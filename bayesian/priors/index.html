<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../img/favicon.ico">

    
    <title>Distribui√ß√µes a priori - Teaching Assistance</title>
    

    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/v4-shims.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3.3.0/build/web/hack.min.css">
    <link href='//rsms.me/inter/inter.css' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="../../css/base.min.css" rel="stylesheet">
    <link href="../../css/cinder.min.css" rel="stylesheet">

    
        
        <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/styles/github.min.css">
        
    

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
        <![endif]-->

    

     
</head>

<body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->

            
              <a class="navbar-brand" href="../..">Teaching Assistance</a>
            
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                
                
                    <li >
                        <a href="../..">Home</a>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Gradua√ß√£o <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../alglin/info/">√Ålgebra Linear</a>
</li>

                        
                            
<li >
    <a href="../../analisenum/info/">An√°lise Num√©rica</a>
</li>

                        
                            
<li >
    <a href="../../curvas/info/">Curvas</a>
</li>

                        
                            
<li >
    <a href="../../edo/info/">Equa√ß√µes Diferenciais Ordin√°rias</a>
</li>

                        
                            
<li >
    <a href="../../edp/info/">Equa√ß√µes Diferenciais Parciais</a>
</li>

                        
                            
<li >
    <a href="../../infestatistica_BSc/info/">Infer√™ncia Estat√≠stica</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">P√≥s-gradua√ß√£o <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../functional_analysis/info/">An√°lise Funcional</a>
</li>

                        
                            
<li >
    <a href="../info/">Estat√≠stica Bayesiana</a>
</li>

                        
                            
<li >
    <a href="../../edp_PhD/info/">Equa√ß√µes Diferenciais Parciais e Aplica√ß√µes</a>
</li>

                        
                            
<li >
    <a href="../../infestatistica_MSc/info/">Infer√™ncia Estat√≠stica</a>
</li>

                        
                            
<li >
    <a href="../../probability/info/">Probabilidade</a>
</li>

                        
                        </ul>
                    </li>
                
                
                </ul>

            <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                            <i class="fas fa-search"></i> Search
                        </a>
                    </li>
                    <li>
                        <a href="https://lucasmoschen.github.io">P√°gina Inicial</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

    <div class="container">
        
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="first-level active"><a href="#distribuicoes-a-priori">Distribui√ß√µes a priori</a></li>
            <li class="second-level"><a href="#determinacao-de-uma-priori">Determina√ß√£o de uma priori</a></li>
                
                <li class="third-level"><a href="#prioris-de-entropia-maxima">Prioris de entropia m√°xima</a></li>
                <li class="third-level"><a href="#aproximacoes-parametricas">Aproxima√ß√µes param√©tricas</a></li>
            <li class="second-level"><a href="#prioris-conjugadas">Prioris conjugadas</a></li>
                
                <li class="third-level"><a href="#familia-exponencial-e-distribuicoes-conjugadas">Fam√≠lia exponencial e distribui√ß√µes conjugadas</a></li>
                <li class="third-level"><a href="#extensoes">Extens√µes</a></li>
            <li class="second-level"><a href="#distribuicoes-a-priori-nao-informativas">Distribui√ß√µes a priori n√£o informativas</a></li>
                
                <li class="third-level"><a href="#priori-de-laplace">Priori de Laplace</a></li>
                <li class="third-level"><a href="#prioris-invariantes">Prioris invariantes</a></li>
                <li class="third-level"><a href="#priori-de-jeffreys">Priori de Jeffreys</a></li>
                <li class="third-level"><a href="#prioris-de-referencia">Prioris de refer√™ncia</a></li>
                <li class="third-level"><a href="#prioris-matching">Prioris matching</a></li>
            <li class="second-level"><a href="#pontuacoes-finais">Pontua√ß√µes finais</a></li>
                
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<h1 id="distribuicoes-a-priori">Distribui√ß√µes a priori</h1>
<p>A determina√ß√£o da distribui√ß√£o a priori para a quantidade de interesse √© um ponto-chave da infer√™ncia bayesiana e, simultaneamente, √© alvo de cr√≠ticas. 
De forma geral, queremos codificar a informa√ß√£o sobre um par√¢metro <script type="math/tex">\theta</script> antes de realizarmos um determinado experimento. 
Dessa forma, a partir de uma informa√ß√£o a priori, queremos definir uma distribui√ß√£o a priori. 
√â claro que raramente conseguimos fazer esse processo de forma exata e √∫nica. 
Quando a informa√ß√£o √© insuficiente para definir uma distribui√ß√£o de probabilidade, o estat√≠stico deve colocar informa√ß√£o subjetiva para, ent√£o, obter uma priori que fa√ßa sentido.
Como a distribui√ß√£o a priori influencia as infer√™ncias a posteriori o tanto quanto se queira (no limite, podemos colocar a massa de probabilidade em um √∫nico ponto), an√°lises de robustez e de sensibilidade s√£o essenciais no dia-a-dia bayesiano.</p>
<h2 id="determinacao-de-uma-priori">Determina√ß√£o de uma priori</h2>
<p>Queremos que a priori <script type="math/tex">\pi</script> resuma a informa√ß√£o dispon√≠vel sobre o fen√¥meno e a incerteza que temos sobre essa informa√ß√£o, mesmo quando <script type="math/tex">\theta</script> n√£o adv√©m de um processo aleat√≥rio.
Usando o conceito de George Box de que <a href="https://en.wikipedia.org/wiki/All_models_are_wrong">todos os modelos s√£o errados</a>, n√£o existe uma priori verdadeira e, portanto, buscamos aproxima√ß√µes para representar <script type="math/tex">\pi</script>.</p>
<hr />
<p><code>üìù</code> <strong>Exemplo (Distribui√ß√£o normal)</strong></p>
<p>Suponha que observamos <script type="math/tex">x_1, \dots, x_n \sim N(\theta, 1)</script> e assumimos que <script type="math/tex">\theta \sim N(\mu, \tau)</script>. A m√©dia a posteriori de <script type="math/tex">\theta</script> √© dada por 
<script type="math/tex; mode=display">
\mathbb{E}[\theta|x] = \frac{\bar{x}\tau + \mu/n}{\tau + 1/n}. 
</script>
Nesse caso, note que estamos fazendo uma m√©dia ponderada de <script type="math/tex">\bar{x}</script> e <script type="math/tex">\mu</script> com pesos <script type="math/tex">\tau</script> e <script type="math/tex">1/n</script>, respectivamente. Dessa forma, poder√≠amos pensar que a priori, ter√≠amos virtualmente uma amostra de tamanho <script type="math/tex">\tau^{-1}</script> e m√©dia <script type="math/tex">\mu</script>. </p>
<hr />
<p>Podemos construir uma medida de probabilidade para <script type="math/tex">\theta</script> atrav√©s de uma rela√ß√£o <script type="math/tex">\preceq</script> que ordena o qu√£o prov√°vel √© um evento definido pela vari√°vel aleat√≥ria <script type="math/tex">\theta</script>, como resumido <a href="https://lucasmoschen.github.io/files/disciplines/bayesian-statistics/probabilidade-subjetiva-resumo.pdf">aqui</a>.</p>
<p>Quando n√£o existe uma informa√ß√£o direta sobre <script type="math/tex">\theta</script>, uma alternativa √© usar a distribui√ß√£o marginal de <script type="math/tex">x</script> dada por 
<script type="math/tex; mode=display">m(x) = \int_{\Theta} f(x|\theta) \pi(\theta) \, d\theta.</script>
Por exemplo, se <script type="math/tex">\theta</script> √© a produ√ß√£o di√°ria m√©dia de leite, informa√ß√£o sobre <script type="math/tex">\theta</script> pode ser obtida a partir da informa√ß√£o que j√° temos sobre o rebanho, o que √© informa√ß√£o sobre a marginal de <script type="math/tex">x</script>.</p>
<h3 id="prioris-de-entropia-maxima">Prioris de entropia m√°xima</h3>
<p>Assuma que temos <script type="math/tex">\mathbb{E}^{\pi}[g_k(\theta)] = \omega_k</script> para <script type="math/tex">k=1,\dots, K</script>. Podemos selecionar <script type="math/tex">\pi</script> que satisfa√ßa essas <script type="math/tex">K</script> rela√ß√µes e maximize a entropia, que mede o n√≠vel de desordem em um sistema. Se <script type="math/tex">\Theta</script> √© finito, temos que 
<script type="math/tex; mode=display">
\mathcal{E}(\pi) = \sum_{\theta_i \in \Theta} \pi(\theta_i) \log(\pi(\theta_i))
</script>
√© a <strong>entropia</strong> de <script type="math/tex">\pi</script>. 
Essa quantidade √© uma medida de incerteza dada por <script type="math/tex">\pi</script>. 
Dessa forma, estamos minimizando a informa√ß√£o de <script type="math/tex">\pi</script> trazida a priori. 
Sendo <script type="math/tex">\lambda_k</script> o multiplicador de Lagrange associado a <script type="math/tex">\mathbb{E}^{\pi}[g_k(\theta)] = \omega_k</script>, temos que a defini√ß√£o da priori √© 
<script type="math/tex; mode=display">
\pi^*(\theta_i) \propto \exp\left\{\sum_{k=1}^K \lambda_k g_k(\theta_i)\right\}.
</script>
No caso cont√≠nuo, as contas se complicam um pouco e precisamos de uma medida de refer√™ncia <script type="math/tex">\pi_0</script>,
que pode ser vista como a distribui√ß√£o a priori sem restri√ß√£o de momentos. 
Quando existe uma estrutura de grupo, a medida de Haar invariante √† direita √© a escolha natural para <script type="math/tex">\pi_0</script>. 
Nesse caso, a entropia √© definida como
<script type="math/tex; mode=display">
\mathcal{E}(\pi) = \int \log\left(\frac{\pi(\theta)}{\pi_0(\theta)}\right) \, \pi_0(d\theta),
</script>
que √© a dist√¢ncia de Kullback-Leibler entre <script type="math/tex">\pi</script> e <script type="math/tex">\pi_0</script> e a distribui√ß√£o a priori que maximiza <script type="math/tex">\mathcal{E}</script> √© 
<script type="math/tex; mode=display">
\pi^*(\theta) \propto \exp\left\{\sum_{k=1}^K \lambda_k g_k(\theta_i)\right\}\pi_0(\theta)
</script>
</p>
<h3 id="aproximacoes-parametricas">Aproxima√ß√µes param√©tricas</h3>
<p>A forma mais utilizada √© provavelmente essa. Definimos uma fam√≠lia param√©trica para a distribui√ß√£o a priori e buscamos definir os par√¢metros atrav√©s dos momentos ou quartis da distribui√ß√£o. 
A base √© mais a tratabilidade matem√°tica do que a subjetividade. 
Outro ponto √© que distribui√ß√µes com caudas muito diferentes para <script type="math/tex">\Theta</script> infinito levam a infer√™ncias bastante distintas.</p>
<h2 id="prioris-conjugadas">Prioris conjugadas</h2>
<p>Prioris conjugadas s√£o baseadas na verossimilhan√ßa e, portanto, j√° definem a fam√≠lia param√©trica da distribui√ß√£o a priori, restando a defini√ß√£o dos par√¢metros. 
Isso limita a informa√ß√£o a priori que deve ser obtida a fim de definir uma distribui√ß√£o de probabilidade. 
Ela tamb√©m auxilia na computa√ß√£o, como veremos na defini√ß√£o:</p>
<p><strong>Fam√≠lia conjugada:</strong> Uma fam√≠lia de distribui√ß√µes <script type="math/tex">\mathcal{F}</script> sobre <script type="math/tex">\Theta</script> √© conjugada para a verossimilhan√ßa <script type="math/tex">f(x|\theta)</script> se para toda priori <script type="math/tex">\pi \in \mathcal{F}</script>, a posteriori <script type="math/tex">p(\cdot \mid x) \in \mathcal{F}</script>. </p>
<p>Essa fam√≠lia se torna interessante quando ela √© a menor poss√≠vel (√© imposs√≠vel encontrar uma m√≠nima propriamente dita, mas a ideia √© que ela tenha dimens√£o de par√¢metros baixa) e parametrizada. Logo, atualizar a informa√ß√£o por <script type="math/tex">f(x|\theta)</script> √© equivalente a atualizar os par√¢metros da distribui√ß√£o. 
Nas fam√≠lias conjugadas, podemos interpretar os hiperpar√¢metros como observa√ß√µes passadas virtuais, o que √© considerado um ponto positivo.</p>
<h3 id="familia-exponencial-e-distribuicoes-conjugadas">Fam√≠lia exponencial e distribui√ß√µes conjugadas</h3>
<p>Seja <script type="math/tex">\mu</script> uma medida <script type="math/tex">\sigma</script>-finita em <script type="math/tex">\mathcal{X}</script> e <script type="math/tex">\Theta</script> o espa√ßo dos par√¢metros. 
Sejam <script type="math/tex">C : \mathcal{X} \to \mathbb{R}_+</script>, <script type="math/tex">h : \mathcal{X} \to \mathbb{R}_+</script>, <script type="math/tex">R : \Theta \to \mathbb{R}^k</script> e <script type="math/tex">T : \mathcal{X} \to \mathbb{R}^k</script> fun√ß√µes. A fam√≠lia de distribui√ß√µes com densidade com respeito a <script type="math/tex">\mu</script>
<script type="math/tex; mode=display">
f(x|\theta) = C(\theta) h(x) \exp\{R(\theta) \cdot T(x)\}
</script>
√© chamada de <strong>fam√≠lia exponencial</strong> de dimens√£o <script type="math/tex">k</script>. Quando <script type="math/tex">R(\theta) = \theta</script> e <script type="math/tex">T(x) = x</script>, a fam√≠lia √© dita <strong>natural</strong>.</p>
<blockquote>
<p><strong>Lema de Pitman‚ÄìKoopman</strong>: Se uma fam√≠lia de distribui√ß√µes <script type="math/tex">f(\cdot | \theta)</script> √© tal que para <script type="math/tex">n</script> suficientemente grande, existe uma estat√≠stica suficiente de dimens√£o constante, ela √© exponencial se o suporte n√£o depende de <script type="math/tex">\theta</script> (essa condi√ß√£o final exclui a distribui√ß√£o <script type="math/tex">U[-\theta, \theta]</script>, por exemplo). </p>
</blockquote>
<p>Al√©m disso, para toda amostra de <script type="math/tex">f</script>, existe uma estat√≠stica suficiente de dimens√£o constante.</p>
<p>O <strong>espa√ßo natural</strong> de par√¢metros √© denotado por 
<script type="math/tex; mode=display">
N = \left\{\theta \in \Theta \mid \int_{\mathcal{X}} e^{\theta\cdot x} h(x) \, d\mu(x) < +\infty \right\}.
</script>
Ela √© regular se <script type="math/tex">N</script> √© aberto e m√≠nimo se <script type="math/tex">dim(N) = dim(C(\mu)) = K</script> em que <script type="math/tex">C(\mu)</script> √© o menor conjunto convexo fechado que cont√©m o suporte de <script type="math/tex">\mu</script>.</p>
<p>Reescreva a densidade como 
<script type="math/tex; mode=display">
f(x|\theta) = h(x) e^{\theta \cdot x - \psi(\theta)},
</script>
em que <script type="math/tex">\psi</script> √© a <strong>fun√ß√£o geradora cumulativa</strong>, pois
<script type="math/tex; mode=display">
\mathbb{E}_{\theta}[X] = \nabla \psi(\theta), \operatorname{Cov}(X_i, X_j) = \psi_{\theta_i \theta_j}(\theta),
</script>
supondo <script type="math/tex">\psi</script> de classe <script type="math/tex">C^2</script> e <script type="math/tex">\theta \in int(N)</script>.</p>
<p><strong>Priori conjugada:</strong> Uma fam√≠lia conjugada para <script type="math/tex">f</script> √© dada pela densidade
<script type="math/tex; mode=display">
\pi(\theta | \mu, \lambda) = K(\mu, \lambda) e^{\theta\cdot \mu - \lambda \psi(\theta)},
</script>
cuja posteriori √© dada por <script type="math/tex">\pi(\theta | \mu + x, \lambda + 1)</script>. Se <script type="math/tex">\lambda > 0</script> e <script type="math/tex">\mu/\lambda</script> pertence ao interior de <script type="math/tex">C(\mu)</script>, <script type="math/tex">\pi</script> define uma distribui√ß√£o de probabilidade <a href="https://statweb.stanford.edu/~cgates/PERSI/papers/conjprior.pdf">[Diaconis, Ylvisaker; 1978]</a>. Para uma tabela completa, consulte <a href="https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions">aqui</a>.</p>
<p>Outro resultado verificado no artigo acima √© que a esperan√ßa a posteriori de <script type="math/tex">\theta</script> √© dada por 
<script type="math/tex; mode=display">
\frac{\mu + n\bar{x}}{\lambda + n}, 
</script>
quando <script type="math/tex">\mu \in \mathcal{X}</script>.</p>
<h3 id="extensoes">Extens√µes</h3>
<p>Considere o seguinte exemplo</p>
<hr />
<p><code>üìù</code> <strong>Exemplo <a href="https://statistics.stanford.edu/technical-reports/quantifying-prior-opinion">[Diaconis, Ylvisaker; 1985]</a></strong></p>
<p>Quando a moeda √© girada pela borda e observamos o resultado quando ela cai, temos um vi√©s maior para um lado do que para o outro devido a irregularidades. Seja <script type="math/tex">x \sim Binomial(n, p)</script> o n√∫mero de caras nesse experimento ap√≥s <script type="math/tex">n</script> jogadas. 
Como sabemos que existe uma irregularidade, poder√≠amos pensar em uma priori para <script type="math/tex">p</script> que tivesse uma cara bimodal, dado peso para as duas possibilidades. Isso n√£o √© poss√≠vel com a fam√≠lia conjugada beta. 
Uma forma de fazer isso √©, portanto, atrav√©s de misturas de distribui√ß√µes beta.</p>
<hr />
<p>Com esse exemplo, podemos ver que misturas de distribui√ß√µes conjugadas definem uma fam√≠lia conjugada maior que d√° maior flexibilidade para o formato de uma distribui√ß√£o a priori. 
Al√©m disso, podemos verificar que a mistura de distribui√ß√µes pode aproximar qualquer distribui√ß√£o a priori sob a <a href="https://en.wikipedia.org/wiki/L%C3%A9vy%E2%80%93Prokhorov_metric">dist√¢ncia de Prokhorov</a>.</p>
<h2 id="distribuicoes-a-priori-nao-informativas">Distribui√ß√µes a priori n√£o informativas</h2>
<p>Quando pouca (ou nenhuma) informa√ß√£o sobre <script type="math/tex">\theta</script> est√° dispon√≠vel, √© dif√≠cil justificar a escolha com base subjetiva.
Nesse caso, uma alternativa √© usar a distribui√ß√£o dos dados para, a partir dela, definir uma distribui√ß√£o a priori.
Chamamos essas prioris de <strong>n√£o informativas</strong>. 
Mas devemos que lembrar que uma distribui√ß√£o ser n√£o informativa n√£o significa que ignor√¢ncia total est√° sendo representada probabilisticamente.
Elas podem ser usadas como prioris de refer√™ncia, todavia.</p>
<h3 id="priori-de-laplace">Priori de Laplace</h3>
<p>Laplace sugeriu construir a distribui√ß√£o a priori baseado no <strong>Princ√≠pio da Raz√£o Insuficiente</strong>, em que eventos elementares s√£o equiprov√°veis. 
Nesse caso, adotamos a priori uniforme.
Se o espa√ßo de par√¢metros n√£o for compacto, isso nos leva √† uma distribui√ß√£o impr√≥pria, por consequ√™ncia. 
Al√©m disso, se fizermos uma transforma√ß√£o biun√≠voca <script type="math/tex">\eta = g(\theta)</script>, a priori para <script type="math/tex">\eta</script> n√£o ser√° uniforme pela F√≥rmula da Mudan√ßa de Vari√°veis, isto √©, a informa√ß√£o sobre <script type="math/tex">\theta</script> n√£o foi criada a partir dessa transforma√ß√£o, mas a distribui√ß√£o n√£o √© uniforme.</p>
<h3 id="prioris-invariantes">Prioris invariantes</h3>
<p>O conceito de invari√¢ncia √© bem profundo na matem√°tica e, com base nesse conceito, podemos construir distribui√ß√µes invariantes a reparametriza√ß√£o. Por exemplo, a fam√≠lia <script type="math/tex">f(x - \theta)</script> √© invariante √† transla√ß√£o, isto √©, <script type="math/tex">y = x - x_0</script> tem distribui√ß√£o da mesma fam√≠lia para todo <script type="math/tex">x_0</script>, <script type="math/tex">f(x - (\theta - x_0))</script>. Chamamos <script type="math/tex">\theta</script> de <em>par√¢metro de loca√ß√£o</em>.</p>
<h3 id="priori-de-jeffreys">Priori de Jeffreys</h3>
<p>A Priori de Jeffreys √© baseada na <a href="https://lucasmoschen.github.io/ta-sessions/infestatistica_BSc/FisherInformation/FisherInformation/">Informa√ß√£o de Fisher</a> dada pela express√£o:
<script type="math/tex; mode=display">
I(\theta) = \mathbb{E}_{\theta}\left[\left(\frac{\partial \log f(X|\theta)}{\partial \theta}\right)^2\right],
</script>
no caso unidimensional. A distribui√ß√£o de Jeffreys √© definida por 
<script type="math/tex; mode=display">
\pi^*(\theta) \propto I^{1/2}(\theta).
</script>
Note que <script type="math/tex">I(\theta) = I(h(\theta))(h'(\theta))^2</script> se <script type="math/tex">h</script> √© uma transforma√ß√£o biun√≠voca. 
Nesse caso, <script type="math/tex">\pi^*(h(\theta)) \propto I^{1/2}(h(\theta)) = I^{1/2}(\theta)/|h'(\theta)|</script>, exatamente o que esper√°vamos usando a mudan√ßa de vari√°veis. 
Logo, essa priori √© invariante √† reparametriza√ß√£o.
Al√©m disso, <script type="math/tex">I(\theta)</script> √© uma medida da quantidade de informa√ß√£o trazida pelo modelo sobre <script type="math/tex">\theta</script>.
Na pr√°tica, prioris de Jeffreys s√£o usualmente impr√≥prias.
No caso multidimensional, 
<script type="math/tex; mode=display">
\pi^*(\theta) \propto [det(I(\theta))]^{1/2}
</script>
√© a priori de Jeffreys. 
Todavia, usar esse procedimento para construir prioris leva a paradoxos e incoer√™ncias.</p>
<p><strong>Observa√ß√£o:</strong> A priori de Jeffreys para fam√≠lia de loca√ß√£o √© constante.</p>
<hr />
<p><code>üìù</code> <strong>Exemplo (caso normal)</strong></p>
<p>Suponha que <script type="math/tex">x \sim N(\mu, \sigma^2)</script> com <script type="math/tex">\theta = (\mu, \sigma^2)</script> desconhecido. Nesse caso, podemos calcular que 
<script type="math/tex; mode=display">
I(\theta) = \begin{bmatrix}
1/\sigma^2 & 0 \\ 0 & 2/\sigma^2
\end{bmatrix},
</script>
e, portanto, <script type="math/tex">\pi(\theta) \propto 1/\sigma^2</script>. Se assumirmos que <script type="math/tex">\mu</script> e <script type="math/tex">\sigma</script> s√£o independentes a priori, todavia, teremos que <script type="math/tex">\pi(\mu, \sigma) = 1/\sigma</script> como priori n√£o informativa e tamb√©m a medida de Haar invariante ao modelo loca√ß√£o-scala.</p>
<hr />
<p>Essa priori, todavia, n√£o obedece ao Princ√≠pio da Verossimilhan√ßa, dado que a informa√ß√£o de Fisher de dois modelos diferentes podem ser diferentes, mesmo que as verossimilhan√ßas sejam proporcionais. <a href="https://stats.stackexchange.com/questions/194448/do-you-have-to-adhere-to-the-likelihood-principle-to-be-a-bayesian">Confira essa resposta.</a>.</p>
<h3 id="prioris-de-referencia">Prioris de refer√™ncia</h3>
<p><a href="https://people.eecs.berkeley.edu/~jordan/sail/readings/bernardo-1979.pdf">[Bernardo; 1979]</a> prop√¥s um procedimento para construir prioris n√£o informativas, ou como ele destaca, um procedimento para obter prioris de forma que a posteriori aproxime aquela que adviria de uma informa√ß√£o a priori vaga.
Seja <script type="math/tex">x \sim f(x | \theta)</script> e <script type="math/tex">\theta = (\theta_1, \theta_2)</script>, em que <script type="math/tex">\theta_1</script> √© o par√¢metro de interesse. A priori de refer√™ncia primeiro define <script type="math/tex">\pi(\theta_2 | \theta_1)</script> usando a priori de Jeffreys atrav√©s <script type="math/tex">f(x|\theta)</script> quando <script type="math/tex">\theta_1</script> √© fixado. 
Depois, ele calcula 
<script type="math/tex; mode=display">
\tilde{f}(x|\theta_1) = \int f(x|\theta) \pi(\theta_2|\theta_1) \, d\theta_2
</script>
e calcula a priori de Jeffreys para <script type="math/tex">\theta_1</script> baseando-se em <script type="math/tex">\tilde{f}</script>. 
Assim, o processo elimina os par√¢metros que n√£o s√£o de interesse.
O algoritmo se generaliza para mais camadas de par√¢metros.</p>
<h3 id="prioris-matching">Prioris matching</h3>
<p>Uma forma, no m√≠nimo curiosa, de construir prioris n√£o informativas √© baseada em propriedades frequentistas, isto √©, em que propriedades funcionem em m√©dia considerando <script type="math/tex">x</script>.
Seja <script type="math/tex">C_x</script> um conjunto de confian√ßa a posteriori n√≠vel <script type="math/tex">\alpha</script>, isto √©, 
<script type="math/tex; mode=display">
p(g(\theta) \in C_x | x) = 1-\alpha.
</script>
Esse conjunto define tem cobertura frequentista <script type="math/tex">\Pr_{\theta}(g(\theta) \in C_x)</script>, em que nesse caso <script type="math/tex">C_x</script> √© a vari√°vel aleat√≥ria. 
Em geral, quando <script type="math/tex">C_x = (-\infty, k_{\alpha}(x))</script>, ent√£o <script type="math/tex">\Pr_{\theta}(\theta \le k_{\alpha}(x)) = 1 - \alpha + O(n^{-1/2})</script>. No caso da priori de Jeffreys, isso se torna <script type="math/tex">O(n^{-1})</script> que decresce mais rapidamente. 
Para mais detalhes, <a href="https://projecteuclid.org/ebook/Download?urlid=10.1214/lnms/1215091929&amp;isFullBook=false">consulte esse link</a>.</p>
<h2 id="pontuacoes-finais">Pontua√ß√µes finais</h2>
<ul>
<li>
<p>Informa√ß√£o a priori n√£o consegue ser traduzida em uma distribui√ß√£o de probabilidade √∫nica. Al√©m disso, ela √© bastante complicada, principalmente em se tratando de definir caudas de forma subjetiva. </p>
</li>
<li>
<p>Se alguma informa√ß√£o √© dispon√≠vel, us√°-la favorece infer√™ncias quando comparado a abordagens n√£o informativas.</p>
</li>
<li>
<p>An√°lise de sensibilidade √© um t√≥pico importante, que verifica a influ√™ncia da escolha da priori nas infer√™ncias. Para essa an√°lise, assumimos que a priori <script type="math/tex">\pi</script> mora em alguma classe de distribui√ß√µes, que mensura a incerteza sobre <script type="math/tex">\pi</script>. Essas classes podem ser: (i) prioris conjugadas: convenientes matematicamente; (ii) momentos definidos: a classe de distribui√ß√µes que tem determinados momentos limitados imp√µe condi√ß√µes fortes sobre a cauda e inclui muitas distribui√ß√µes imposs√≠veis; (iii) classes de vizinhan√ßa: pondera por um <script type="math/tex">\epsilon</script> uma outra classe de distribui√ß√µes <script type="math/tex">Q</script> a ser escolhida; entre outras.</p>
</li>
</ul></div>
        
        
    </div>

    
      <footer class="col-md-12 text-center">
          
          
            <hr>
            <p>
            <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</small>
            </p>
          

          
          
      </footer>
    
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="../../js/bootstrap-3.0.3.min.js"></script>

    
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/highlight.min.js"></script>
        
    <script>hljs.initHighlightingOnLoad();</script>
    

    <script>var base_url = "../.."</script>
    
    <script src="../../js/base.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../search/main.js"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal">
                    <span aria-hidden="true">&times;</span>
                    <span class="sr-only">Close</span>
                </button>
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>
    </body>

</html>
