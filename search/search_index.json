{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Monitorias Este website foi desenvolvido para concentrar as informa\u00e7\u00f5es sobre as monitorias que dei ao longo da minha vida acad\u00eamica. Em particular, na Escola de Matem\u00e1tica Aplicada na Funda\u00e7\u00e3o Getulio Vargas ( FGV/EMAp ). Eu espero que a informa\u00e7\u00e3o aqui contida seja de interesse! Para sugest\u00f5es e altera\u00e7\u00f5es, voc\u00ea pode usar o Issues do Github. T\u00f3picos abordados Equa\u00e7\u00f5es Diferenciais Parciais (2022.2) Professor Moacyr Alvim Horta Barbosa da Silva An\u00e1lise Funcional (2022.5) Professora Maria Soledad Aronna Infer\u00eancia Estat\u00edstica (2022.5) Professor Luiz Max de Carvalho Estat\u00edstica Bayesiana (2022.3) Professor Eduardo Fonseca Mendes Curvas e Superf\u00edcies (2022.1) Professora Asla Medeiros e S\u00e1 Equa\u00e7\u00f5es Diferenciais Parciais (2021.2) Professor Moacyr Alvim Horta Barbosa da Silva Introdu\u00e7\u00e3o \u00e0 An\u00e1lise Num\u00e9rica (2021.2) Professor Hugo A. de la Cruz Cancino Curvas e Superf\u00edcies (2021.1) Professora Asla Medeiros e S\u00e1 Infer\u00eancia Estat\u00edstica (2020.2) Professor Luiz Max de Carvalho Equa\u00e7\u00f5es Diferenciais Ordin\u00e1rias (2020.1) Professora Maria Izabel Camacho \u00c1lgebra Linear (2019.2) Professor Eduardo Wagner Refer\u00eancias interessantes de matem\u00e1tica SageMath : software matem\u00e1tico que facilita c\u00e1lculos. \u00c9 uma possibilidade alternativa ao WolframAlpha , por\u00e9m com c\u00f3digo em Python. Manin : ferramenta para criar v\u00eddeos matem\u00e1ticos explicativos com anima\u00e7\u00e3o program\u00e1tica, uma biblioteca para Python. Seeing Theory : visualiza\u00e7\u00e3o de conceitos de probabilidade e estat\u00edstica. Softwares de Geometria Alg\u00e9brica : lista de softwares relacionados com Geometria Alg\u00e9brica.","title":"Home"},{"location":"#monitorias","text":"Este website foi desenvolvido para concentrar as informa\u00e7\u00f5es sobre as monitorias que dei ao longo da minha vida acad\u00eamica. Em particular, na Escola de Matem\u00e1tica Aplicada na Funda\u00e7\u00e3o Getulio Vargas ( FGV/EMAp ). Eu espero que a informa\u00e7\u00e3o aqui contida seja de interesse! Para sugest\u00f5es e altera\u00e7\u00f5es, voc\u00ea pode usar o Issues do Github.","title":"Monitorias"},{"location":"#topicos-abordados","text":"Equa\u00e7\u00f5es Diferenciais Parciais (2022.2) Professor Moacyr Alvim Horta Barbosa da Silva An\u00e1lise Funcional (2022.5) Professora Maria Soledad Aronna Infer\u00eancia Estat\u00edstica (2022.5) Professor Luiz Max de Carvalho Estat\u00edstica Bayesiana (2022.3) Professor Eduardo Fonseca Mendes Curvas e Superf\u00edcies (2022.1) Professora Asla Medeiros e S\u00e1 Equa\u00e7\u00f5es Diferenciais Parciais (2021.2) Professor Moacyr Alvim Horta Barbosa da Silva Introdu\u00e7\u00e3o \u00e0 An\u00e1lise Num\u00e9rica (2021.2) Professor Hugo A. de la Cruz Cancino Curvas e Superf\u00edcies (2021.1) Professora Asla Medeiros e S\u00e1 Infer\u00eancia Estat\u00edstica (2020.2) Professor Luiz Max de Carvalho Equa\u00e7\u00f5es Diferenciais Ordin\u00e1rias (2020.1) Professora Maria Izabel Camacho \u00c1lgebra Linear (2019.2) Professor Eduardo Wagner","title":"T\u00f3picos abordados"},{"location":"#referencias-interessantes-de-matematica","text":"SageMath : software matem\u00e1tico que facilita c\u00e1lculos. \u00c9 uma possibilidade alternativa ao WolframAlpha , por\u00e9m com c\u00f3digo em Python. Manin : ferramenta para criar v\u00eddeos matem\u00e1ticos explicativos com anima\u00e7\u00e3o program\u00e1tica, uma biblioteca para Python. Seeing Theory : visualiza\u00e7\u00e3o de conceitos de probabilidade e estat\u00edstica. Softwares de Geometria Alg\u00e9brica : lista de softwares relacionados com Geometria Alg\u00e9brica.","title":"Refer\u00eancias interessantes de matem\u00e1tica"},{"location":"unavailable/","text":"Arquivo indispon\u00edvel Crie um Issue solicitando-o.","title":"Arquivo indispon\u00edvel"},{"location":"unavailable/#arquivo-indisponivel","text":"Crie um Issue solicitando-o.","title":"Arquivo indispon\u00edvel"},{"location":"alglin/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de \u00c1lgebra Linear correspondente ao per\u00edodo de 2019.2. Os temas abordados s\u00e3o: Matrizes Sistemas lineares Espa\u00e7os vetoriais e com produto interno Bases Transforma\u00e7\u00f5es lineares N\u00facleo e imagem de uma transforma\u00e7\u00e3o Autovalores e autovetores Proje\u00e7\u00f5es Monitorias Monitorias Monitoria 4 Monitoria 6 Monitoria 7 Monitorias 10 e 11 Monitorias 12 e 13 Decomposi\u00e7\u00e3o LU","title":"\u00c1lgebra Linear"},{"location":"alglin/info/#informacoes-gerais","text":"Monitoria de \u00c1lgebra Linear correspondente ao per\u00edodo de 2019.2. Os temas abordados s\u00e3o: Matrizes Sistemas lineares Espa\u00e7os vetoriais e com produto interno Bases Transforma\u00e7\u00f5es lineares N\u00facleo e imagem de uma transforma\u00e7\u00e3o Autovalores e autovetores Proje\u00e7\u00f5es","title":"Informa\u00e7\u00f5es Gerais"},{"location":"alglin/info/#monitorias","text":"Monitorias Monitoria 4 Monitoria 6 Monitoria 7 Monitorias 10 e 11 Monitorias 12 e 13 Decomposi\u00e7\u00e3o LU","title":"Monitorias"},{"location":"alglin/files/decompositionLU/","text":"Decomposi\u00e7\u00e3o LU Fatoriza\u00e7\u00e3o de uma matriz A como um produto P\\cdot A = L\\cdot U , onde a primeira matrix \u00e9 triangular inferior (lower), enquanto a segunda \u00e9 triangular superior (upper). A matriz P \u00e9 uma matriz de permuta\u00e7\u00e3o que garante a exist\u00eancia dessa fatoriza\u00e7\u00e3o. import scipy.linalg A = scipy.array([ [7, 3, -1, 2], [3, 8, 1, -4], [-1, 1, 4, -1], [2, -4, -1, 6] ]) print(A) [[ 7 3 -1 2] [ 3 8 1 -4] [-1 1 4 -1] [ 2 -4 -1 6]] P, L, U = scipy.linalg.lu(A) print(P), print(L), print(U) [[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]] [[ 1. 0. 0. 0. ] [ 0.42857143 1. 0. 0. ] [-0.14285714 0.21276596 1. 0. ] [ 0.28571429 -0.72340426 0.08982036 1. ]] [[ 7. 3. -1. 2. ] [ 0. 6.71428571 1.42857143 -4.85714286] [ 0. 0. 3.55319149 0.31914894] [ 0. 0. 0. 1.88622754]] (None, None, None)","title":"Decomposi\u00e7\u00e3o LU"},{"location":"alglin/files/decompositionLU/#decomposicao-lu","text":"Fatoriza\u00e7\u00e3o de uma matriz A como um produto P\\cdot A = L\\cdot U , onde a primeira matrix \u00e9 triangular inferior (lower), enquanto a segunda \u00e9 triangular superior (upper). A matriz P \u00e9 uma matriz de permuta\u00e7\u00e3o que garante a exist\u00eancia dessa fatoriza\u00e7\u00e3o. import scipy.linalg A = scipy.array([ [7, 3, -1, 2], [3, 8, 1, -4], [-1, 1, 4, -1], [2, -4, -1, 6] ]) print(A) [[ 7 3 -1 2] [ 3 8 1 -4] [-1 1 4 -1] [ 2 -4 -1 6]] P, L, U = scipy.linalg.lu(A) print(P), print(L), print(U) [[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]] [[ 1. 0. 0. 0. ] [ 0.42857143 1. 0. 0. ] [-0.14285714 0.21276596 1. 0. ] [ 0.28571429 -0.72340426 0.08982036 1. ]] [[ 7. 3. -1. 2. ] [ 0. 6.71428571 1.42857143 -4.85714286] [ 0. 0. 3.55319149 0.31914894] [ 0. 0. 0. 1.88622754]] (None, None, None)","title":"Decomposi\u00e7\u00e3o LU"},{"location":"alglin/files/monitoria10/","text":"Monitorias 10 e 11 Defini\u00e7\u00f5es e Teoremas Lembrando: Uma transforma\u00e7\u00e3o linear T: \\mathbb{R}^n \\to \\mathbb{R}^m fica inteiramente determinada por uma matriz A = [a_{ij}] \\in M(m \\times n) . Os vetores coluna dessa matriz s\u00e3o as imagens A \\cdot e_j dos vetores da base can\u00f4nica. Definimos A como matriz de transforma\u00e7\u00e3o. Assim, A \\cdot e_j = \\sum_{i=1}^m a_{ij}e_i (j=1,...,n) , onde e_i \\in \\mathbb{R}^m . Simetrias: Matrizes de tranforma\u00e7\u00e3o referentes \u00e0 simetria em rela\u00e7\u00e3o aos eixos x e y, e em rela\u00e7\u00e3o \u00e0 origem, respectivamente: S_x = \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & -1 \\end{array} \\right] S_y = \\left[ \\begin{array}{cc} -1 & 0 \\\\ 0 & 1 \\end{array} \\right] S_o = \\left[ \\begin{array}{cc} -1 & 0 \\\\ 0 & -1 \\end{array} \\right] Dilata\u00e7\u00f5es: Basta multiplicar uma coluna que se quer dilatar por r . Podemos chamar r de coeficiente de dilata\u00e7\u00e3o. Rota\u00e7\u00e3o: Para montar essa matriz, basta conhecer a transforma\u00e7\u00e3o dos vetores (1,0) e (0,1) . R_{\\theta} = \\left[ \\begin{array}{cc} \\cos \\theta & - \\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{array} \\right] A rota\u00e7\u00e3o tem algumas propriedades: R_{\\theta}^{-1} = R_{-\\theta} R_{\\alpha}R_{\\beta} = R_{\\alpha + \\beta} (R_{\\theta})^n = R_{n\\theta} Proje\u00e7\u00f5es: Podemos considerar a transforma\u00e7\u00e3o que projeta os vetores sobre a reta y = ax . P = \\frac{1}{1+a^2} \\left[ \\begin{array}{cc} 1 & a \\\\ a & a^2 \\end{array} \\right] Se quisermos que a proje\u00e7\u00e3o sobre um eixo e paralelo a uma reta, temos que P_p = \\left[ \\begin{array}{cc} 1 & -\\frac{1}{a} \\\\ 0 & 0 \\end{array} \\right] N\u00facleo de A : N(A) = \\{v \\in E | Av = 0\\} . \u00c9 o espa\u00e7o anulado da matriz A . Imagem de A : Im(A) = \\{Av | v \\in E\\} \\implies \\exists v \\in E; Av = w \\implies w \\in Im(A) . Notemos que posto(A) = dim~Im(A) = dim~col(A) . Isto ocorre, pois w \\in Im(A) \u00e9 combina\u00e7\u00e3o linear das colunas da matriz A . Transforma\u00e7\u00e3o Injetiva: A: E \\to F \u00e9 injetiva se \\forall v, v', v \\neq v' \\implies Av \\neq Av' . Uma transforma\u00e7\u00e3o \u00e9 injetiva se, e s\u00f3 se, transforma vetores LI em vetores LI. Para essa demonstra\u00e7\u00e3o, \u00e9 necess\u00e1rio mostrar que uma transforma\u00e7\u00e3o \u00e9 injetiva se, e s\u00f3 se, seu n\u00facleo possui apenas o vetor nulo. Transforma\u00e7\u00e3o Sobrejetiva: Ocorre quando Im(A) = F , onde F \u00e9 o espa\u00e7o vetorial contradom\u00ednio. Teorema do N\u00facleo e da Imagem: Como dim~Im(A) = posto(A) , podemos usar no teorema do posto. Podemos alterar n para dim~E , sendo E o dom\u00ednio da transforma\u00e7\u00e3o. Laplace: Escolhe-se uma linha uma coluna e para cada elemento, calcula-se o seu cofator. A_{ij} = (-1)^{i+j}D_{ij} . Propriedades Importantes: det(A) = det(A^{T}) ; trocar duas linhas ou colunas inverte o sinal do determinante; duas linhas proporcionais indica determinante 0; multiplicar uma linha por \\alpha implicar\u00e1 multiplicar o determinante pelo mesmo fator; determinante do produto de matrizes \u00e9 o produto dos determinantes; o determinante de uma matriz com a opera\u00e7\u00e3o de somar com m\u00faltiplo de outra linha \u00e9 id\u00eantico; determinante da inversa \u00e9 o inverso do determinante Lembretes para exerc\u00edcios: Para calcular uma matriz de tranforma\u00e7\u00e3o, precisamos apenas saber a transforma\u00e7\u00e3o linear de uma base do dom\u00ednio. Com essa transforma\u00e7\u00e3o, precisamos obter a transforma\u00e7\u00e3o da base can\u00f4nica, para que a matriz seja constr\u00edda nessa base. Essa matriz de tranforma\u00e7\u00e3o tamb\u00e9m pode ser obtida por T = AP^{-1} , onde P tem como colunas os vetores da base, e A os vetores da base ap\u00f3s a transforma\u00e7\u00e3o. Para mostrar injetividade, podemos usar a contrapositiva da defini\u00e7\u00e3o. Voc\u00ea sabe encontrar uma base para o n\u00facleo e uma base para a imagem de uma transforma\u00e7\u00e3o? A base da imagem \u00e9 basicamente a base para o espa\u00e7o coluna (consegue enxergar o porqu\u00ea? Tente representar um vetor da imagem como combina\u00e7\u00e3o linear das colunas. E a base para o n\u00facleo? Exerc\u00edcios: Reflex\u00e3o em torno de uma reta: Seja S: \\mathbb{R}^2 \\to \\mathbb{R}^2 a transforma\u00e7\u00e3o que reflete um veotr em torno da reta y = ax . Assim, a reta \u00e9 a bissetriz do \u00e2ngulo entre v e Sv e \u00e9 perpendicular \u00e0 reta que liga v a Sv . Solu\u00e7\u00e3o: Seja P a matriz de proje\u00e7\u00e3o. Projetamos ortogonalmente v sobre a reta y = ax . Assim, teremos que v + Sv = 2Pv \\implies I + S = 2P \\implies S = 2P - I . Outra forma \u00e9 fazer as tranforma\u00e7\u00f5es dos vetores da base can\u00f4nica. Considere 5 l\u00e2mpadas, cada uma com um bot\u00e3o. Cada bot\u00e3o muda o estado da l\u00e2mpada e das vizinhas. Todas est\u00e3o apagadas. Como deixar a primeira, terceira e quinta acesas. Encontre os n\u00fameros a,b,c,d de modo que o operador A: \\mathbb{R}^2 \\to \\mathbb{R}^2 , dado por A(x,y) = (ax + by, cx + dy) tenha como n\u00facleo a reta y = 3x . A transforma\u00e7\u00e3o A: \\mathbb{R} \\to \\mathbb{R}^n; A(x) = (x,2x,...,nx) \u00e9 uma transforma\u00e7\u00e3o injetiva? E B(x,y) = (x + 2y, x + y, x - y) ? Considere uma transforma\u00e7\u00e3o A: E \\to F na base can\u00f4nica. Considere V uma base de vetores de E . Determine a matriz de transforma\u00e7\u00e3o A' nessa base. Ou seja, se Av = w \\to A'v_V = w_V . Ache uma transforma\u00e7\u00e3o A: \\mathbb{R}^2 \\to \\mathbb{R}^2 tal que a imagem e o n\u00facleo sejam o eixo x.","title":"Monitorias 10 e 11"},{"location":"alglin/files/monitoria10/#monitorias-10-e-11","text":"","title":"Monitorias 10 e 11"},{"location":"alglin/files/monitoria10/#definicoes-e-teoremas","text":"Lembrando: Uma transforma\u00e7\u00e3o linear T: \\mathbb{R}^n \\to \\mathbb{R}^m fica inteiramente determinada por uma matriz A = [a_{ij}] \\in M(m \\times n) . Os vetores coluna dessa matriz s\u00e3o as imagens A \\cdot e_j dos vetores da base can\u00f4nica. Definimos A como matriz de transforma\u00e7\u00e3o. Assim, A \\cdot e_j = \\sum_{i=1}^m a_{ij}e_i (j=1,...,n) , onde e_i \\in \\mathbb{R}^m . Simetrias: Matrizes de tranforma\u00e7\u00e3o referentes \u00e0 simetria em rela\u00e7\u00e3o aos eixos x e y, e em rela\u00e7\u00e3o \u00e0 origem, respectivamente: S_x = \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & -1 \\end{array} \\right] S_y = \\left[ \\begin{array}{cc} -1 & 0 \\\\ 0 & 1 \\end{array} \\right] S_o = \\left[ \\begin{array}{cc} -1 & 0 \\\\ 0 & -1 \\end{array} \\right] Dilata\u00e7\u00f5es: Basta multiplicar uma coluna que se quer dilatar por r . Podemos chamar r de coeficiente de dilata\u00e7\u00e3o. Rota\u00e7\u00e3o: Para montar essa matriz, basta conhecer a transforma\u00e7\u00e3o dos vetores (1,0) e (0,1) . R_{\\theta} = \\left[ \\begin{array}{cc} \\cos \\theta & - \\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{array} \\right] A rota\u00e7\u00e3o tem algumas propriedades: R_{\\theta}^{-1} = R_{-\\theta} R_{\\alpha}R_{\\beta} = R_{\\alpha + \\beta} (R_{\\theta})^n = R_{n\\theta} Proje\u00e7\u00f5es: Podemos considerar a transforma\u00e7\u00e3o que projeta os vetores sobre a reta y = ax . P = \\frac{1}{1+a^2} \\left[ \\begin{array}{cc} 1 & a \\\\ a & a^2 \\end{array} \\right] Se quisermos que a proje\u00e7\u00e3o sobre um eixo e paralelo a uma reta, temos que P_p = \\left[ \\begin{array}{cc} 1 & -\\frac{1}{a} \\\\ 0 & 0 \\end{array} \\right] N\u00facleo de A : N(A) = \\{v \\in E | Av = 0\\} . \u00c9 o espa\u00e7o anulado da matriz A . Imagem de A : Im(A) = \\{Av | v \\in E\\} \\implies \\exists v \\in E; Av = w \\implies w \\in Im(A) . Notemos que posto(A) = dim~Im(A) = dim~col(A) . Isto ocorre, pois w \\in Im(A) \u00e9 combina\u00e7\u00e3o linear das colunas da matriz A . Transforma\u00e7\u00e3o Injetiva: A: E \\to F \u00e9 injetiva se \\forall v, v', v \\neq v' \\implies Av \\neq Av' . Uma transforma\u00e7\u00e3o \u00e9 injetiva se, e s\u00f3 se, transforma vetores LI em vetores LI. Para essa demonstra\u00e7\u00e3o, \u00e9 necess\u00e1rio mostrar que uma transforma\u00e7\u00e3o \u00e9 injetiva se, e s\u00f3 se, seu n\u00facleo possui apenas o vetor nulo. Transforma\u00e7\u00e3o Sobrejetiva: Ocorre quando Im(A) = F , onde F \u00e9 o espa\u00e7o vetorial contradom\u00ednio. Teorema do N\u00facleo e da Imagem: Como dim~Im(A) = posto(A) , podemos usar no teorema do posto. Podemos alterar n para dim~E , sendo E o dom\u00ednio da transforma\u00e7\u00e3o. Laplace: Escolhe-se uma linha uma coluna e para cada elemento, calcula-se o seu cofator. A_{ij} = (-1)^{i+j}D_{ij} .","title":"Defini\u00e7\u00f5es e Teoremas"},{"location":"alglin/files/monitoria10/#propriedades-importantes","text":"det(A) = det(A^{T}) ; trocar duas linhas ou colunas inverte o sinal do determinante; duas linhas proporcionais indica determinante 0; multiplicar uma linha por \\alpha implicar\u00e1 multiplicar o determinante pelo mesmo fator; determinante do produto de matrizes \u00e9 o produto dos determinantes; o determinante de uma matriz com a opera\u00e7\u00e3o de somar com m\u00faltiplo de outra linha \u00e9 id\u00eantico; determinante da inversa \u00e9 o inverso do determinante","title":"Propriedades Importantes:"},{"location":"alglin/files/monitoria10/#lembretes-para-exercicios","text":"Para calcular uma matriz de tranforma\u00e7\u00e3o, precisamos apenas saber a transforma\u00e7\u00e3o linear de uma base do dom\u00ednio. Com essa transforma\u00e7\u00e3o, precisamos obter a transforma\u00e7\u00e3o da base can\u00f4nica, para que a matriz seja constr\u00edda nessa base. Essa matriz de tranforma\u00e7\u00e3o tamb\u00e9m pode ser obtida por T = AP^{-1} , onde P tem como colunas os vetores da base, e A os vetores da base ap\u00f3s a transforma\u00e7\u00e3o. Para mostrar injetividade, podemos usar a contrapositiva da defini\u00e7\u00e3o. Voc\u00ea sabe encontrar uma base para o n\u00facleo e uma base para a imagem de uma transforma\u00e7\u00e3o? A base da imagem \u00e9 basicamente a base para o espa\u00e7o coluna (consegue enxergar o porqu\u00ea? Tente representar um vetor da imagem como combina\u00e7\u00e3o linear das colunas. E a base para o n\u00facleo?","title":"Lembretes para exerc\u00edcios:"},{"location":"alglin/files/monitoria10/#exercicios","text":"Reflex\u00e3o em torno de uma reta: Seja S: \\mathbb{R}^2 \\to \\mathbb{R}^2 a transforma\u00e7\u00e3o que reflete um veotr em torno da reta y = ax . Assim, a reta \u00e9 a bissetriz do \u00e2ngulo entre v e Sv e \u00e9 perpendicular \u00e0 reta que liga v a Sv . Solu\u00e7\u00e3o: Seja P a matriz de proje\u00e7\u00e3o. Projetamos ortogonalmente v sobre a reta y = ax . Assim, teremos que v + Sv = 2Pv \\implies I + S = 2P \\implies S = 2P - I . Outra forma \u00e9 fazer as tranforma\u00e7\u00f5es dos vetores da base can\u00f4nica. Considere 5 l\u00e2mpadas, cada uma com um bot\u00e3o. Cada bot\u00e3o muda o estado da l\u00e2mpada e das vizinhas. Todas est\u00e3o apagadas. Como deixar a primeira, terceira e quinta acesas. Encontre os n\u00fameros a,b,c,d de modo que o operador A: \\mathbb{R}^2 \\to \\mathbb{R}^2 , dado por A(x,y) = (ax + by, cx + dy) tenha como n\u00facleo a reta y = 3x . A transforma\u00e7\u00e3o A: \\mathbb{R} \\to \\mathbb{R}^n; A(x) = (x,2x,...,nx) \u00e9 uma transforma\u00e7\u00e3o injetiva? E B(x,y) = (x + 2y, x + y, x - y) ? Considere uma transforma\u00e7\u00e3o A: E \\to F na base can\u00f4nica. Considere V uma base de vetores de E . Determine a matriz de transforma\u00e7\u00e3o A' nessa base. Ou seja, se Av = w \\to A'v_V = w_V . Ache uma transforma\u00e7\u00e3o A: \\mathbb{R}^2 \\to \\mathbb{R}^2 tal que a imagem e o n\u00facleo sejam o eixo x.","title":"Exerc\u00edcios:"},{"location":"alglin/files/monitoria12/","text":"Monitorias 12 e 13 Autovalores e Autovetores Defini\u00e7\u00e3o: Autovetor \u00e9 um vetor ( \\neq \\vec{0} ) que tem como imagem de uma transforma\u00e7\u00e3o linear um vetor proporcional. A propor\u00e7\u00e3o \u00e9 chamada de autovalor. Polin\u00f4mio Caracter\u00edstico: Polin\u00f4mio cujas ra\u00edzes s\u00e3o os autovalores de uma transforma\u00e7\u00e3o linear. Subespa\u00e7o invariante: Tamb\u00e9m conhecido como auto-espa\u00e7o, \u00e9 formado pela combina\u00e7\u00e3o dos autovetores associados ao mesmo autovalor. Teorema 1: Seja A um operador linear, \\lambda um autovalor e v um autovetor. Av = \\lambda v \\implies A^nv = \\lambda^n v . Teorema 2: A autovalores diferentes do mesmo operador correspondem autovetores linearmente independentes. Mudan\u00e7a de Base Considere as seguintes bases: E = \\{e_1, ..., e_n\\} U = \\{u_1, ..., u_n\\} V = \\{v_1,...,v_n\\} Considere w = (x_1,...,x_n) . Isso significa que w \u00e9 escrito como uma combina\u00e7\u00e3o linear dos vetores da base E , can\u00f4nica, e os coeficientes s\u00e3o x_1, ..., x_n . Imagine que queiramos escrever na base U . Para isso, basta encontrarmos os coeficientes de cada vetor da base U . Para isso, basta resolver o sistema linear onde cada vetor de U \u00e9 uma coluna e o vetor restultado \u00e9 o vetor na base can\u00f4nica. Assim, a matriz formada pelos vetores da base U formam uma matriz que transforma vetores da base U em vetores da base can\u00f4nica. A inversa faz o processo contr\u00e1rio. Se quis\u00e9ssemos mudar da base E para a base U sem o uso da inversa, s\u00f3 precisamos saber a transforma\u00e7\u00e3o dos vetores da base can\u00f4nica. Para fazer a transforma\u00e7\u00e3o de uma base em outra, basta transformarmos na can\u00f4nica como interm\u00e9dio. Matrizes Semelhantes e Diagonaliza\u00e7\u00e3o Defini\u00e7\u00e3o: Duas matrizes s\u00e3o semelhantes se existe P invert\u00edvel tal que B = P^{-1}AP (AP = PB) , que tem o mesmo polin\u00f4mio caracter\u00edstico e o mesmo determinante. Diagonaliza\u00e7\u00e3o: Uma matriz \u00e9 diagonaliz\u00e1vel se existe uma matriz semelhante que seja diagonal. A \u00e9 diagon\u00e1liz\u00e1vel se, e s\u00f3 se, tiver n autovetores LI. Nesse caso P \u00e9 a matriz cujas colunas s\u00e3o os autovetores de A e D os autovalores correspontes. Observe que P^{-1}AP = D , logo para transformar um vetor na base V em outro na base V correspondente a imagem desse vetor na base can\u00f4nica da matriz A , basta usar a transforma\u00e7\u00e3o D . Recorr\u00eancias Podemos utilizar matrizes para representar recorr\u00eancias. Um exemplo famoso \u00e9 a sequ\u00eancia de Fibonight. Produto Interno Seja E um espa\u00e7o vetorial e u, v \\in E . Define-se produto interno com <u,v> com um n\u00famero real que satisfaz as seguintes condi\u00e7\u00f5es: <u,v + v'> = <u,v> + <u,v'> e <u + u',v> = <u,v> + <u',v> <u,v> = <v,u> <\\alpha u,v> = \\alpha<u,v> e <u,\\alpha v> = \\alpha<u,v> Se u \\neq 0 , <u,u> > 0 Proje\u00e7\u00f5es J\u00e1 sabemos que p = (\\frac{u\\cdot v}{v\\cdot v})v \u00e9 a proje\u00e7\u00e3o do vetor u sobre a reta gerada por v . Quando queremos projetar um vetor v sobre um hiperplano \\pi , com vetor nornal n , temos que v = p + tn , onde t \u00e9 uma constante. Logo, podemos montar um sistema com n equa\u00e7\u00f5es. Ortogonalidade Se <u,v> = 0 , dizemos que u e v s\u00e3o ortogonais. Um conjunto \u00e9 dito ortogonal se a cada par de vetores, eles s\u00e3o ortogonais. Ele ser\u00e1 ortonormal quando seus vetores ortogonais forem normalizados. Note que se X \u00e9 um conjunto ortogonal, ent\u00e3o \u00e9 LI. Ortogonaliza\u00e7\u00e3o de Gram-Schimidt Lembre-se: Defina um vetor inicial e utilize a ideia de que cada outro vetor ser\u00e1 subtra\u00eddo das proje\u00e7\u00f5es do vetor calculado previamente. Proje\u00e7\u00e3o de um vetor sobre um subespa\u00e7o Seja W um subespa\u00e7o de E e \\alpha uma base ortogonal desse subespa\u00e7o. p = proj_W v = \\sum_{i=1}^n proj_{\\alpha_i} v Informa\u00e7\u00f5es Adicionais Extendendo a ideia dos autovalores: Dado um operador linear A: E \\to E ou existe um vetor u \\in E tal que Au = \\lambda u . Ou, ent\u00e3o, existem u, v \\in E linearmente independentes, tais que Au = \\alpha u + \\beta v e Av = \\gamma u + \\delta v . Invariante: Diz-se que um subespa\u00e7o vetorial F \\subset E \u00e9 invariante pelo operador A: E \\to F quando A(F) \\subset F . Isto \u00e9, quando a imagem dos vetorres desse subespa\u00e7o est\u00e3o nesse subespa\u00e7o. Um subespa\u00e7o de dimens\u00e3o 1 \u00e9 invariante por A se, e somente se, existe um n\u00famero \\lambda tal que Av = \\lambda v, \\forall v \\in F . Se u,v formam um subespa\u00e7o de dimens\u00e3o 2 , ele ser\u00e1 invariante se, e s\u00f3 se, Au \\in F e Av \\in F . Teorema: Todo operador linear num espa\u00e7o vetorial de dimens\u00e3o finita possui um subespa\u00e7o invariante de dimens\u00e3o 1 ou 2. Para provar esse teorema, temos que provar o lema que diz que existem um polin\u00f4mio de grau 1 ou 2 e um vetor v tal que p(A)\\cdot v = 0 . Cadeias de Markov Defini\u00e7\u00e3o: \u00c9 uma s\u00e9rie temporal discreta no qual a distribui\u00e7\u00e3o de uma popula\u00e7\u00e3o pode ser calculada por recorr\u00eancia. Ad condi\u00e7\u00f5es s\u00e3o que a popula\u00e7\u00e3o nunca torna-se negativa e que a popula\u00e7\u00e3o total \u00e9 fixa. Podemos utilizar uma matriz de trani\u00e7\u00e3o que descreva a movimenta\u00e7\u00e3o probil\u00edstica dessa popula\u00e7\u00e3o. Requere-se que a soma de cada coluna seja 1 e que n\u00e3o haja entradas negativas. O elemento ij da matriz descreve a probabilidade da popula\u00e7\u00e3o passar do estado j para o estadp i . Se T possui alguma pot\u00eancua com todas as entradas positivas, \u00e9 dito regular. Uma matrzi de transi\u00e7\u00e3o regular ter\u00e1 um estado estacion\u00e1rio. Ts = s . \u00c9 poss\u00edvel mostrar que qualquer matriz de transi\u00e7\u00e3o com as condi\u00e7\u00f5es dadas deve ter um autovalor 1 .","title":"Monitorias 12 e 13"},{"location":"alglin/files/monitoria12/#monitorias-12-e-13","text":"","title":"Monitorias 12 e 13"},{"location":"alglin/files/monitoria12/#autovalores-e-autovetores","text":"Defini\u00e7\u00e3o: Autovetor \u00e9 um vetor ( \\neq \\vec{0} ) que tem como imagem de uma transforma\u00e7\u00e3o linear um vetor proporcional. A propor\u00e7\u00e3o \u00e9 chamada de autovalor. Polin\u00f4mio Caracter\u00edstico: Polin\u00f4mio cujas ra\u00edzes s\u00e3o os autovalores de uma transforma\u00e7\u00e3o linear. Subespa\u00e7o invariante: Tamb\u00e9m conhecido como auto-espa\u00e7o, \u00e9 formado pela combina\u00e7\u00e3o dos autovetores associados ao mesmo autovalor. Teorema 1: Seja A um operador linear, \\lambda um autovalor e v um autovetor. Av = \\lambda v \\implies A^nv = \\lambda^n v . Teorema 2: A autovalores diferentes do mesmo operador correspondem autovetores linearmente independentes.","title":"Autovalores e Autovetores"},{"location":"alglin/files/monitoria12/#mudanca-de-base","text":"Considere as seguintes bases: E = \\{e_1, ..., e_n\\} U = \\{u_1, ..., u_n\\} V = \\{v_1,...,v_n\\} Considere w = (x_1,...,x_n) . Isso significa que w \u00e9 escrito como uma combina\u00e7\u00e3o linear dos vetores da base E , can\u00f4nica, e os coeficientes s\u00e3o x_1, ..., x_n . Imagine que queiramos escrever na base U . Para isso, basta encontrarmos os coeficientes de cada vetor da base U . Para isso, basta resolver o sistema linear onde cada vetor de U \u00e9 uma coluna e o vetor restultado \u00e9 o vetor na base can\u00f4nica. Assim, a matriz formada pelos vetores da base U formam uma matriz que transforma vetores da base U em vetores da base can\u00f4nica. A inversa faz o processo contr\u00e1rio. Se quis\u00e9ssemos mudar da base E para a base U sem o uso da inversa, s\u00f3 precisamos saber a transforma\u00e7\u00e3o dos vetores da base can\u00f4nica. Para fazer a transforma\u00e7\u00e3o de uma base em outra, basta transformarmos na can\u00f4nica como interm\u00e9dio.","title":"Mudan\u00e7a de Base"},{"location":"alglin/files/monitoria12/#matrizes-semelhantes-e-diagonalizacao","text":"Defini\u00e7\u00e3o: Duas matrizes s\u00e3o semelhantes se existe P invert\u00edvel tal que B = P^{-1}AP (AP = PB) , que tem o mesmo polin\u00f4mio caracter\u00edstico e o mesmo determinante. Diagonaliza\u00e7\u00e3o: Uma matriz \u00e9 diagonaliz\u00e1vel se existe uma matriz semelhante que seja diagonal. A \u00e9 diagon\u00e1liz\u00e1vel se, e s\u00f3 se, tiver n autovetores LI. Nesse caso P \u00e9 a matriz cujas colunas s\u00e3o os autovetores de A e D os autovalores correspontes. Observe que P^{-1}AP = D , logo para transformar um vetor na base V em outro na base V correspondente a imagem desse vetor na base can\u00f4nica da matriz A , basta usar a transforma\u00e7\u00e3o D .","title":"Matrizes Semelhantes e Diagonaliza\u00e7\u00e3o"},{"location":"alglin/files/monitoria12/#recorrencias","text":"Podemos utilizar matrizes para representar recorr\u00eancias. Um exemplo famoso \u00e9 a sequ\u00eancia de Fibonight.","title":"Recorr\u00eancias"},{"location":"alglin/files/monitoria12/#produto-interno","text":"Seja E um espa\u00e7o vetorial e u, v \\in E . Define-se produto interno com <u,v> com um n\u00famero real que satisfaz as seguintes condi\u00e7\u00f5es: <u,v + v'> = <u,v> + <u,v'> e <u + u',v> = <u,v> + <u',v> <u,v> = <v,u> <\\alpha u,v> = \\alpha<u,v> e <u,\\alpha v> = \\alpha<u,v> Se u \\neq 0 , <u,u> > 0","title":"Produto Interno"},{"location":"alglin/files/monitoria12/#projecoes","text":"J\u00e1 sabemos que p = (\\frac{u\\cdot v}{v\\cdot v})v \u00e9 a proje\u00e7\u00e3o do vetor u sobre a reta gerada por v . Quando queremos projetar um vetor v sobre um hiperplano \\pi , com vetor nornal n , temos que v = p + tn , onde t \u00e9 uma constante. Logo, podemos montar um sistema com n equa\u00e7\u00f5es.","title":"Proje\u00e7\u00f5es"},{"location":"alglin/files/monitoria12/#ortogonalidade","text":"Se <u,v> = 0 , dizemos que u e v s\u00e3o ortogonais. Um conjunto \u00e9 dito ortogonal se a cada par de vetores, eles s\u00e3o ortogonais. Ele ser\u00e1 ortonormal quando seus vetores ortogonais forem normalizados. Note que se X \u00e9 um conjunto ortogonal, ent\u00e3o \u00e9 LI.","title":"Ortogonalidade"},{"location":"alglin/files/monitoria12/#ortogonalizacao-de-gram-schimidt","text":"Lembre-se: Defina um vetor inicial e utilize a ideia de que cada outro vetor ser\u00e1 subtra\u00eddo das proje\u00e7\u00f5es do vetor calculado previamente.","title":"Ortogonaliza\u00e7\u00e3o de Gram-Schimidt"},{"location":"alglin/files/monitoria12/#projecao-de-um-vetor-sobre-um-subespaco","text":"Seja W um subespa\u00e7o de E e \\alpha uma base ortogonal desse subespa\u00e7o. p = proj_W v = \\sum_{i=1}^n proj_{\\alpha_i} v","title":"Proje\u00e7\u00e3o de um vetor sobre um subespa\u00e7o"},{"location":"alglin/files/monitoria12/#informacoes-adicionais","text":"Extendendo a ideia dos autovalores: Dado um operador linear A: E \\to E ou existe um vetor u \\in E tal que Au = \\lambda u . Ou, ent\u00e3o, existem u, v \\in E linearmente independentes, tais que Au = \\alpha u + \\beta v e Av = \\gamma u + \\delta v . Invariante: Diz-se que um subespa\u00e7o vetorial F \\subset E \u00e9 invariante pelo operador A: E \\to F quando A(F) \\subset F . Isto \u00e9, quando a imagem dos vetorres desse subespa\u00e7o est\u00e3o nesse subespa\u00e7o. Um subespa\u00e7o de dimens\u00e3o 1 \u00e9 invariante por A se, e somente se, existe um n\u00famero \\lambda tal que Av = \\lambda v, \\forall v \\in F . Se u,v formam um subespa\u00e7o de dimens\u00e3o 2 , ele ser\u00e1 invariante se, e s\u00f3 se, Au \\in F e Av \\in F . Teorema: Todo operador linear num espa\u00e7o vetorial de dimens\u00e3o finita possui um subespa\u00e7o invariante de dimens\u00e3o 1 ou 2. Para provar esse teorema, temos que provar o lema que diz que existem um polin\u00f4mio de grau 1 ou 2 e um vetor v tal que p(A)\\cdot v = 0 .","title":"Informa\u00e7\u00f5es Adicionais"},{"location":"alglin/files/monitoria12/#cadeias-de-markov","text":"Defini\u00e7\u00e3o: \u00c9 uma s\u00e9rie temporal discreta no qual a distribui\u00e7\u00e3o de uma popula\u00e7\u00e3o pode ser calculada por recorr\u00eancia. Ad condi\u00e7\u00f5es s\u00e3o que a popula\u00e7\u00e3o nunca torna-se negativa e que a popula\u00e7\u00e3o total \u00e9 fixa. Podemos utilizar uma matriz de trani\u00e7\u00e3o que descreva a movimenta\u00e7\u00e3o probil\u00edstica dessa popula\u00e7\u00e3o. Requere-se que a soma de cada coluna seja 1 e que n\u00e3o haja entradas negativas. O elemento ij da matriz descreve a probabilidade da popula\u00e7\u00e3o passar do estado j para o estadp i . Se T possui alguma pot\u00eancua com todas as entradas positivas, \u00e9 dito regular. Uma matrzi de transi\u00e7\u00e3o regular ter\u00e1 um estado estacion\u00e1rio. Ts = s . \u00c9 poss\u00edvel mostrar que qualquer matriz de transi\u00e7\u00e3o com as condi\u00e7\u00f5es dadas deve ter um autovalor 1 .","title":"Cadeias de Markov"},{"location":"alglin/files/monitoria6/","text":"Monitoria 6 Defini\u00e7\u00f5es e Teoremas Linearmente Independente: Um cojunto X \\subset E \u00e9 dito linearmente independente, quando nenhum vetor do conjunto \u00e9 combina\u00e7\u00e3o linear dos outros vetores. O conjunto unit\u00e1rio \u00e9 dito LI. Para isso, existe o teorema de que: \\alpha_1v_1 + ... + \\alpha_nv_n = 0 \\to \\alpha_1 = ... = \\alpha_n = 0 , se e s\u00f3 se, X \u00e9 LI. A partir disso, conclue-se que a representa\u00e7\u00e3o de um vetor como combina\u00e7\u00e3o de outros vetores \u00e9 sempre \u00fanica (se os vetores formarem um conjunto LI). Se um conjunto n\u00e3o \u00e9 LI, ele \u00e9 dito linearmente dependente. Teorema 1 Seja X = \\{x_1,x_2,...,x_m\\} . Se, \\forall k \\leq m, v_k n\u00e3o \u00e9 combina\u00e7\u00e3o linear de seus antecessores, ent\u00e3o X \u00e9 LI. Observa\u00e7\u00e3o Considere X = \\{(1,2),(3,4),(2,4)\\} \\subset \\mathbb{R}^2 , Note que X \u00e9 LD, por\u00e9m (3,4) n\u00e3o \u00e9 combina\u00e7\u00e3o linear dos outros vetores (verifique!). Por que isso n\u00e3o \u00e9 contradit\u00f3rio? Base: \u00c9 um conjunto linearmente independente que gera E. Os coeficientes s\u00e3o chamados de coordenadas do vetor nessa base. Como veremos a seguir, toda base de um espa\u00e7o vetorial apresenta o mesmo n\u00famero de elementos. Este n\u00famero \u00e9 chamado de \\textit{dimens\u00e3o}. Lema 2.1: Todo sistema homog\u00eaneo cujo n\u00famero de inc\u00f3gnitas \u00e9 maior que o n\u00famero de equa\u00e7\u00f5es admite solu\u00e7\u00e3o n\u00e3o trivial (a prova \u00e9 por indu\u00e7\u00e3o em m , o n\u00famero de equa\u00e7\u00f5es. Teorema 2.2: Se um conjunto de n vetores gera o espa\u00e7o E, ent\u00e3o qualquer conjunto com mais de n elementos \u00e9 LD. Corol\u00e1rio 2.3: Assim, se os vetores v_1,...,v_n geram o espa\u00e7o vetorial E e os vetores u_1,...,u_m s\u00e3o LI, m\\leq n . Daqui tiramos que se E admite uma base \\beta = \\{u_1,...,u_n\\} , qualquer outra base tamb\u00e9m possui n elementos. Teorema 3: Considere um espa\u00e7o vetorial de dimens\u00e3o finita: Considere o conjunto de todos os geradores de E. Ele cont\u00e9m uma base. Todo conjunto LI est\u00e1 contido numa base. Todo subespa\u00e7o vetorial tem dimens\u00e3o finita. Se a dimens\u00e3o de um subespa\u00e7o \u00e9 n , ent\u00e3o o subespa\u00e7o \u00e9 o pr\u00f3prio espa\u00e7o. Exerc\u00edcios Prove que os seguintes polin\u00f4mios s\u00e3o linearmente independentes: p(x) = x^3 - 5x^2 + 1, q(x) = 2x^4 + 5x - 6, r(x) = x^2 - 5x + 2 . Dica: Considere a base X = \\{1, x, x^2, x^3, x^4\\} Seja X um conjunto de polin\u00f4mios. Se dois polin\u00f4mios quaisquer de X t\u00eam graus diferentes, X \u00e9 LI. Dado X \\subset E , seja Y o conjunto obtido de X substituindo um dos seus elementos v por v + \\alpha u , onde u \\in X e \\alpha \\in \\mathbb{R} . Prove que X e Y geram o mesmo subespa\u00e7o vetorial de E . Conclua, ent\u00e3o que \\{v_1,...,v_k\\} \\subset E e \\{v_1, v_2 - v_1, ..., v_k - v_1\\} \\subset E geram o mesmo subespa\u00e7o vetorial de E . Mostre que os vetores u = (1,1) e v = (-1,1) formam uma base de \\mathbb{R}^2 . Considere a afirma\u00e7\u00e3o: \"A uni\u00e3o de dois conjuntos subconjuntos LI do espa\u00e7o vetorial E \u00e9 ainda um conjunto LI\". Assinale verdadeiro e falso: ( ) Nunca; ( ) Quando um deles \u00e9 disjunto do outro; ( ) Quanto um deles \u00e9 parte do outro; ( ) Quando um deles \u00e9 disjunto do subespa\u00e7o gerado pelo outro; ( ) Quando o n\u00famero de elementos de um deles mais o n\u00famero de elementos do outro \u00e9 igual \u00e0 dimens\u00e3o de E. Encontre uma base para o espa\u00e7o vetorial W = \\{\\begin{pmatrix} a \\\\ b \\\\ -b \\\\ a\\end{pmatrix}, \\forall a,b \\in \\mathbb{R}^2\\} Se f e g est\u00e3o no espa\u00e7o vetorial de todas as fun\u00e7\u00f5es com derivadas cont\u00ednuas, ent\u00e3o o determinante de \\begin{pmatrix} f(x) & g(x) \\\\ f'(x) & g'(x) \\end{pmatrix} \u00e9 conhecido como Wronskiano de f e g . Prove que f e g s\u00e3o linearmente independentes, se seu Wronskiano n\u00e3o for identicamente nulo. Esse estudo \u00e9 estremamente importante no estudo de solu\u00e7\u00f5es de sistemas de equa\u00e7\u00f5es diferenci\u00e1veis, pois identifica se duas solu\u00e7\u00f5es s\u00e3o linearmente independentes. Aplica\u00e7\u00e3o: Quadrados M\u00e1gicos Observe a imagem da Melancolia I, de Albrecht Durer de 1514: Link da obra Observa-se o quadrado m\u00e1gico: \\begin{pmatrix} 16 & 3 & 2 & 13 \\\\ 5 & 10 & 11 & 8 \\\\ 9 & 6 & 7 & 12 \\\\ 4 & 15 & 14 & 1 \\end{pmatrix} Primeira coisa interessante \u00e9 ver 15 e 14 lado a lado. A soma de cada coluna, linha e diagoral \u00e9 34 . Podemos definir uma matriz n\\times n sendo quadrado m\u00e1gico quando a soma de cada linha, coluna e diagonal \u00e9 igual. Essa soma se chama peso. Considere Mag_n o conjunto de todos os quadrados m\u00e1gicos de ordem n . Prove que Mag_3 \u00e9 um subespa\u00e7o de M_{3x3} .","title":"Monitoria 6"},{"location":"alglin/files/monitoria6/#monitoria-6","text":"","title":"Monitoria 6"},{"location":"alglin/files/monitoria6/#definicoes-e-teoremas","text":"Linearmente Independente: Um cojunto X \\subset E \u00e9 dito linearmente independente, quando nenhum vetor do conjunto \u00e9 combina\u00e7\u00e3o linear dos outros vetores. O conjunto unit\u00e1rio \u00e9 dito LI. Para isso, existe o teorema de que: \\alpha_1v_1 + ... + \\alpha_nv_n = 0 \\to \\alpha_1 = ... = \\alpha_n = 0 , se e s\u00f3 se, X \u00e9 LI. A partir disso, conclue-se que a representa\u00e7\u00e3o de um vetor como combina\u00e7\u00e3o de outros vetores \u00e9 sempre \u00fanica (se os vetores formarem um conjunto LI). Se um conjunto n\u00e3o \u00e9 LI, ele \u00e9 dito linearmente dependente. Teorema 1 Seja X = \\{x_1,x_2,...,x_m\\} . Se, \\forall k \\leq m, v_k n\u00e3o \u00e9 combina\u00e7\u00e3o linear de seus antecessores, ent\u00e3o X \u00e9 LI. Observa\u00e7\u00e3o Considere X = \\{(1,2),(3,4),(2,4)\\} \\subset \\mathbb{R}^2 , Note que X \u00e9 LD, por\u00e9m (3,4) n\u00e3o \u00e9 combina\u00e7\u00e3o linear dos outros vetores (verifique!). Por que isso n\u00e3o \u00e9 contradit\u00f3rio? Base: \u00c9 um conjunto linearmente independente que gera E. Os coeficientes s\u00e3o chamados de coordenadas do vetor nessa base. Como veremos a seguir, toda base de um espa\u00e7o vetorial apresenta o mesmo n\u00famero de elementos. Este n\u00famero \u00e9 chamado de \\textit{dimens\u00e3o}. Lema 2.1: Todo sistema homog\u00eaneo cujo n\u00famero de inc\u00f3gnitas \u00e9 maior que o n\u00famero de equa\u00e7\u00f5es admite solu\u00e7\u00e3o n\u00e3o trivial (a prova \u00e9 por indu\u00e7\u00e3o em m , o n\u00famero de equa\u00e7\u00f5es. Teorema 2.2: Se um conjunto de n vetores gera o espa\u00e7o E, ent\u00e3o qualquer conjunto com mais de n elementos \u00e9 LD. Corol\u00e1rio 2.3: Assim, se os vetores v_1,...,v_n geram o espa\u00e7o vetorial E e os vetores u_1,...,u_m s\u00e3o LI, m\\leq n . Daqui tiramos que se E admite uma base \\beta = \\{u_1,...,u_n\\} , qualquer outra base tamb\u00e9m possui n elementos. Teorema 3: Considere um espa\u00e7o vetorial de dimens\u00e3o finita: Considere o conjunto de todos os geradores de E. Ele cont\u00e9m uma base. Todo conjunto LI est\u00e1 contido numa base. Todo subespa\u00e7o vetorial tem dimens\u00e3o finita. Se a dimens\u00e3o de um subespa\u00e7o \u00e9 n , ent\u00e3o o subespa\u00e7o \u00e9 o pr\u00f3prio espa\u00e7o.","title":"Defini\u00e7\u00f5es e Teoremas"},{"location":"alglin/files/monitoria6/#exercicios","text":"Prove que os seguintes polin\u00f4mios s\u00e3o linearmente independentes: p(x) = x^3 - 5x^2 + 1, q(x) = 2x^4 + 5x - 6, r(x) = x^2 - 5x + 2 . Dica: Considere a base X = \\{1, x, x^2, x^3, x^4\\} Seja X um conjunto de polin\u00f4mios. Se dois polin\u00f4mios quaisquer de X t\u00eam graus diferentes, X \u00e9 LI. Dado X \\subset E , seja Y o conjunto obtido de X substituindo um dos seus elementos v por v + \\alpha u , onde u \\in X e \\alpha \\in \\mathbb{R} . Prove que X e Y geram o mesmo subespa\u00e7o vetorial de E . Conclua, ent\u00e3o que \\{v_1,...,v_k\\} \\subset E e \\{v_1, v_2 - v_1, ..., v_k - v_1\\} \\subset E geram o mesmo subespa\u00e7o vetorial de E . Mostre que os vetores u = (1,1) e v = (-1,1) formam uma base de \\mathbb{R}^2 . Considere a afirma\u00e7\u00e3o: \"A uni\u00e3o de dois conjuntos subconjuntos LI do espa\u00e7o vetorial E \u00e9 ainda um conjunto LI\". Assinale verdadeiro e falso: ( ) Nunca; ( ) Quando um deles \u00e9 disjunto do outro; ( ) Quanto um deles \u00e9 parte do outro; ( ) Quando um deles \u00e9 disjunto do subespa\u00e7o gerado pelo outro; ( ) Quando o n\u00famero de elementos de um deles mais o n\u00famero de elementos do outro \u00e9 igual \u00e0 dimens\u00e3o de E. Encontre uma base para o espa\u00e7o vetorial W = \\{\\begin{pmatrix} a \\\\ b \\\\ -b \\\\ a\\end{pmatrix}, \\forall a,b \\in \\mathbb{R}^2\\} Se f e g est\u00e3o no espa\u00e7o vetorial de todas as fun\u00e7\u00f5es com derivadas cont\u00ednuas, ent\u00e3o o determinante de \\begin{pmatrix} f(x) & g(x) \\\\ f'(x) & g'(x) \\end{pmatrix} \u00e9 conhecido como Wronskiano de f e g . Prove que f e g s\u00e3o linearmente independentes, se seu Wronskiano n\u00e3o for identicamente nulo. Esse estudo \u00e9 estremamente importante no estudo de solu\u00e7\u00f5es de sistemas de equa\u00e7\u00f5es diferenci\u00e1veis, pois identifica se duas solu\u00e7\u00f5es s\u00e3o linearmente independentes.","title":"Exerc\u00edcios"},{"location":"alglin/files/monitoria6/#aplicacao-quadrados-magicos","text":"Observe a imagem da Melancolia I, de Albrecht Durer de 1514: Link da obra Observa-se o quadrado m\u00e1gico: \\begin{pmatrix} 16 & 3 & 2 & 13 \\\\ 5 & 10 & 11 & 8 \\\\ 9 & 6 & 7 & 12 \\\\ 4 & 15 & 14 & 1 \\end{pmatrix} Primeira coisa interessante \u00e9 ver 15 e 14 lado a lado. A soma de cada coluna, linha e diagoral \u00e9 34 . Podemos definir uma matriz n\\times n sendo quadrado m\u00e1gico quando a soma de cada linha, coluna e diagonal \u00e9 igual. Essa soma se chama peso. Considere Mag_n o conjunto de todos os quadrados m\u00e1gicos de ordem n . Prove que Mag_3 \u00e9 um subespa\u00e7o de M_{3x3} .","title":"Aplica\u00e7\u00e3o: Quadrados M\u00e1gicos"},{"location":"alglin/files/monitoria7/","text":"Monitoria 7 Defini\u00e7\u00f5es e Teoremas Quando uma linha \u00e9 substitu\u00edda pela soma dela com um m\u00faltiplo de outra, a nova linha pertence ao mesmo subespa\u00e7o gerado pelas primeiras. Mais do que isso, o subespa\u00e7o gerado \u00e9 o mesmo. Nulidade da Matriz: Dimens\u00e3o do espa\u00e7o anulado da matriz A ( anul(A) ). Voc\u00ea sabe encontrar a nulidade de uma matriz? Teorema do Posto ! anul(A) + posto(A) = n . Considere Ax = v_E . Ent\u00e3o, A \u00e9 a matriz de passagem da base B para a base E , can\u00f4nica. Assim, v_B = A^{-1}v_E . Transforma\u00e7\u00e3o Linear \u00e9 uma fun\u00e7\u00e3o linear entre os espa\u00e7os vetorias E e F com as propriedades de soma T(u+v)=T(u)+T(v) e T(\\alpha u) = \\alpha T(u) . Lembre que T(v) = T(\\alpha_1 e_1 + ... + \\alpha_n e_n) = \\alpha_1 T(e_1) + ... + \\alpha_n T(e_n) . Logo, a transforma\u00e7\u00e3o linear est\u00e1 definida quando conhcemos as imagens dos elementos de uma base. Da\u00ed sa\u00ed a matriz de transforma\u00e7\u00e3o. O escalonamento mant\u00e9m a rela\u00e7\u00e3o entre as colunas das matrizes. Para se ter a intui\u00e7\u00e3o, basta pensar que para resolver sistemas, escalonamos as matrizes, e as inc\u00f3gnitas permanecem as mesmas para o sistema escalonado. Suponha que temos uma vetor w_\\beta = (a,b)_\\beta e queremos reescrever w_E = (a',b') , na base can\u00f4nica. Para isso, precisamos fazer uma mudan\u00e7a de bases que envolve uma matriz de tranforma\u00e7\u00e3o. Essa matriz \u00e9 simples, pois \u00e9 composta pelos vetores da base \\beta . Teorema: Seja uma transforma\u00e7\u00e3o linear A: E \\to F . A cada vetor u \\in \\beta base de E , fa\u00e7amos corresponder um vetor u' \\in F . Ent\u00e3o essa tranforma\u00e7\u00e3o, tal que Au = u' , \u00e9 \u00fanica. Importante Saber encontrar bases do espa\u00e7o-coluna, do espa\u00e7o-linha e do espa\u00e7o-anulado (logo suas dimens\u00f5es). Um funcional linear \u00e9 T: E \\to \\mathbb{R} . Um operador linear \u00e9 T: E \\to E . Lembrar de conjunto gerador. Exerc\u00edcios: Prove que os seguintes polin\u00f4mios s\u00e3o linearmente independentes: p(x) = x^3 - 5x^2 + 1, q(x) = 2x^4 + 5x - 6, r(x) = x^2 - 5x + 2 . Considere a base X = \\{1, x, x^2, x^3, x^4\\} Seja X um conjunto de polin\u00f4mios. Se dois polin\u00f4mios quaisquer de X t\u00eam graus diferentes, X \u00e9 LI. Mostre que os vetores u = (1,1) e v = (-1,1) formam uma base de \\mathbb{R}^2 . Encontre os espa\u00e7os linha, coluna e anulado da matriz: A = \\left[ \\begin{array}{ccccc} -1 & 2 & 0 & 4 & 5\\\\ 3 & -7 & 2 & 0 & 1\\\\ 2 & -5 & 2 & 4 & 6 \\end{array} \\right] Seja U = \\{u_1,u_2,u_3\\} . Como representar o vetor (a,b,c) , como combina\u00e7\u00e3o linear dos vetores de U . Exiba uma base para o espa\u00e7o vetorial formado pelos polin\u00f4mios de grau \\leq n que se anulam em x=2 e x=3 . Qual a dimens\u00e3o dessa base? Tem-se uma transforma\u00e7\u00e3o linear A(-1,1) = (1,2,3) e A(2,3) = (1,1,1) . Qual a matriz de tranforma\u00e7\u00e3o de A , em rela\u00e7\u00e3o \u00e0s bases can\u00f4nicas. Avalie as afirma\u00e7\u00f5es: ( ) Seja C = \\{(x_1,x_2,x_3,x_4,x_5); x_i = 3\\cdot x_{i-1}, i=2,...,5\\} . \u00c9 um subespa\u00e7o do \\mathbb{R}^5 . (Se verdadeiro, apresente uma base). ( ) \u00c9 poss\u00edvel encontrar dois planos do \\mathbb{R}^4 que se interesectem em apenas um ponto. ( Pense em planos com dois par\u00e2metos livres ). ( ) O conjunto de todas as matrizes cujo determinante \u00e9 maior do que zero \u00e9 um subespa\u00e7o das matrizes. ( ) A uni\u00e3o de dois conjuntos LI \u00e9 um conjunto LI, se um deles \u00e9 disjunto do subespa\u00e7o gerado pelo outro. ( ) Existe apenas uma transforma\u00e7\u00e3o linear com T(0,0,1) = (1,2) , T(0,1,0) = (2,3), T(1,0,0) = (4,7) , onde T: \\mathbb{R}^3\\to \\mathbb{R}^2 . Isto \u00e9, n\u00e3o existem x,y,z , tal que T(x,y,z) \\neq T'(x,y,z) com essas propriedades. ( ) Se u, v, w \\in E s\u00e3o colineares, ent\u00e3o Au,Av,Aw tamb\u00e9m s\u00e3o. ( ) Se Aw = Au + Av , ent\u00e3o w = u + v . Aplica\u00e7\u00e3o: Quadrados M\u00e1gicos Na monitoria 6, observamos a imagem da Melancolia I, de Albrecht Durer de 1514, em que apareceia um quadrado m\u00e1gico cl\u00e1ssico. Para lembrar: definimos uma matriz n\\times n como quadrado m\u00e1gico quando a soma de cada linha, coluna e diagonal \u00e9 igual. Essa soma se chama peso. Mag_n o conjunto de todos os quadrados m\u00e1gicos de ordem n .O quadrado ser\u00e1 cl\u00e1ssico se usarmos os todos on n\u00fameros entre 1 e n^2 . Considere Mag_2 . Voc\u00ea conseguiria encontrar uma base para esse subespa\u00e7o das matrizes 2 por 2 (provamos que \u00e9 um subespa\u00e7o na semana passada)? E Mag_3 ? Exemplo: X = \\{(1,1,1),(1,2,1)\\}, Y = \\{(1,0,0),(0,0,1)\\}","title":"Monitoria 7"},{"location":"alglin/files/monitoria7/#monitoria-7","text":"","title":"Monitoria 7"},{"location":"alglin/files/monitoria7/#definicoes-e-teoremas","text":"Quando uma linha \u00e9 substitu\u00edda pela soma dela com um m\u00faltiplo de outra, a nova linha pertence ao mesmo subespa\u00e7o gerado pelas primeiras. Mais do que isso, o subespa\u00e7o gerado \u00e9 o mesmo. Nulidade da Matriz: Dimens\u00e3o do espa\u00e7o anulado da matriz A ( anul(A) ). Voc\u00ea sabe encontrar a nulidade de uma matriz? Teorema do Posto ! anul(A) + posto(A) = n . Considere Ax = v_E . Ent\u00e3o, A \u00e9 a matriz de passagem da base B para a base E , can\u00f4nica. Assim, v_B = A^{-1}v_E . Transforma\u00e7\u00e3o Linear \u00e9 uma fun\u00e7\u00e3o linear entre os espa\u00e7os vetorias E e F com as propriedades de soma T(u+v)=T(u)+T(v) e T(\\alpha u) = \\alpha T(u) . Lembre que T(v) = T(\\alpha_1 e_1 + ... + \\alpha_n e_n) = \\alpha_1 T(e_1) + ... + \\alpha_n T(e_n) . Logo, a transforma\u00e7\u00e3o linear est\u00e1 definida quando conhcemos as imagens dos elementos de uma base. Da\u00ed sa\u00ed a matriz de transforma\u00e7\u00e3o. O escalonamento mant\u00e9m a rela\u00e7\u00e3o entre as colunas das matrizes. Para se ter a intui\u00e7\u00e3o, basta pensar que para resolver sistemas, escalonamos as matrizes, e as inc\u00f3gnitas permanecem as mesmas para o sistema escalonado. Suponha que temos uma vetor w_\\beta = (a,b)_\\beta e queremos reescrever w_E = (a',b') , na base can\u00f4nica. Para isso, precisamos fazer uma mudan\u00e7a de bases que envolve uma matriz de tranforma\u00e7\u00e3o. Essa matriz \u00e9 simples, pois \u00e9 composta pelos vetores da base \\beta . Teorema: Seja uma transforma\u00e7\u00e3o linear A: E \\to F . A cada vetor u \\in \\beta base de E , fa\u00e7amos corresponder um vetor u' \\in F . Ent\u00e3o essa tranforma\u00e7\u00e3o, tal que Au = u' , \u00e9 \u00fanica.","title":"Defini\u00e7\u00f5es e Teoremas"},{"location":"alglin/files/monitoria7/#importante","text":"Saber encontrar bases do espa\u00e7o-coluna, do espa\u00e7o-linha e do espa\u00e7o-anulado (logo suas dimens\u00f5es). Um funcional linear \u00e9 T: E \\to \\mathbb{R} . Um operador linear \u00e9 T: E \\to E . Lembrar de conjunto gerador.","title":"Importante"},{"location":"alglin/files/monitoria7/#exercicios","text":"Prove que os seguintes polin\u00f4mios s\u00e3o linearmente independentes: p(x) = x^3 - 5x^2 + 1, q(x) = 2x^4 + 5x - 6, r(x) = x^2 - 5x + 2 . Considere a base X = \\{1, x, x^2, x^3, x^4\\} Seja X um conjunto de polin\u00f4mios. Se dois polin\u00f4mios quaisquer de X t\u00eam graus diferentes, X \u00e9 LI. Mostre que os vetores u = (1,1) e v = (-1,1) formam uma base de \\mathbb{R}^2 . Encontre os espa\u00e7os linha, coluna e anulado da matriz: A = \\left[ \\begin{array}{ccccc} -1 & 2 & 0 & 4 & 5\\\\ 3 & -7 & 2 & 0 & 1\\\\ 2 & -5 & 2 & 4 & 6 \\end{array} \\right] Seja U = \\{u_1,u_2,u_3\\} . Como representar o vetor (a,b,c) , como combina\u00e7\u00e3o linear dos vetores de U . Exiba uma base para o espa\u00e7o vetorial formado pelos polin\u00f4mios de grau \\leq n que se anulam em x=2 e x=3 . Qual a dimens\u00e3o dessa base? Tem-se uma transforma\u00e7\u00e3o linear A(-1,1) = (1,2,3) e A(2,3) = (1,1,1) . Qual a matriz de tranforma\u00e7\u00e3o de A , em rela\u00e7\u00e3o \u00e0s bases can\u00f4nicas. Avalie as afirma\u00e7\u00f5es: ( ) Seja C = \\{(x_1,x_2,x_3,x_4,x_5); x_i = 3\\cdot x_{i-1}, i=2,...,5\\} . \u00c9 um subespa\u00e7o do \\mathbb{R}^5 . (Se verdadeiro, apresente uma base). ( ) \u00c9 poss\u00edvel encontrar dois planos do \\mathbb{R}^4 que se interesectem em apenas um ponto. ( Pense em planos com dois par\u00e2metos livres ). ( ) O conjunto de todas as matrizes cujo determinante \u00e9 maior do que zero \u00e9 um subespa\u00e7o das matrizes. ( ) A uni\u00e3o de dois conjuntos LI \u00e9 um conjunto LI, se um deles \u00e9 disjunto do subespa\u00e7o gerado pelo outro. ( ) Existe apenas uma transforma\u00e7\u00e3o linear com T(0,0,1) = (1,2) , T(0,1,0) = (2,3), T(1,0,0) = (4,7) , onde T: \\mathbb{R}^3\\to \\mathbb{R}^2 . Isto \u00e9, n\u00e3o existem x,y,z , tal que T(x,y,z) \\neq T'(x,y,z) com essas propriedades. ( ) Se u, v, w \\in E s\u00e3o colineares, ent\u00e3o Au,Av,Aw tamb\u00e9m s\u00e3o. ( ) Se Aw = Au + Av , ent\u00e3o w = u + v .","title":"Exerc\u00edcios:"},{"location":"alglin/files/monitoria7/#aplicacao-quadrados-magicos","text":"Na monitoria 6, observamos a imagem da Melancolia I, de Albrecht Durer de 1514, em que apareceia um quadrado m\u00e1gico cl\u00e1ssico. Para lembrar: definimos uma matriz n\\times n como quadrado m\u00e1gico quando a soma de cada linha, coluna e diagonal \u00e9 igual. Essa soma se chama peso. Mag_n o conjunto de todos os quadrados m\u00e1gicos de ordem n .O quadrado ser\u00e1 cl\u00e1ssico se usarmos os todos on n\u00fameros entre 1 e n^2 . Considere Mag_2 . Voc\u00ea conseguiria encontrar uma base para esse subespa\u00e7o das matrizes 2 por 2 (provamos que \u00e9 um subespa\u00e7o na semana passada)? E Mag_3 ? Exemplo: X = \\{(1,1,1),(1,2,1)\\}, Y = \\{(1,0,0),(0,0,1)\\}","title":"Aplica\u00e7\u00e3o: Quadrados M\u00e1gicos"},{"location":"alglin/files/monitoria4/monitoria4/","text":"Monitoria 4 T\u00f3picos j\u00e1 estudados Matrizes e suas propriedade Nota\u00e7\u00e3o Opera\u00e7\u00f5es com matrizes e suas propriedades A transposta e suas propriedades: (AB)^T = B^TA^T Sim\u00e9trica e Antissim\u00e9trica: (A + A^T) \u00e9 sim\u00e9trica e (A - A^T) \u00e9 antissim\u00e9trica, \\forall A . A inversa, quando existe, e suas propriedades M\u00e9todo para obten\u00e7\u00e3o da inversa utilizando matrizes elementares Matrizes em outros conjuntos, como Z_5 . Matriz singular: matriz n\u00e3o invert\u00edvel. Decomposi\u00e7\u00e3o LU Sistemas Lineares e sua representa\u00e7\u00e3o matricial Escalonamento: m\u00e9todo de Gauss e m\u00e9todo de Gauss-Jordan A \u00e9 equivalente a B se, e s\u00f3 se, existe uma sequ\u00eancia de opera\u00e7\u00f5es elementares que transformam A em B. Posto de uma matriz e grau de liberdade (Teorema do Posto) Posto (rank em ingl\u00eas) pode ser definido como a dimens\u00e3o do espa\u00e7o linha ou a dimens\u00e3o do espa\u00e7o coluna de uma matriz. Sistema homog\u00eaneo: sempre existe solu\u00e7\u00e3o (trivial) Posto(A) = Posto(A^T) Matrizes Elementares: representantes matriciais das tr\u00eas opera\u00e7\u00f5es elementares, trocar linha, multiplica\u00e7\u00e3o por constante e somar a linha a um m\u00faltiplo de outra linha Matriz Completa: [A|b] Espa\u00e7os Vetorias e vetores Combina\u00e7\u00e3o Linear de vetores O produto escalar -> m\u00f3dulo Proje\u00e7\u00e3o: \\vec{z} = \\frac{\\vec{v}\\cdot\\vec{u}}{\\vec{u}\\cdot\\vec{u}}\\cdot u Desigualdade de Cauchy-Schwarz e cosseno: \\|u\\|\\|v\\| \\geq \\|u\\cdot v\\| Conjuntos geradores: S = \\{v_1,v_2,...,v_k\\} \\subset\\mathbb{R}^n . O conjunto de todas as combina\u00e7\u00f5es dos elementos de S \u00e9 ger(S) . Depen\u00eancia: um conjunto \u00e9 linearmente dependente se um vetor do conjunto pode ser escrito como combina\u00e7\u00e3o linear de outros. Independente, caso contr\u00e1rio. Lembrar que se \\alpha_1\\vec{v_1} + \\alpha_2\\vec{v_2} + ... + \\alpha_k\\vec{v_k} = 0 \\implies \\alpha_1 = \\alpha_2 = ... = \\alpha_k = 0 , o conjuntos dos vetores \u00e9 linearmente independente. Espa\u00e7os vetorias dotam-se de: comutatividade e associatividade da soma, exist\u00eancia do nulo e da identidade, exist\u00eancia do sim\u00e9trico, distribuitividade com real. Subespa\u00e7os vetoriais: tem o vetor nulo e a soma e produto por real s\u00e3o fechados. Subespa\u00e7os associados a matrizes: Espa\u00e7o linha: conjunto gerado por suas linhas: lin(A) = ger(\\{L_1,L_2,...,L_m\\}) Espa\u00e7o coluna: conjunto gerado por suas colunas: col(A) = ger(\\{c_1,c_2,...,c_m\\}) Espa\u00e7o anulado por uma matriz: Tamb\u00e9m chamado de n\u00facleo, \u00e9 o conjunto solu\u00e7\u00e3o do sistema Ax = 0 Bases de um espa\u00e7o vetorial: \u00c9 um conjunto LIque gera um espa\u00e7o vetorial --- Pr\u00f3xima monitoria import numpy as np import scipy as sp import scipy.linalg as splin import sympy as sc Exerc\u00edcios Exemplo 1.4: Espa\u00e7os Vetorias Seja X um conjunto n\u00e3o vazio. O s\u00edmbolo \\mathbb{F}(X;\\mathbb{R}) representa o conjunto das fun\u00e7\u00f5es reais f,g: X \\to \\mathbb{R} . Defino a soma: f + g como (f + g)(x) = f(x) + g(x) e o produto \\alpha\\cdot f como (\\alpha f)(x) = \\alpha\\cdot f(x) Exerc\u00edcio 2.13: Combina\u00e7\u00e3o Linear Mostre que a matriz d = <script type=\"math/tex; mode=display\">\\begin{pmatrix} 4 & -4 \\\\ -6 & 16 \\end{pmatrix} pode ser escrita como combina\u00e7\u00e3o linear das matrizes: a = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, b = \\begin{pmatrix} -1 & 2 \\\\ 3 & -4\\end{pmatrix}, c =\\begin{pmatrix} 1 & -2 \\\\ -3 & 4 \\end{pmatrix} Exemplo: Subespa\u00e7os Seja P_4 o espa\u00e7o vetorial de todos os polin\u00f4mios de grau 4 ou menos com coeficientes reais. A soma e o produto por real s\u00e3o definidos naturalmente. S_5 \u00e9 um subespa\u00e7o? S_5 = \\{f(x) \\in P_4 \\| f(1)~ is~ a ~rational~ number\\} Quest\u00e3o Teste 2: Opera\u00e7\u00f5es com matrizes A matriz A = \\frac{1}{\\sqrt{2}} <script type=\"math/tex; mode=display\">\\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix} . Quest\u00e3o: \\exists n \\in \\mathbb{N} , tal que A^n = A ? Obs.: Observe que \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2} , e se eu tratar \\theta = \\frac{\\pi}{4} , temos que A \u00e9 matriz de rota\u00e7\u00e3o. Logo, basta encontrar n tal que n\\cdot\\frac{\\pi}{4} = k\\cdot2\\cdot\\pi \\to n = 8\\cdot k \\to n = 8 \u00e9 solu\u00e7\u00e3o, logo A^8 = I \\to A^9 = A Quest\u00e3o 2.37 Dado X \\subset E , seja Y o conjunto obtido de X substituindo um dos seus elementos v por v + \\alpha u , onde u\\in X e \\alpha \\in \\mathbb{R} . Prove que X e Y geram o mesmo subespa\u00e7o vetorial de E . Conclua, ent\u00e3o que \\{v_1,...,v_k\\} \\subset E e \\{v_1, v_2 - v_1, ..., v_k - v_1\\} \\subset E geram o mesmo subespa\u00e7o vetorial de E . Algu\u00e9m percebe a implica\u00e7\u00e3o pr\u00e1tica dessa quest\u00e3o? Lembre que S(Y) \u00e9 o subespa\u00e7o gerado por Y. Uma ajuda para desenvolver essa quest\u00e3o \u00e9 reconhecer que X \\subset S(Y) \\implies S(X) \\subset S(Y) . Voc\u00ea consegue demonstrar? Conjuntos Geradores u_1 = (1,1,2,3) u_2 = (2,-1,1,2) \\pi = ger(\\{u_1, u_2\\}) w = (-2,7,5,6) \\in \\pi Queremos saber se, \\exists ~\\alpha, \\beta , tal que: \\alpha \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 3 \\end{pmatrix} + \\beta \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 7 \\\\ 5 \\\\ 6 \\end{pmatrix} Teste 1 2018 Exerc\u00edcio 1: Escreva a seguinte matriz como a soma de uma matriz sim\u00e9trica e uma antissim\u00e9trica: A = <script type=\"math/tex; mode=display\">\\begin{pmatrix} 4 & 6 & 3\\\\ -2 & 0 & 5 \\\\ 5 & -1 & 2 \\end{pmatrix} Resolu\u00e7\u00e3o A = \\frac{1}{2}((A + A^T) + (A - A^T)) Ora, mas A + A^T e A - A^T s\u00e3o sim\u00e9trica e antissim\u00e9trica, respectivamente. Logo: A = np.array([[4,6,3],[-2,0,5],[5,-1,2]]) A = sc.Matrix(A) A_T = A.T 1/2*(A + A_T) \\left[\\begin{matrix}4.0 & 2.0 & 4.0\\\\2.0 & 0 & 2.0\\\\4.0 & 2.0 & 2.0\\end{matrix}\\right] 1/2*(A - A_T) \\left[\\begin{matrix}0 & 4.0 & -1.0\\\\-4.0 & 0 & 3.0\\\\1.0 & -3.0 & 0\\end{matrix}\\right] Exerc\u00edcio 2: Encontre a inversa da seguinte matriz e depois resolva AX = B : A = \\begin{pmatrix} 4 & 6 & 3\\\\ -2 & 0 & 5 \\\\ 5 & -1 & 2 \\end{pmatrix} \\\\ B = \\begin{pmatrix} 7 & 1 & 4\\\\ -5 & 2 & 4 \\\\ 9 & 3 & -1 \\end{pmatrix} Resolu\u00e7\u00e3o A = np.array([[4,6,3],[-2,0 , 5], [5, -1, 2]]) B = np.array([[7,1,4],[-5,2,5],[9,3,1]]) A_inv = np.linalg.inv(A) X = np.matmul(A_inv,B) sc.Matrix(A_inv) \\left[\\begin{matrix}0.025 & -0.075 & 0.15\\\\0.145 & -0.035 & -0.13\\\\0.01 & 0.17 & 0.06\\end{matrix}\\right] sc.Matrix(X) \\left[\\begin{matrix}1.9 & 0.325 & -0.125\\\\0.0199999999999999 & -0.315 & 0.275\\\\-0.24 & 0.53 & 0.95\\end{matrix}\\right] Exerc\u00edcio 3: Resolva o sistema em \\mathbb{Z}_5 : \\begin{pmatrix} 4 & 1 & 3\\\\ 3 & 0 & 2 \\\\ 0 & 4 & 2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 4 \\end{pmatrix} Exerc\u00edcio 4: Uma rede consiste em um n\u00famero finito de n\u00f3s conectados por arestas . Cada aresta \u00e9 rotulada com um fluxo , que relaciona uma dire\u00e7\u00e3o e uma quantidade. A regra fundamental \u00e9 a conserva\u00e7\u00e3o de fluxo , isto \u00e9, em cada n\u00f3, o fluxo de entrada \u00e9 igual ao fluxo de sa\u00edda. Considere a seguinte rede: Encontre os intervalos de f_1, f_2, f_3, f_4 Exerc\u00edcio 5: Fa\u00e7a a fatora\u00e7\u00e3o LU da matriz \\begin{pmatrix} 1 & 2 & 4 \\\\ 3 & 8 & 14 \\\\ 2 & 6 & 13 \\end{pmatrix} Resolu\u00e7\u00e3o: A = sp.array([[1,2,4], [3,8,14], [2,6,13]]) P, L, U = sp.linalg.lu(A) print(\"L:\") sc.Matrix(L) L: \\left[\\begin{matrix}1.0 & 0.0 & 0.0\\\\0.666666666666667 & 1.0 & 0.0\\\\0.333333333333333 & -0.999999999999999 & 1.0\\end{matrix}\\right] print(\"U:\") sc.Matrix(U) U: \\left[\\begin{matrix}3.0 & 8.0 & 14.0\\\\0.0 & 0.666666666666667 & 3.66666666666667\\\\0.0 & 0.0 & 3.0\\end{matrix}\\right] Curiosidades O conjunto H dos pontos x = (x_1,...,x_n) \\in \\mathbb{R^n} , tais que a_1x_1 + ... + a_nx+n = b \u00e9 uma variedade afim . Se a_i n\u00e3o s\u00e3o todos nulos, H \u00e9 um hiperplano. Por que \u00e9 necess\u00e1rio que 0 \\in F , sendo F \\subset E um subespa\u00e7o? Essa condi\u00e7\u00e3o pode ser alterada po outra? Sugest\u00e3o: Demonstrar que essa condi\u00e7\u00e3o pode ser alterada por F \\neq \\emptyset Defini\u00e7\u00e3o de segmento de reta: [u,v] = \\{(1 - t)\\cdot u + t\\cdot v ~; ~ 0 \\leq t \\leq 1\\} , sendo u, v \\in E , espa\u00e7o vetorial.","title":"Monitoria 4"},{"location":"alglin/files/monitoria4/monitoria4/#monitoria-4","text":"","title":"Monitoria 4"},{"location":"alglin/files/monitoria4/monitoria4/#topicos-ja-estudados","text":"Matrizes e suas propriedade Nota\u00e7\u00e3o Opera\u00e7\u00f5es com matrizes e suas propriedades A transposta e suas propriedades: (AB)^T = B^TA^T Sim\u00e9trica e Antissim\u00e9trica: (A + A^T) \u00e9 sim\u00e9trica e (A - A^T) \u00e9 antissim\u00e9trica, \\forall A . A inversa, quando existe, e suas propriedades M\u00e9todo para obten\u00e7\u00e3o da inversa utilizando matrizes elementares Matrizes em outros conjuntos, como Z_5 . Matriz singular: matriz n\u00e3o invert\u00edvel. Decomposi\u00e7\u00e3o LU Sistemas Lineares e sua representa\u00e7\u00e3o matricial Escalonamento: m\u00e9todo de Gauss e m\u00e9todo de Gauss-Jordan A \u00e9 equivalente a B se, e s\u00f3 se, existe uma sequ\u00eancia de opera\u00e7\u00f5es elementares que transformam A em B. Posto de uma matriz e grau de liberdade (Teorema do Posto) Posto (rank em ingl\u00eas) pode ser definido como a dimens\u00e3o do espa\u00e7o linha ou a dimens\u00e3o do espa\u00e7o coluna de uma matriz. Sistema homog\u00eaneo: sempre existe solu\u00e7\u00e3o (trivial) Posto(A) = Posto(A^T) Matrizes Elementares: representantes matriciais das tr\u00eas opera\u00e7\u00f5es elementares, trocar linha, multiplica\u00e7\u00e3o por constante e somar a linha a um m\u00faltiplo de outra linha Matriz Completa: [A|b] Espa\u00e7os Vetorias e vetores Combina\u00e7\u00e3o Linear de vetores O produto escalar -> m\u00f3dulo Proje\u00e7\u00e3o: \\vec{z} = \\frac{\\vec{v}\\cdot\\vec{u}}{\\vec{u}\\cdot\\vec{u}}\\cdot u Desigualdade de Cauchy-Schwarz e cosseno: \\|u\\|\\|v\\| \\geq \\|u\\cdot v\\| Conjuntos geradores: S = \\{v_1,v_2,...,v_k\\} \\subset\\mathbb{R}^n . O conjunto de todas as combina\u00e7\u00f5es dos elementos de S \u00e9 ger(S) . Depen\u00eancia: um conjunto \u00e9 linearmente dependente se um vetor do conjunto pode ser escrito como combina\u00e7\u00e3o linear de outros. Independente, caso contr\u00e1rio. Lembrar que se \\alpha_1\\vec{v_1} + \\alpha_2\\vec{v_2} + ... + \\alpha_k\\vec{v_k} = 0 \\implies \\alpha_1 = \\alpha_2 = ... = \\alpha_k = 0 , o conjuntos dos vetores \u00e9 linearmente independente. Espa\u00e7os vetorias dotam-se de: comutatividade e associatividade da soma, exist\u00eancia do nulo e da identidade, exist\u00eancia do sim\u00e9trico, distribuitividade com real. Subespa\u00e7os vetoriais: tem o vetor nulo e a soma e produto por real s\u00e3o fechados. Subespa\u00e7os associados a matrizes: Espa\u00e7o linha: conjunto gerado por suas linhas: lin(A) = ger(\\{L_1,L_2,...,L_m\\}) Espa\u00e7o coluna: conjunto gerado por suas colunas: col(A) = ger(\\{c_1,c_2,...,c_m\\}) Espa\u00e7o anulado por uma matriz: Tamb\u00e9m chamado de n\u00facleo, \u00e9 o conjunto solu\u00e7\u00e3o do sistema Ax = 0 Bases de um espa\u00e7o vetorial: \u00c9 um conjunto LIque gera um espa\u00e7o vetorial --- Pr\u00f3xima monitoria import numpy as np import scipy as sp import scipy.linalg as splin import sympy as sc","title":"T\u00f3picos j\u00e1 estudados"},{"location":"alglin/files/monitoria4/monitoria4/#exercicios","text":"","title":"Exerc\u00edcios"},{"location":"alglin/files/monitoria4/monitoria4/#exemplo-14-espacos-vetorias","text":"Seja X um conjunto n\u00e3o vazio. O s\u00edmbolo \\mathbb{F}(X;\\mathbb{R}) representa o conjunto das fun\u00e7\u00f5es reais f,g: X \\to \\mathbb{R} . Defino a soma: f + g como (f + g)(x) = f(x) + g(x) e o produto \\alpha\\cdot f como (\\alpha f)(x) = \\alpha\\cdot f(x)","title":"Exemplo 1.4: Espa\u00e7os Vetorias"},{"location":"alglin/files/monitoria4/monitoria4/#exercicio-213-combinacao-linear","text":"Mostre que a matriz d = <script type=\"math/tex; mode=display\">\\begin{pmatrix} 4 & -4 \\\\ -6 & 16 \\end{pmatrix} pode ser escrita como combina\u00e7\u00e3o linear das matrizes: a = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, b = \\begin{pmatrix} -1 & 2 \\\\ 3 & -4\\end{pmatrix}, c =\\begin{pmatrix} 1 & -2 \\\\ -3 & 4 \\end{pmatrix}","title":"Exerc\u00edcio 2.13: Combina\u00e7\u00e3o Linear"},{"location":"alglin/files/monitoria4/monitoria4/#exemplo-subespacos","text":"Seja P_4 o espa\u00e7o vetorial de todos os polin\u00f4mios de grau 4 ou menos com coeficientes reais. A soma e o produto por real s\u00e3o definidos naturalmente. S_5 \u00e9 um subespa\u00e7o? S_5 = \\{f(x) \\in P_4 \\| f(1)~ is~ a ~rational~ number\\}","title":"Exemplo: Subespa\u00e7os"},{"location":"alglin/files/monitoria4/monitoria4/#questao-teste-2-operacoes-com-matrizes","text":"A matriz A = \\frac{1}{\\sqrt{2}} <script type=\"math/tex; mode=display\">\\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix} . Quest\u00e3o: \\exists n \\in \\mathbb{N} , tal que A^n = A ? Obs.: Observe que \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2} , e se eu tratar \\theta = \\frac{\\pi}{4} , temos que A \u00e9 matriz de rota\u00e7\u00e3o. Logo, basta encontrar n tal que n\\cdot\\frac{\\pi}{4} = k\\cdot2\\cdot\\pi \\to n = 8\\cdot k \\to n = 8 \u00e9 solu\u00e7\u00e3o, logo A^8 = I \\to A^9 = A","title":"Quest\u00e3o Teste 2: Opera\u00e7\u00f5es com matrizes"},{"location":"alglin/files/monitoria4/monitoria4/#questao-237","text":"Dado X \\subset E , seja Y o conjunto obtido de X substituindo um dos seus elementos v por v + \\alpha u , onde u\\in X e \\alpha \\in \\mathbb{R} . Prove que X e Y geram o mesmo subespa\u00e7o vetorial de E . Conclua, ent\u00e3o que \\{v_1,...,v_k\\} \\subset E e \\{v_1, v_2 - v_1, ..., v_k - v_1\\} \\subset E geram o mesmo subespa\u00e7o vetorial de E . Algu\u00e9m percebe a implica\u00e7\u00e3o pr\u00e1tica dessa quest\u00e3o? Lembre que S(Y) \u00e9 o subespa\u00e7o gerado por Y. Uma ajuda para desenvolver essa quest\u00e3o \u00e9 reconhecer que X \\subset S(Y) \\implies S(X) \\subset S(Y) . Voc\u00ea consegue demonstrar?","title":"Quest\u00e3o 2.37"},{"location":"alglin/files/monitoria4/monitoria4/#conjuntos-geradores","text":"u_1 = (1,1,2,3) u_2 = (2,-1,1,2) \\pi = ger(\\{u_1, u_2\\}) w = (-2,7,5,6) \\in \\pi Queremos saber se, \\exists ~\\alpha, \\beta , tal que: \\alpha \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 3 \\end{pmatrix} + \\beta \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 7 \\\\ 5 \\\\ 6 \\end{pmatrix}","title":"Conjuntos Geradores"},{"location":"alglin/files/monitoria4/monitoria4/#teste-1-2018","text":"","title":"Teste 1 2018"},{"location":"alglin/files/monitoria4/monitoria4/#exercicio-1","text":"Escreva a seguinte matriz como a soma de uma matriz sim\u00e9trica e uma antissim\u00e9trica: A = <script type=\"math/tex; mode=display\">\\begin{pmatrix} 4 & 6 & 3\\\\ -2 & 0 & 5 \\\\ 5 & -1 & 2 \\end{pmatrix}","title":"Exerc\u00edcio 1:"},{"location":"alglin/files/monitoria4/monitoria4/#resolucao","text":"A = \\frac{1}{2}((A + A^T) + (A - A^T)) Ora, mas A + A^T e A - A^T s\u00e3o sim\u00e9trica e antissim\u00e9trica, respectivamente. Logo: A = np.array([[4,6,3],[-2,0,5],[5,-1,2]]) A = sc.Matrix(A) A_T = A.T 1/2*(A + A_T) \\left[\\begin{matrix}4.0 & 2.0 & 4.0\\\\2.0 & 0 & 2.0\\\\4.0 & 2.0 & 2.0\\end{matrix}\\right] 1/2*(A - A_T) \\left[\\begin{matrix}0 & 4.0 & -1.0\\\\-4.0 & 0 & 3.0\\\\1.0 & -3.0 & 0\\end{matrix}\\right]","title":"Resolu\u00e7\u00e3o"},{"location":"alglin/files/monitoria4/monitoria4/#exercicio-2","text":"Encontre a inversa da seguinte matriz e depois resolva AX = B : A = \\begin{pmatrix} 4 & 6 & 3\\\\ -2 & 0 & 5 \\\\ 5 & -1 & 2 \\end{pmatrix} \\\\ B = \\begin{pmatrix} 7 & 1 & 4\\\\ -5 & 2 & 4 \\\\ 9 & 3 & -1 \\end{pmatrix}","title":"Exerc\u00edcio 2:"},{"location":"alglin/files/monitoria4/monitoria4/#resolucao_1","text":"A = np.array([[4,6,3],[-2,0 , 5], [5, -1, 2]]) B = np.array([[7,1,4],[-5,2,5],[9,3,1]]) A_inv = np.linalg.inv(A) X = np.matmul(A_inv,B) sc.Matrix(A_inv) \\left[\\begin{matrix}0.025 & -0.075 & 0.15\\\\0.145 & -0.035 & -0.13\\\\0.01 & 0.17 & 0.06\\end{matrix}\\right] sc.Matrix(X) \\left[\\begin{matrix}1.9 & 0.325 & -0.125\\\\0.0199999999999999 & -0.315 & 0.275\\\\-0.24 & 0.53 & 0.95\\end{matrix}\\right]","title":"Resolu\u00e7\u00e3o"},{"location":"alglin/files/monitoria4/monitoria4/#exercicio-3","text":"Resolva o sistema em \\mathbb{Z}_5 : \\begin{pmatrix} 4 & 1 & 3\\\\ 3 & 0 & 2 \\\\ 0 & 4 & 2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 4 \\end{pmatrix}","title":"Exerc\u00edcio 3:"},{"location":"alglin/files/monitoria4/monitoria4/#exercicio-4","text":"Uma rede consiste em um n\u00famero finito de n\u00f3s conectados por arestas . Cada aresta \u00e9 rotulada com um fluxo , que relaciona uma dire\u00e7\u00e3o e uma quantidade. A regra fundamental \u00e9 a conserva\u00e7\u00e3o de fluxo , isto \u00e9, em cada n\u00f3, o fluxo de entrada \u00e9 igual ao fluxo de sa\u00edda. Considere a seguinte rede: Encontre os intervalos de f_1, f_2, f_3, f_4","title":"Exerc\u00edcio 4:"},{"location":"alglin/files/monitoria4/monitoria4/#exercicio-5","text":"Fa\u00e7a a fatora\u00e7\u00e3o LU da matriz \\begin{pmatrix} 1 & 2 & 4 \\\\ 3 & 8 & 14 \\\\ 2 & 6 & 13 \\end{pmatrix}","title":"Exerc\u00edcio 5:"},{"location":"alglin/files/monitoria4/monitoria4/#resolucao_2","text":"A = sp.array([[1,2,4], [3,8,14], [2,6,13]]) P, L, U = sp.linalg.lu(A) print(\"L:\") sc.Matrix(L) L: \\left[\\begin{matrix}1.0 & 0.0 & 0.0\\\\0.666666666666667 & 1.0 & 0.0\\\\0.333333333333333 & -0.999999999999999 & 1.0\\end{matrix}\\right] print(\"U:\") sc.Matrix(U) U: \\left[\\begin{matrix}3.0 & 8.0 & 14.0\\\\0.0 & 0.666666666666667 & 3.66666666666667\\\\0.0 & 0.0 & 3.0\\end{matrix}\\right]","title":"Resolu\u00e7\u00e3o:"},{"location":"alglin/files/monitoria4/monitoria4/#curiosidades","text":"O conjunto H dos pontos x = (x_1,...,x_n) \\in \\mathbb{R^n} , tais que a_1x_1 + ... + a_nx+n = b \u00e9 uma variedade afim . Se a_i n\u00e3o s\u00e3o todos nulos, H \u00e9 um hiperplano. Por que \u00e9 necess\u00e1rio que 0 \\in F , sendo F \\subset E um subespa\u00e7o? Essa condi\u00e7\u00e3o pode ser alterada po outra? Sugest\u00e3o: Demonstrar que essa condi\u00e7\u00e3o pode ser alterada por F \\neq \\emptyset Defini\u00e7\u00e3o de segmento de reta: [u,v] = \\{(1 - t)\\cdot u + t\\cdot v ~; ~ 0 \\leq t \\leq 1\\} , sendo u, v \\in E , espa\u00e7o vetorial.","title":"Curiosidades"},{"location":"analisenum/computing/","text":"Aritm\u00e9tica do computador No computador, express\u00f5es como (\\sqrt{3})^2 = 3 n\u00e3o s\u00e3o verdadeiras. Em Python, >>> (3**(1/2))**2 2.9999999999999996 Erros de c\u00e1lculo realizados pela m\u00e1quina s\u00e3o chamados de erros de arredondamento (round-off error) . Isso acontece porque n\u00fameros reais s\u00e3o representados de forma finita, e n\u00e3o cobrem todos os n\u00fameros reais de fato. N\u00fameros bin\u00e1rios Em 1985, IEEE (Institute for Electrical and Electronic Engineers) publicou um relat\u00f3rio nomeado Binary Floating Point Arithmetic Standard 754\u20131985 que estabelece padr\u00f5es para pontos flutuantes, algoritmos de arredondamento de opera\u00e7\u00f5es aritm\u00e9ticas e para lidar com exce\u00e7\u00f5es. A representa\u00e7\u00e3o usual dos n\u00fameros reais \u00e9 64-bit ( double ). O primeiro bit \u00e9 o sinal do n\u00famero. Depois vem a caracter\u00edstica que \u00e9 representa o expoente (11-bit) e por fim a fra\u00e7\u00e3o bin\u00e1ria chamada de mantissa (52-bit). Os n\u00fameros s\u00e3o armazenados na base 2. A representa\u00e7\u00e3o ponto flutuante dos n\u00fameros \u00e9, portanto, (-1)^s2^{c-1023}(1 + f), em que s \u00e9 o sinal (0 para positivo e 1 para negativo), c \u00e9 o expoente (tiramos o vi\u00e9s 1023 para assegurar que o intervalo de c fique entre -1023 e 1024) e f \u00e9 a mantissa. Exemplo: s c f 0 10000000011 1011100100010000000000000000000000000000000000000000 Nesse caso o sinal \u00e9 0, c = 1\\cdot 2^{10} + 0\\cdot 2^9 + \\dots + 1\\cdot 2 + 1 = 1027 , e f = 1\\cdot 2^{-1} + 1\\cdot 2^{-3} + \\dots 1\\cdot 2^{-12} = 0.722900390625 e, portanto, (-1)^s2^{c-1023}(1 + f) = 2^4(1 + 0.722900390625) = 27.56640625. O menor n\u00famero positivo que pode ser representado \u00e9, portanto, 2^{0-1022}(1 + 0) \\approx 10^{-307} e o maior 2^{2047 - 1023}(1 + (1 -2^{-52})) \\approx 10^{309} Quando s = c = f = 0 , representamos o 0. Note que quando um n\u00famero tem representa\u00e7\u00e3o infinita, ele deve ser, de alguma forma, colocado em forma finita. Duas maneiras comuns de se fazer isso \u00e9 chopping , quando se simplesmente corta os d\u00edgitos a mais, ou arredondamento . A representa\u00e7\u00e3o de x em ponto flutuante \u00e9 denotada por fl(x) . O infinito \u00e9 representado quando todos os valores do expoente s\u00e3o 1. OS valores de NaN (QNaN e SNaN) variam a mantissa e mant\u00e9m os expoentes todos 1. Um ponto interessante \u00e9 a densidade dos n\u00fameros ponto flutuante. Quanto maior fica o n\u00famero, menor a densidade. Isso acontece porque o tamanho da mantissa \u00e9 fixo, ent\u00e3o sempre v\u00e3o existir 2^{52} n\u00fameros representados para cada expoente. Assim o intervalo [2^{k}, 2^{k+1}) tem n\u00famero fixo de n\u00fameros que independe de k , apesar da dist\u00e2ncia crescer. Por esse motivo tamb\u00e9m, 2^{-52} \u00e9 o epsilon no sistema 64-bit. Representa\u00e7\u00e3o ponto flutuante Vamos mostrar um exemplo de como transformar um n\u00famero real 10.1 em um ponto flutuante. Separar parte inteira da parte decimal. Transformar a parte inteira e a parte fracion\u00e1ria em sua representa\u00e7\u00e3o bin\u00e1ria . Deixar o n\u00famero no formato 1.x_1x_2\\dots \\cdot 2^{\\text{expoente}} . Adicionar o vi\u00e9s 1023 ao expoente. Converter o expoente em bin\u00e1rio. Casa 53: se for 1, soma-se a casa 52. Para conferir, voc\u00ea pode verificar a representa\u00e7\u00e3o utilizando alguma linguagem de programa\u00e7\u00e3o. Por exemplo, em Python, >>> import struct def binary(num): s = struct.pack('!d', num) # packs the num to the decimal format s = struct.unpack('!Q', s)[0] # unpacks in long integer format. s = bin(s) # converts to binary form. s = s[2:].zfill(64) # fills with zeros until reaches length 64. return s[0] + ' ' + s[1:12] + ' ' + s[12:] >>> binary(1/2) '0 01111111110 0000000000000000000000000000000000000000000000000000' >>> binary(1/7) '0 01111111100 0010010010010010010010010010010010010010010010010010' Mensura\u00e7\u00e3o de erros Seja \\bar{x} uma aproxima\u00e7\u00e3o para x . Dizemos que o erro absoluto \u00e9 |x - \\bar{x}| e o erro relativo \u00e9 |x - \\bar{x}|/|x| quando x \\neq 0 . Dizemos que \\bar{x} aproxima x em t d\u00edgitos significativos se t = \\max\\left\\{s \\ge 0 \\, \\bigg| \\, \\frac{|x - \\bar{x}|}{|x|} \\le 5 \\cdot 10^{-s} \\right\\} Algumas dicas com Python Para printar todos os n\u00fameros da representa\u00e7\u00e3o num\u00e9rica (considerando arredondamento) em Python, use >>> import numpy >>> format(numpy.pi, .20) '3.141592653589793116' Para verificar o valor exatamente armazenado para o n\u00famero em quest\u00e3o: >>> from decimal import Decimal >>> Decimal.from_float(0.1) Decimal('0.1000000000000000055511151231257827021181583404541015625') Para mais detalhes, consulte o material adicional","title":"Aritm\u00e9tica do computador"},{"location":"analisenum/computing/#aritmetica-do-computador","text":"No computador, express\u00f5es como (\\sqrt{3})^2 = 3 n\u00e3o s\u00e3o verdadeiras. Em Python, >>> (3**(1/2))**2 2.9999999999999996 Erros de c\u00e1lculo realizados pela m\u00e1quina s\u00e3o chamados de erros de arredondamento (round-off error) . Isso acontece porque n\u00fameros reais s\u00e3o representados de forma finita, e n\u00e3o cobrem todos os n\u00fameros reais de fato.","title":"Aritm\u00e9tica do computador"},{"location":"analisenum/computing/#numeros-binarios","text":"Em 1985, IEEE (Institute for Electrical and Electronic Engineers) publicou um relat\u00f3rio nomeado Binary Floating Point Arithmetic Standard 754\u20131985 que estabelece padr\u00f5es para pontos flutuantes, algoritmos de arredondamento de opera\u00e7\u00f5es aritm\u00e9ticas e para lidar com exce\u00e7\u00f5es. A representa\u00e7\u00e3o usual dos n\u00fameros reais \u00e9 64-bit ( double ). O primeiro bit \u00e9 o sinal do n\u00famero. Depois vem a caracter\u00edstica que \u00e9 representa o expoente (11-bit) e por fim a fra\u00e7\u00e3o bin\u00e1ria chamada de mantissa (52-bit). Os n\u00fameros s\u00e3o armazenados na base 2. A representa\u00e7\u00e3o ponto flutuante dos n\u00fameros \u00e9, portanto, (-1)^s2^{c-1023}(1 + f), em que s \u00e9 o sinal (0 para positivo e 1 para negativo), c \u00e9 o expoente (tiramos o vi\u00e9s 1023 para assegurar que o intervalo de c fique entre -1023 e 1024) e f \u00e9 a mantissa. Exemplo: s c f 0 10000000011 1011100100010000000000000000000000000000000000000000 Nesse caso o sinal \u00e9 0, c = 1\\cdot 2^{10} + 0\\cdot 2^9 + \\dots + 1\\cdot 2 + 1 = 1027 , e f = 1\\cdot 2^{-1} + 1\\cdot 2^{-3} + \\dots 1\\cdot 2^{-12} = 0.722900390625 e, portanto, (-1)^s2^{c-1023}(1 + f) = 2^4(1 + 0.722900390625) = 27.56640625. O menor n\u00famero positivo que pode ser representado \u00e9, portanto, 2^{0-1022}(1 + 0) \\approx 10^{-307} e o maior 2^{2047 - 1023}(1 + (1 -2^{-52})) \\approx 10^{309} Quando s = c = f = 0 , representamos o 0. Note que quando um n\u00famero tem representa\u00e7\u00e3o infinita, ele deve ser, de alguma forma, colocado em forma finita. Duas maneiras comuns de se fazer isso \u00e9 chopping , quando se simplesmente corta os d\u00edgitos a mais, ou arredondamento . A representa\u00e7\u00e3o de x em ponto flutuante \u00e9 denotada por fl(x) . O infinito \u00e9 representado quando todos os valores do expoente s\u00e3o 1. OS valores de NaN (QNaN e SNaN) variam a mantissa e mant\u00e9m os expoentes todos 1. Um ponto interessante \u00e9 a densidade dos n\u00fameros ponto flutuante. Quanto maior fica o n\u00famero, menor a densidade. Isso acontece porque o tamanho da mantissa \u00e9 fixo, ent\u00e3o sempre v\u00e3o existir 2^{52} n\u00fameros representados para cada expoente. Assim o intervalo [2^{k}, 2^{k+1}) tem n\u00famero fixo de n\u00fameros que independe de k , apesar da dist\u00e2ncia crescer. Por esse motivo tamb\u00e9m, 2^{-52} \u00e9 o epsilon no sistema 64-bit.","title":"N\u00fameros bin\u00e1rios"},{"location":"analisenum/computing/#representacao-ponto-flutuante","text":"Vamos mostrar um exemplo de como transformar um n\u00famero real 10.1 em um ponto flutuante. Separar parte inteira da parte decimal. Transformar a parte inteira e a parte fracion\u00e1ria em sua representa\u00e7\u00e3o bin\u00e1ria . Deixar o n\u00famero no formato 1.x_1x_2\\dots \\cdot 2^{\\text{expoente}} . Adicionar o vi\u00e9s 1023 ao expoente. Converter o expoente em bin\u00e1rio. Casa 53: se for 1, soma-se a casa 52. Para conferir, voc\u00ea pode verificar a representa\u00e7\u00e3o utilizando alguma linguagem de programa\u00e7\u00e3o. Por exemplo, em Python, >>> import struct def binary(num): s = struct.pack('!d', num) # packs the num to the decimal format s = struct.unpack('!Q', s)[0] # unpacks in long integer format. s = bin(s) # converts to binary form. s = s[2:].zfill(64) # fills with zeros until reaches length 64. return s[0] + ' ' + s[1:12] + ' ' + s[12:] >>> binary(1/2) '0 01111111110 0000000000000000000000000000000000000000000000000000' >>> binary(1/7) '0 01111111100 0010010010010010010010010010010010010010010010010010'","title":"Representa\u00e7\u00e3o ponto flutuante"},{"location":"analisenum/computing/#mensuracao-de-erros","text":"Seja \\bar{x} uma aproxima\u00e7\u00e3o para x . Dizemos que o erro absoluto \u00e9 |x - \\bar{x}| e o erro relativo \u00e9 |x - \\bar{x}|/|x| quando x \\neq 0 . Dizemos que \\bar{x} aproxima x em t d\u00edgitos significativos se t = \\max\\left\\{s \\ge 0 \\, \\bigg| \\, \\frac{|x - \\bar{x}|}{|x|} \\le 5 \\cdot 10^{-s} \\right\\}","title":"Mensura\u00e7\u00e3o de erros"},{"location":"analisenum/computing/#algumas-dicas-com-python","text":"Para printar todos os n\u00fameros da representa\u00e7\u00e3o num\u00e9rica (considerando arredondamento) em Python, use >>> import numpy >>> format(numpy.pi, .20) '3.141592653589793116' Para verificar o valor exatamente armazenado para o n\u00famero em quest\u00e3o: >>> from decimal import Decimal >>> Decimal.from_float(0.1) Decimal('0.1000000000000000055511151231257827021181583404541015625') Para mais detalhes, consulte o material adicional","title":"Algumas dicas com Python"},{"location":"analisenum/edo/","text":"M\u00e9todos num\u00e9ricos para solu\u00e7\u00e3o de EDOs Problemas de valor inicial para equa\u00e7\u00f5es diferenciais ordin\u00e1rias (EDOs) ocorrem em quase todas as ci\u00eancias, tal como a mec\u00e2nica de Newton, o movimento das part\u00edculas, evolu\u00e7\u00f5es de doen\u00e7as, mec\u00e2nica qu\u00e2ntica (aqui uma equa\u00e7\u00e3o parcial \u00e9 am ias conhecida), entre outros. No c\u00e1lculo das varia\u00e7\u00f5es , essa teoria tamb\u00e9m \u00e9 muito importante, como vimos no curso de EDP. Mais motiva\u00e7\u00f5es dessa \u00e1rea s\u00e3o deixados para outros materiais, como esse aqui , esse aqui , ou algum livro de EDOs recomendado pelo curso. O problema de valor inicial padr\u00e3o envolve um sistema de equa\u00e7\u00f5es diferenciais de primeira ordem, \\frac{dy_i}{dx} = f_i(x,y_1, \\dots, y_n), \\quad i = 1, \\dots, n e suas condi\u00e7\u00f5es iniciais, y_i(a) = y_i^{(0)}, \\quad i = 1, \\dots n. Uma equa\u00e7\u00e3o de segunda ordem, tal como y'' + y = a pode ser transformada em um sistema de primeira ordem introduzindo a vari\u00e1vel z = y' . A exist\u00eancia de solu\u00e7\u00f5es em um sistema de EDOs \u00e9 garantida quando a fun\u00e7\u00e3o \u00e9 cont\u00ednua, enquanto a unicidade \u00e9 garantida quando as \u00faltimas n-1 componentes s\u00e3o Lipshitz Cont\u00ednuas. Mais detalhes podem ser consultados aqui para um resumo, ou algum texto-livro de EDOs. M\u00e9todos num\u00e9ricos Suponha que queremos resolver \\frac{dy}{dx} = f(x,y), y \\in \\mathbb{R}^n, x \\in \\mathbb{R}, y(0) = y_0. Existem basicamente dois tipos de m\u00e9todos para solu\u00e7\u00e3o num\u00e9rica de EDOs, os m\u00e9todos por aproxima\u00e7\u00e3o anal\u00edtica que buscam aproximar a fun\u00e7\u00e3o y(x) para qualquer ponto x \\in [a,b] , tomando a forma de uma expans\u00e3o de s\u00e9rie truncada; e os m\u00e9todos vari\u00e1vel-discreta que procuram estimar a fun\u00e7\u00e3o em pontos y(x_i) para x_i \\in \\{x_1, \\dots, x_p\\} \\subset [a,b] . M\u00e9todos de um passo Para x \\in [a,b] , definimos esses tipos de m\u00e9todo por y_{\\mathrm{next}} = y + h\\Phi(x, y, h), h > 0, em que \\Phi : [a,b] \\times \\mathbb{R}^n \\times \\mathbb{R}_{+} \\to \\mathbb{R}^n com uma incrementa\u00e7\u00e3o aproximada por passo. Chamamos de solu\u00e7\u00e3o refer\u00eancia a fun\u00e7\u00e3o u(t) tal que \\frac{du}{dt} = f(t,u), x \\le t \\le x + h, u(x) = y. Temos que y_{\\mathrm{next}} procura estimar u(x+h) . Para avaliar essa aproxima\u00e7\u00e3o, definimos o erro de truncamento dado pela express\u00e3o T(x,y,h) = \\frac{1}{h}[y_{\\mathrm{next}} - u(x+h)]. Dizemos que o m\u00e9todo, dado pela descri\u00e7\u00e3o de \\Phi , \u00e9 consistente se T(x,y,h) \\to 0, \\; \\text{ quando } h \\to 0. Tamb\u00e9m dizemos que o m\u00e9todo \u00e9 de ordem p se para alguma norma (n\u00e3o importa qual nesse contexto pela equival\u00eancia das normas) e para algum C \\in \\mathbb{R} , vale que ||T(x,y,h)|| \\le Ch^p. M\u00e9todo de Euler No m\u00e9todo de Euler, temos que \\Phi(x,y,h) = f(x,y) . Note que o passo ser\u00e1 a expans\u00e3o de Taylor de primeira ordem: y_{\\mathrm{next}} = y + hf(x,y). O erro de truncamento \u00e9 T(x,y,h) = \\frac{1}{h}(y + hf(x,y) - u(x+h)) = f(x,y) - \\frac{1}{h}(u(x+h) - u(x)). que \u00e9 claramente consistente, porque quando h tende a 0, temos que a express\u00e3o de u converge para u'(x) = f(x,y) . Al\u00e9m disso, note que T(x,y,h) = u'(x) - \\frac{1}{h}\\left(u(x) + hu'(x) + \\frac{h^2}{2}u''(\\xi(x)) - u(x)\\right) = -\\frac{h}{2}u''(\\xi(x)), em que \\xi(x) \\in (x, x+h) pelo Teorema do Valor M\u00e9dio e usando a expans\u00e3o de Taylor. Isso nos permite mostrar que ||T(x,y,h)|| \\le Ch e o m\u00e9todo de Euler \u00e9 de ordem p=1 . Para verificar a ordem, uma maneira relativamente f\u00e1cil \u00e9 verificar que 0 \\le |y(x_k) - y_k| \\le (1 + hL)|y(x_{k-1}) - y_{k-1}| + Dh^2 , em que L \u00e9 a constante de Lipschitz de f e D = \\max_{[a,b]}||y''(\\xi)|| . Na vida real, o m\u00e9todo de Euler \u00e9 pouco utilizado, pois o seu erro se acumula ao longo das itera\u00e7\u00f5es e a curva fica bem diferente ao longo do tempo. Lembre que ele \u00e9 uma boa aproxima\u00e7\u00e3o local, mas uma p\u00e9ssima global. M\u00e9todo de Heun Vamos tentar corrigir a separa\u00e7\u00e3o da curva real com a aproxima\u00e7\u00e3o por Euler. O algoritmo Heun atende a esse requisito de corre\u00e7\u00e3o. Em vez de focar no ponto inicial \u00e0 esquerda do intervalo em que calculamos f(x,y) , usamos a informa\u00e7\u00e3o do ponto final da curva. A ideia \u00e9 que quando a tangente subestima o valor da fun\u00e7\u00e3o no ponto \u00e0 esquerda do intervalo, teremos que ela superestimar\u00e1 quando pegarmos do ponto \u00e0 direita (pelo menos em fun\u00e7\u00f5es bem comportadas e h suficientemente pequeno). A imagem abaixo ilustra esse fato: Com isso, o m\u00e9todo fica y_n = y_{n-1} + h\\frac{f(x_{n-1}, y_{n-1}) + f(x_n, y_n)}{2}, que \u00e9 um m\u00e9todo impl\u00edcito. Para isso, aproximamos y_n na equa\u00e7\u00e3o da direita por y_{n-1} + hf(x_{n-1}, y_{n-1}), que \u00e9 o m\u00e9todo de Euler. Esse m\u00e9todo tem ordem p=2 . Para mais detalhes, confira esse site . M\u00e9todos de Taylor Considere as derivadas totais de f dadas por f^{[0]}(x,y) = f(x,y), \\\\ f^{[k+1]}(x,y) = f_x^{[k]}(x,y) + f_y^{[k]}(x,y)f(x,y), k \\in \\mathbb{Z}_{+}. Portanto, u^{(k+1)}(t) = f^{[k]}(t, u(t)) e o m\u00e9todo se torna \\Phi(x,y,h) = f^{[0]}(x,y) + \\frac{1}{2}hf^{[1]}(x,y) + \\cdots +\\frac{1}{p!}h^{p-1}f^{[p-1]}(x,y). Obteremos que o erro de truncagem \u00e9 ||T(x,y,h)|| \\le \\frac{C_p}{(p+1)!}h^p. M\u00e9todos Runge-Kutta A ideia desses m\u00e9todos \u00e9 escrever \\Phi(x,y,h) = \\sum_{i=1}^r \\alpha_s k_s, tal que k_1(x,y) = f(x,y) e k_s(x,y,h) = f\\left(x + \\mu_s h, y + h\\sum_{j=1}^{s-1}\\lambda_{s_j}k_j\\right), 2 \\le s \\le r. \u00c9 natural impor que \\mu_s = \\sum_{j=1}^{s-1} \\lambda_{s_j}, \\quad \\sum_{s=1}^r \\alpha_s = 1. Por fim, basta definir esses par\u00e2metros adicionais introduzidos. O Runge-Kutta cl\u00e1ssico de ordem 4 tem que r = 4 , \\alpha_1 = \\alpha_4 = 1/6, \\alpha_2 = \\alpha_3 = 2/6, \\mu_2 = \\mu_3 = 1/2, \\mu_4 = 1. Esse \u00e9 um tutorial que pode ser \u00fatil. N\u00e3o se deixe enganar pelo \"tutorial\", tem bastante matem\u00e1tica. Esse tamb\u00e9m \u00e9 um bom resumo . Equa\u00e7\u00f5es diferenciais Stiff (r\u00edgida, dif\u00edcil) Os m\u00e9todos num\u00e9ricos para resolver EDOs t\u00eam erros inerentes que envolvem derivadas de ordem maior. Se essas derivadas s\u00e3o razoavelmente limitadas, o erro vai ficar controlado. Problemas de valor inicial cuja magnitude da derivada cresce, mas a fun\u00e7\u00e3o n\u00e3o, s\u00e3o chamados de equa\u00e7\u00f5es stiff ou equa\u00e7\u00f5es r\u00edgidas/dif\u00edceis. A solu\u00e7\u00e3o exata delas tem termo com forma e^{-ct} em que c \u00e9 grande. Vamos ilustrar esse problema com o seguinte exemplo: x_1' = 9x_1 + 24x_2 + 5\\cos(t) - \\frac{1}{3}\\sin(t), x_1(0) = 4/3, x_2' = - 24x_1 - 51u_2 - 9\\cos(t) + \\frac{1}{3}\\sin(t), x_1(0) = 2/3. Sabemos calcular a solu\u00e7\u00e3o desse sistema exatamente usando EDOs. Nesse caso, x_1(t) = 2e^{-3t} -e^{-39t} + \\frac{1}{3}\\cos(t), \\quad x_2(t) = -e^{-3t} + 2e^{-39t} - \\frac{1}{3}\\cos(t). A solu\u00e7\u00e3o \u00e9 a seguinte: \u00c9 f\u00e1cil ver que quando t cresce, ambas as fun\u00e7\u00f5es convergem para a fun\u00e7\u00e3o cosseno: Vamos aplicar o Runge Kutta com o passo h=0.1 e com h=0.05 . Note como a solu\u00e7\u00e3o fica bem ruim para o primeiro caso: Isso d\u00e1 uma ideia do qu\u00e3o ruim uma solu\u00e7\u00e3o pode acabar ficando dependendo do h escolhido. Para analisar o erro produzido por equa\u00e7\u00f5es desse tipo, usamos uma equa\u00e7\u00e3o de teste : x'(t) = Ax(t), \\quad x(0) = x_0 Note que se a parte real dos autovalores de A forem todos negativos, teremos que a solu\u00e7\u00e3o converge para 0. No m\u00e9todo de Euler, por exemplo, x_n = x_{n-1} + hAx_{n-1} = (I + hA)x_{n-1}, que estabelece uma recursividade cuja solu\u00e7\u00e3o \u00e9 x_n = (I + hA)^n x_0. Queremos, em particular, que (I + hA)^n \\to 0 quando n \\to \\infty . Se A = PDP^{-1} para uma matriz diagonal D e uma invert\u00edvel P , ent\u00e3o (I +hA) = (PIP^{-1} + hPDP^{-1}) = P(I + hD)P^{-1} \\implies (I +hA)^n = P(I+hD)^nP^{-1}. Isso permite concluir que x_n \\to 0 somente se para cada autovalor \\lambda_i de A , tenhamos que |1 + \\lambda_i h| < 1 . Dom\u00ednio de estabilidade: D = \\{z \\in \\mathbb{C} \\mid |Re(z)| < 1\\} . Dizemos que um m\u00e9todo \u00e9 A -est\u00e1vel quando for aplicado \u00e0 equa\u00e7\u00e3o de teste x'(t) = \\lambda x(t) para Re(\\lambda) < 0 e \\lim x_n = 0 para a sequ\u00eancia gerada pelo m\u00e9todo, para qualquer h > 0 escolhido. O m\u00e9todo de Euler impl\u00edcito \u00e9 A -est\u00e1vel, pois x_{n+1} = x_n + \\lambda x_{n+1} h \\implies x_{n+1} = \\frac{1}{1 - \\lambda h}x_n e, portanto, x_{n+1} = \\left(\\frac{1}{1 - \\lambda h}\\right)^{n+1}x_0.","title":"M\u00e9todos num\u00e9ricos para solu\u00e7\u00e3o de EDOs"},{"location":"analisenum/edo/#metodos-numericos-para-solucao-de-edos","text":"Problemas de valor inicial para equa\u00e7\u00f5es diferenciais ordin\u00e1rias (EDOs) ocorrem em quase todas as ci\u00eancias, tal como a mec\u00e2nica de Newton, o movimento das part\u00edculas, evolu\u00e7\u00f5es de doen\u00e7as, mec\u00e2nica qu\u00e2ntica (aqui uma equa\u00e7\u00e3o parcial \u00e9 am ias conhecida), entre outros. No c\u00e1lculo das varia\u00e7\u00f5es , essa teoria tamb\u00e9m \u00e9 muito importante, como vimos no curso de EDP. Mais motiva\u00e7\u00f5es dessa \u00e1rea s\u00e3o deixados para outros materiais, como esse aqui , esse aqui , ou algum livro de EDOs recomendado pelo curso. O problema de valor inicial padr\u00e3o envolve um sistema de equa\u00e7\u00f5es diferenciais de primeira ordem, \\frac{dy_i}{dx} = f_i(x,y_1, \\dots, y_n), \\quad i = 1, \\dots, n e suas condi\u00e7\u00f5es iniciais, y_i(a) = y_i^{(0)}, \\quad i = 1, \\dots n. Uma equa\u00e7\u00e3o de segunda ordem, tal como y'' + y = a pode ser transformada em um sistema de primeira ordem introduzindo a vari\u00e1vel z = y' . A exist\u00eancia de solu\u00e7\u00f5es em um sistema de EDOs \u00e9 garantida quando a fun\u00e7\u00e3o \u00e9 cont\u00ednua, enquanto a unicidade \u00e9 garantida quando as \u00faltimas n-1 componentes s\u00e3o Lipshitz Cont\u00ednuas. Mais detalhes podem ser consultados aqui para um resumo, ou algum texto-livro de EDOs.","title":"M\u00e9todos num\u00e9ricos para solu\u00e7\u00e3o de EDOs"},{"location":"analisenum/edo/#metodos-numericos","text":"Suponha que queremos resolver \\frac{dy}{dx} = f(x,y), y \\in \\mathbb{R}^n, x \\in \\mathbb{R}, y(0) = y_0. Existem basicamente dois tipos de m\u00e9todos para solu\u00e7\u00e3o num\u00e9rica de EDOs, os m\u00e9todos por aproxima\u00e7\u00e3o anal\u00edtica que buscam aproximar a fun\u00e7\u00e3o y(x) para qualquer ponto x \\in [a,b] , tomando a forma de uma expans\u00e3o de s\u00e9rie truncada; e os m\u00e9todos vari\u00e1vel-discreta que procuram estimar a fun\u00e7\u00e3o em pontos y(x_i) para x_i \\in \\{x_1, \\dots, x_p\\} \\subset [a,b] .","title":"M\u00e9todos num\u00e9ricos"},{"location":"analisenum/edo/#metodos-de-um-passo","text":"Para x \\in [a,b] , definimos esses tipos de m\u00e9todo por y_{\\mathrm{next}} = y + h\\Phi(x, y, h), h > 0, em que \\Phi : [a,b] \\times \\mathbb{R}^n \\times \\mathbb{R}_{+} \\to \\mathbb{R}^n com uma incrementa\u00e7\u00e3o aproximada por passo. Chamamos de solu\u00e7\u00e3o refer\u00eancia a fun\u00e7\u00e3o u(t) tal que \\frac{du}{dt} = f(t,u), x \\le t \\le x + h, u(x) = y. Temos que y_{\\mathrm{next}} procura estimar u(x+h) . Para avaliar essa aproxima\u00e7\u00e3o, definimos o erro de truncamento dado pela express\u00e3o T(x,y,h) = \\frac{1}{h}[y_{\\mathrm{next}} - u(x+h)]. Dizemos que o m\u00e9todo, dado pela descri\u00e7\u00e3o de \\Phi , \u00e9 consistente se T(x,y,h) \\to 0, \\; \\text{ quando } h \\to 0. Tamb\u00e9m dizemos que o m\u00e9todo \u00e9 de ordem p se para alguma norma (n\u00e3o importa qual nesse contexto pela equival\u00eancia das normas) e para algum C \\in \\mathbb{R} , vale que ||T(x,y,h)|| \\le Ch^p.","title":"M\u00e9todos de um passo"},{"location":"analisenum/edo/#metodo-de-euler","text":"No m\u00e9todo de Euler, temos que \\Phi(x,y,h) = f(x,y) . Note que o passo ser\u00e1 a expans\u00e3o de Taylor de primeira ordem: y_{\\mathrm{next}} = y + hf(x,y). O erro de truncamento \u00e9 T(x,y,h) = \\frac{1}{h}(y + hf(x,y) - u(x+h)) = f(x,y) - \\frac{1}{h}(u(x+h) - u(x)). que \u00e9 claramente consistente, porque quando h tende a 0, temos que a express\u00e3o de u converge para u'(x) = f(x,y) . Al\u00e9m disso, note que T(x,y,h) = u'(x) - \\frac{1}{h}\\left(u(x) + hu'(x) + \\frac{h^2}{2}u''(\\xi(x)) - u(x)\\right) = -\\frac{h}{2}u''(\\xi(x)), em que \\xi(x) \\in (x, x+h) pelo Teorema do Valor M\u00e9dio e usando a expans\u00e3o de Taylor. Isso nos permite mostrar que ||T(x,y,h)|| \\le Ch e o m\u00e9todo de Euler \u00e9 de ordem p=1 . Para verificar a ordem, uma maneira relativamente f\u00e1cil \u00e9 verificar que 0 \\le |y(x_k) - y_k| \\le (1 + hL)|y(x_{k-1}) - y_{k-1}| + Dh^2 , em que L \u00e9 a constante de Lipschitz de f e D = \\max_{[a,b]}||y''(\\xi)|| . Na vida real, o m\u00e9todo de Euler \u00e9 pouco utilizado, pois o seu erro se acumula ao longo das itera\u00e7\u00f5es e a curva fica bem diferente ao longo do tempo. Lembre que ele \u00e9 uma boa aproxima\u00e7\u00e3o local, mas uma p\u00e9ssima global.","title":"M\u00e9todo de Euler"},{"location":"analisenum/edo/#metodo-de-heun","text":"Vamos tentar corrigir a separa\u00e7\u00e3o da curva real com a aproxima\u00e7\u00e3o por Euler. O algoritmo Heun atende a esse requisito de corre\u00e7\u00e3o. Em vez de focar no ponto inicial \u00e0 esquerda do intervalo em que calculamos f(x,y) , usamos a informa\u00e7\u00e3o do ponto final da curva. A ideia \u00e9 que quando a tangente subestima o valor da fun\u00e7\u00e3o no ponto \u00e0 esquerda do intervalo, teremos que ela superestimar\u00e1 quando pegarmos do ponto \u00e0 direita (pelo menos em fun\u00e7\u00f5es bem comportadas e h suficientemente pequeno). A imagem abaixo ilustra esse fato: Com isso, o m\u00e9todo fica y_n = y_{n-1} + h\\frac{f(x_{n-1}, y_{n-1}) + f(x_n, y_n)}{2}, que \u00e9 um m\u00e9todo impl\u00edcito. Para isso, aproximamos y_n na equa\u00e7\u00e3o da direita por y_{n-1} + hf(x_{n-1}, y_{n-1}), que \u00e9 o m\u00e9todo de Euler. Esse m\u00e9todo tem ordem p=2 . Para mais detalhes, confira esse site .","title":"M\u00e9todo de Heun"},{"location":"analisenum/edo/#metodos-de-taylor","text":"Considere as derivadas totais de f dadas por f^{[0]}(x,y) = f(x,y), \\\\ f^{[k+1]}(x,y) = f_x^{[k]}(x,y) + f_y^{[k]}(x,y)f(x,y), k \\in \\mathbb{Z}_{+}. Portanto, u^{(k+1)}(t) = f^{[k]}(t, u(t)) e o m\u00e9todo se torna \\Phi(x,y,h) = f^{[0]}(x,y) + \\frac{1}{2}hf^{[1]}(x,y) + \\cdots +\\frac{1}{p!}h^{p-1}f^{[p-1]}(x,y). Obteremos que o erro de truncagem \u00e9 ||T(x,y,h)|| \\le \\frac{C_p}{(p+1)!}h^p.","title":"M\u00e9todos de Taylor"},{"location":"analisenum/edo/#metodos-runge-kutta","text":"A ideia desses m\u00e9todos \u00e9 escrever \\Phi(x,y,h) = \\sum_{i=1}^r \\alpha_s k_s, tal que k_1(x,y) = f(x,y) e k_s(x,y,h) = f\\left(x + \\mu_s h, y + h\\sum_{j=1}^{s-1}\\lambda_{s_j}k_j\\right), 2 \\le s \\le r. \u00c9 natural impor que \\mu_s = \\sum_{j=1}^{s-1} \\lambda_{s_j}, \\quad \\sum_{s=1}^r \\alpha_s = 1. Por fim, basta definir esses par\u00e2metros adicionais introduzidos. O Runge-Kutta cl\u00e1ssico de ordem 4 tem que r = 4 , \\alpha_1 = \\alpha_4 = 1/6, \\alpha_2 = \\alpha_3 = 2/6, \\mu_2 = \\mu_3 = 1/2, \\mu_4 = 1. Esse \u00e9 um tutorial que pode ser \u00fatil. N\u00e3o se deixe enganar pelo \"tutorial\", tem bastante matem\u00e1tica. Esse tamb\u00e9m \u00e9 um bom resumo .","title":"M\u00e9todos Runge-Kutta"},{"location":"analisenum/edo/#equacoes-diferenciais-stiff-rigida-dificil","text":"Os m\u00e9todos num\u00e9ricos para resolver EDOs t\u00eam erros inerentes que envolvem derivadas de ordem maior. Se essas derivadas s\u00e3o razoavelmente limitadas, o erro vai ficar controlado. Problemas de valor inicial cuja magnitude da derivada cresce, mas a fun\u00e7\u00e3o n\u00e3o, s\u00e3o chamados de equa\u00e7\u00f5es stiff ou equa\u00e7\u00f5es r\u00edgidas/dif\u00edceis. A solu\u00e7\u00e3o exata delas tem termo com forma e^{-ct} em que c \u00e9 grande. Vamos ilustrar esse problema com o seguinte exemplo: x_1' = 9x_1 + 24x_2 + 5\\cos(t) - \\frac{1}{3}\\sin(t), x_1(0) = 4/3, x_2' = - 24x_1 - 51u_2 - 9\\cos(t) + \\frac{1}{3}\\sin(t), x_1(0) = 2/3. Sabemos calcular a solu\u00e7\u00e3o desse sistema exatamente usando EDOs. Nesse caso, x_1(t) = 2e^{-3t} -e^{-39t} + \\frac{1}{3}\\cos(t), \\quad x_2(t) = -e^{-3t} + 2e^{-39t} - \\frac{1}{3}\\cos(t). A solu\u00e7\u00e3o \u00e9 a seguinte: \u00c9 f\u00e1cil ver que quando t cresce, ambas as fun\u00e7\u00f5es convergem para a fun\u00e7\u00e3o cosseno: Vamos aplicar o Runge Kutta com o passo h=0.1 e com h=0.05 . Note como a solu\u00e7\u00e3o fica bem ruim para o primeiro caso: Isso d\u00e1 uma ideia do qu\u00e3o ruim uma solu\u00e7\u00e3o pode acabar ficando dependendo do h escolhido. Para analisar o erro produzido por equa\u00e7\u00f5es desse tipo, usamos uma equa\u00e7\u00e3o de teste : x'(t) = Ax(t), \\quad x(0) = x_0 Note que se a parte real dos autovalores de A forem todos negativos, teremos que a solu\u00e7\u00e3o converge para 0. No m\u00e9todo de Euler, por exemplo, x_n = x_{n-1} + hAx_{n-1} = (I + hA)x_{n-1}, que estabelece uma recursividade cuja solu\u00e7\u00e3o \u00e9 x_n = (I + hA)^n x_0. Queremos, em particular, que (I + hA)^n \\to 0 quando n \\to \\infty . Se A = PDP^{-1} para uma matriz diagonal D e uma invert\u00edvel P , ent\u00e3o (I +hA) = (PIP^{-1} + hPDP^{-1}) = P(I + hD)P^{-1} \\implies (I +hA)^n = P(I+hD)^nP^{-1}. Isso permite concluir que x_n \\to 0 somente se para cada autovalor \\lambda_i de A , tenhamos que |1 + \\lambda_i h| < 1 . Dom\u00ednio de estabilidade: D = \\{z \\in \\mathbb{C} \\mid |Re(z)| < 1\\} . Dizemos que um m\u00e9todo \u00e9 A -est\u00e1vel quando for aplicado \u00e0 equa\u00e7\u00e3o de teste x'(t) = \\lambda x(t) para Re(\\lambda) < 0 e \\lim x_n = 0 para a sequ\u00eancia gerada pelo m\u00e9todo, para qualquer h > 0 escolhido. O m\u00e9todo de Euler impl\u00edcito \u00e9 A -est\u00e1vel, pois x_{n+1} = x_n + \\lambda x_{n+1} h \\implies x_{n+1} = \\frac{1}{1 - \\lambda h}x_n e, portanto, x_{n+1} = \\left(\\frac{1}{1 - \\lambda h}\\right)^{n+1}x_0.","title":"Equa\u00e7\u00f5es diferenciais Stiff (r\u00edgida, dif\u00edcil)"},{"location":"analisenum/edp/","text":"M\u00e9todos num\u00e9ricos para EDPs Nessa se\u00e7\u00e3o, desenvolveremos m\u00e9todos de aproxima\u00e7\u00e3o num\u00e9rica para Equa\u00e7\u00f5es Diferenciais Parciais. Para detalhes sobre os tipos de equa\u00e7\u00f5es, consulte esse link . Equa\u00e7\u00f5es El\u00edpticas Considere a equa\u00e7\u00e3o de Poisson \\nabla^2 u(x,y) := u_{xx}(x,y) + u_{yy}(x,y) = f(x,y) definida em um conjunto U tal que u(x,y) = g(x,y) em (x,y) na fronteira de U . Em geral, tomamos U = \\{(x,y) \\mid a < x < b, c < y < d\\} . O m\u00e9todo consiste na aplica\u00e7\u00e3o do m\u00e9todo das Diferen\u00e7as Finitas em um grid de duas dimens\u00f5es. Seja h = (b-a)/n e k = (d-c)/m o tamanho do passo, em que n e m indicam a quantidade deles em cada dire\u00e7\u00e3o. A representa\u00e7\u00e3o do desenho foi retirada de https://sites.me.ucsb.edu/~moehlis/APC591/tutorials/tutorial5/node3.html e segue abaixo: Nessa imagem \\delta x = h e \\delta t = k . Com o grid, obtemos que x_i = a + ih e y_j = c + jk , para i, j = 0, \\dots, m . Usando a expans\u00e3o de Taylor na vari\u00e1vel x e y , geramos a f\u00f3rmula de diferen\u00e7as centrada \\frac{\\partial ^2 u}{\\partial x^2}u(x_i, y_j) = \\frac{u(x_{i+1}, y_j) - 2u(x_{i}, y_j) + u(x_{i-1}, y_j)}{h^2} - \\frac{h^2}{12}\\frac{\\partial^4 u}{\\partial x^4}(\\xi_i, y_j), em que \\xi_i \\in (x_{i-1}, x_{i+1}) , e \\frac{\\partial ^2 u}{\\partial y^2}u(x_i, y_j) = \\frac{u(x_{i}, y_{j+1}) - 2u(x_{i}, y_j) + u(x_{i}, y_{j-1})}{k^2} - \\frac{k^2}{12}\\frac{\\partial^4 u}{\\partial y^4}(x_i, \\eta_j), em que \\eta_i \\in (y_{i-1}, y_{i+1}) . Aplicando na equa\u00e7\u00e3o de Poisson, \\frac{u(x_{i+1}, y_j) - 2u(x_{i}, y_j) + u(x_{i-1}, y_j)}{h^2} + \\frac{u(x_{i}, y_{j+1}) - 2u(x_{i}, y_j) + u(x_{i}, y_{j-1})}{k^2} = f(x_i, y_j) + \\frac{h^2}{12}\\frac{\\partial^4 u}{\\partial x^4}(\\xi_i, y_j) + \\frac{k^2}{12}\\frac{\\partial^4 u}{\\partial y^4}(x_i, \\eta_j), com as restri\u00e7\u00f5es de contorno. Usando a nota\u00e7\u00e3o w_{ij} \\approx u(x_{i}, y_j) , multiplicando ambos os lados pro h^2 e juntando os termos, obtemos que 2\\left[\\left(\\frac{h}{k}\\right)^2 + 1\\right]w_{ij} - (w_{i+1,j} + w_{i-1,j})- \\left(\\frac{h}{k}\\right)^2(w_{i,j+1} + w_{i,j-1}) = -h^2f(x_i, y_j), que tem erro local de ordem h^2 + k^2 . Note que isso formar\u00e1 um sistema de equa\u00e7\u00f5es com vari\u00e1veis desconhecidas w_{ij} . \u00c9 usual denotar w_l = w_{ij}, l = i + (m-1-j)(n-1) para i = 1,2, \\dots, n-1 e j=1,2,\\dots,m-1 . Equa\u00e7\u00f5es Parab\u00f3licas A primeira equa\u00e7\u00e3o \u00e9 a do calor ou difus\u00e3o, dada por \\frac{\\partial u}{\\partial t} = \\frac{\\partial^2 u}{\\partial x^2}, com condi\u00e7\u00f5es u(x,0) = f(x), u(0,t) = a(t), \\text{ e } u(1,t) = b(t). Usando a F\u00f3rmula de Taylor mais uma vez e a aproxima\u00e7\u00e3o w_{ij} \\approx u(x_i, y_j) , obtemos que \\frac{w_{i, j+1} - w_{ij}}{k} - \\alpha^2 \\frac{w_{i+1,j} - 2w_{ij} + w_{i-1,j}}{h^2} = 0, que pode ser reescrito como w_{i,j+1} = \\left(1 - \\frac{2\\alpha^2 k }{h^2}\\right)w_{ij} + \\alpha^2 \\frac{k}{h^2}(w_{i+1,j} + w_{i-1,j}) para i=1,2, \\dots, m-1 e j=1,2,\\dots . Al\u00e9m, disso temos os valores de contorno, w_{i0} = f(x_i) para i= 0, 1, \\dots, m, w_{m j} = b(kj), \\text{ and } w_{0 j} = a(kj) . Esse m\u00e9todo de atualiza\u00e7\u00e3o \u00e9 conhecido como Forward-Difference e o m\u00e9todo tem erro local de ordem O(k + h^2) . Note que a atualiza\u00e7\u00e3o pode ser escrita em formato matricial: w^{(j)} = Aw^{(j-1)} , em que w^{(j)} = (w_{1j}, \\dots, w_{mj}) . Estabilidade Suponha que o valor inicial w^{(0)} seja representado com erro num\u00e9rico e^{(0)} . Esse erro vai se propagar pelas itera\u00e7\u00f5es, dado que w^{(1)} = Aw^{(0)} + Ae^{(0)}, \\quad w^{(2)} = A^2w^{(0)} + A^2e^{(0)}, \\quad w^{(3)} = A^3w^{(0)} + A^3e^{(0)}, e assim por diante. Isso significa que n\u00e3o queremos que ||A^n e^{(0)}|| cres\u00e7a com n . Isso acontece se para qualquer erro, ||A^ne^{(0)}|| \\le ||e^{(0)}||, \\text{ for all } n. Com isso, gostar\u00edamos que ||A^n|| \\le 1 e, por conseguinte, \\rho(A) \\le 1 . Essa restri\u00e7\u00e3o implica na condi\u00e7\u00e3o de estabilidade para esse m\u00e9todo: \\alpha^2 \\frac{k}{h^2} \\le \\frac{1}{2}. Backward-Difference Nesse m\u00e9todo, usamos que \\frac{\\partial u}{\\partial t}(x_i, t_j) = \\frac{u(x_i, t_j) - u(x_i, t_{j-1}}{k} + \\frac{k}{2}\\frac{\\partial ^2 u}{\\partial t^2}(x_i, \\mu_j). Assim, chegamos na f\u00f3rmula de atualiza\u00e7\u00e3o \\frac{w_{i, j} - w_{i,j-1}}{k} - \\alpha^2 \\frac{w_{i+1,j} - 2w_{ij} + w_{i-1,j}}{h^2} = 0 Denote \\lambda = \\alpha^2(k/h^2) . Ent\u00e3o o m\u00e9todo vira (1+2\\lambda)w_{ij} - \\lambda w_{i+1, j} - \\lambda w_{i-1,j} = w_{i, j-1} Note que nesse caso, teremos que Aw^{(j)} = w^{(j-1)}. Podemos verificar que quando \\lambda > 0 , a matriz \u00e9 A \u00e9 estritamente diagonalmente dominante e positiva definida. Um Solver de sistema linear deve ser utilizado para encontrar o pr\u00f3ximo passo w^{(j)} . Uma an\u00e1lise nos autovalores de A prova que o m\u00e9todo backward \u00e9 est\u00e1vel incondicionalmente. Al\u00e9m dela, ainda temos a f\u00f3rmula Centered-Difference que diz que \\frac{w_{i,j+1} - w_{i,j-1}}{2k} = \\alpha^2\\frac{w_{i+1,j} - 2w_{ij} + w_{i-1, j}}{h^2}. Esse m\u00e9todo tem a vantagem de ter erro local O(h^2 + k^2) , mas tem problemas de estabilidade como o Forward. Crank-Nicolson A ideia desse m\u00e9todo \u00e9 tomar a m\u00e9dia dos passo j do m\u00e9todo Forward e do passo j+1 do m\u00e9todo Backward, dados por \\frac{w_{i, j+1} - w_{i,j}}{k} - \\alpha^2 \\frac{w_{i+1,j} - 2w_{ij} + w_{i-1,j}}{h^2} = 0 e \\frac{w_{i, j+1} - w_{i,j-1}}{k} - \\alpha^2 \\frac{w_{i+1,j+1} - 2w_{i,j+1} + w_{i-1,j+1}}{h^2} = 0 que leva a \\frac{w_{i, j+1} - w_{i,j}}{k} - \\frac{\\alpha^2}{2}\\left[\\frac{w_{i+1,j} - 2w_{ij} + w_{i-1,j}}{h^2} + \\frac{w_{i+1,j+1} - 2w_{i,j+1} + w_{i-1,j+1}}{h^2}\\right] = 0. Esse m\u00e9todo tem erro local O(k^2 + h^2) . Em forma matricial, esse m\u00e9todo \u00e9 da forma Aw^{(j+1)} = Bw^{(j)} . \u00c9 f\u00e1cil verificar que A \u00e9 positiva definida com diagonal estritamente dominante. Al\u00e9m disso, o m\u00e9todo \u00e9 incondicionalmente est\u00e1vel. Equa\u00e7\u00f5es Hiperb\u00f3licas O principal exemplo desse tipo de equa\u00e7\u00e3o \u00e9 a equa\u00e7\u00e3o da onda que \u00e9 dada por u_{tt}(x,t) = \\alpha^2 u_{xx}(x,t) com condi\u00e7\u00f5es iniciais e finais u(x,0) = f(x), u_t(x,0) = g(x), u(0,t) = p(t), u(l,t) = q(t) , em que 0 < x < l . As ideias do grid e da aproxima\u00e7\u00e3o de Taylor s\u00e3o as mesmas, o que leva ao m\u00e9todo das diferen\u00e7as \\frac{w_{i,j+1} - 2w_{ij} + w_{i,j-1}}{k^2} - \\alpha^2\\frac{w_{i+1,j} - 2w_{ij} + w_{i-1, j}}{h^2} = 0. Ponha \\lambda = \\alpha k / h . Ent\u00e3o reescrevemos a equa\u00e7\u00e3o acima como w_{i,j+1} = 2(1 - \\lambda^2)w_{i,j} + \\lambda^2(w_{i+1,j} + w_{i-1,j}) -w_{i,j-1}. Note que para o passo j+1 s\u00e3o necess\u00e1rios conhecer dois passos anteriores. Para calcular w_{i2} precisamos saber w_{i0} = f(x_i) e w_{i1} . Esse \u00faltimo valor vai ser aproximado usando a condi\u00e7\u00e3o inicial da derivada g(x) , obtida atrav\u00e9s de w_{i,1} = w_{i,0} + kg(x_i). Tamb\u00e9m podemos melhor essa aproxima\u00e7\u00e3o de Euler usando o polin\u00f4mio de Maclaurin em t : w_{i,1} = w_{i,0} + kg(x_i) + \\frac{\\alpha^2 k^2}{2}f''(x_i), que tem erro de aproxima\u00e7\u00e3o O(k^3) . Esse m\u00e9todo \u00e9 est\u00e1vel se \\lambda \\le 1 .","title":"M\u00e9todos num\u00e9ricos para EDPs"},{"location":"analisenum/edp/#metodos-numericos-para-edps","text":"Nessa se\u00e7\u00e3o, desenvolveremos m\u00e9todos de aproxima\u00e7\u00e3o num\u00e9rica para Equa\u00e7\u00f5es Diferenciais Parciais. Para detalhes sobre os tipos de equa\u00e7\u00f5es, consulte esse link .","title":"M\u00e9todos num\u00e9ricos para EDPs"},{"location":"analisenum/edp/#equacoes-elipticas","text":"Considere a equa\u00e7\u00e3o de Poisson \\nabla^2 u(x,y) := u_{xx}(x,y) + u_{yy}(x,y) = f(x,y) definida em um conjunto U tal que u(x,y) = g(x,y) em (x,y) na fronteira de U . Em geral, tomamos U = \\{(x,y) \\mid a < x < b, c < y < d\\} . O m\u00e9todo consiste na aplica\u00e7\u00e3o do m\u00e9todo das Diferen\u00e7as Finitas em um grid de duas dimens\u00f5es. Seja h = (b-a)/n e k = (d-c)/m o tamanho do passo, em que n e m indicam a quantidade deles em cada dire\u00e7\u00e3o. A representa\u00e7\u00e3o do desenho foi retirada de https://sites.me.ucsb.edu/~moehlis/APC591/tutorials/tutorial5/node3.html e segue abaixo: Nessa imagem \\delta x = h e \\delta t = k . Com o grid, obtemos que x_i = a + ih e y_j = c + jk , para i, j = 0, \\dots, m . Usando a expans\u00e3o de Taylor na vari\u00e1vel x e y , geramos a f\u00f3rmula de diferen\u00e7as centrada \\frac{\\partial ^2 u}{\\partial x^2}u(x_i, y_j) = \\frac{u(x_{i+1}, y_j) - 2u(x_{i}, y_j) + u(x_{i-1}, y_j)}{h^2} - \\frac{h^2}{12}\\frac{\\partial^4 u}{\\partial x^4}(\\xi_i, y_j), em que \\xi_i \\in (x_{i-1}, x_{i+1}) , e \\frac{\\partial ^2 u}{\\partial y^2}u(x_i, y_j) = \\frac{u(x_{i}, y_{j+1}) - 2u(x_{i}, y_j) + u(x_{i}, y_{j-1})}{k^2} - \\frac{k^2}{12}\\frac{\\partial^4 u}{\\partial y^4}(x_i, \\eta_j), em que \\eta_i \\in (y_{i-1}, y_{i+1}) . Aplicando na equa\u00e7\u00e3o de Poisson, \\frac{u(x_{i+1}, y_j) - 2u(x_{i}, y_j) + u(x_{i-1}, y_j)}{h^2} + \\frac{u(x_{i}, y_{j+1}) - 2u(x_{i}, y_j) + u(x_{i}, y_{j-1})}{k^2} = f(x_i, y_j) + \\frac{h^2}{12}\\frac{\\partial^4 u}{\\partial x^4}(\\xi_i, y_j) + \\frac{k^2}{12}\\frac{\\partial^4 u}{\\partial y^4}(x_i, \\eta_j), com as restri\u00e7\u00f5es de contorno. Usando a nota\u00e7\u00e3o w_{ij} \\approx u(x_{i}, y_j) , multiplicando ambos os lados pro h^2 e juntando os termos, obtemos que 2\\left[\\left(\\frac{h}{k}\\right)^2 + 1\\right]w_{ij} - (w_{i+1,j} + w_{i-1,j})- \\left(\\frac{h}{k}\\right)^2(w_{i,j+1} + w_{i,j-1}) = -h^2f(x_i, y_j), que tem erro local de ordem h^2 + k^2 . Note que isso formar\u00e1 um sistema de equa\u00e7\u00f5es com vari\u00e1veis desconhecidas w_{ij} . \u00c9 usual denotar w_l = w_{ij}, l = i + (m-1-j)(n-1) para i = 1,2, \\dots, n-1 e j=1,2,\\dots,m-1 .","title":"Equa\u00e7\u00f5es El\u00edpticas"},{"location":"analisenum/edp/#equacoes-parabolicas","text":"A primeira equa\u00e7\u00e3o \u00e9 a do calor ou difus\u00e3o, dada por \\frac{\\partial u}{\\partial t} = \\frac{\\partial^2 u}{\\partial x^2}, com condi\u00e7\u00f5es u(x,0) = f(x), u(0,t) = a(t), \\text{ e } u(1,t) = b(t). Usando a F\u00f3rmula de Taylor mais uma vez e a aproxima\u00e7\u00e3o w_{ij} \\approx u(x_i, y_j) , obtemos que \\frac{w_{i, j+1} - w_{ij}}{k} - \\alpha^2 \\frac{w_{i+1,j} - 2w_{ij} + w_{i-1,j}}{h^2} = 0, que pode ser reescrito como w_{i,j+1} = \\left(1 - \\frac{2\\alpha^2 k }{h^2}\\right)w_{ij} + \\alpha^2 \\frac{k}{h^2}(w_{i+1,j} + w_{i-1,j}) para i=1,2, \\dots, m-1 e j=1,2,\\dots . Al\u00e9m, disso temos os valores de contorno, w_{i0} = f(x_i) para i= 0, 1, \\dots, m, w_{m j} = b(kj), \\text{ and } w_{0 j} = a(kj) . Esse m\u00e9todo de atualiza\u00e7\u00e3o \u00e9 conhecido como Forward-Difference e o m\u00e9todo tem erro local de ordem O(k + h^2) . Note que a atualiza\u00e7\u00e3o pode ser escrita em formato matricial: w^{(j)} = Aw^{(j-1)} , em que w^{(j)} = (w_{1j}, \\dots, w_{mj}) .","title":"Equa\u00e7\u00f5es Parab\u00f3licas"},{"location":"analisenum/edp/#estabilidade","text":"Suponha que o valor inicial w^{(0)} seja representado com erro num\u00e9rico e^{(0)} . Esse erro vai se propagar pelas itera\u00e7\u00f5es, dado que w^{(1)} = Aw^{(0)} + Ae^{(0)}, \\quad w^{(2)} = A^2w^{(0)} + A^2e^{(0)}, \\quad w^{(3)} = A^3w^{(0)} + A^3e^{(0)}, e assim por diante. Isso significa que n\u00e3o queremos que ||A^n e^{(0)}|| cres\u00e7a com n . Isso acontece se para qualquer erro, ||A^ne^{(0)}|| \\le ||e^{(0)}||, \\text{ for all } n. Com isso, gostar\u00edamos que ||A^n|| \\le 1 e, por conseguinte, \\rho(A) \\le 1 . Essa restri\u00e7\u00e3o implica na condi\u00e7\u00e3o de estabilidade para esse m\u00e9todo: \\alpha^2 \\frac{k}{h^2} \\le \\frac{1}{2}.","title":"Estabilidade"},{"location":"analisenum/edp/#backward-difference","text":"Nesse m\u00e9todo, usamos que \\frac{\\partial u}{\\partial t}(x_i, t_j) = \\frac{u(x_i, t_j) - u(x_i, t_{j-1}}{k} + \\frac{k}{2}\\frac{\\partial ^2 u}{\\partial t^2}(x_i, \\mu_j). Assim, chegamos na f\u00f3rmula de atualiza\u00e7\u00e3o \\frac{w_{i, j} - w_{i,j-1}}{k} - \\alpha^2 \\frac{w_{i+1,j} - 2w_{ij} + w_{i-1,j}}{h^2} = 0 Denote \\lambda = \\alpha^2(k/h^2) . Ent\u00e3o o m\u00e9todo vira (1+2\\lambda)w_{ij} - \\lambda w_{i+1, j} - \\lambda w_{i-1,j} = w_{i, j-1} Note que nesse caso, teremos que Aw^{(j)} = w^{(j-1)}. Podemos verificar que quando \\lambda > 0 , a matriz \u00e9 A \u00e9 estritamente diagonalmente dominante e positiva definida. Um Solver de sistema linear deve ser utilizado para encontrar o pr\u00f3ximo passo w^{(j)} . Uma an\u00e1lise nos autovalores de A prova que o m\u00e9todo backward \u00e9 est\u00e1vel incondicionalmente. Al\u00e9m dela, ainda temos a f\u00f3rmula Centered-Difference que diz que \\frac{w_{i,j+1} - w_{i,j-1}}{2k} = \\alpha^2\\frac{w_{i+1,j} - 2w_{ij} + w_{i-1, j}}{h^2}. Esse m\u00e9todo tem a vantagem de ter erro local O(h^2 + k^2) , mas tem problemas de estabilidade como o Forward.","title":"Backward-Difference"},{"location":"analisenum/edp/#crank-nicolson","text":"A ideia desse m\u00e9todo \u00e9 tomar a m\u00e9dia dos passo j do m\u00e9todo Forward e do passo j+1 do m\u00e9todo Backward, dados por \\frac{w_{i, j+1} - w_{i,j}}{k} - \\alpha^2 \\frac{w_{i+1,j} - 2w_{ij} + w_{i-1,j}}{h^2} = 0 e \\frac{w_{i, j+1} - w_{i,j-1}}{k} - \\alpha^2 \\frac{w_{i+1,j+1} - 2w_{i,j+1} + w_{i-1,j+1}}{h^2} = 0 que leva a \\frac{w_{i, j+1} - w_{i,j}}{k} - \\frac{\\alpha^2}{2}\\left[\\frac{w_{i+1,j} - 2w_{ij} + w_{i-1,j}}{h^2} + \\frac{w_{i+1,j+1} - 2w_{i,j+1} + w_{i-1,j+1}}{h^2}\\right] = 0. Esse m\u00e9todo tem erro local O(k^2 + h^2) . Em forma matricial, esse m\u00e9todo \u00e9 da forma Aw^{(j+1)} = Bw^{(j)} . \u00c9 f\u00e1cil verificar que A \u00e9 positiva definida com diagonal estritamente dominante. Al\u00e9m disso, o m\u00e9todo \u00e9 incondicionalmente est\u00e1vel.","title":"Crank-Nicolson"},{"location":"analisenum/edp/#equacoes-hiperbolicas","text":"O principal exemplo desse tipo de equa\u00e7\u00e3o \u00e9 a equa\u00e7\u00e3o da onda que \u00e9 dada por u_{tt}(x,t) = \\alpha^2 u_{xx}(x,t) com condi\u00e7\u00f5es iniciais e finais u(x,0) = f(x), u_t(x,0) = g(x), u(0,t) = p(t), u(l,t) = q(t) , em que 0 < x < l . As ideias do grid e da aproxima\u00e7\u00e3o de Taylor s\u00e3o as mesmas, o que leva ao m\u00e9todo das diferen\u00e7as \\frac{w_{i,j+1} - 2w_{ij} + w_{i,j-1}}{k^2} - \\alpha^2\\frac{w_{i+1,j} - 2w_{ij} + w_{i-1, j}}{h^2} = 0. Ponha \\lambda = \\alpha k / h . Ent\u00e3o reescrevemos a equa\u00e7\u00e3o acima como w_{i,j+1} = 2(1 - \\lambda^2)w_{i,j} + \\lambda^2(w_{i+1,j} + w_{i-1,j}) -w_{i,j-1}. Note que para o passo j+1 s\u00e3o necess\u00e1rios conhecer dois passos anteriores. Para calcular w_{i2} precisamos saber w_{i0} = f(x_i) e w_{i1} . Esse \u00faltimo valor vai ser aproximado usando a condi\u00e7\u00e3o inicial da derivada g(x) , obtida atrav\u00e9s de w_{i,1} = w_{i,0} + kg(x_i). Tamb\u00e9m podemos melhor essa aproxima\u00e7\u00e3o de Euler usando o polin\u00f4mio de Maclaurin em t : w_{i,1} = w_{i,0} + kg(x_i) + \\frac{\\alpha^2 k^2}{2}f''(x_i), que tem erro de aproxima\u00e7\u00e3o O(k^3) . Esse m\u00e9todo \u00e9 est\u00e1vel se \\lambda \\le 1 .","title":"Equa\u00e7\u00f5es Hiperb\u00f3licas"},{"location":"analisenum/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de Introdu\u00e7\u00e3o \u00e0 An\u00e1lise Num\u00e9rica correspondente ao per\u00edodo de 2021.2. Dia: Sexta-feira, 11:10am at\u00e9 12:50pm Ementa da disciplina T\u00f3picos T\u00f3picos Aritm\u00e9tica do computador M\u00e9todos iterativos para sistemas lineares Solu\u00e7\u00e3o num\u00e9rica de equa\u00e7\u00f5es n\u00e3o lineares Aplica\u00e7\u00e3o m\u00e9todo de Newton Interpola\u00e7\u00e3o polonomial Integra\u00e7\u00e3o num\u00e9rica Solu\u00e7\u00e3o de Equa\u00e7\u00f5es diferenciais ordin\u00e1rias Solu\u00e7\u00e3o de Equa\u00e7\u00f5es diferenciais parciais Simula\u00e7\u00e3o estoc\u00e1stica Listas N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Aritm\u00e9tica do computador e equa\u00e7\u00e3o recursiva 1 2 M\u00e9todos iterativos para sistemas de equa\u00e7\u00f5es lineares 2 3 Solu\u00e7\u00f5es de equa\u00e7\u00f5es n\u00e3o lineares 3 4 Interpola\u00e7\u00e3o polinomial 4 5 M\u00e9todos num\u00e9ricos para EDOs 5 6 M\u00e9todos num\u00e9ricos para EDPs 6 Notas Monitoria Itens discutidos Arquivo V\u00eddeo 20/08/2021 Lista 1 e an\u00e1lise de estabilidade Visualizar N\u00e3o Provas Ano Bimestre Prova Solu\u00e7\u00f5es 2020 A1 ver arquivo ver arquivo 2021 A1 ver arquivo ver arquivo Sugest\u00f5es Adicionais Disasters attributable to bad numerical computing M\u00e9todo Gradiente Conjugado","title":"An\u00e1lise Num\u00e9rica"},{"location":"analisenum/info/#informacoes-gerais","text":"Monitoria de Introdu\u00e7\u00e3o \u00e0 An\u00e1lise Num\u00e9rica correspondente ao per\u00edodo de 2021.2. Dia: Sexta-feira, 11:10am at\u00e9 12:50pm Ementa da disciplina","title":"Informa\u00e7\u00f5es Gerais"},{"location":"analisenum/info/#topicos","text":"T\u00f3picos Aritm\u00e9tica do computador M\u00e9todos iterativos para sistemas lineares Solu\u00e7\u00e3o num\u00e9rica de equa\u00e7\u00f5es n\u00e3o lineares Aplica\u00e7\u00e3o m\u00e9todo de Newton Interpola\u00e7\u00e3o polonomial Integra\u00e7\u00e3o num\u00e9rica Solu\u00e7\u00e3o de Equa\u00e7\u00f5es diferenciais ordin\u00e1rias Solu\u00e7\u00e3o de Equa\u00e7\u00f5es diferenciais parciais Simula\u00e7\u00e3o estoc\u00e1stica","title":"T\u00f3picos"},{"location":"analisenum/info/#listas","text":"N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Aritm\u00e9tica do computador e equa\u00e7\u00e3o recursiva 1 2 M\u00e9todos iterativos para sistemas de equa\u00e7\u00f5es lineares 2 3 Solu\u00e7\u00f5es de equa\u00e7\u00f5es n\u00e3o lineares 3 4 Interpola\u00e7\u00e3o polinomial 4 5 M\u00e9todos num\u00e9ricos para EDOs 5 6 M\u00e9todos num\u00e9ricos para EDPs 6","title":"Listas"},{"location":"analisenum/info/#notas","text":"Monitoria Itens discutidos Arquivo V\u00eddeo 20/08/2021 Lista 1 e an\u00e1lise de estabilidade Visualizar N\u00e3o","title":"Notas"},{"location":"analisenum/info/#provas","text":"Ano Bimestre Prova Solu\u00e7\u00f5es 2020 A1 ver arquivo ver arquivo 2021 A1 ver arquivo ver arquivo","title":"Provas"},{"location":"analisenum/info/#sugestoes-adicionais","text":"Disasters attributable to bad numerical computing M\u00e9todo Gradiente Conjugado","title":"Sugest\u00f5es Adicionais"},{"location":"analisenum/linear-systems/","text":"M\u00e9todos iterativos para resolver sistemas lineares Suponha que queremos resolver um problema do tipo Ax = b , em que A \u00e9 uma matriz real n \\times n e b \u00e9 um vetor em \\mathbb{R}^n . Matematicamente, se estamos interessados em encontrar x , podemos apenas calcular a inversa de A , caso exista. Se sim x = A^{-1}b . Apesar de ser f\u00e1cil de calcular, esse procedimento precisa fazer O(n^3) opera\u00e7\u00f5es, o que pode ser muito custoso quando n aumenta. Nesse caso, precisamos de alternativas mais palat\u00e1veis para resolver esse problema, principalmente para n grande. Utilizamos m\u00e9todos iterativos para ajudar! De forma geral, vamos querer reescrever o problema da forma: x = Cx + D, em que s\u00e3o matrizes. Se conseguirmos expressar Ax = b dessa forma, estaremos interessados nos pontos fixos do operador L(x) = Cx + D . Teorema: O processo iterativo x^{k+1} = Cx^k + D satisfaz o seguinte: Para todo valor inicial x^0 , a sequ\u00eancia \\{x^k\\}_{k \\in \\mathbb{N}} converge para o ponto fixo x^* = Cx^* + D se, e somente se, \\rho(C) < 1 , em que \\rho(C) \u00e9 o raio espectral da matriz C , isto \u00e9, o maior autovalor em m\u00f3dulo. Uma demonstra\u00e7\u00e3o desse resultado pode ser encontrado no livro de Richard L.Burden Numerical Analysis (p\u00e1gina 457). Corol\u00e1rio: Se ||C|| < 1 para qualquer norma matricial induzida (induzida por uma norma vetorial), ent\u00e3o a itera\u00e7\u00e3o anterior converge para o ponto fixo do operador L para qualquer chute inicial x^0 . Esse resultado \u00e9 uma consequ\u00eancia de \\rho(A) \\le ||A|| para todo norma natural ||\\cdot|| . M\u00e9todo de Jacobi M\u00e9todo de Gauss-Seidel M\u00e9todo Successive Over-Relaxation (SOR) M\u00e9todo Gradiente Conjugado Teorema: Se A \u00e9 diagonalmente estritamente dominante, ent\u00e3o para qualquer escolha de x^0 , os m\u00e9todos de Jacobi e Gauss-Seidel convergem. M\u00e9todo de Jacobi Esse m\u00e9todo \u00e9 derivado resolvendo a i th equa\u00e7\u00e3o de Ax = b para x_i (dado que a_{ii} \\neq 0 ) x_i = \\sum_{j\\neq i} \\left(-\\frac{a_{ij}x_j}{a_{ii}}\\right) + \\frac{b_i}{a_{ii}}, ~~~ \\text{ for } i = 1, 2, \\dots, n. Assim, geramos iterativamente, x_i^{(k)} = \\frac{1}{a_{ii}} \\left[ \\sum_{j\\neq i} \\left(-a_{ij} x_j^{(k-1)}\\right) + b_i\\right] Vamos escrever em formato matricial. Observe que x^{(k)} = \\begin{bmatrix} 0 & -a_{12}/a_{11} & -a_{13}/a_{11} & \\dots & -a_{1n}/a_{11} \\\\ -a_{21}/a_{22} & 0 & -a_{23}/a_{22} & \\dots & -a_{2n}/a_{22} \\\\ -a_{31}/a_{33} & -a_{32}/a_{33} & 0 & \\dots & -a_{3n}/a_{33} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ -a_{n1}/a_{nn} & -a_{n2}/a_{nn} & -a_{n3}/a_{nn} & \\dots & 0\\\\ \\end{bmatrix}x^{(k-1)} + \\begin{bmatrix} b_1/a_{11} \\\\ b_2/a_{22} \\\\ b_3/a_{33} \\\\ \\vdots \\\\ b_n/a_{nn} \\end{bmatrix} e sendo U e L as matrizes triangulares superiores e inferiores, e D a matriz diagonal de A , x^{(k)} = -D^{-1}(L + U)x^{(k-1)} + D^{-1}b Assim, o procedimento Jacobi pode ser sumarizado da seguinte forma: Sejam as entradas as matrizes A e b com uma aproxima\u00e7\u00e3o inicial x^{(0)} . Defina uma toler\u00e2ncia para o crit\u00e9rio de parada. Realize a itera\u00e7\u00e3o at\u00e9 o crit\u00e9rio de parada. Lembre de pr\u00e9-calcular as matrizes D^{-1}(L + U) e D^{-1}b . Observa\u00e7\u00e3o: Na nota\u00e7\u00e3o do livro, troca-se L e U por -L e -U para n\u00e3o aparecer o sinal na express\u00e3o acima. Acelerar a converg\u00eancia: Colocar a_{ii} o m\u00e1ximo poss\u00edvel acelera a converg\u00eancia do m\u00e9todo. M\u00e9todo de Gauss-Seidel Observe que quando iteramos o processo no m\u00e9todo de Jacobi, fixamos x^{(k-1)} e fazemos a opera\u00e7\u00e3o x^{(k)} . Por esse motivo, podemos utilizar os valores j\u00e1 calculados x_1^{(k)}, \\dots, x_{i-1}^{(k)} para atualizar o valor de x_i^{(k)} . Assim x_i^{(k)} = \\frac{1}{a_{ii}} \\left[ -\\sum_{j=1}^{i-1} \\left(a_{ij} x_j^{(k)}\\right) - \\sum_{j=i+1}^{n} \\left(a_{ij} x_j^{(k-1)}\\right) + b_i\\right] Essa simples modifica\u00e7\u00e3o gerou o m\u00e9todo de Gauss-Seidel. Para obter essa equa\u00e7\u00e3o em formato matricial, precisamos de \\sum_{j=1}^{i} \\left(a_{ij} x_j^{(k)}\\right) = - \\sum_{j=i+1}^{n} \\left(a_{ij} x_j^{(k-1)}\\right) + b_i Note que para cada linha i , estamos tomando todas as colunas j \\le i no lado esquerdo. No lado direito \u00e9 o contr\u00e1rio. Assim, (D + L)x^{(k)} = Ux^{(k-1)} + b \\implies x^{(k)} = -(D+L)^{-1}Ux^{(k-1)} + (D + L)^{-1}b O processo iterativo \u00e9 similar ao M\u00e9todo de Jordan. Teorema (Stein-Rosenberg): Se a_{ij} \\le 0 para cada i \\neq j e a_{ii} > 0 , para cada i=1,2,\\dots,n , ent\u00e3o uma, e somente uma, das afirma\u00e7\u00f5es vale: (i) 0 \\le \\rho(T_g) < \\rho(T_j) < 1 ; (ii) 1 < \\rho(T_j) < \\rho(T_g) ; (iii) \\rho(T_g) = \\rho(T_j) = 0 ; (iv) \\rho(T_g) = \\rho(T_j) = 1 , em que T_j = -D^{-1}(L+U) e T_g = -(D+L)^{-1}U s\u00e3o as matrizes dos m\u00e9todos de Jacobi e Gauss-Seidel. Converg\u00eancia Seja Ax = b o sistema com solu\u00e7\u00e3o x^* e o m\u00e9todo iterativo x^{k+1} = Cx^k +D com x^* = Cx^* + D . Assim se ||C|| < 1 , o m\u00e9todo converge para a solu\u00e7\u00e3o com os seguintes limites no erro: (i) ||x^* - x^{k}|| \\le ||C||^k||x^* - x^0|| . (ii) ||x^* - x^{k}|| \\le \\frac{||C||^k}{1 - ||C||}||x^1 - x^0|| . Assim, percebemos que a converg\u00eancia depende de ||C|| \\approx \\rho(C) (isso \u00e9 verdade porque para todo \\epsilon > 0 , existe uma norma matricial natural tal que \\rho(C) < ||C|| < \\rho(C) + \\epsilon . M\u00e9todo Successive Over-Relaxation (SOR) Baseada na an\u00e1lise de converg\u00eancia da \u00faltima se\u00e7\u00e3o, estamos interessados em minimizar \\rho(C) de maneira geral. Para isso introduz-se o SOR. O vetor res\u00edduo \u00e9 dado por r = b - A\\tilde{x} , em que \\tilde{x} \u00e9 uma aproxima\u00e7\u00e3o para a solu\u00e7\u00e3o de Ax = b . Seja r_{ii}^{(k)} = (r_{1i}^{(k)}, r_{2i}^{(k)}, \\dots, r_{ni}^{(k)}) o vetor res\u00edduo para x_{i}^{(k)} = (x_1^{(k)}, \\dots, x_{i-1}^{(k)}, x_i^{(k-1)}, \\dots, x_n^{(k)}) . Em particular, o m\u00e9todo de Gauss-Seidel pode ser reescrito de forma a x_{i}^{(k)} = x_i^{(k-1)} + \\frac{r_{ii}^{(k)}}{a_{ii}}. Para isso, a ideia ser\u00e1 escolher \\omega de forma a acelerar a converg\u00eancia e x_{i}^{(k)} = x_i^{(k-1)} + \\omega\\frac{r_{ii}^{(k)}}{a_{ii}}. Se \\omega \\in (0,1) , o m\u00e9todo \u00e9 sob-relaxamento. Se \\omega \\in (1,2) , o m\u00e9todo \u00e9 sobre-relaxamento. Em formato iterativo, x_i^{(k)} = (1 - \\omega)x_{i}^{(k-1)} + \\frac{\\omega}{a_{ii}}\\left[b_i - \\sum_{j-1}^{i-1} a_{ij}x_j^{(k)} - \\sum_{j=i+1}^n a_{ij}x_{j}^{(k-1)} \\right] que em formato matricial se reduz a x^{(k)} = (D + \\omega L)^{-1}[(1-\\omega)D - \\omega U]x^{(k-1)} + \\omega(D + \\omega L)^{-1}b. Teorema (Kahan): Se a_{ii} \\neq 0 para todo i=1,\\dots,n , ent\u00e3o o m\u00e9todo converge somente se 0 < \\omega < 2 . Esse resultado pode ser obtido calculando o raio espectral da matriz da itera\u00e7\u00e3o SOR. Teorema (Ostrowski-Reich): Se A \u00e9 matriz positiva definida e 0 < \\omega < 2 , ent\u00e3o o m\u00e9todo SOR converge para todo x^{(0)} . Por fim, \u00e9 importante destacar que para minimizarmos o raio espectral, a escolha \u00f3tima de \\omega \u00e9 dada por \\omega = \\frac{2}{1 + \\sqrt{1 - [\\rho(T_j)]^2}}, em que T_j \u00e9 a matriz de itera\u00e7\u00e3o de Jacobi. Gradiente Conjugado","title":"M\u00e9todos iterativos para resolver sistemas lineares"},{"location":"analisenum/linear-systems/#metodos-iterativos-para-resolver-sistemas-lineares","text":"Suponha que queremos resolver um problema do tipo Ax = b , em que A \u00e9 uma matriz real n \\times n e b \u00e9 um vetor em \\mathbb{R}^n . Matematicamente, se estamos interessados em encontrar x , podemos apenas calcular a inversa de A , caso exista. Se sim x = A^{-1}b . Apesar de ser f\u00e1cil de calcular, esse procedimento precisa fazer O(n^3) opera\u00e7\u00f5es, o que pode ser muito custoso quando n aumenta. Nesse caso, precisamos de alternativas mais palat\u00e1veis para resolver esse problema, principalmente para n grande. Utilizamos m\u00e9todos iterativos para ajudar! De forma geral, vamos querer reescrever o problema da forma: x = Cx + D, em que s\u00e3o matrizes. Se conseguirmos expressar Ax = b dessa forma, estaremos interessados nos pontos fixos do operador L(x) = Cx + D . Teorema: O processo iterativo x^{k+1} = Cx^k + D satisfaz o seguinte: Para todo valor inicial x^0 , a sequ\u00eancia \\{x^k\\}_{k \\in \\mathbb{N}} converge para o ponto fixo x^* = Cx^* + D se, e somente se, \\rho(C) < 1 , em que \\rho(C) \u00e9 o raio espectral da matriz C , isto \u00e9, o maior autovalor em m\u00f3dulo. Uma demonstra\u00e7\u00e3o desse resultado pode ser encontrado no livro de Richard L.Burden Numerical Analysis (p\u00e1gina 457). Corol\u00e1rio: Se ||C|| < 1 para qualquer norma matricial induzida (induzida por uma norma vetorial), ent\u00e3o a itera\u00e7\u00e3o anterior converge para o ponto fixo do operador L para qualquer chute inicial x^0 . Esse resultado \u00e9 uma consequ\u00eancia de \\rho(A) \\le ||A|| para todo norma natural ||\\cdot|| . M\u00e9todo de Jacobi M\u00e9todo de Gauss-Seidel M\u00e9todo Successive Over-Relaxation (SOR) M\u00e9todo Gradiente Conjugado Teorema: Se A \u00e9 diagonalmente estritamente dominante, ent\u00e3o para qualquer escolha de x^0 , os m\u00e9todos de Jacobi e Gauss-Seidel convergem.","title":"M\u00e9todos iterativos para resolver sistemas lineares"},{"location":"analisenum/linear-systems/#metodo-de-jacobi","text":"Esse m\u00e9todo \u00e9 derivado resolvendo a i th equa\u00e7\u00e3o de Ax = b para x_i (dado que a_{ii} \\neq 0 ) x_i = \\sum_{j\\neq i} \\left(-\\frac{a_{ij}x_j}{a_{ii}}\\right) + \\frac{b_i}{a_{ii}}, ~~~ \\text{ for } i = 1, 2, \\dots, n. Assim, geramos iterativamente, x_i^{(k)} = \\frac{1}{a_{ii}} \\left[ \\sum_{j\\neq i} \\left(-a_{ij} x_j^{(k-1)}\\right) + b_i\\right] Vamos escrever em formato matricial. Observe que x^{(k)} = \\begin{bmatrix} 0 & -a_{12}/a_{11} & -a_{13}/a_{11} & \\dots & -a_{1n}/a_{11} \\\\ -a_{21}/a_{22} & 0 & -a_{23}/a_{22} & \\dots & -a_{2n}/a_{22} \\\\ -a_{31}/a_{33} & -a_{32}/a_{33} & 0 & \\dots & -a_{3n}/a_{33} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ -a_{n1}/a_{nn} & -a_{n2}/a_{nn} & -a_{n3}/a_{nn} & \\dots & 0\\\\ \\end{bmatrix}x^{(k-1)} + \\begin{bmatrix} b_1/a_{11} \\\\ b_2/a_{22} \\\\ b_3/a_{33} \\\\ \\vdots \\\\ b_n/a_{nn} \\end{bmatrix} e sendo U e L as matrizes triangulares superiores e inferiores, e D a matriz diagonal de A , x^{(k)} = -D^{-1}(L + U)x^{(k-1)} + D^{-1}b Assim, o procedimento Jacobi pode ser sumarizado da seguinte forma: Sejam as entradas as matrizes A e b com uma aproxima\u00e7\u00e3o inicial x^{(0)} . Defina uma toler\u00e2ncia para o crit\u00e9rio de parada. Realize a itera\u00e7\u00e3o at\u00e9 o crit\u00e9rio de parada. Lembre de pr\u00e9-calcular as matrizes D^{-1}(L + U) e D^{-1}b . Observa\u00e7\u00e3o: Na nota\u00e7\u00e3o do livro, troca-se L e U por -L e -U para n\u00e3o aparecer o sinal na express\u00e3o acima. Acelerar a converg\u00eancia: Colocar a_{ii} o m\u00e1ximo poss\u00edvel acelera a converg\u00eancia do m\u00e9todo.","title":"M\u00e9todo de Jacobi"},{"location":"analisenum/linear-systems/#metodo-de-gauss-seidel","text":"Observe que quando iteramos o processo no m\u00e9todo de Jacobi, fixamos x^{(k-1)} e fazemos a opera\u00e7\u00e3o x^{(k)} . Por esse motivo, podemos utilizar os valores j\u00e1 calculados x_1^{(k)}, \\dots, x_{i-1}^{(k)} para atualizar o valor de x_i^{(k)} . Assim x_i^{(k)} = \\frac{1}{a_{ii}} \\left[ -\\sum_{j=1}^{i-1} \\left(a_{ij} x_j^{(k)}\\right) - \\sum_{j=i+1}^{n} \\left(a_{ij} x_j^{(k-1)}\\right) + b_i\\right] Essa simples modifica\u00e7\u00e3o gerou o m\u00e9todo de Gauss-Seidel. Para obter essa equa\u00e7\u00e3o em formato matricial, precisamos de \\sum_{j=1}^{i} \\left(a_{ij} x_j^{(k)}\\right) = - \\sum_{j=i+1}^{n} \\left(a_{ij} x_j^{(k-1)}\\right) + b_i Note que para cada linha i , estamos tomando todas as colunas j \\le i no lado esquerdo. No lado direito \u00e9 o contr\u00e1rio. Assim, (D + L)x^{(k)} = Ux^{(k-1)} + b \\implies x^{(k)} = -(D+L)^{-1}Ux^{(k-1)} + (D + L)^{-1}b O processo iterativo \u00e9 similar ao M\u00e9todo de Jordan. Teorema (Stein-Rosenberg): Se a_{ij} \\le 0 para cada i \\neq j e a_{ii} > 0 , para cada i=1,2,\\dots,n , ent\u00e3o uma, e somente uma, das afirma\u00e7\u00f5es vale: (i) 0 \\le \\rho(T_g) < \\rho(T_j) < 1 ; (ii) 1 < \\rho(T_j) < \\rho(T_g) ; (iii) \\rho(T_g) = \\rho(T_j) = 0 ; (iv) \\rho(T_g) = \\rho(T_j) = 1 , em que T_j = -D^{-1}(L+U) e T_g = -(D+L)^{-1}U s\u00e3o as matrizes dos m\u00e9todos de Jacobi e Gauss-Seidel.","title":"M\u00e9todo de Gauss-Seidel"},{"location":"analisenum/linear-systems/#convergencia","text":"Seja Ax = b o sistema com solu\u00e7\u00e3o x^* e o m\u00e9todo iterativo x^{k+1} = Cx^k +D com x^* = Cx^* + D . Assim se ||C|| < 1 , o m\u00e9todo converge para a solu\u00e7\u00e3o com os seguintes limites no erro: (i) ||x^* - x^{k}|| \\le ||C||^k||x^* - x^0|| . (ii) ||x^* - x^{k}|| \\le \\frac{||C||^k}{1 - ||C||}||x^1 - x^0|| . Assim, percebemos que a converg\u00eancia depende de ||C|| \\approx \\rho(C) (isso \u00e9 verdade porque para todo \\epsilon > 0 , existe uma norma matricial natural tal que \\rho(C) < ||C|| < \\rho(C) + \\epsilon .","title":"Converg\u00eancia"},{"location":"analisenum/linear-systems/#metodo-successive-over-relaxation-sor","text":"Baseada na an\u00e1lise de converg\u00eancia da \u00faltima se\u00e7\u00e3o, estamos interessados em minimizar \\rho(C) de maneira geral. Para isso introduz-se o SOR. O vetor res\u00edduo \u00e9 dado por r = b - A\\tilde{x} , em que \\tilde{x} \u00e9 uma aproxima\u00e7\u00e3o para a solu\u00e7\u00e3o de Ax = b . Seja r_{ii}^{(k)} = (r_{1i}^{(k)}, r_{2i}^{(k)}, \\dots, r_{ni}^{(k)}) o vetor res\u00edduo para x_{i}^{(k)} = (x_1^{(k)}, \\dots, x_{i-1}^{(k)}, x_i^{(k-1)}, \\dots, x_n^{(k)}) . Em particular, o m\u00e9todo de Gauss-Seidel pode ser reescrito de forma a x_{i}^{(k)} = x_i^{(k-1)} + \\frac{r_{ii}^{(k)}}{a_{ii}}. Para isso, a ideia ser\u00e1 escolher \\omega de forma a acelerar a converg\u00eancia e x_{i}^{(k)} = x_i^{(k-1)} + \\omega\\frac{r_{ii}^{(k)}}{a_{ii}}. Se \\omega \\in (0,1) , o m\u00e9todo \u00e9 sob-relaxamento. Se \\omega \\in (1,2) , o m\u00e9todo \u00e9 sobre-relaxamento. Em formato iterativo, x_i^{(k)} = (1 - \\omega)x_{i}^{(k-1)} + \\frac{\\omega}{a_{ii}}\\left[b_i - \\sum_{j-1}^{i-1} a_{ij}x_j^{(k)} - \\sum_{j=i+1}^n a_{ij}x_{j}^{(k-1)} \\right] que em formato matricial se reduz a x^{(k)} = (D + \\omega L)^{-1}[(1-\\omega)D - \\omega U]x^{(k-1)} + \\omega(D + \\omega L)^{-1}b. Teorema (Kahan): Se a_{ii} \\neq 0 para todo i=1,\\dots,n , ent\u00e3o o m\u00e9todo converge somente se 0 < \\omega < 2 . Esse resultado pode ser obtido calculando o raio espectral da matriz da itera\u00e7\u00e3o SOR. Teorema (Ostrowski-Reich): Se A \u00e9 matriz positiva definida e 0 < \\omega < 2 , ent\u00e3o o m\u00e9todo SOR converge para todo x^{(0)} . Por fim, \u00e9 importante destacar que para minimizarmos o raio espectral, a escolha \u00f3tima de \\omega \u00e9 dada por \\omega = \\frac{2}{1 + \\sqrt{1 - [\\rho(T_j)]^2}}, em que T_j \u00e9 a matriz de itera\u00e7\u00e3o de Jacobi.","title":"M\u00e9todo Successive Over-Relaxation (SOR)"},{"location":"analisenum/linear-systems/#gradiente-conjugado","text":"","title":"Gradiente Conjugado"},{"location":"analisenum/lista2/","text":"Lista 2 - An\u00e1lise Num\u00e9rica Escola de Matem\u00e1tica Aplicada, Funda\u00e7\u00e3o Getulio Vargas Professor: Hugo A. de la Cruz Cancino Monitor: Lucas Machado Moschen Data da entrega: 12/09/2021 Considere a matriz diagonal por blocos A \\in \\mathbb{R}^{m^2 \\times m^2} A = \\begin{bmatrix} T & -I & 0 & 0 & \\dots & 0 \\\\ -I & T & -I & 0 & \\dots & 0 \\\\ 0 & -I & T & -I & \\dots & 0 \\\\ &&\\dots&\\dots&& \\\\ 0 & \\dots & 0 & 0 & -I & T \\end{bmatrix} onde I \\in \\mathbb{R}^{m\\times m} \u00e9 a matriz identidade T \\in \\mathbb{R}^{m\\times m} \u00e9 dada por T = \\begin{bmatrix} 4 & -1 & 0 & 0 & \\dots & 0 \\\\ -1 & 4 & -1 & 0 & \\dots & 0 \\\\ 0 & -1 & 4 & -1 & \\dots & 0 \\\\ &&\\dots&\\dots&& \\\\ 0 & \\dots & 0 & 0 & -1 & 4 \\end{bmatrix} (a) O m\u00e9todo de Gauss-Seidel aplicado ao sistema Ax = b (b \\in R^m) \u00e9 convergente \\forall m \\in N ? Justifique bem sua resposta. Existem algumas formas de verificar esse resultado usando alguns resultados da literatura. De forma geral para m\u00e9todos iterativos, podemos fazer: (i) Mostrar que a matriz do m\u00e9todo T = -(D + L)^{-1}U tem o maior autovalor em m\u00f3dulo \u00e9 menor do que 1. Seja x autovetor de T cujo autovalor correspondente \u00e9 \\lambda . Assim -(D+L)^{-1}Ux = \\lambda x \\implies -Ux = \\lambda(D+ L)x . De fato, ter\u00edamos que ||U||||x|| \\ge |\\lambda|||(D+L)x|| para todo autovetor x , que pode ser considerado de norma 1. Assim, se consegu\u00edssemos mostrar que ||U||/||(D+L)x|| < 1 para todo autovetor x , ter\u00edamos que \\rho(T) < 1 . Se tomarmos a norma m\u00e1ximo, \u00e9 f\u00e1cil ver que ||U|| = 6 . Ser\u00e1 que \u00e9 f\u00e1cil mostrar que ||(D+L)x|| > 6 ? Eu acredito que n\u00e3o. Para isso ter\u00edamos que encontrar o formato dos autovalores, pois, de forma geral, se x \u00e9 um vetor qualquer, essa desigualdade n\u00e3o \u00e9 observada. (ii) Mostrar que ||T|| = ||(D+L)^{-1}U|| < 1 para alguma norma induzida. Infelizmente, isso tamb\u00e9m n\u00e3o parece trivial de se fazer, mesmo sabendo que a inversa de uma matriz triangular inferior \u00e9 triangular inferior. (iii) A matriz A \u00e9 estritamente diagonalmente dominante: isso n\u00e3o \u00e9 verdade para essa matriz. (iv) A matriz A \u00e9 irredutivelmente diagonalmente dominante: aqui precisamos mostrar que ela \u00e9 diagonalmente dominante (o que de fato \u00e9, pois a diagonal \u00e9 4, enquanto ou outros elementos da mesma linha s\u00e3o no m\u00e1ximo quatro valores -1) e que pelo menos uma linha vale a desigualdade estrita, o que vale para a primeira linha. Assim j\u00e1 provamos que vale a converg\u00eancia de Gauss-Seidel. Para verificar esse teorema, esse artigo pode ser um bom in\u00edcio. (v) A matriz A \u00e9 positiva definida. Provamos que se 0 < \\omega < 2 , o m\u00e9todo SOR converge nesse caso. Como Gauss-Seidel \u00e9 um casso particular \\omega = 1 , basta verificar a positividade. Um bom material nesse sentido \u00e9 esse aqui . Nesse caso, procurar os autovalores de A (que t\u00eam um formato bem conveniente) e mostrar que eles s\u00e3o positivos \u00e9 uma boa. (b) Uma boa escolha do par\u00e2metro \\omega no m\u00e9todo SOR pode levar a uma converg\u00eancia mais r\u00e1pida, comparado com Jacobi e Seidel. Em geral determinar o valor \u00f3timo de \\omega n\u00e3o \u00e9 um problema f\u00e1cil, mas em alguns casos em que a matriz do sistema tem uma estrutura espec\u00edfica \u00e9 poss\u00edvel achar esse valor \u00f3timo \\omega \u00f3timo. Por exemplo, para a matriz A acima \u00e9 conhecido que \\omega_{\\mathrm{otimo}} = \\frac{2}{1 + \\sin\\left(\\frac{\\pi}{m+1}\\right)}. Implemente o m\u00e9todo SOR para resolver o sistema Ax = b ; onde b \u00e9 um vetor tal que o sistema tem a solu\u00e7\u00e3o exata x = (2, 2, \\dots, 2) . Compare a performance do m\u00e9todo SOR usando 4 valores diferentes do par\u00e2metro \\omega : i) \\omega = \\omega_{\\mathrm{otimo}} ii) \\omega = 1 (o m\u00e9todo de Seidel) iii) \\omega = 0.5 iv) \\omega = 0 , para 4 valores diferentes de m : m = 50, 100, 1000, 5000 . Como crit\u00e9rio de compara\u00e7\u00e3o use o n\u00famero de itera\u00e7\u00f5es necess\u00e1rias para que o erro na norma \\infty seja \\le 10^{-6} . Comente sobre os resultados obtidos. A implementa\u00e7\u00e3o desse problema pode ser encontrado na pasta do reposit\u00f3rio . Vamos lembrar que a itera\u00e7\u00e3o do m\u00e9todo SOR \u00e9 dada por x_i^{(k)} = (1 - \\omega)x_{i}^{(k-1)} + \\frac{\\omega}{a_{ii}}\\left[b_i - \\sum_{j-1}^{i-1} a_{ij}x_j^{(k)} - \\sum_{j=i+1}^n a_{ij}x_{j}^{(k-1)} \\right] Para essa matriz em particular, temos uma estrutura interessante. Observe que para linha i temos: Um valor 4 na diagonal, isto \u00e9, a_{ii} = 4 . Um valor -1 \u00e0 esquerda de 4 (exceto na primeira linha de cada T ), isto \u00e9, a_{i,i-1} = -1 se i \\neq 1 \\mod m . Um valor -1 \u00e0 direita de 4 (exceto na \u00faltima linha de cada T ), isto \u00e9, a_{i,i+1} = -1 se i \\neq -1 \\mod m . Um valor -1 na identidade ao lado esquerdo de T , isto \u00e9, a_{i,i-m} = -1 se i > m . Um valor -1 na identidade ao lado direito de T , isto \u00e9, a_{i,i+m} = -1 se i + m \\le m^2 . A itera\u00e7\u00e3o se reduz a x_i^{(k)} = (1 - \\omega)x_{i}^{(k-1)} + \\frac{\\omega}{4}\\left[b_i + x_{i-1}^{(k)} + x_{i-m}^{(k)} + x_{i+1}^{(k-1)} + x_{i+m}^{(k-1)} \\right], com as restri\u00e7\u00f5es j\u00e1 citadas a cima. Al\u00e9m disso, b tamb\u00e9m tem uma estrutura bem particular que pode ser tamb\u00e9m utilizada. Multiplicando pelo vetor com s\u00f3 valores 1 faz com que somemos as linhas. O que fazemos ent\u00e3o \u00e9 subtrair de 4 o n\u00famero de condi\u00e7\u00f5es verdadeiras: i \\neq 1 \\mod m , i \\neq -1 \\mod m , i > m , e i \\le m^2 - m . Uma visualiza\u00e7\u00e3o interessante do processo que podemos ter \u00e9 sobre o vetor x . Ele \u00e9 um vetor em \\mathbb{R}^{m^2} . Por\u00e9m podemos imagina-lo como uma matriz \\mathbb{R}^{m \\times m} . Nesse caso se j = mq + r , dizemos que x_j = x_{q+1,r} a menos que j = mq . Nesse caso x_j = x_{q,m} . Com essa estrutura, veja que x_{i-m} fica na mesma coluna de x_i , mas uma linha acima. Isso nos permite escrever x_{i,j}^{(k)} = x_{i,j}^{(k-1)} + \\frac{\\omega}{4}\\left[x_{i, j-1}^{(k)} + x_{i-1,j}^{(k)} + x_{i,j+1}^{(k-1)} + x_{i+1,j}^{(k-1)} + b_{i,j} - 4x_{i,j}^{(k-1)}\\right], e as condi\u00e7\u00f5es se restringem a condi\u00e7\u00f5es de borda (isto \u00e9, 1 \\le i,j \\le m ). Do jeito que calculamos agora, n\u00e3o \u00e9 poss\u00edvel paralelizar o c\u00e1lculo. Para fazer isso, uma estrat\u00e9gia \u00e9 montar o Grid Red-Black . Esse problema \u00e9 muito custoso quando m cresce. Em particular, colocando x_0 a uma dist\u00e2ncia de Normal(0, sd = 0.01), para fazer a seguinte figura, levou 11s. Por\u00e9m, para m = 1000 , esse tempo j\u00e1 foi muito superior. Outras quest\u00f5es da lista M\u00e9todos iterativos para sistemas de equa\u00e7\u00f5es lineares","title":"Lista 2 - An\u00e1lise Num\u00e9rica"},{"location":"analisenum/lista2/#lista-2-analise-numerica","text":"Escola de Matem\u00e1tica Aplicada, Funda\u00e7\u00e3o Getulio Vargas Professor: Hugo A. de la Cruz Cancino Monitor: Lucas Machado Moschen Data da entrega: 12/09/2021 Considere a matriz diagonal por blocos A \\in \\mathbb{R}^{m^2 \\times m^2} A = \\begin{bmatrix} T & -I & 0 & 0 & \\dots & 0 \\\\ -I & T & -I & 0 & \\dots & 0 \\\\ 0 & -I & T & -I & \\dots & 0 \\\\ &&\\dots&\\dots&& \\\\ 0 & \\dots & 0 & 0 & -I & T \\end{bmatrix} onde I \\in \\mathbb{R}^{m\\times m} \u00e9 a matriz identidade T \\in \\mathbb{R}^{m\\times m} \u00e9 dada por T = \\begin{bmatrix} 4 & -1 & 0 & 0 & \\dots & 0 \\\\ -1 & 4 & -1 & 0 & \\dots & 0 \\\\ 0 & -1 & 4 & -1 & \\dots & 0 \\\\ &&\\dots&\\dots&& \\\\ 0 & \\dots & 0 & 0 & -1 & 4 \\end{bmatrix} (a) O m\u00e9todo de Gauss-Seidel aplicado ao sistema Ax = b (b \\in R^m) \u00e9 convergente \\forall m \\in N ? Justifique bem sua resposta. Existem algumas formas de verificar esse resultado usando alguns resultados da literatura. De forma geral para m\u00e9todos iterativos, podemos fazer: (i) Mostrar que a matriz do m\u00e9todo T = -(D + L)^{-1}U tem o maior autovalor em m\u00f3dulo \u00e9 menor do que 1. Seja x autovetor de T cujo autovalor correspondente \u00e9 \\lambda . Assim -(D+L)^{-1}Ux = \\lambda x \\implies -Ux = \\lambda(D+ L)x . De fato, ter\u00edamos que ||U||||x|| \\ge |\\lambda|||(D+L)x|| para todo autovetor x , que pode ser considerado de norma 1. Assim, se consegu\u00edssemos mostrar que ||U||/||(D+L)x|| < 1 para todo autovetor x , ter\u00edamos que \\rho(T) < 1 . Se tomarmos a norma m\u00e1ximo, \u00e9 f\u00e1cil ver que ||U|| = 6 . Ser\u00e1 que \u00e9 f\u00e1cil mostrar que ||(D+L)x|| > 6 ? Eu acredito que n\u00e3o. Para isso ter\u00edamos que encontrar o formato dos autovalores, pois, de forma geral, se x \u00e9 um vetor qualquer, essa desigualdade n\u00e3o \u00e9 observada. (ii) Mostrar que ||T|| = ||(D+L)^{-1}U|| < 1 para alguma norma induzida. Infelizmente, isso tamb\u00e9m n\u00e3o parece trivial de se fazer, mesmo sabendo que a inversa de uma matriz triangular inferior \u00e9 triangular inferior. (iii) A matriz A \u00e9 estritamente diagonalmente dominante: isso n\u00e3o \u00e9 verdade para essa matriz. (iv) A matriz A \u00e9 irredutivelmente diagonalmente dominante: aqui precisamos mostrar que ela \u00e9 diagonalmente dominante (o que de fato \u00e9, pois a diagonal \u00e9 4, enquanto ou outros elementos da mesma linha s\u00e3o no m\u00e1ximo quatro valores -1) e que pelo menos uma linha vale a desigualdade estrita, o que vale para a primeira linha. Assim j\u00e1 provamos que vale a converg\u00eancia de Gauss-Seidel. Para verificar esse teorema, esse artigo pode ser um bom in\u00edcio. (v) A matriz A \u00e9 positiva definida. Provamos que se 0 < \\omega < 2 , o m\u00e9todo SOR converge nesse caso. Como Gauss-Seidel \u00e9 um casso particular \\omega = 1 , basta verificar a positividade. Um bom material nesse sentido \u00e9 esse aqui . Nesse caso, procurar os autovalores de A (que t\u00eam um formato bem conveniente) e mostrar que eles s\u00e3o positivos \u00e9 uma boa. (b) Uma boa escolha do par\u00e2metro \\omega no m\u00e9todo SOR pode levar a uma converg\u00eancia mais r\u00e1pida, comparado com Jacobi e Seidel. Em geral determinar o valor \u00f3timo de \\omega n\u00e3o \u00e9 um problema f\u00e1cil, mas em alguns casos em que a matriz do sistema tem uma estrutura espec\u00edfica \u00e9 poss\u00edvel achar esse valor \u00f3timo \\omega \u00f3timo. Por exemplo, para a matriz A acima \u00e9 conhecido que \\omega_{\\mathrm{otimo}} = \\frac{2}{1 + \\sin\\left(\\frac{\\pi}{m+1}\\right)}. Implemente o m\u00e9todo SOR para resolver o sistema Ax = b ; onde b \u00e9 um vetor tal que o sistema tem a solu\u00e7\u00e3o exata x = (2, 2, \\dots, 2) . Compare a performance do m\u00e9todo SOR usando 4 valores diferentes do par\u00e2metro \\omega : i) \\omega = \\omega_{\\mathrm{otimo}} ii) \\omega = 1 (o m\u00e9todo de Seidel) iii) \\omega = 0.5 iv) \\omega = 0 , para 4 valores diferentes de m : m = 50, 100, 1000, 5000 . Como crit\u00e9rio de compara\u00e7\u00e3o use o n\u00famero de itera\u00e7\u00f5es necess\u00e1rias para que o erro na norma \\infty seja \\le 10^{-6} . Comente sobre os resultados obtidos. A implementa\u00e7\u00e3o desse problema pode ser encontrado na pasta do reposit\u00f3rio . Vamos lembrar que a itera\u00e7\u00e3o do m\u00e9todo SOR \u00e9 dada por x_i^{(k)} = (1 - \\omega)x_{i}^{(k-1)} + \\frac{\\omega}{a_{ii}}\\left[b_i - \\sum_{j-1}^{i-1} a_{ij}x_j^{(k)} - \\sum_{j=i+1}^n a_{ij}x_{j}^{(k-1)} \\right] Para essa matriz em particular, temos uma estrutura interessante. Observe que para linha i temos: Um valor 4 na diagonal, isto \u00e9, a_{ii} = 4 . Um valor -1 \u00e0 esquerda de 4 (exceto na primeira linha de cada T ), isto \u00e9, a_{i,i-1} = -1 se i \\neq 1 \\mod m . Um valor -1 \u00e0 direita de 4 (exceto na \u00faltima linha de cada T ), isto \u00e9, a_{i,i+1} = -1 se i \\neq -1 \\mod m . Um valor -1 na identidade ao lado esquerdo de T , isto \u00e9, a_{i,i-m} = -1 se i > m . Um valor -1 na identidade ao lado direito de T , isto \u00e9, a_{i,i+m} = -1 se i + m \\le m^2 . A itera\u00e7\u00e3o se reduz a x_i^{(k)} = (1 - \\omega)x_{i}^{(k-1)} + \\frac{\\omega}{4}\\left[b_i + x_{i-1}^{(k)} + x_{i-m}^{(k)} + x_{i+1}^{(k-1)} + x_{i+m}^{(k-1)} \\right], com as restri\u00e7\u00f5es j\u00e1 citadas a cima. Al\u00e9m disso, b tamb\u00e9m tem uma estrutura bem particular que pode ser tamb\u00e9m utilizada. Multiplicando pelo vetor com s\u00f3 valores 1 faz com que somemos as linhas. O que fazemos ent\u00e3o \u00e9 subtrair de 4 o n\u00famero de condi\u00e7\u00f5es verdadeiras: i \\neq 1 \\mod m , i \\neq -1 \\mod m , i > m , e i \\le m^2 - m . Uma visualiza\u00e7\u00e3o interessante do processo que podemos ter \u00e9 sobre o vetor x . Ele \u00e9 um vetor em \\mathbb{R}^{m^2} . Por\u00e9m podemos imagina-lo como uma matriz \\mathbb{R}^{m \\times m} . Nesse caso se j = mq + r , dizemos que x_j = x_{q+1,r} a menos que j = mq . Nesse caso x_j = x_{q,m} . Com essa estrutura, veja que x_{i-m} fica na mesma coluna de x_i , mas uma linha acima. Isso nos permite escrever x_{i,j}^{(k)} = x_{i,j}^{(k-1)} + \\frac{\\omega}{4}\\left[x_{i, j-1}^{(k)} + x_{i-1,j}^{(k)} + x_{i,j+1}^{(k-1)} + x_{i+1,j}^{(k-1)} + b_{i,j} - 4x_{i,j}^{(k-1)}\\right], e as condi\u00e7\u00f5es se restringem a condi\u00e7\u00f5es de borda (isto \u00e9, 1 \\le i,j \\le m ). Do jeito que calculamos agora, n\u00e3o \u00e9 poss\u00edvel paralelizar o c\u00e1lculo. Para fazer isso, uma estrat\u00e9gia \u00e9 montar o Grid Red-Black . Esse problema \u00e9 muito custoso quando m cresce. Em particular, colocando x_0 a uma dist\u00e2ncia de Normal(0, sd = 0.01), para fazer a seguinte figura, levou 11s. Por\u00e9m, para m = 1000 , esse tempo j\u00e1 foi muito superior.","title":"Lista 2 - An\u00e1lise Num\u00e9rica"},{"location":"analisenum/lista2/#outras-questoes-da-lista","text":"M\u00e9todos iterativos para sistemas de equa\u00e7\u00f5es lineares","title":"Outras quest\u00f5es da lista"},{"location":"analisenum/non_linear_equations/","text":"Solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares Considere uma popula\u00e7\u00e3o com crescimento proporcional ao seu tamanho a cada tempo t sujeita a migra\u00e7\u00f5es constantes, isto, se x(t) \u00e9 o tamanho da popula\u00e7\u00e3o no tempo t , \\dot{x}(t) = \\lambda x(t) + \\nu \\implies x(t) = x(0)e^{\\lambda t } + \\frac{\\nu}{\\lambda}(e^{\\lambda t} - 1) Se sabemos x(t), x(0) e \\nu , ainda n\u00e3o conseguimos resolver para \\lambda esse sistema. Obseva\u00e7\u00e3o: Muitas vezes, simplifica\u00e7\u00f5es em sistemas de equa\u00e7\u00f5es ou equa\u00e7\u00f5es podem ajudar a resolver o problema analiticamente, que \u00e9 sempre mais preciso. Nem sempre s\u00f3 jogar um sistema num solver vai resolver o problema. De forma geral, queremos resolver f(x) = 0 para f : [a,b] \\to \\mathbb{R} , em geral, precisamos que f(a) e f(b) tenham sinais diferentes e f seja pelo menos cont\u00ednua, para garantir exist\u00eancia de solu\u00e7\u00e3o atrav\u00e9s do Teorema do Valor Intermedi\u00e1rio. M\u00e9todo da Bisse\u00e7\u00e3o Esse \u00e9 um m\u00e9todo bem simples que se embasa totalmente no Teorema do Valor Intermedi\u00e1rio. Seja x^* a solu\u00e7\u00e3o do problema, isto \u00e9, f(x^*) = 0 . Tome a_1 = a, b_1 = b, x_1 = \\frac{a_1 + b_1}{2} . A partir de x_1 , verificamos o sinal de f(x_1) . Assim f(x_1) = 0 : Nesse caso a solu\u00e7\u00e3o \u00e9 x^* = x_1 . f(x_1) < 0 : Nesse caso, se f(a) < 0 e f(b) > 0 , deve haver uma solu\u00e7\u00e3o no intervalo [x_1, b] . Por isso, definimos a_2 = x_1 e b_2 = b e seguimos o procedimento. Se f(a) > 0 e f(b) < 0 , deve haver uma solu\u00e7\u00e3o em [a, x_1] e, portanto, a_2 = a, b_2 = x_1 e seguimos o procedimento. f(x_1) > 0 : Nesse caso, se f(a) > 0 e f(b) < 0 , deve haver uma solu\u00e7\u00e3o no intervalo [x_1, b] . Por isso, definimos a_2 = x_1 e b_2 = b e seguimos o procedimento. Se f(a) < 0 e f(b) > 0 , deve haver uma solu\u00e7\u00e3o em [a, x_1] e, portanto, a_2 = a, b_2 = x_1 e seguimos o procedimento. Assim, esse processo se resume a tomar o ponto m\u00e9dio do intervalo e ir cortando pela metade o intervalo a cada itera\u00e7\u00e3o. Precisamos de um crit\u00e9rio de parada melhor do que f(x_k) = 0 , pois estamos num \u00e2mbito cont\u00ednuo. Como cortamos o intervalo pela metade a cada itera\u00e7\u00e3o, \u00e9 f\u00e1cil ver que |x_n - x^*| \\le \\frac{b-a}{2^n}, \\forall n \\ge 1. Nesse caso, uma condi\u00e7\u00e3o de parada poss\u00edvel \u00e9 (b-a) \\le 2^n \\epsilon , em que \\epsilon \u00e9 a minha toler\u00e2ncia a erro. Alguns coment\u00e1rios importantes: calcular x_n = a_n + \\frac{b_n - a_n}{2} \u00e9 melhor numericamente do que x_n = (a_n + b_n)/2 . Al\u00e9m disso, cuidado com a condi\u00e7\u00e3o f(a_n)f(b_n) < 0 , pois pode haver problema de underflow na multiplica\u00e7\u00e3o. Regula-Falsi (M\u00e9todo da posi\u00e7\u00e3o falsa) Esse \u00e9 outro m\u00e9todo bem antigo, com as primeiras apari\u00e7\u00f5es nos registros babil\u00f4nicos. A ideia era encontrar x tal que ax + b = 0 . Essa ideia foi trazida para resolver o problema de encontrar ra\u00edzes de f . Nesse caso, dados os pontos (a, f(a)), (b, f(b)) , sabemos que existe x^* \\in (a,b) tal que f(x^*) = 0 quando os sinais s\u00e3o trocados e f \u00e9 cont\u00ednua. Tra\u00e7ando um segmento entre esses pontos, em algum momento ele vai atingir o eixo x e \u00e9 nesse ponto que teremos a itera\u00e7\u00e3o. A continua\u00e7\u00e3o do algoritmo \u00e9 o mesmo do m\u00e9todo da Bisse\u00e7\u00e3o, isto \u00e9, o intervalo vai sendo reduzido, n\u00e3o mais pelo ponto m\u00e9dio, mas pelo ponto de intersec\u00e7\u00e3o da reta que passa por (a,f(a)) e (b, f(b)) e o eixo x . A itera\u00e7\u00e3o desse m\u00e9todo \u00e9 dada por x_k = a_k - \\frac{f(a_k)}{f(b_k) - f(a_k)}(b_k-a_k), que pode ser reescrita como x_k = \\frac{a_k\\cdot f(b_k) - b_k\\cdot f(a_k)}{f(b_k) - f(a_k)}, em que a_k e b_k s\u00e3o obtidos conforme o m\u00e9todo da Bisse\u00e7\u00e3o. Se |f'(x)| \\ge d > 0 para todo x \\in [a,b] , asseguramos que |x^* - x_k| \\le |f(x_k)|/d. Itera\u00e7\u00e3o de Ponto Fixo Se f : \\mathbb{R}^n \\to \\mathbb{R} \u00e9 uma fun\u00e7\u00e3o, dizemos que x \u00e9 ponto fixo de f quando f(x) = x . Esse nome fica claro pois f(f(\\dots(f(x)))) = x . Note que se g(x) = f(x) - x , temos que g(x) = 0 \\iff f(x) = x , isto \u00e9, encontrar pontos fixos de f equivale a encontrar as ra\u00edzes de g . Teorema do Ponto Fixo Existem alguns teoremas de garantia de exist\u00eancia e unicidade de pontos fixos, com diferentes hip\u00f3teses. Aqui tem uma lista no Wikipedia . Teorema do Ponto Fixo de Brower Seja f cont\u00ednua em um conjunto convexo compacto C com imagem f(C) \\subseteq C . Ent\u00e3o f possui ponto fixo. Aqui uma demonstra\u00e7\u00e3o interessante com um pouquinho de topologia. No nosso caso, tomamos C = [a,b] , o intervalo limitado e fechado na reta, isto \u00e9, se f \u00e9 cont\u00ednua em [a,b] e f(x) \\in [a,b] para todo x \\in [a,b] , isto \u00e9, f([a,b]) \\subseteq [a,b] , ent\u00e3o f possui ponto fixo. Teorema do Ponto Fixo de Banach Dizemos que uma fun\u00e7\u00e3o f : X \\to X , em que X \u00e9 um espa\u00e7o normado completo \u00e9 uma contra\u00e7\u00e3o se existe L \\in [0,1) tal que ||f(x) - f(y)|| \\le L||x-y||. Como exemplo, podemos tomar X = [a,b] . Se X n\u00e3o for vazio e f for uma contra\u00e7\u00e3o, ent\u00e3o f admite um \u00fanico ponto fixo x^* . Al\u00e9m do mais, para todo x_0 \\in X , a sequ\u00eancia iniciada em x_0 que segue a itera\u00e7\u00e3o x_k = f(x_{k-1}) converge para x^* (o que permite desenvolver um m\u00e9todo). Podemos demonstrar que ||x^* - x_n|| \\le \\dfrac{L^n}{1 -L}||x_1 - x_0|| . A prova pode ser facilmente encontrada . Como j\u00e1 afirmei, se conseguirmos assegurar que f satisfaz as condi\u00e7\u00f5es do Teorema, ent\u00e3o a sequ\u00eancia x_0, x_1, x_2, \\dots, x_n, \\dots com f(x_k) = x_{k-1} converge para o ponto fixo x^* , que implica g(x^*) = 0 . M\u00e9todo de Newton-Raphson Tamb\u00e9m chamado de m\u00e9todo de Newton. Por Taylor, seja f pelo menos duas vezes deriv\u00e1vel. Assim f(x^*) = f(x) + (x^* - x)f'(x) + \\frac{(x^*-x)^2}{2}f''(x) + o((x^*-x)^2), em que o(x) \u00e9 qualquer fun\u00e7\u00e3o tal que \\lim_{x\\to 0} o(x)/x = 0 . O m\u00e9todo de Netwon assume que (x^* - x)^2 \u00e9 suficientemente pequeno, isto \u00e9, x est\u00e1 suficientemente pr\u00f3ximo de x^* . Assim, 0 \\approx f(x) + f'(x)(x^* - x) \\implies x^* \\approx x - \\frac{f(x)}{f'(x)}. Com essa ideia em mente, definimos a itera\u00e7\u00e3o x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}, em que se x_0 \u00e9 suficientemente pr\u00f3ximo de x^* , ent\u00e3o \\lim_{k \\to +\\infty} x_k = x^* . A ideia \u00e9 que as aproxima\u00e7\u00f5es s\u00e3o dadas atrav\u00e9s da tangente, isto \u00e9, x_{k+1} \u00e9 o ponto da intersec\u00e7\u00e3o do eixo x com a tangente de f no ponto x_k . Teorema de converg\u00eancia: Seja f duas vezes continuamente diferenci\u00e1vel em [a,b] . Se f'(x^*) \\neq 0 , existe \\delta > 0 tal que a sequ\u00eancia de gerada pelo m\u00e9todo de Newton converge para todo x_0 \\in [x^* - \\delta, x^* + \\delta] . A ideia dessa demonstra\u00e7\u00e3o \u00e9 introduzir a fun\u00e7\u00e3o g(x) = x -\\frac{f(x)}{f'(x)} Note que quando f(x) =0 , teremos que g(x) = x , isto \u00e9, x \u00e9 ponto fixo de g , isto \u00e9, a prova se resume a verificar as condi\u00e7\u00f5es do Teorema do Ponto Fixo de Banach para algum \\delta > 0 . A derivada de g vai ser controlada em um intervalo suficientemente pequeno. Condi\u00e7\u00e3o suficiente para converg\u00eancia: Voltando a expans\u00e3o de Taylor, f(x^*) = f(x) + (x^* - x)f'(x) + \\frac{(x^*-x)^2}{2}f''(z) para algum z entre x e x^* . Usando a itera\u00e7\u00e3o de Newton, isto \u00e9, f(x_n) = f'(x_n)(x_n - x_{n+1}) , obtemos que 0 = f'(x_n)(x^* - x_{n+1}) + \\frac{(x^*-x_n)^2}{2}f''(z_n). Definindo e_n = (x^* - x_n) , temos que e_{n+1} = \\frac{e_n^2}{2f'(x_n)}f''(z_n). Assumindo que f'(x), f''(x) \\neq 0 para x \\in [a,b] , temos que para todo x_0 tal que f(x_0)f''(x_0) > 0 , o m\u00e9todo converge, o que \u00e9 um resultado de converg\u00eancia global. Sugest\u00f5es Aplica\u00e7\u00e3o m\u00e9todo de Newton Compara\u00e7\u00e3o de m\u00e9todos num\u00e9ricos Converg\u00eancia de m\u00e9todos num\u00e9ricos . Exemplos de aplica\u00e7\u00e3o do m\u00e9todo de Newton-Raphson","title":"Solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares"},{"location":"analisenum/non_linear_equations/#solucao-de-equacoes-nao-lineares","text":"Considere uma popula\u00e7\u00e3o com crescimento proporcional ao seu tamanho a cada tempo t sujeita a migra\u00e7\u00f5es constantes, isto, se x(t) \u00e9 o tamanho da popula\u00e7\u00e3o no tempo t , \\dot{x}(t) = \\lambda x(t) + \\nu \\implies x(t) = x(0)e^{\\lambda t } + \\frac{\\nu}{\\lambda}(e^{\\lambda t} - 1) Se sabemos x(t), x(0) e \\nu , ainda n\u00e3o conseguimos resolver para \\lambda esse sistema. Obseva\u00e7\u00e3o: Muitas vezes, simplifica\u00e7\u00f5es em sistemas de equa\u00e7\u00f5es ou equa\u00e7\u00f5es podem ajudar a resolver o problema analiticamente, que \u00e9 sempre mais preciso. Nem sempre s\u00f3 jogar um sistema num solver vai resolver o problema. De forma geral, queremos resolver f(x) = 0 para f : [a,b] \\to \\mathbb{R} , em geral, precisamos que f(a) e f(b) tenham sinais diferentes e f seja pelo menos cont\u00ednua, para garantir exist\u00eancia de solu\u00e7\u00e3o atrav\u00e9s do Teorema do Valor Intermedi\u00e1rio.","title":"Solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares"},{"location":"analisenum/non_linear_equations/#metodo-da-bissecao","text":"Esse \u00e9 um m\u00e9todo bem simples que se embasa totalmente no Teorema do Valor Intermedi\u00e1rio. Seja x^* a solu\u00e7\u00e3o do problema, isto \u00e9, f(x^*) = 0 . Tome a_1 = a, b_1 = b, x_1 = \\frac{a_1 + b_1}{2} . A partir de x_1 , verificamos o sinal de f(x_1) . Assim f(x_1) = 0 : Nesse caso a solu\u00e7\u00e3o \u00e9 x^* = x_1 . f(x_1) < 0 : Nesse caso, se f(a) < 0 e f(b) > 0 , deve haver uma solu\u00e7\u00e3o no intervalo [x_1, b] . Por isso, definimos a_2 = x_1 e b_2 = b e seguimos o procedimento. Se f(a) > 0 e f(b) < 0 , deve haver uma solu\u00e7\u00e3o em [a, x_1] e, portanto, a_2 = a, b_2 = x_1 e seguimos o procedimento. f(x_1) > 0 : Nesse caso, se f(a) > 0 e f(b) < 0 , deve haver uma solu\u00e7\u00e3o no intervalo [x_1, b] . Por isso, definimos a_2 = x_1 e b_2 = b e seguimos o procedimento. Se f(a) < 0 e f(b) > 0 , deve haver uma solu\u00e7\u00e3o em [a, x_1] e, portanto, a_2 = a, b_2 = x_1 e seguimos o procedimento. Assim, esse processo se resume a tomar o ponto m\u00e9dio do intervalo e ir cortando pela metade o intervalo a cada itera\u00e7\u00e3o. Precisamos de um crit\u00e9rio de parada melhor do que f(x_k) = 0 , pois estamos num \u00e2mbito cont\u00ednuo. Como cortamos o intervalo pela metade a cada itera\u00e7\u00e3o, \u00e9 f\u00e1cil ver que |x_n - x^*| \\le \\frac{b-a}{2^n}, \\forall n \\ge 1. Nesse caso, uma condi\u00e7\u00e3o de parada poss\u00edvel \u00e9 (b-a) \\le 2^n \\epsilon , em que \\epsilon \u00e9 a minha toler\u00e2ncia a erro. Alguns coment\u00e1rios importantes: calcular x_n = a_n + \\frac{b_n - a_n}{2} \u00e9 melhor numericamente do que x_n = (a_n + b_n)/2 . Al\u00e9m disso, cuidado com a condi\u00e7\u00e3o f(a_n)f(b_n) < 0 , pois pode haver problema de underflow na multiplica\u00e7\u00e3o.","title":"M\u00e9todo da Bisse\u00e7\u00e3o"},{"location":"analisenum/non_linear_equations/#regula-falsi-metodo-da-posicao-falsa","text":"Esse \u00e9 outro m\u00e9todo bem antigo, com as primeiras apari\u00e7\u00f5es nos registros babil\u00f4nicos. A ideia era encontrar x tal que ax + b = 0 . Essa ideia foi trazida para resolver o problema de encontrar ra\u00edzes de f . Nesse caso, dados os pontos (a, f(a)), (b, f(b)) , sabemos que existe x^* \\in (a,b) tal que f(x^*) = 0 quando os sinais s\u00e3o trocados e f \u00e9 cont\u00ednua. Tra\u00e7ando um segmento entre esses pontos, em algum momento ele vai atingir o eixo x e \u00e9 nesse ponto que teremos a itera\u00e7\u00e3o. A continua\u00e7\u00e3o do algoritmo \u00e9 o mesmo do m\u00e9todo da Bisse\u00e7\u00e3o, isto \u00e9, o intervalo vai sendo reduzido, n\u00e3o mais pelo ponto m\u00e9dio, mas pelo ponto de intersec\u00e7\u00e3o da reta que passa por (a,f(a)) e (b, f(b)) e o eixo x . A itera\u00e7\u00e3o desse m\u00e9todo \u00e9 dada por x_k = a_k - \\frac{f(a_k)}{f(b_k) - f(a_k)}(b_k-a_k), que pode ser reescrita como x_k = \\frac{a_k\\cdot f(b_k) - b_k\\cdot f(a_k)}{f(b_k) - f(a_k)}, em que a_k e b_k s\u00e3o obtidos conforme o m\u00e9todo da Bisse\u00e7\u00e3o. Se |f'(x)| \\ge d > 0 para todo x \\in [a,b] , asseguramos que |x^* - x_k| \\le |f(x_k)|/d.","title":"Regula-Falsi (M\u00e9todo da posi\u00e7\u00e3o falsa)"},{"location":"analisenum/non_linear_equations/#iteracao-de-ponto-fixo","text":"Se f : \\mathbb{R}^n \\to \\mathbb{R} \u00e9 uma fun\u00e7\u00e3o, dizemos que x \u00e9 ponto fixo de f quando f(x) = x . Esse nome fica claro pois f(f(\\dots(f(x)))) = x . Note que se g(x) = f(x) - x , temos que g(x) = 0 \\iff f(x) = x , isto \u00e9, encontrar pontos fixos de f equivale a encontrar as ra\u00edzes de g .","title":"Itera\u00e7\u00e3o de Ponto Fixo"},{"location":"analisenum/non_linear_equations/#teorema-do-ponto-fixo","text":"Existem alguns teoremas de garantia de exist\u00eancia e unicidade de pontos fixos, com diferentes hip\u00f3teses. Aqui tem uma lista no Wikipedia . Teorema do Ponto Fixo de Brower Seja f cont\u00ednua em um conjunto convexo compacto C com imagem f(C) \\subseteq C . Ent\u00e3o f possui ponto fixo. Aqui uma demonstra\u00e7\u00e3o interessante com um pouquinho de topologia. No nosso caso, tomamos C = [a,b] , o intervalo limitado e fechado na reta, isto \u00e9, se f \u00e9 cont\u00ednua em [a,b] e f(x) \\in [a,b] para todo x \\in [a,b] , isto \u00e9, f([a,b]) \\subseteq [a,b] , ent\u00e3o f possui ponto fixo. Teorema do Ponto Fixo de Banach Dizemos que uma fun\u00e7\u00e3o f : X \\to X , em que X \u00e9 um espa\u00e7o normado completo \u00e9 uma contra\u00e7\u00e3o se existe L \\in [0,1) tal que ||f(x) - f(y)|| \\le L||x-y||. Como exemplo, podemos tomar X = [a,b] . Se X n\u00e3o for vazio e f for uma contra\u00e7\u00e3o, ent\u00e3o f admite um \u00fanico ponto fixo x^* . Al\u00e9m do mais, para todo x_0 \\in X , a sequ\u00eancia iniciada em x_0 que segue a itera\u00e7\u00e3o x_k = f(x_{k-1}) converge para x^* (o que permite desenvolver um m\u00e9todo). Podemos demonstrar que ||x^* - x_n|| \\le \\dfrac{L^n}{1 -L}||x_1 - x_0|| . A prova pode ser facilmente encontrada . Como j\u00e1 afirmei, se conseguirmos assegurar que f satisfaz as condi\u00e7\u00f5es do Teorema, ent\u00e3o a sequ\u00eancia x_0, x_1, x_2, \\dots, x_n, \\dots com f(x_k) = x_{k-1} converge para o ponto fixo x^* , que implica g(x^*) = 0 .","title":"Teorema do Ponto Fixo"},{"location":"analisenum/non_linear_equations/#metodo-de-newton-raphson","text":"Tamb\u00e9m chamado de m\u00e9todo de Newton. Por Taylor, seja f pelo menos duas vezes deriv\u00e1vel. Assim f(x^*) = f(x) + (x^* - x)f'(x) + \\frac{(x^*-x)^2}{2}f''(x) + o((x^*-x)^2), em que o(x) \u00e9 qualquer fun\u00e7\u00e3o tal que \\lim_{x\\to 0} o(x)/x = 0 . O m\u00e9todo de Netwon assume que (x^* - x)^2 \u00e9 suficientemente pequeno, isto \u00e9, x est\u00e1 suficientemente pr\u00f3ximo de x^* . Assim, 0 \\approx f(x) + f'(x)(x^* - x) \\implies x^* \\approx x - \\frac{f(x)}{f'(x)}. Com essa ideia em mente, definimos a itera\u00e7\u00e3o x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}, em que se x_0 \u00e9 suficientemente pr\u00f3ximo de x^* , ent\u00e3o \\lim_{k \\to +\\infty} x_k = x^* . A ideia \u00e9 que as aproxima\u00e7\u00f5es s\u00e3o dadas atrav\u00e9s da tangente, isto \u00e9, x_{k+1} \u00e9 o ponto da intersec\u00e7\u00e3o do eixo x com a tangente de f no ponto x_k . Teorema de converg\u00eancia: Seja f duas vezes continuamente diferenci\u00e1vel em [a,b] . Se f'(x^*) \\neq 0 , existe \\delta > 0 tal que a sequ\u00eancia de gerada pelo m\u00e9todo de Newton converge para todo x_0 \\in [x^* - \\delta, x^* + \\delta] . A ideia dessa demonstra\u00e7\u00e3o \u00e9 introduzir a fun\u00e7\u00e3o g(x) = x -\\frac{f(x)}{f'(x)} Note que quando f(x) =0 , teremos que g(x) = x , isto \u00e9, x \u00e9 ponto fixo de g , isto \u00e9, a prova se resume a verificar as condi\u00e7\u00f5es do Teorema do Ponto Fixo de Banach para algum \\delta > 0 . A derivada de g vai ser controlada em um intervalo suficientemente pequeno. Condi\u00e7\u00e3o suficiente para converg\u00eancia: Voltando a expans\u00e3o de Taylor, f(x^*) = f(x) + (x^* - x)f'(x) + \\frac{(x^*-x)^2}{2}f''(z) para algum z entre x e x^* . Usando a itera\u00e7\u00e3o de Newton, isto \u00e9, f(x_n) = f'(x_n)(x_n - x_{n+1}) , obtemos que 0 = f'(x_n)(x^* - x_{n+1}) + \\frac{(x^*-x_n)^2}{2}f''(z_n). Definindo e_n = (x^* - x_n) , temos que e_{n+1} = \\frac{e_n^2}{2f'(x_n)}f''(z_n). Assumindo que f'(x), f''(x) \\neq 0 para x \\in [a,b] , temos que para todo x_0 tal que f(x_0)f''(x_0) > 0 , o m\u00e9todo converge, o que \u00e9 um resultado de converg\u00eancia global.","title":"M\u00e9todo de Newton-Raphson"},{"location":"analisenum/non_linear_equations/#sugestoes","text":"Aplica\u00e7\u00e3o m\u00e9todo de Newton Compara\u00e7\u00e3o de m\u00e9todos num\u00e9ricos Converg\u00eancia de m\u00e9todos num\u00e9ricos . Exemplos de aplica\u00e7\u00e3o do m\u00e9todo de Newton-Raphson","title":"Sugest\u00f5es"},{"location":"analisenum/numeric_integration/","text":"Integra\u00e7\u00e3o Num\u00e9rica O objetivo da integra\u00e7\u00e3o num\u00e9rica \u00e9 estimar I = \\int_a^b f(x) \\, dx, principalmente quando n\u00e3o sabemos a antiderivada de f . Podemos extender para problemas de dimens\u00e3o maior, isto \u00e9, integrar I = \\int_R f(x) \\, dx, em algum ret\u00e2ngulo R em dimens\u00e3o \\mathbb{R}^n . Chamamos de quadratura num\u00e9rica o m\u00e9todo b\u00e1sico de aproxima\u00e7\u00e3o de I atrav\u00e9s da soma \\sum_{i=0}^n a_i f(x_i) . F\u00f3rmula de Newton-Cotes De forma geral, podemos integrar o polin\u00f4mio interpolador de Lagrange que aproxima para aproximar a integra\u00e7\u00e3o da fun\u00e7\u00e3o aproximada pela fun\u00e7\u00e3o, isto \u00e9, \\int_a^b f(x) \\, dx = \\int_a^b \\sum_{i=0}^n f(x_i) L_i(x) + \\int_a^b \\prod_{i=0}^n (x - x_i)\\frac{f^{(n+1)}(\\xi(x))}{(n+1)!} \\, dx em que \\{L_i(x)\\}_{i=0}^n forma a base dos polin\u00f4mios de Lagrange . Nesse caso, uma poss\u00edvel aproxima\u00e7\u00e3o para essa integral \u00e9 \\int_a^b \\sum_{i=0}^n f(x_i) L_i(x) \\, dx = \\sum_{i=0}^n a_if(x_i), em que a_i = \\int_a^b L_i(x) \\, dx . O erro dessa estima\u00e7\u00e3o \u00e9 dado por E(f) = \\int_a^b \\prod_{i=0}^n (x - x_i)\\frac{f^{(n+1)}(\\xi(x))}{(n+1)!} \\, dx. A f\u00f3rmula de Newton-Cotes (n+1) -pontos fechada usa os pontos interpoladores x_i = x_0 + ih em que h = (b-a)/n, x_0 = a e x_n = b . Ela \u00e9 dita fechada por incluir os endpoints do intervalo. Apresentamos dois m\u00e9todos com esse tipo de f\u00f3rmula. Al\u00e9m do fechado, podemos tomar os pontos igualmente espa\u00e7ados no interior do intervalo, isto \u00e9, h = (b-a)/(n+2) e x_i = x_0 + (i+1)h para i = 0, \\dots, n . Essas s\u00e3o ditas f\u00f3rmula de Newton-Cotes (n+1) -pontos aberta . Um exemplo conhecida \u00e9 a regra do ponto m\u00e9dio que toma apenas um ponto x_0 = (a + b)/2 . Trap\u00e9zio Simples Nesse caso, definimos x_0 = a, x_1 = b e h=b-a . Assim a_0 = \\int_{x_0}^{x_1} \\frac{(x - x_1)}{(x_0-x_1)} \\quad \\text{ e } a_1 = \\int_{x_0}^{x_1} \\frac{(x - x_0)}{(x_1-x_0)}. Vamos utilizar o Teorema do Valor M\u00e9dio Ponderado para Integrais (p\u00e1gina 5) para estimar o erro. Nesse caso, ele \u00e9 da seguinte forma: \\begin{split} E(f) &= \\int_{x_0}^{x_1} (x - x_0)(x - x_1)\\frac{f^{(2)}(\\xi(x))}{2!} \\, dx \\\\ &= \\frac{f^{(2)}(\\xi)}{2}\\left[\\frac{x^3}{3} - \\frac{(x_0 + x_1)}{2}x^2 + x_0x_1x\\right]_{x_0}^{x_1} \\\\ &= -\\frac{h^3}{6}f^{(2)}(\\xi), \\end{split} para algum \\xi \\in (a,b) . A regra do trap\u00e9zio se resume, ent\u00e3o, a \\int_a^b f(x) \\, dx = \\frac{h}{2}[f(a) + f(b)] - \\frac{h^3}{12}f''(\\xi). F\u00f3rmula de Simpson Simples Para a f\u00f3rmula de Simpson, fazemos x_0 = a, x_2 = b e x_1 = a + h , para h = (b-a)/2 , isto \u00e9, adicionamos mais um ponto para a aproxima\u00e7\u00e3o polinomial. Nesse caso, a_0 = \\int_{x_0}^{x_2} \\frac{(x - x_1)(x - x_2)}{(x_0-x_1)(x_0 - x_2)}, a_1 = \\int_{x_0}^{x_2} \\frac{(x - x_0)(x - x_2)}{(x_1-x_0)(x_1 - x_2)} \\text{ e }a_2 = \\int_{x_0}^{x_2} \\frac{(x - x_0)(x - x_1)}{(x_2-x_0)(x_2 - x_1)}. Com essa deriva\u00e7\u00e3o, vamos obter um erro O(h^4) com um termo relacionado a f^{(3)} . Podemos reduzir esse erro com esses mesmos pontos usando uma abordagem levemente diferente. \u00c9 usada a expans\u00e3o de Taylor em x_1 at\u00e9 ordem e um pouco de simetria x_2 - x_1 = x_1 - x_0 para verificar que \\int_{x_0}^{x_2} f(x) \\, dx = 2hf(x_1) + \\frac{h^3}{3}f''(x_1) + \\frac{f^{(4)}(\\xi')}{60}h^5. Al\u00e9m disso, \u00e9 usada a seguinte aproxima\u00e7\u00e3o para f''(x_1) : f''(x_1) = \\frac{1}{h^2}[f(x_0) - 2f(x_1) + f(x_2)] - \\frac{h^2}{12}f^{(4)}(\\xi'') a fim de obter a f\u00f3rmula de Simpson \\int_{a}^{b} f(x) \\, dx = \\frac{h}{3}[f(a) + 4f(a/2+b/2) + f(b)] - \\frac{h^5}{90}f^{(4)}(\\xi). Precis\u00e3o O grau de precis\u00e3o da f\u00f3rmula de quadratura \u00e9 o maior inteiro positivo n tal que a f\u00f3rmula seja exata para x^k para cada k = 0,\\dots,n . Para os exemplos desenvolvidos acima, basta olhar qual o maior inteiro n tal que f^{(2)} \\equiv 0 no caso do trap\u00e9zio e f^{(4)} = 0 no caso de Simpson. F\u00f3rmulas compostas \u00c9 claro que as f\u00f3rmulas deduzidas acima ficam bem ruins quando b - a \u00e9 grande. Para isso, a ideia \u00e9 subdividir o intervalo [a,b] em a = x_0 < \\dots < x_n = b (de forma igualmente espa\u00e7ada, em geral) e aplicar a quadratura num\u00e9rica a cada um deles. Essa abordagem tira proveito da seguinte propriedade: \\int_a^b f(x) \\, dx = \\int_a^c f(x) \\, dx + \\int_c^b f(x) \\, dx. Para a regra de Simpson composta teremos que \\int_a^b f(x) \\, dx = \\frac{h}{3}\\left[f(a) + 2\\sum_{j=1}^{n/2 - 1} f(x_{2j}) + 4\\sum_{j=1}^{n/2} f(x_{2j-1}) + f(b)\\right] - \\frac{b-a}{180}h^4f^{(4)}(\\xi), para \\xi \\in (a,b) e h = (b-a)/n . O erro, portanto, \u00e9 O(h^4) . A f\u00f3rmula do trap\u00e9zio \u00e9 similar: \\int_a^b f(x) \\, dx = \\frac{h}{2}\\left[f(a) + 2\\sum_{j=1}^{n - 1} f(x_{j}) + f(b)\\right] - \\frac{b-a}{12}h^2f^{(2)}(\\xi). Quadratura de Gauss-Legendre Este m\u00e9todo escolhe os pontos para a interpola\u00e7\u00e3o de forma \u00f3tima ao inv\u00e9s de igualmente espa\u00e7ada. Assim, os pontos x_i e os coeficiente c_i s\u00e3o escolhidos a minimizar \\left|\\int_a^b f(x) \\, dx - \\sum_{i=1}^n c_i f(x_i)\\right|. Para medir essa acur\u00e1cia, assumimos que a escolha \u00f3tima desses valores produzo maior grau de precis\u00e3o, definido acima. Polin\u00f4mios Ortogonais Considere o espa\u00e7o das fun\u00e7\u00f5es cont\u00ednuas definidas em [a,b] (isso significa que um elemento desse conjunto \u00e9 uma fun\u00e7\u00e3o cont\u00ednua definida em [a,b] ) com o seguinte produto interno: \\langle f, g \\rangle = \\int_a^b f(x)g(x) \\, dx. Observe como esse produto interno \u00e9 uma extens\u00e3o do somat\u00f3rio dos produtos dos componentes de dois vetores. Dizemos que duas fun\u00e7\u00f5es s\u00e3o ortogonais quando \\langle f,g \\rangle = 0 . Considere o espa\u00e7o dos polin\u00f4mios de grau at\u00e9 n com o produto interno apresentado acima no intervalo [-1,1] . Os polin\u00f4mios de Legendre s\u00e3o polin\u00f4mios ortogonais e s\u00e3o obtidos fazendo o processo de Gram-Schmidt sobre a base \\{1, x, x^2, x^3, \\dots, x^n\\} . Abaixo est\u00e3o alguns dos polin\u00f4mios de Legendre: P_0(x) = 1, P_1(x) = x, P_2(x) = x^2 - \\frac{1}{3}, P_3(x) = x^3 - \\frac{3}{5}x, P_4(x) = x^4 - \\frac{6}{7}x^2 + \\frac{3}{35}. Suponha que x_1, \\dots, x_n s\u00e3o as ra\u00edzes do polin\u00f4mio de Legendre P_n(x) e que c_i = \\int_{-1}^1 \\prod_{j \\neq i} \\frac{x-x_j}{x_i-x_j} \\, dx. Se P(x) \u00e9 qualquer polin\u00f4mio de grau menor do que 2n , ent\u00e3o, \\int_{-1}^1 P(x) \\, dx = \\sum_{i=1}^n c_iP(x_i). F\u00f3rmula de Gauss-Legendre para intervalo qualquer Basta observar que \\int_a^b f(x) \\, dx = \\frac{b-a}{2}\\int_{-1}^1 f\\left(\\frac{(b-a)t + (b+a)}{2}\\right) \\, dt e aplicar Gauss-Legendre para a integral \u00e0 direita.","title":"Integra\u00e7\u00e3o Num\u00e9rica"},{"location":"analisenum/numeric_integration/#integracao-numerica","text":"O objetivo da integra\u00e7\u00e3o num\u00e9rica \u00e9 estimar I = \\int_a^b f(x) \\, dx, principalmente quando n\u00e3o sabemos a antiderivada de f . Podemos extender para problemas de dimens\u00e3o maior, isto \u00e9, integrar I = \\int_R f(x) \\, dx, em algum ret\u00e2ngulo R em dimens\u00e3o \\mathbb{R}^n . Chamamos de quadratura num\u00e9rica o m\u00e9todo b\u00e1sico de aproxima\u00e7\u00e3o de I atrav\u00e9s da soma \\sum_{i=0}^n a_i f(x_i) .","title":"Integra\u00e7\u00e3o Num\u00e9rica"},{"location":"analisenum/numeric_integration/#formula-de-newton-cotes","text":"De forma geral, podemos integrar o polin\u00f4mio interpolador de Lagrange que aproxima para aproximar a integra\u00e7\u00e3o da fun\u00e7\u00e3o aproximada pela fun\u00e7\u00e3o, isto \u00e9, \\int_a^b f(x) \\, dx = \\int_a^b \\sum_{i=0}^n f(x_i) L_i(x) + \\int_a^b \\prod_{i=0}^n (x - x_i)\\frac{f^{(n+1)}(\\xi(x))}{(n+1)!} \\, dx em que \\{L_i(x)\\}_{i=0}^n forma a base dos polin\u00f4mios de Lagrange . Nesse caso, uma poss\u00edvel aproxima\u00e7\u00e3o para essa integral \u00e9 \\int_a^b \\sum_{i=0}^n f(x_i) L_i(x) \\, dx = \\sum_{i=0}^n a_if(x_i), em que a_i = \\int_a^b L_i(x) \\, dx . O erro dessa estima\u00e7\u00e3o \u00e9 dado por E(f) = \\int_a^b \\prod_{i=0}^n (x - x_i)\\frac{f^{(n+1)}(\\xi(x))}{(n+1)!} \\, dx. A f\u00f3rmula de Newton-Cotes (n+1) -pontos fechada usa os pontos interpoladores x_i = x_0 + ih em que h = (b-a)/n, x_0 = a e x_n = b . Ela \u00e9 dita fechada por incluir os endpoints do intervalo. Apresentamos dois m\u00e9todos com esse tipo de f\u00f3rmula. Al\u00e9m do fechado, podemos tomar os pontos igualmente espa\u00e7ados no interior do intervalo, isto \u00e9, h = (b-a)/(n+2) e x_i = x_0 + (i+1)h para i = 0, \\dots, n . Essas s\u00e3o ditas f\u00f3rmula de Newton-Cotes (n+1) -pontos aberta . Um exemplo conhecida \u00e9 a regra do ponto m\u00e9dio que toma apenas um ponto x_0 = (a + b)/2 .","title":"F\u00f3rmula de Newton-Cotes"},{"location":"analisenum/numeric_integration/#trapezio-simples","text":"Nesse caso, definimos x_0 = a, x_1 = b e h=b-a . Assim a_0 = \\int_{x_0}^{x_1} \\frac{(x - x_1)}{(x_0-x_1)} \\quad \\text{ e } a_1 = \\int_{x_0}^{x_1} \\frac{(x - x_0)}{(x_1-x_0)}. Vamos utilizar o Teorema do Valor M\u00e9dio Ponderado para Integrais (p\u00e1gina 5) para estimar o erro. Nesse caso, ele \u00e9 da seguinte forma: \\begin{split} E(f) &= \\int_{x_0}^{x_1} (x - x_0)(x - x_1)\\frac{f^{(2)}(\\xi(x))}{2!} \\, dx \\\\ &= \\frac{f^{(2)}(\\xi)}{2}\\left[\\frac{x^3}{3} - \\frac{(x_0 + x_1)}{2}x^2 + x_0x_1x\\right]_{x_0}^{x_1} \\\\ &= -\\frac{h^3}{6}f^{(2)}(\\xi), \\end{split} para algum \\xi \\in (a,b) . A regra do trap\u00e9zio se resume, ent\u00e3o, a \\int_a^b f(x) \\, dx = \\frac{h}{2}[f(a) + f(b)] - \\frac{h^3}{12}f''(\\xi).","title":"Trap\u00e9zio Simples"},{"location":"analisenum/numeric_integration/#formula-de-simpson-simples","text":"Para a f\u00f3rmula de Simpson, fazemos x_0 = a, x_2 = b e x_1 = a + h , para h = (b-a)/2 , isto \u00e9, adicionamos mais um ponto para a aproxima\u00e7\u00e3o polinomial. Nesse caso, a_0 = \\int_{x_0}^{x_2} \\frac{(x - x_1)(x - x_2)}{(x_0-x_1)(x_0 - x_2)}, a_1 = \\int_{x_0}^{x_2} \\frac{(x - x_0)(x - x_2)}{(x_1-x_0)(x_1 - x_2)} \\text{ e }a_2 = \\int_{x_0}^{x_2} \\frac{(x - x_0)(x - x_1)}{(x_2-x_0)(x_2 - x_1)}. Com essa deriva\u00e7\u00e3o, vamos obter um erro O(h^4) com um termo relacionado a f^{(3)} . Podemos reduzir esse erro com esses mesmos pontos usando uma abordagem levemente diferente. \u00c9 usada a expans\u00e3o de Taylor em x_1 at\u00e9 ordem e um pouco de simetria x_2 - x_1 = x_1 - x_0 para verificar que \\int_{x_0}^{x_2} f(x) \\, dx = 2hf(x_1) + \\frac{h^3}{3}f''(x_1) + \\frac{f^{(4)}(\\xi')}{60}h^5. Al\u00e9m disso, \u00e9 usada a seguinte aproxima\u00e7\u00e3o para f''(x_1) : f''(x_1) = \\frac{1}{h^2}[f(x_0) - 2f(x_1) + f(x_2)] - \\frac{h^2}{12}f^{(4)}(\\xi'') a fim de obter a f\u00f3rmula de Simpson \\int_{a}^{b} f(x) \\, dx = \\frac{h}{3}[f(a) + 4f(a/2+b/2) + f(b)] - \\frac{h^5}{90}f^{(4)}(\\xi).","title":"F\u00f3rmula de Simpson Simples"},{"location":"analisenum/numeric_integration/#precisao","text":"O grau de precis\u00e3o da f\u00f3rmula de quadratura \u00e9 o maior inteiro positivo n tal que a f\u00f3rmula seja exata para x^k para cada k = 0,\\dots,n . Para os exemplos desenvolvidos acima, basta olhar qual o maior inteiro n tal que f^{(2)} \\equiv 0 no caso do trap\u00e9zio e f^{(4)} = 0 no caso de Simpson.","title":"Precis\u00e3o"},{"location":"analisenum/numeric_integration/#formulas-compostas","text":"\u00c9 claro que as f\u00f3rmulas deduzidas acima ficam bem ruins quando b - a \u00e9 grande. Para isso, a ideia \u00e9 subdividir o intervalo [a,b] em a = x_0 < \\dots < x_n = b (de forma igualmente espa\u00e7ada, em geral) e aplicar a quadratura num\u00e9rica a cada um deles. Essa abordagem tira proveito da seguinte propriedade: \\int_a^b f(x) \\, dx = \\int_a^c f(x) \\, dx + \\int_c^b f(x) \\, dx. Para a regra de Simpson composta teremos que \\int_a^b f(x) \\, dx = \\frac{h}{3}\\left[f(a) + 2\\sum_{j=1}^{n/2 - 1} f(x_{2j}) + 4\\sum_{j=1}^{n/2} f(x_{2j-1}) + f(b)\\right] - \\frac{b-a}{180}h^4f^{(4)}(\\xi), para \\xi \\in (a,b) e h = (b-a)/n . O erro, portanto, \u00e9 O(h^4) . A f\u00f3rmula do trap\u00e9zio \u00e9 similar: \\int_a^b f(x) \\, dx = \\frac{h}{2}\\left[f(a) + 2\\sum_{j=1}^{n - 1} f(x_{j}) + f(b)\\right] - \\frac{b-a}{12}h^2f^{(2)}(\\xi).","title":"F\u00f3rmulas compostas"},{"location":"analisenum/numeric_integration/#quadratura-de-gauss-legendre","text":"Este m\u00e9todo escolhe os pontos para a interpola\u00e7\u00e3o de forma \u00f3tima ao inv\u00e9s de igualmente espa\u00e7ada. Assim, os pontos x_i e os coeficiente c_i s\u00e3o escolhidos a minimizar \\left|\\int_a^b f(x) \\, dx - \\sum_{i=1}^n c_i f(x_i)\\right|. Para medir essa acur\u00e1cia, assumimos que a escolha \u00f3tima desses valores produzo maior grau de precis\u00e3o, definido acima.","title":"Quadratura de Gauss-Legendre"},{"location":"analisenum/numeric_integration/#polinomios-ortogonais","text":"Considere o espa\u00e7o das fun\u00e7\u00f5es cont\u00ednuas definidas em [a,b] (isso significa que um elemento desse conjunto \u00e9 uma fun\u00e7\u00e3o cont\u00ednua definida em [a,b] ) com o seguinte produto interno: \\langle f, g \\rangle = \\int_a^b f(x)g(x) \\, dx. Observe como esse produto interno \u00e9 uma extens\u00e3o do somat\u00f3rio dos produtos dos componentes de dois vetores. Dizemos que duas fun\u00e7\u00f5es s\u00e3o ortogonais quando \\langle f,g \\rangle = 0 . Considere o espa\u00e7o dos polin\u00f4mios de grau at\u00e9 n com o produto interno apresentado acima no intervalo [-1,1] . Os polin\u00f4mios de Legendre s\u00e3o polin\u00f4mios ortogonais e s\u00e3o obtidos fazendo o processo de Gram-Schmidt sobre a base \\{1, x, x^2, x^3, \\dots, x^n\\} . Abaixo est\u00e3o alguns dos polin\u00f4mios de Legendre: P_0(x) = 1, P_1(x) = x, P_2(x) = x^2 - \\frac{1}{3}, P_3(x) = x^3 - \\frac{3}{5}x, P_4(x) = x^4 - \\frac{6}{7}x^2 + \\frac{3}{35}. Suponha que x_1, \\dots, x_n s\u00e3o as ra\u00edzes do polin\u00f4mio de Legendre P_n(x) e que c_i = \\int_{-1}^1 \\prod_{j \\neq i} \\frac{x-x_j}{x_i-x_j} \\, dx. Se P(x) \u00e9 qualquer polin\u00f4mio de grau menor do que 2n , ent\u00e3o, \\int_{-1}^1 P(x) \\, dx = \\sum_{i=1}^n c_iP(x_i).","title":"Polin\u00f4mios Ortogonais"},{"location":"analisenum/numeric_integration/#formula-de-gauss-legendre-para-intervalo-qualquer","text":"Basta observar que \\int_a^b f(x) \\, dx = \\frac{b-a}{2}\\int_{-1}^1 f\\left(\\frac{(b-a)t + (b+a)}{2}\\right) \\, dt e aplicar Gauss-Legendre para a integral \u00e0 direita.","title":"F\u00f3rmula de Gauss-Legendre para intervalo qualquer"},{"location":"analisenum/polynomial_interpolation/","text":"Interpola\u00e7\u00e3o polinomial Interpola\u00e7\u00e3o \u00e9 um tipo de estima\u00e7\u00e3o de curva que constr\u00f3i novos pontos a partir de um conjunto finito. Em geral, a partir de uma amostragem ou experimento, obtemos um sequ\u00eancia de pontos e os valores da fun\u00e7\u00e3o correspondentes. No caso de polin\u00f4mios, a ideia \u00e9 encontrar um polin\u00f4mio que passe pelos dados a serem analisados. P_n(x) = a_0 + a_1 x + a_2 x^2 + \\dots + a_n x^n \u00e9 polin\u00f4mio de grau no m\u00e1ximo n e a_0, a_1, \\dots, a_n s\u00e3o constantes reais. Dizemos que P_n[x] \\in \\mathbb{R}[x] , isto \u00e9, \u00e9 um polin\u00f4mio na vari\u00e1vel x com constantes reais. Teorema da Aproxima\u00e7\u00e3o de Weierstrass Seja f cont\u00ednua em [a,b] . Para cada \\epsilon > 0 , existe um polin\u00f4mio p(x) tal que |f(x) - p(x)| < \\epsilon, \\forall x \\in [a,b]. Demonstra\u00e7\u00e3o por Dunham Jackson. Exist\u00eancia de polin\u00f4mio interpolador Considere os pontos (x_i, y_i), i = 1, \\dots, n com pontos x_i todos diferentes. Ent\u00e3o existe um \u00fanico polin\u00f4mio p(x) de grau no m\u00e1ximo n-1 tal que p(x_i) = y_i para i = 1, \\dots, n . Esse problema tem solu\u00e7\u00e3o se o sistema Xa = b tem solu\u00e7\u00e3o em que X = \\begin{bmatrix} 1 & x_0 & \\cdots & x_0^n \\\\ 1 & x_1 & \\cdots & x_1^n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_n & \\cdots & x_n^n \\end{bmatrix}, a = \\begin{bmatrix} a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\text{ e } b = \\begin{bmatrix} f(x_0) \\\\ f(x_1) \\\\ \\vdots \\\\ f(x_n). \\end{bmatrix} Esse sistema tem solu\u00e7\u00e3o se \\det(X) \\neq 0 , isto \u00e9, o determinante de Vandermonde \u00e9 n\u00e3o nulo. \\det(X) = \\prod_{i > j} (x_i - x_j) = \\prod_{j=0}^{n-1} \\left[\\prod_{i=j+1}^n (x_i - x_j) \\right] \\neq 0 \\iff x_i \\neq x_j, j \\neq i. Nesse caso, existe um \u00fanico a solu\u00e7\u00e3o desse sistema que define o polin\u00f4mio interpolador p(x) . Se P_n(x_i) = Q_n(x_i) para n+1 pontos x_0, x_1, \\dots, x_n , ent\u00e3o P_n(x) = Q_n(x) para todo x . Esse resultado \u00e9 direto se considerarmos as ra\u00edzes do polin\u00f4mio D(x) = P_n(x) - Q_n(x) (que tem no m\u00e1ximo) n ra\u00edzes se n\u00e3o for identicamente nulo. Polin\u00f4mios de Lagrange Defina L_{n,k}(x) = \\frac{(x-x_0)\\cdots(x - x_{k-1})(x - x_{k+1})\\cdots(x - x_n)}{(x_k-x_0)\\cdots(x_k - x_{k-1})(x_k - x_{k+1})\\cdots(x_k - x_n)}. \u00c9 f\u00e1cil ver que L_{n,k}(x_i) = \\begin{cases} 0, & \\text{se } i \\neq k \\\\ 1, & \\text{se } i = k. \\end{cases} Assim, podemos definir p(x) = \\sum_{k=0}^n f(x_k)L_{n,k}(x). que \u00e9 o polin\u00f4mio interpolador de Lagrange. Se os pontos s\u00e3o diferentes, isto \u00e9, x_0 \\neq x_1 \\neq \\cdots \\neq x_n . Teorema de erro pontual na interpola\u00e7\u00e3o Seja f(x) \\in C^{(n+1)}[a,b] e p_n(x) o polin\u00f4mio interpolador com os pontos distintos x_0, x_1, \\dots, x_n . Ent\u00e3o para todo x \\in [a,b] , existe um ponto \\xi = \\xi(x) tal que \\min(x_0, x_1, \\dots, x_n, x) < \\xi < \\max(x_0, x_1, \\dots, x_n, x), e f(x) - p_n(x) = \\frac{(x-x_0)(x-x_1)\\cdots(x-x_n)}{(n+1)!}f^{(n+1)}(\\xi). Fen\u00f4meno de Runge \u00c9 um problema de oscila\u00e7\u00e3o nas bordas de um intervalo que ocorre ao se usar a interpola\u00e7\u00e3o polinomial com polin\u00f4mio de alto grau em um conjunto de pontos equidistantes . Foi descoberto por Carl David Tolm\u00e9 Runge que mostrou que aumentar o grau n\u00e3o aumenta precis\u00e3o necessariamente. Considere a fun\u00e7\u00e3o f(x) = \\frac{1}{1 + 25x^2} e defina x_i = \\frac{2i}{n} - 1, i \\in \\{0, 1, \\dots, n\\} com um polin\u00f4mio de grau no m\u00e1ximo n . Podemos provar que \\lim_{n \\to \\infty}\\left(\\max_{-1 \\le x \\le 1} |f(x) - P_n(x)|\\right) = + \\infty. Vamos verificar empiricamente, \u00e9 claro. Polin\u00f4mios de Chebyshev Um t\u00f3pico importante dentro da interpola\u00e7\u00e3o e da estima\u00e7\u00e3o de curvas de forma geral s\u00e3o os polin\u00f4mios ortogonais . Lembrando que uma base ortogonal ocorre quando o produto interno de dois elementos da base diferentes \u00e9 nulo. Isso acontece com os polin\u00f4mios tamb\u00e9m, s\u00f3 que o produto interno \u00e9 \\langle \\phi_k, \\phi_j \\rangle = \\int_a^b w(x)\\phi_k(x)\\phi_j(x) \\, dx, em que w(x) \u00e9 uma fun\u00e7\u00e3o peso. Note que essa \u00e9 uma extens\u00e3o do produto escalar que \u00e9 a soma finita dos produtos dos componentes dos vetores. Esse \u00e9 um assunto muito interessante, mas vamos ao t\u00f3pico. Os polin\u00f4mios de CHebyshev formam uma base ortogonal em [-1,1] com respeito a fun\u00e7\u00e3o peso w(x) = (1 - x^2)^{-1/2} . Definimos T_n(x) = \\cos(n\\arccos(x)), n \\ge 0, x \\in [-1,1]. Para verificar que T_n \u00e9 um polin\u00f4mio, note que T_0(x) = \\cos(0) = 1, \\quad T_1(x) = \\cos(\\arccos(x)) = x. Al\u00e9m do mais, sabemos que T_{n+1}(x) = \\cos((n+1)\\theta) = \\cos(\\theta)\\cos(n\\theta) - \\sin(\\theta)\\sin(n\\theta) e T_{n-1}(x) = \\cos((n-1)\\theta) = \\cos(\\theta)\\cos(n\\theta) + \\sin(\\theta)\\sin(n\\theta), pondo \\theta = \\arccos(x) . Assim, \u00e9 f\u00e1cil ver que T_{n+1}(x) + T_{n-1}(x) = 2xT_n(x) \\implies T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x). Por indu\u00e7\u00e3o, conseguimos ver que T_n \u00e9 de fato um polin\u00f4mio. Esses polin\u00f4mios s\u00e3o usados para minimizar o erro de aproxima\u00e7\u00e3o de uma interpola\u00e7\u00e3o. No caso dos polin\u00f4mios de Lagrange, isso se d\u00e1 pela escolha apropriada de x_1, x_2, \\dots, x_n . Zeros do polin\u00f4mio de Chebyshev O polin\u00f4mio T_n(x) de grau n \\ge 1 tem n zeros simples (multiplicidade 1 ) em [-1,1] em \\bar{x}_k = \\cos\\left(\\frac{2k-1}{2n}\\pi\\right), k = 1,\\dots, n. Al\u00e9m do mais, ele assume valores absolutos extremos em x_k = \\cos\\left(\\frac{k\\pi}{n}\\right) \\text{ com } T_n(x_k)= (-1)^k, k =0,1,\\dots,n. M\u00e1ximo atingido Seja \\tilde{T}_n(x) = T_n(x)2^{1-n} . Temos que \\frac{1}{2^{n-1}} = \\max_{x \\in [-1,1]} |\\tilde{T}_n(x)| \\le \\max_{x \\in [-1,1]} |P_n(x)|, para todo polin\u00f4mio m\u00f4nico P_n(x) . Al\u00e9m disso a igualdade s\u00f3 ocorre para o polin\u00f4mio de Chebyshev. Escolha de x_i para Lagrange Lembre que f(x) - p(x) = \\frac{f^{(n+1)}(\\xi(x))}{(n+1)!}(x-x_0)\\cdots(x-x_n). Como Q(x) = (x-x_0)\\cdots(x-x_n) \u00e9 um polin\u00f4mio m\u00f4nico de grau n+1 , escolhendo x_i para serem as ra\u00edzes do polin\u00f4mio de Chebyshev, estaremos fazendo Q(x) = \\tilde{T}_{n+1}(x) para todo x \\in [-1,1] . Assim, obtemos que \\frac{1}{2^{n}} = \\max_{x \\in [-1,1]} |(x-\\bar{x}_0)(x-\\bar{x}_1)\\cdots(x-\\bar{x}_n)|. O que implica que \\max_{x \\in [-1,1]} |f(x) - p(x)| \\le \\frac{1}{2^{n}(n+1)!}\\max_{x \\in [-1,1]} |f^{n+1}(x)|. Intervalo [a,b] Agora que entendemos como escolher os pontos em [-1,1] , precisamos extender para um intervalo fechado qualquer. Para isso, basta fazer a mudan\u00e7a de vari\u00e1veis \\tilde{x} = \\frac{1}{2}[(b-a)x + a + b]. Sugest\u00f5es Material original de 1901: o texto n\u00e3o est\u00e1 em portugu\u00eas, mas vale a pena olhar. C\u00f3digos para os gr\u00e1ficos.","title":"Interpola\u00e7\u00e3o polinomial"},{"location":"analisenum/polynomial_interpolation/#interpolacao-polinomial","text":"Interpola\u00e7\u00e3o \u00e9 um tipo de estima\u00e7\u00e3o de curva que constr\u00f3i novos pontos a partir de um conjunto finito. Em geral, a partir de uma amostragem ou experimento, obtemos um sequ\u00eancia de pontos e os valores da fun\u00e7\u00e3o correspondentes. No caso de polin\u00f4mios, a ideia \u00e9 encontrar um polin\u00f4mio que passe pelos dados a serem analisados. P_n(x) = a_0 + a_1 x + a_2 x^2 + \\dots + a_n x^n \u00e9 polin\u00f4mio de grau no m\u00e1ximo n e a_0, a_1, \\dots, a_n s\u00e3o constantes reais. Dizemos que P_n[x] \\in \\mathbb{R}[x] , isto \u00e9, \u00e9 um polin\u00f4mio na vari\u00e1vel x com constantes reais. Teorema da Aproxima\u00e7\u00e3o de Weierstrass Seja f cont\u00ednua em [a,b] . Para cada \\epsilon > 0 , existe um polin\u00f4mio p(x) tal que |f(x) - p(x)| < \\epsilon, \\forall x \\in [a,b]. Demonstra\u00e7\u00e3o por Dunham Jackson. Exist\u00eancia de polin\u00f4mio interpolador Considere os pontos (x_i, y_i), i = 1, \\dots, n com pontos x_i todos diferentes. Ent\u00e3o existe um \u00fanico polin\u00f4mio p(x) de grau no m\u00e1ximo n-1 tal que p(x_i) = y_i para i = 1, \\dots, n . Esse problema tem solu\u00e7\u00e3o se o sistema Xa = b tem solu\u00e7\u00e3o em que X = \\begin{bmatrix} 1 & x_0 & \\cdots & x_0^n \\\\ 1 & x_1 & \\cdots & x_1^n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_n & \\cdots & x_n^n \\end{bmatrix}, a = \\begin{bmatrix} a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\text{ e } b = \\begin{bmatrix} f(x_0) \\\\ f(x_1) \\\\ \\vdots \\\\ f(x_n). \\end{bmatrix} Esse sistema tem solu\u00e7\u00e3o se \\det(X) \\neq 0 , isto \u00e9, o determinante de Vandermonde \u00e9 n\u00e3o nulo. \\det(X) = \\prod_{i > j} (x_i - x_j) = \\prod_{j=0}^{n-1} \\left[\\prod_{i=j+1}^n (x_i - x_j) \\right] \\neq 0 \\iff x_i \\neq x_j, j \\neq i. Nesse caso, existe um \u00fanico a solu\u00e7\u00e3o desse sistema que define o polin\u00f4mio interpolador p(x) . Se P_n(x_i) = Q_n(x_i) para n+1 pontos x_0, x_1, \\dots, x_n , ent\u00e3o P_n(x) = Q_n(x) para todo x . Esse resultado \u00e9 direto se considerarmos as ra\u00edzes do polin\u00f4mio D(x) = P_n(x) - Q_n(x) (que tem no m\u00e1ximo) n ra\u00edzes se n\u00e3o for identicamente nulo.","title":"Interpola\u00e7\u00e3o polinomial"},{"location":"analisenum/polynomial_interpolation/#polinomios-de-lagrange","text":"Defina L_{n,k}(x) = \\frac{(x-x_0)\\cdots(x - x_{k-1})(x - x_{k+1})\\cdots(x - x_n)}{(x_k-x_0)\\cdots(x_k - x_{k-1})(x_k - x_{k+1})\\cdots(x_k - x_n)}. \u00c9 f\u00e1cil ver que L_{n,k}(x_i) = \\begin{cases} 0, & \\text{se } i \\neq k \\\\ 1, & \\text{se } i = k. \\end{cases} Assim, podemos definir p(x) = \\sum_{k=0}^n f(x_k)L_{n,k}(x). que \u00e9 o polin\u00f4mio interpolador de Lagrange. Se os pontos s\u00e3o diferentes, isto \u00e9, x_0 \\neq x_1 \\neq \\cdots \\neq x_n . Teorema de erro pontual na interpola\u00e7\u00e3o Seja f(x) \\in C^{(n+1)}[a,b] e p_n(x) o polin\u00f4mio interpolador com os pontos distintos x_0, x_1, \\dots, x_n . Ent\u00e3o para todo x \\in [a,b] , existe um ponto \\xi = \\xi(x) tal que \\min(x_0, x_1, \\dots, x_n, x) < \\xi < \\max(x_0, x_1, \\dots, x_n, x), e f(x) - p_n(x) = \\frac{(x-x_0)(x-x_1)\\cdots(x-x_n)}{(n+1)!}f^{(n+1)}(\\xi).","title":"Polin\u00f4mios de Lagrange"},{"location":"analisenum/polynomial_interpolation/#fenomeno-de-runge","text":"\u00c9 um problema de oscila\u00e7\u00e3o nas bordas de um intervalo que ocorre ao se usar a interpola\u00e7\u00e3o polinomial com polin\u00f4mio de alto grau em um conjunto de pontos equidistantes . Foi descoberto por Carl David Tolm\u00e9 Runge que mostrou que aumentar o grau n\u00e3o aumenta precis\u00e3o necessariamente. Considere a fun\u00e7\u00e3o f(x) = \\frac{1}{1 + 25x^2} e defina x_i = \\frac{2i}{n} - 1, i \\in \\{0, 1, \\dots, n\\} com um polin\u00f4mio de grau no m\u00e1ximo n . Podemos provar que \\lim_{n \\to \\infty}\\left(\\max_{-1 \\le x \\le 1} |f(x) - P_n(x)|\\right) = + \\infty. Vamos verificar empiricamente, \u00e9 claro.","title":"Fen\u00f4meno de Runge"},{"location":"analisenum/polynomial_interpolation/#polinomios-de-chebyshev","text":"Um t\u00f3pico importante dentro da interpola\u00e7\u00e3o e da estima\u00e7\u00e3o de curvas de forma geral s\u00e3o os polin\u00f4mios ortogonais . Lembrando que uma base ortogonal ocorre quando o produto interno de dois elementos da base diferentes \u00e9 nulo. Isso acontece com os polin\u00f4mios tamb\u00e9m, s\u00f3 que o produto interno \u00e9 \\langle \\phi_k, \\phi_j \\rangle = \\int_a^b w(x)\\phi_k(x)\\phi_j(x) \\, dx, em que w(x) \u00e9 uma fun\u00e7\u00e3o peso. Note que essa \u00e9 uma extens\u00e3o do produto escalar que \u00e9 a soma finita dos produtos dos componentes dos vetores. Esse \u00e9 um assunto muito interessante, mas vamos ao t\u00f3pico. Os polin\u00f4mios de CHebyshev formam uma base ortogonal em [-1,1] com respeito a fun\u00e7\u00e3o peso w(x) = (1 - x^2)^{-1/2} . Definimos T_n(x) = \\cos(n\\arccos(x)), n \\ge 0, x \\in [-1,1]. Para verificar que T_n \u00e9 um polin\u00f4mio, note que T_0(x) = \\cos(0) = 1, \\quad T_1(x) = \\cos(\\arccos(x)) = x. Al\u00e9m do mais, sabemos que T_{n+1}(x) = \\cos((n+1)\\theta) = \\cos(\\theta)\\cos(n\\theta) - \\sin(\\theta)\\sin(n\\theta) e T_{n-1}(x) = \\cos((n-1)\\theta) = \\cos(\\theta)\\cos(n\\theta) + \\sin(\\theta)\\sin(n\\theta), pondo \\theta = \\arccos(x) . Assim, \u00e9 f\u00e1cil ver que T_{n+1}(x) + T_{n-1}(x) = 2xT_n(x) \\implies T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x). Por indu\u00e7\u00e3o, conseguimos ver que T_n \u00e9 de fato um polin\u00f4mio. Esses polin\u00f4mios s\u00e3o usados para minimizar o erro de aproxima\u00e7\u00e3o de uma interpola\u00e7\u00e3o. No caso dos polin\u00f4mios de Lagrange, isso se d\u00e1 pela escolha apropriada de x_1, x_2, \\dots, x_n . Zeros do polin\u00f4mio de Chebyshev O polin\u00f4mio T_n(x) de grau n \\ge 1 tem n zeros simples (multiplicidade 1 ) em [-1,1] em \\bar{x}_k = \\cos\\left(\\frac{2k-1}{2n}\\pi\\right), k = 1,\\dots, n. Al\u00e9m do mais, ele assume valores absolutos extremos em x_k = \\cos\\left(\\frac{k\\pi}{n}\\right) \\text{ com } T_n(x_k)= (-1)^k, k =0,1,\\dots,n. M\u00e1ximo atingido Seja \\tilde{T}_n(x) = T_n(x)2^{1-n} . Temos que \\frac{1}{2^{n-1}} = \\max_{x \\in [-1,1]} |\\tilde{T}_n(x)| \\le \\max_{x \\in [-1,1]} |P_n(x)|, para todo polin\u00f4mio m\u00f4nico P_n(x) . Al\u00e9m disso a igualdade s\u00f3 ocorre para o polin\u00f4mio de Chebyshev. Escolha de x_i para Lagrange Lembre que f(x) - p(x) = \\frac{f^{(n+1)}(\\xi(x))}{(n+1)!}(x-x_0)\\cdots(x-x_n). Como Q(x) = (x-x_0)\\cdots(x-x_n) \u00e9 um polin\u00f4mio m\u00f4nico de grau n+1 , escolhendo x_i para serem as ra\u00edzes do polin\u00f4mio de Chebyshev, estaremos fazendo Q(x) = \\tilde{T}_{n+1}(x) para todo x \\in [-1,1] . Assim, obtemos que \\frac{1}{2^{n}} = \\max_{x \\in [-1,1]} |(x-\\bar{x}_0)(x-\\bar{x}_1)\\cdots(x-\\bar{x}_n)|. O que implica que \\max_{x \\in [-1,1]} |f(x) - p(x)| \\le \\frac{1}{2^{n}(n+1)!}\\max_{x \\in [-1,1]} |f^{n+1}(x)|.","title":"Polin\u00f4mios de Chebyshev"},{"location":"analisenum/polynomial_interpolation/#intervalo-ab","text":"Agora que entendemos como escolher os pontos em [-1,1] , precisamos extender para um intervalo fechado qualquer. Para isso, basta fazer a mudan\u00e7a de vari\u00e1veis \\tilde{x} = \\frac{1}{2}[(b-a)x + a + b].","title":"Intervalo [a,b]"},{"location":"analisenum/polynomial_interpolation/#sugestoes","text":"Material original de 1901: o texto n\u00e3o est\u00e1 em portugu\u00eas, mas vale a pena olhar. C\u00f3digos para os gr\u00e1ficos.","title":"Sugest\u00f5es"},{"location":"analisenum/stochastic/","text":"Simula\u00e7\u00e3o estoc\u00e1stica Uso para c\u00e1lculo de integrais. Para integrais de dimens\u00e3o alta, \u00e9 o \u00fanico m\u00e9todo computacionalmente acess\u00edvel. Para essa parte do, boas refer\u00eancias, que s\u00e3o usadas nos cursos de Estat\u00edstica Computacional, s\u00e3o as seguintes: Introdu\u00e7\u00e3o a m\u00e9todos de Monte Carlo M\u00e9todo da invers\u00e3o e da transforma\u00e7\u00e3o Gera\u00e7\u00e3o de n\u00fameros aleat\u00f3rios Concentra\u00e7\u00e3o de medida","title":"Simula\u00e7\u00e3o estoc\u00e1stica"},{"location":"analisenum/stochastic/#simulacao-estocastica","text":"Uso para c\u00e1lculo de integrais. Para integrais de dimens\u00e3o alta, \u00e9 o \u00fanico m\u00e9todo computacionalmente acess\u00edvel. Para essa parte do, boas refer\u00eancias, que s\u00e3o usadas nos cursos de Estat\u00edstica Computacional, s\u00e3o as seguintes: Introdu\u00e7\u00e3o a m\u00e9todos de Monte Carlo M\u00e9todo da invers\u00e3o e da transforma\u00e7\u00e3o Gera\u00e7\u00e3o de n\u00fameros aleat\u00f3rios Concentra\u00e7\u00e3o de medida","title":"Simula\u00e7\u00e3o estoc\u00e1stica"},{"location":"analisenum/application_newton/non_linear_equations/","text":"M\u00e9todos num\u00e9ricos para solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares Escrevemos no parte anterior o resumo dos algoritmos de alguns m\u00e9todos para solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares, e n\u00e3o s\u00e3o os \u00fanicos. Modifica\u00e7\u00f5es desses m\u00e9todos, em especial do M\u00e9todo de Newton s\u00e3o constantemente sugeridas para melhorar a converg\u00eancia. Fica claro que muitos sistemas no mundo real s\u00e3o n\u00e3o lineares. Uma aplica\u00e7\u00e3o comum \u00e9 resolver problemas de otimiza\u00e7\u00e3o. Por exemplo, quando queremos maximizar em conjuntos abertos, se conseguirmos provar algumas condi\u00e7\u00f5es, podemos assegurar que o m\u00e1ximo se encontra quando f'(x) = 0 . Logo, o problema de otimiza\u00e7\u00e3o se resume a um problema de encontrar ra\u00edzes. import numpy as np import scipy.special as scis import scipy.optimize as scop import matplotlib.pyplot as plt %matplotlib inline Um jogador A ganha com placar (21-0) do jogador B em um jogo de raquetebol com probabilidade . P = \\frac{p+1}{2}\\left(\\frac{p}{1-p+p^2}\\right)^{21}, em que p \u00e9 a probabilidade de A ganhar um rally qualquer. Qual o valor de p que assegura que A vencer\u00e1 com esse placar em pelo menos metade dos jogos? Esse \u00e9 um problema real proposto por Ralph Levine para Joseph Keller. def P(p): return (p+1)/2 * (p/(1 + p**2-p))**(21) p_values = np.linspace(0, 1, 50) P_values = P(p_values) plt.plot(p_values, P_values, color='r') plt.axhline(0.5, linestyle = '--', color = 'darkblue') plt.title('Probabilidade de 21-0 para cada p') plt.text(0.7, 0.55, '$P = 0.5$', fontsize=12) plt.xlabel('$p$') plt.ylabel('$P$') plt.show() Vamos explorar m\u00e9todos de resolver a equa\u00e7\u00e3o P(p) = 0.5 para p , isto \u00e9, P(p) - 0.5 = 0 , atrav\u00e9s do m\u00e9todo do Ponto Fixo e do M\u00e9todo de Newton. Itera\u00e7\u00e3o de Ponto Fixo Temos que f(p) = 0.5 - \\frac{p+1}{2}\\left(\\frac{p}{1-p+p^2}\\right)^{21} + p = p, \u00e9 a fun\u00e7\u00e3o que admite o ponto fixo que queremos. Um ponto importante seria provar as condi\u00e7\u00f5es de funcionamento do m\u00e9todo. Por\u00e9m, n\u00e3o \u00e9 dif\u00edcil ver que f([0,1]) \\not \\subseteq [0,1] , o que j\u00e1 quebra nosso teorema. def f(p): return 0.5 - P(p) + p def fixed_point(x0, tol = 1e-5, max_ite = 1e4): x1 = f(x0) err = [abs(x1 - x0)] sols = [x0, x1] while err[-1] > tol and len(err) <= max_ite: sols.append(f(sols[-1])) err.append(abs(sols[-1] - sols[-2])) return {'sol': sols, 'errors': err} print(f(0.7)) 1.1329638832103943 Bom, mas vamos supor que mesmo assim quis\u00e9ssemos usar esse m\u00e9todo. res = fixed_point(0.7) plt.scatter(range(len(res['sol'])), res['sol']) <matplotlib.collections.PathCollection at 0x7f6a17b5eb38> Ele n\u00e3o converge! Mas a gente j\u00e1 devia ter ficado desconfiado, pois justamente as condi\u00e7\u00f5es do Teorema n\u00e3o functionavam. Em particular, \u00e9 f\u00e1cil ver que ele n\u00e3o \u00e9 nem n\u00e3o expansivo. Usando Scipy: scop.fixed_point(func = f, x0 = 0.81, method = 'iteration') --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-6-93ff8160efab> in <module> ----> 1 scop.fixed_point(func = f, x0 = 0.81, method = 'iteration') ~/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py in fixed_point(func, x0, args, xtol, maxiter, method) 935 use_accel = {'del2': True, 'iteration': False}[method] 936 x0 = _asarray_validated(x0, as_inexact=True) --> 937 return _fixed_point_helper(func, x0, args, xtol, maxiter, use_accel) ~/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py in _fixed_point_helper(func, x0, args, xtol, maxiter, use_accel) 889 p0 = p 890 msg = \"Failed to converge after %d iterations, value is %s\" % (maxiter, p) --> 891 raise RuntimeError(msg) 892 893 RuntimeError: Failed to converge after 500 iterations, value is 0.4998542288112865 Existe uma varia\u00e7\u00e3o desse dado pelo m\u00e9todo Steffensen com Aitken's \\Delta^2 , que constr\u00f3i uma sequ\u00eancia com onverg\u00eancia mais r\u00e1pida, a partir da inicial. %timeit scop.fixed_point(func = f, x0 = 0.3, method = 'del2') 1.08 ms \u00b1 120 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop.fixed_point(func = f, x0 = 0.3, method = 'del2') array(0.84230479) %timeit scop.fixed_point(func = f, x0 = 0.8, method = 'del2') 1.21 ms \u00b1 165 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop.fixed_point(func = f, x0 = 0.8, method = 'del2') array(0.84230479) Olhe o que acontece com x_0 = 0.1 . scop.fixed_point(func = f, x0 = 0.1, method = 'del2') array(-3.51843721e+13) M\u00e9todo de Newton Agora vamos usar informa\u00e7\u00e3o da derivada de P para nos ajudar com o problema de encontrar a ra\u00edz de P(p) - 0.5 = 0 . Note que P(0) < 0.5 e P(1) > 0.5 . }Para nos ajudar com as contas, vamos considerar a vers\u00e3o com log, isto \u00e9, \\log(P(p)) = - \\log(2) . Assim, \\log(p+1) - \\log(2) + 21(\\log(p) - \\log(1 - p + p^2)) = -\\log(2), o que simplica para g(p) = \\log(p+1) + 21(\\log(p) - \\log(1 - p + p^2)) = 0 . A\u00ed temos que g'(p) = \\frac{1}{p+1} + \\frac{21}{p} - \\frac{21}{1 - p + p^2}(2p - 1) def g(p): return np.log(p+1) + 21*(np.log(p) - np.log1p(p**2 - p)) def g_prime(p): return 1 / (p+1) + 21 / p - 21 * (2*p - 1) / (1 - p + p**2) Podemos provar analiticamente que a derivada \u00e9 estritamente positiva, mas faremos a observa\u00e7\u00e3o num\u00e9rica atrav\u00e9s do seguinte gr\u00e1fico. g_values = g_prime(p_values[1:]) plt.plot(p_values[1:], g_values, color='r') plt.title('Derivada da fun\u00e7\u00e3o derivada') plt.xlabel('$p$') plt.ylabel(\"$g\\'(p)$\") plt.show() Aplicando o m\u00e9todo de newton atrav\u00e9s do Scipy. Observe que o seu tempo deu bem menor que o anterior, mesmo com um valor de x_0 bem distante. %timeit scop.newton(func = g, x0 = 0.1, fprime = g_prime) 836 \u00b5s \u00b1 117 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop.newton(func = g, x0 = 0.1, fprime = g_prime) 0.8423047910355657 Observe o m\u00e9todo da Secante, um m\u00e9todo que tamb\u00e9m usa a ideia de tangente, mas sem consultar a derivada. %timeit scop.newton(func = g, x0 = 0.1) 631 \u00b5s \u00b1 111 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop.newton(func = g, x0 = 0.1) 0.8423047910355633 Exemplo adicional Nesse exemplo, a ideia \u00e9 verificar que Newton pode dar errado. Considere: f(x) = x\\sin(\\pi x) - \\exp(-x) f = lambda x: x * np.sin(np.pi * x) - np.exp(-x) x = np.linspace(-1, 2, 100) y = f(x) fig, ax = plt.subplots() ax.plot(x, y, color='r',zorder=0) xs = [0.57, 0.83, -0.27] texts = ['raiz$_1$', 'raiz$_2$', 'm\u00ednimo local'] for i in range(len(xs)): ax.scatter([xs[i]], [f(xs[i])],marker='x',s=60) ax.text(xs[i]+0.05, f(xs[i])-0.1, texts[i]) ax.plot(x,np.zeros_like(x),color='gray',ls='-.',alpha=0.75) ax.set_xlabel('$x$') ax.set_ylabel('$f(x)$') plt.title('$f(x)= x sin(\\pi x) - exp(-x)$') plt.show() xspace = np.linspace(-0.7, 0.7, 15) for x in xspace: print('Com x_0 = {0:5.2f}, a ra\u00edz \u00e9 {1:5.2f}'.format(x, scop.newton(f,x))) Com x_0 = -0.70, a ra\u00edz \u00e9 2.02 Com x_0 = -0.60, a ra\u00edz \u00e9 0.58 Com x_0 = -0.50, a ra\u00edz \u00e9 1.27 Com x_0 = -0.40, a ra\u00edz \u00e9 0.82 Com x_0 = -0.30, a ra\u00edz \u00e9 -0.30 Com x_0 = -0.20, a ra\u00edz \u00e9 0.82 Com x_0 = -0.10, a ra\u00edz \u00e9 2.02 Com x_0 = 0.00, a ra\u00edz \u00e9 0.82 Com x_0 = 0.10, a ra\u00edz \u00e9 0.58 Com x_0 = 0.20, a ra\u00edz \u00e9 0.58 Com x_0 = 0.30, a ra\u00edz \u00e9 0.58 Com x_0 = 0.40, a ra\u00edz \u00e9 0.58 Com x_0 = 0.50, a ra\u00edz \u00e9 0.58 Com x_0 = 0.60, a ra\u00edz \u00e9 0.58 Com x_0 = 0.70, a ra\u00edz \u00e9 0.58 Um outro exemplo Vamos comparar os m\u00e9todos agora com a fun\u00e7\u00e3o f(x) = x - \\cos(x) . Para o m\u00e9todo do ponto fixo, vamos utilizar a fun\u00e7\u00e3o g(x) = \\cos(x) , naturalmente. Note que g([0,1]) \\subseteq [0,1] e |g'([0,1])| \\subseteq [0,0.9] , isto \u00e9, temos que as hip\u00f3teses para a itera\u00e7\u00e3o do ponto fixo s\u00e3o v\u00e1lidas. def f(x, info): res = x - np.cos(x) if info['print']: info['iter_x'].append(x) info['iter_res'].append(res) return res def g(x, info): res = np.cos(x) if info['print']: info['iter_x'].append(x) info['iter_res'].append(res) return res x = np.linspace(0, 1, 100) y = f(x, {'iter_x': [], 'iter_res': [], 'print': False}) fig, ax = plt.subplots() ax.plot(x, y, color='r', zorder=0) ax.axhline(0, color = 'k', linestyle = '-.') ax.set_title('$f(x)= x - cos(x)$') plt.show() info_newton = {'iter_x': [], 'iter_res': [], 'print': True} info_secant = {'iter_x': [], 'iter_res': [], 'print': True} info_bisect = {'iter_x': [], 'iter_res': [], 'print': True} info_fixed = {'iter_x': [], 'iter_res': [], 'print': True} newton = scop.newton(func = f, x0 = 0.5, fprime = lambda x, info: 1 + np.sin(x), tol = 1e-10, maxiter = 200, args = (info_newton,)) secant = scop.newton(func = f, x0 = 0.5, tol = 1e-10, maxiter = 200, args = (info_secant,)) bisect = scop.bisect(f = f, a = 0, b = 1, xtol = 1e-10, maxiter = 200, args = (info_bisect,)) fixed_point = scop.fixed_point(func = g, x0 =0.5, xtol = 1e-10, bisectmethod = 'iteration', maxiter = 200, args = (info_fixed,)) plt.plot(info_bisect['iter_x'], label = 'bisect: {}'.format(len(info_bisect['iter_x']))) plt.plot(info_newton['iter_x'], label = 'newton: {}'.format(len(info_newton['iter_x']))) plt.plot(np.array(info_fixed['iter_x']), label = 'fixed point: {}'.format(len(info_fixed['iter_x']))) plt.plot(info_secant['iter_x'], label = 'secant: {}'.format(len(info_secant['iter_x']))) plt.legend() plt.title('Comparando alguns m\u00e9todos') plt.xscale('log') plt.show()","title":"M\u00e9todos num\u00e9ricos para solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares"},{"location":"analisenum/application_newton/non_linear_equations/#metodos-numericos-para-solucao-de-equacoes-nao-lineares","text":"Escrevemos no parte anterior o resumo dos algoritmos de alguns m\u00e9todos para solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares, e n\u00e3o s\u00e3o os \u00fanicos. Modifica\u00e7\u00f5es desses m\u00e9todos, em especial do M\u00e9todo de Newton s\u00e3o constantemente sugeridas para melhorar a converg\u00eancia. Fica claro que muitos sistemas no mundo real s\u00e3o n\u00e3o lineares. Uma aplica\u00e7\u00e3o comum \u00e9 resolver problemas de otimiza\u00e7\u00e3o. Por exemplo, quando queremos maximizar em conjuntos abertos, se conseguirmos provar algumas condi\u00e7\u00f5es, podemos assegurar que o m\u00e1ximo se encontra quando f'(x) = 0 . Logo, o problema de otimiza\u00e7\u00e3o se resume a um problema de encontrar ra\u00edzes. import numpy as np import scipy.special as scis import scipy.optimize as scop import matplotlib.pyplot as plt %matplotlib inline Um jogador A ganha com placar (21-0) do jogador B em um jogo de raquetebol com probabilidade . P = \\frac{p+1}{2}\\left(\\frac{p}{1-p+p^2}\\right)^{21}, em que p \u00e9 a probabilidade de A ganhar um rally qualquer. Qual o valor de p que assegura que A vencer\u00e1 com esse placar em pelo menos metade dos jogos? Esse \u00e9 um problema real proposto por Ralph Levine para Joseph Keller. def P(p): return (p+1)/2 * (p/(1 + p**2-p))**(21) p_values = np.linspace(0, 1, 50) P_values = P(p_values) plt.plot(p_values, P_values, color='r') plt.axhline(0.5, linestyle = '--', color = 'darkblue') plt.title('Probabilidade de 21-0 para cada p') plt.text(0.7, 0.55, '$P = 0.5$', fontsize=12) plt.xlabel('$p$') plt.ylabel('$P$') plt.show() Vamos explorar m\u00e9todos de resolver a equa\u00e7\u00e3o P(p) = 0.5 para p , isto \u00e9, P(p) - 0.5 = 0 , atrav\u00e9s do m\u00e9todo do Ponto Fixo e do M\u00e9todo de Newton.","title":"M\u00e9todos num\u00e9ricos para solu\u00e7\u00e3o de equa\u00e7\u00f5es n\u00e3o lineares"},{"location":"analisenum/application_newton/non_linear_equations/#iteracao-de-ponto-fixo","text":"Temos que f(p) = 0.5 - \\frac{p+1}{2}\\left(\\frac{p}{1-p+p^2}\\right)^{21} + p = p, \u00e9 a fun\u00e7\u00e3o que admite o ponto fixo que queremos. Um ponto importante seria provar as condi\u00e7\u00f5es de funcionamento do m\u00e9todo. Por\u00e9m, n\u00e3o \u00e9 dif\u00edcil ver que f([0,1]) \\not \\subseteq [0,1] , o que j\u00e1 quebra nosso teorema. def f(p): return 0.5 - P(p) + p def fixed_point(x0, tol = 1e-5, max_ite = 1e4): x1 = f(x0) err = [abs(x1 - x0)] sols = [x0, x1] while err[-1] > tol and len(err) <= max_ite: sols.append(f(sols[-1])) err.append(abs(sols[-1] - sols[-2])) return {'sol': sols, 'errors': err} print(f(0.7)) 1.1329638832103943 Bom, mas vamos supor que mesmo assim quis\u00e9ssemos usar esse m\u00e9todo. res = fixed_point(0.7) plt.scatter(range(len(res['sol'])), res['sol']) <matplotlib.collections.PathCollection at 0x7f6a17b5eb38> Ele n\u00e3o converge! Mas a gente j\u00e1 devia ter ficado desconfiado, pois justamente as condi\u00e7\u00f5es do Teorema n\u00e3o functionavam. Em particular, \u00e9 f\u00e1cil ver que ele n\u00e3o \u00e9 nem n\u00e3o expansivo. Usando Scipy: scop.fixed_point(func = f, x0 = 0.81, method = 'iteration') --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-6-93ff8160efab> in <module> ----> 1 scop.fixed_point(func = f, x0 = 0.81, method = 'iteration') ~/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py in fixed_point(func, x0, args, xtol, maxiter, method) 935 use_accel = {'del2': True, 'iteration': False}[method] 936 x0 = _asarray_validated(x0, as_inexact=True) --> 937 return _fixed_point_helper(func, x0, args, xtol, maxiter, use_accel) ~/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py in _fixed_point_helper(func, x0, args, xtol, maxiter, use_accel) 889 p0 = p 890 msg = \"Failed to converge after %d iterations, value is %s\" % (maxiter, p) --> 891 raise RuntimeError(msg) 892 893 RuntimeError: Failed to converge after 500 iterations, value is 0.4998542288112865 Existe uma varia\u00e7\u00e3o desse dado pelo m\u00e9todo Steffensen com Aitken's \\Delta^2 , que constr\u00f3i uma sequ\u00eancia com onverg\u00eancia mais r\u00e1pida, a partir da inicial. %timeit scop.fixed_point(func = f, x0 = 0.3, method = 'del2') 1.08 ms \u00b1 120 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop.fixed_point(func = f, x0 = 0.3, method = 'del2') array(0.84230479) %timeit scop.fixed_point(func = f, x0 = 0.8, method = 'del2') 1.21 ms \u00b1 165 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop.fixed_point(func = f, x0 = 0.8, method = 'del2') array(0.84230479) Olhe o que acontece com x_0 = 0.1 . scop.fixed_point(func = f, x0 = 0.1, method = 'del2') array(-3.51843721e+13)","title":"Itera\u00e7\u00e3o de Ponto Fixo"},{"location":"analisenum/application_newton/non_linear_equations/#metodo-de-newton","text":"Agora vamos usar informa\u00e7\u00e3o da derivada de P para nos ajudar com o problema de encontrar a ra\u00edz de P(p) - 0.5 = 0 . Note que P(0) < 0.5 e P(1) > 0.5 . }Para nos ajudar com as contas, vamos considerar a vers\u00e3o com log, isto \u00e9, \\log(P(p)) = - \\log(2) . Assim, \\log(p+1) - \\log(2) + 21(\\log(p) - \\log(1 - p + p^2)) = -\\log(2), o que simplica para g(p) = \\log(p+1) + 21(\\log(p) - \\log(1 - p + p^2)) = 0 . A\u00ed temos que g'(p) = \\frac{1}{p+1} + \\frac{21}{p} - \\frac{21}{1 - p + p^2}(2p - 1) def g(p): return np.log(p+1) + 21*(np.log(p) - np.log1p(p**2 - p)) def g_prime(p): return 1 / (p+1) + 21 / p - 21 * (2*p - 1) / (1 - p + p**2) Podemos provar analiticamente que a derivada \u00e9 estritamente positiva, mas faremos a observa\u00e7\u00e3o num\u00e9rica atrav\u00e9s do seguinte gr\u00e1fico. g_values = g_prime(p_values[1:]) plt.plot(p_values[1:], g_values, color='r') plt.title('Derivada da fun\u00e7\u00e3o derivada') plt.xlabel('$p$') plt.ylabel(\"$g\\'(p)$\") plt.show() Aplicando o m\u00e9todo de newton atrav\u00e9s do Scipy. Observe que o seu tempo deu bem menor que o anterior, mesmo com um valor de x_0 bem distante. %timeit scop.newton(func = g, x0 = 0.1, fprime = g_prime) 836 \u00b5s \u00b1 117 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop.newton(func = g, x0 = 0.1, fprime = g_prime) 0.8423047910355657 Observe o m\u00e9todo da Secante, um m\u00e9todo que tamb\u00e9m usa a ideia de tangente, mas sem consultar a derivada. %timeit scop.newton(func = g, x0 = 0.1) 631 \u00b5s \u00b1 111 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) scop.newton(func = g, x0 = 0.1) 0.8423047910355633","title":"M\u00e9todo de Newton"},{"location":"analisenum/application_newton/non_linear_equations/#exemplo-adicional","text":"Nesse exemplo, a ideia \u00e9 verificar que Newton pode dar errado. Considere: f(x) = x\\sin(\\pi x) - \\exp(-x) f = lambda x: x * np.sin(np.pi * x) - np.exp(-x) x = np.linspace(-1, 2, 100) y = f(x) fig, ax = plt.subplots() ax.plot(x, y, color='r',zorder=0) xs = [0.57, 0.83, -0.27] texts = ['raiz$_1$', 'raiz$_2$', 'm\u00ednimo local'] for i in range(len(xs)): ax.scatter([xs[i]], [f(xs[i])],marker='x',s=60) ax.text(xs[i]+0.05, f(xs[i])-0.1, texts[i]) ax.plot(x,np.zeros_like(x),color='gray',ls='-.',alpha=0.75) ax.set_xlabel('$x$') ax.set_ylabel('$f(x)$') plt.title('$f(x)= x sin(\\pi x) - exp(-x)$') plt.show() xspace = np.linspace(-0.7, 0.7, 15) for x in xspace: print('Com x_0 = {0:5.2f}, a ra\u00edz \u00e9 {1:5.2f}'.format(x, scop.newton(f,x))) Com x_0 = -0.70, a ra\u00edz \u00e9 2.02 Com x_0 = -0.60, a ra\u00edz \u00e9 0.58 Com x_0 = -0.50, a ra\u00edz \u00e9 1.27 Com x_0 = -0.40, a ra\u00edz \u00e9 0.82 Com x_0 = -0.30, a ra\u00edz \u00e9 -0.30 Com x_0 = -0.20, a ra\u00edz \u00e9 0.82 Com x_0 = -0.10, a ra\u00edz \u00e9 2.02 Com x_0 = 0.00, a ra\u00edz \u00e9 0.82 Com x_0 = 0.10, a ra\u00edz \u00e9 0.58 Com x_0 = 0.20, a ra\u00edz \u00e9 0.58 Com x_0 = 0.30, a ra\u00edz \u00e9 0.58 Com x_0 = 0.40, a ra\u00edz \u00e9 0.58 Com x_0 = 0.50, a ra\u00edz \u00e9 0.58 Com x_0 = 0.60, a ra\u00edz \u00e9 0.58 Com x_0 = 0.70, a ra\u00edz \u00e9 0.58","title":"Exemplo adicional"},{"location":"analisenum/application_newton/non_linear_equations/#um-outro-exemplo","text":"Vamos comparar os m\u00e9todos agora com a fun\u00e7\u00e3o f(x) = x - \\cos(x) . Para o m\u00e9todo do ponto fixo, vamos utilizar a fun\u00e7\u00e3o g(x) = \\cos(x) , naturalmente. Note que g([0,1]) \\subseteq [0,1] e |g'([0,1])| \\subseteq [0,0.9] , isto \u00e9, temos que as hip\u00f3teses para a itera\u00e7\u00e3o do ponto fixo s\u00e3o v\u00e1lidas. def f(x, info): res = x - np.cos(x) if info['print']: info['iter_x'].append(x) info['iter_res'].append(res) return res def g(x, info): res = np.cos(x) if info['print']: info['iter_x'].append(x) info['iter_res'].append(res) return res x = np.linspace(0, 1, 100) y = f(x, {'iter_x': [], 'iter_res': [], 'print': False}) fig, ax = plt.subplots() ax.plot(x, y, color='r', zorder=0) ax.axhline(0, color = 'k', linestyle = '-.') ax.set_title('$f(x)= x - cos(x)$') plt.show() info_newton = {'iter_x': [], 'iter_res': [], 'print': True} info_secant = {'iter_x': [], 'iter_res': [], 'print': True} info_bisect = {'iter_x': [], 'iter_res': [], 'print': True} info_fixed = {'iter_x': [], 'iter_res': [], 'print': True} newton = scop.newton(func = f, x0 = 0.5, fprime = lambda x, info: 1 + np.sin(x), tol = 1e-10, maxiter = 200, args = (info_newton,)) secant = scop.newton(func = f, x0 = 0.5, tol = 1e-10, maxiter = 200, args = (info_secant,)) bisect = scop.bisect(f = f, a = 0, b = 1, xtol = 1e-10, maxiter = 200, args = (info_bisect,)) fixed_point = scop.fixed_point(func = g, x0 =0.5, xtol = 1e-10, bisectmethod = 'iteration', maxiter = 200, args = (info_fixed,)) plt.plot(info_bisect['iter_x'], label = 'bisect: {}'.format(len(info_bisect['iter_x']))) plt.plot(info_newton['iter_x'], label = 'newton: {}'.format(len(info_newton['iter_x']))) plt.plot(np.array(info_fixed['iter_x']), label = 'fixed point: {}'.format(len(info_fixed['iter_x']))) plt.plot(info_secant['iter_x'], label = 'secant: {}'.format(len(info_secant['iter_x']))) plt.legend() plt.title('Comparando alguns m\u00e9todos') plt.xscale('log') plt.show()","title":"Um outro exemplo"},{"location":"bayesian/decision-theory/","text":"Teoria da Decis\u00e3o Em geral, a estat\u00edstica se preocupa em propor uma decis\u00e3o frente a um problema apresentado. Nesse caso, a avalia\u00e7\u00e3o deve estar clara, como, por exemplo, com a descri\u00e7\u00e3o do procedimento e suas consequ\u00eancias. A Teoria da Decis\u00e3o entra para axiomatizar a estrutura de avaliar um estimador para algum par\u00e2metro. O crit\u00e9rio para avaliar uma tomada de decis\u00e3o \u00e9 usualmente atrav\u00e9s de uma fun\u00e7\u00e3o de perda . Alguns estat\u00edsticos discordam de us\u00e1-la, justamente porque defini-la para um problema pode levar a resultados inesperados. Seja \\mathcal{D} o espa\u00e7o das decis\u00f5es (por exemplo, uma estimativa \u00e9 uma decis\u00e3o) e \\Omega o espa\u00e7o dos par\u00e2metros. Uma fun\u00e7\u00e3o de perda \u00e9 uma fun\u00e7\u00e3o L : \\Omega \\times \\mathcal{D} \\to [0, +\\infty] e avalia uma penalidade L(\\theta, d) em tomar a decis\u00e3o d com respeito a \\theta . Quando \\mathcal{D} = h(\\Omega) , temos que L(\\theta, d) mede o erro em obter h(\\theta) por d . Escolher uma fun\u00e7\u00e3o de perda de maneira a considerar o problema em quest\u00e3o n\u00e3o \u00e9 uma tarefa f\u00e1cil. A complexidade envolvida em defini-la a partir de conceitos subjetivos leva ao uso de perdas matematicamente trat\u00e1veis, como a perda quadr\u00e1tica ou absoluta, por exemplo. Ent\u00e3o, em uma infer\u00eancia bayesiana, do ponto de vista da Teoria da Decis\u00e3o, os tr\u00eas fatores principais s\u00e3o: a fam\u00edlia param\u00e9trica de distribui\u00e7\u00f5es das observa\u00e7\u00f5es, a distribui\u00e7\u00e3o a priori dos par\u00e2metros, e a fun\u00e7\u00e3o de perda associada \u00e0s decis\u00f5es. Inclusive a subjetividade em definir a fun\u00e7\u00e3o de perda e a priori n\u00e3o pode ser separada, conforme destacado por Lindley (1985) . \ud83d\udcdd Exemplo (Fun\u00e7\u00e3o de perda) Considere o problema de estimar a \\mathbb{E}[x] = \\theta , em que x \\sim Normal(\\theta, \\sigma^2) , em que \\sigma^2 \u00e9 conhecido. Nesse caso, \\mathcal{D} = \\Omega = \\mathbb{R} . Uma ideia \u00e9 usar a perda da forma L\\left(\\frac{\\delta - \\theta}{\\sigma}\\right), \\delta \\in \\mathcal{D}, \\theta \\in \\Omega, em que o m\u00ednimo de L \u00e9 em 0. Al\u00e9m disso, a divis\u00e3o pelo desvio padr\u00e3o reduz o vi\u00e9s de vari\u00e2ncia grande, principalmente quando a dimens\u00e3o de \\theta aumenta. Usualmente L(t) = t^2 \u00e9 a perda escolhida. Fun\u00e7\u00e3o utilidade Utilidade \u00e9 definida como o oposto de perda e \u00e9 utilizada quando se pretende ordenar consequ\u00eancias de decis\u00f5es. Ou seja, a utilidade sumariza os poss\u00edveis resultados de uma decis\u00e3o, como, por exemplo, o lucro da empresa. Seja \\mathcal{R} o espa\u00e7o das recompensas, que assumimos possuir uma ordena\u00e7\u00e3o total \\le de forma que para todo r_1, r_2 \\in R , tem-se que r_1 \\le r_2 ou r_2 \\le r_1 e r_1 \\le r_2 com r_2 \\le r_3 implica r_1 \\le r_3 . A primeira propriedade permite comparar qualquer duas recompensas, enquanto a segunda \u00e9 a transitividade e fixa uma no\u00e7\u00e3o que esperar\u00edamos de recompensas. Agora, vamos estender essa no\u00e7\u00e3o de ordem para \\mathcal{P} , o espa\u00e7o das distribui\u00e7\u00f5es de probabilidade em \\mathcal{R} . Assumimos que \\le est\u00e1 dispon\u00edvel em \\mathcal{P} e que satisfaz (H1) ordem total; (H2) transitividade. No caso, de certa forma, \\mathcal{R} \\subseteq \\mathcal{P} atrav\u00e9s das distribui\u00e7\u00f5es de Dirac com massa em um ponto r \\in \\mathcal{R} espec\u00edfico. Queremos construir uma fun\u00e7\u00e3o U em \\mathcal{R} que chamaremos de fun\u00e7\u00e3o de utilidade atrav\u00e9s da rela\u00e7\u00e3o \\le . Atrav\u00e9s da seguinte axiomatiza\u00e7\u00e3o, conseguimos assegurar tal exist\u00eancia. Observe que se a rela\u00e7\u00e3o \\mathbb{E}^{P_1}[U(r)] \\le \\mathbb{E}^{P_2}[U(r)] for equivalente a P_1 \\le P_2 , ent\u00e3o conseguimos determinar a exist\u00eancia dessa rela\u00e7\u00e3o em \\mathcal{P} , o que d\u00e1 uma esp\u00e9cie de rec\u00edproca do que queremos encontrar. Considere o conjunto das distribui\u00e7\u00f5es definidas em um suporte limitado \\mathcal{P}_{B} . Uma mistura \u00e9 definida como P = \\alpha P_1 + (1-\\alpha)P_2 , em que \\alpha \\in (0,1) . Assumimos que (H3) Se P_1 \\le P_2 , temos que \\forall P \\in \\mathcal{P} , \\alpha P_1 + (1-\\alpha) P \\le \\alpha P_2 + (1-\\alpha) P . (H4) Se P_1 \\le P_2 \\le P_3 , ent\u00e3o existem \\alpha, \\beta \\in (0,1) de forma que, \\alpha P_1 + (1-\\alpha) P_3 \\le P_2 \\le \\beta P_1 + (1-\\beta)P_3. Note que H4 implica que se r_1 \\le r \\le r_2 com r_1 < r_2 , ent\u00e3o existe um \u00fanico \\alpha \\in [0,1] de forma que r = \\alpha r_1 + (1-\\alpha)r_2 . Para demonstrar esse resultado, basta supor a n\u00e3o exist\u00eancia e usar um argumento de supremo e \u00ednfimo associado \u00e0 hip\u00f3tese de (H4). Com esse resultado, dados r_1 < r_2 , defina U da seguinte forma: \\begin{cases} U(r) = v, &\\text{se } r_1 \\le r \\le r_2 \\text{ e } r = vr_2 + (1-v)r_1 \\\\ U(r) = -\\frac{v}{1-v}, &\\text{se } r \\le r_1 \\text{ e } r_1 = vr_2 + (1-v)r \\\\ U(r) = \\frac{1}{v}, &\\text{se } r_2 \\le r \\text{ e } r_2 = vr + (1-v)r_1. \\end{cases} Com essa defini\u00e7\u00e3o, temos que U(r_1) = 1 e U(r_2) = 0 . Al\u00e9m do mais, U preserva rela\u00e7\u00e3o de ordem e se r = \\alpha r_1 + (1-\\alpha)r_2 , ent\u00e3o teremos que U(r) = \\alpha U(r_1) + (1-\\alpha) U(r_2) . Agora, precisamos extender a defini\u00e7\u00e3o de U para \\mathcal{P}_{B} , o que existe uma hip\u00f3tese adicional. Defina \\alpha(r) = \\frac{U(r) - U(r_1)}{U(r_2) - U(r_1)}, \\beta = \\int_{[r_1, r_2]} \\alpha(r) dP(r), em que P([r_1, r_2]) = 1 . Note que \\alpha \u00e9 obtido a partir das rela\u00e7\u00f5es do par\u00e1grafo anterior, isto \u00e9, sabemos que para cada r , existe \\alpha(r) , de forma que r = \\alpha(r) r_1 + (1-\\alpha(r))r_2 e obtemos \\alpha atrav\u00e9s da f\u00f3rmula U(r) = \\alpha(r) U(r_1) + (1-\\alpha(r)) U(r_2) . Al\u00e9m disso \\beta indica a probabilidade de selecionarmos r_1 quando escolhemos uma loteria r = \\alpha(r) r_1 + (1-\\alpha(r))r_2 segundo a distribui\u00e7\u00e3o de probabilidade P . Assumimos que (H5) P = \\beta \\delta_{r_2} + (1-\\beta) \\delta_{r_1} . Com isso, \u00e9 poss\u00edvel definir a fun\u00e7\u00e3o de utilidade em \\mathcal{P}_B . Dos resultados que se seguem, considere o seguinte teorema: Teorema: Sejam P_1, P_2 \\in \\mathcal{P}_B . Ent\u00e3o P_1 \\le P_2 se, e somente se, \\mathbb{E}^{P_1}[U(r)] \\le \\mathbb{E}^{P_2}[U(r)] . Al\u00e9m do mais, se outra fun\u00e7\u00e3o de utilidade U^* satisfaz a rela\u00e7\u00e3o de equival\u00eancia, ent\u00e3o existem constantes a > 0 e b de forma que U^*(\\cdot) = a U(\\cdot) + b . Com duas hip\u00f3teses adicionais, podemos extender esse resultado para \\mathcal{P}_E que \u00e9 o conjunto das distribui\u00e7\u00f5es P que tem \\mathbb{E}^P[U(r)] finita. Algumas cr\u00edticas ao formalismo incluem: \u00e9 imposs\u00edvel que um indiv\u00edduo consiga comparar quaisquer duas recompensas. Al\u00e9m do mais, a transitividade \u00e9 algo forte demais. \u00c0s vezes, resultados da vida real levam \u00e0 n\u00e3o transitividade. A extens\u00e3o de \\mathcal{R} para \\mathcal{P} tamb\u00e9m \u00e9 bastante problematizada, mas explica um pouco da rela\u00e7\u00e3o da priori com a escolha da fun\u00e7\u00e3o de perda, no sentido bayesiano. Um exemplo interessante \u00e9 o paradoxo de Saint Petersburg que argumenta que o valor esperado do pr\u00eamio \u00e9 infinito, mas a quantidade que os jogadores recebem \u00e9 em geral baixa. Uma solu\u00e7\u00e3o poss\u00edvel para esse paradoxo \u00e9 mudar a fun\u00e7\u00e3o de utilidade para uma limitada. Rela\u00e7\u00e3o entre utilidade e perda A Teoria da Decis\u00e3o assume que cada a\u00e7\u00e3o d \\in \\mathcal{D} pode ser avaliada e leva a uma recompensa r com utilidade U(r) . Seja U(\\theta, d) = \\mathbb{E}_{\\theta, d}[U(r)] . Temos que U mede uma proximidade entre d e h(\\theta) . Ap\u00f3s definir a fun\u00e7\u00e3o de utilidade, fazemos L(\\theta, d) = -U(\\theta, d) \\ge 0 como a fun\u00e7\u00e3o de perda. Note que essa desigualdade implica que U \u00e9 limitada superiormente por 0 . \u00c9 claro que \\min_d L(\\theta, d) , quando \\theta \u00e9 desconhecido, \u00e9 praticamente imposs\u00edvel, pois dever\u00edamos ter um resultado uniforme em \\Omega . Por isso, os frequentistas usam a no\u00e7\u00e3o de perda m\u00e9dia ou risco frequentista : R(\\theta, \\delta) = \\mathbb{E}_{\\theta}[L(\\theta, \\delta(x))] = \\int_{\\mathcal{X}} L(\\theta, \\delta(x)) f(x|\\theta) \\, dx, em que \\delta(x) \u00e9 a decis\u00e3o baseada em x quando x \\sim f(x|\\theta) . Chamamos \\delta de estimador , enquanto \\delta(x) de estimativa. No cen\u00e1rio frequentista, estimadores s\u00e3o comparados segundo a performance a longo-prazo, para todos os valores de \\theta . Note que R(\\theta, \\delta) \u00e9 uma perda m\u00e9dia ponderada sobre a distribui\u00e7\u00e3o de x . Logo, o dado observado n\u00e3o \u00e9 considerado nesse caso, o que \u00e9 uma cr\u00edtica ao m\u00e9todo. Al\u00e9m disso, existe uma controv\u00e9rsia sobre a ideia de repetir experimentos, conceito importante para o frequentismo. Por fim, para cada \\delta , temos que R(\\cdot, \\delta) \u00e9 uma fun\u00e7\u00e3o e, portanto, n\u00e3o induz uma ordem total no conjunto de procedimentos. No procedimento bayesiano, j\u00e1 integramos sobre o espa\u00e7o de \\Omega dos par\u00e2metros. Assim, usamos a perda esperada a posteriori : \\varrho(\\pi, d \\mid x) = \\mathbb{E}^{\\pi}[L(\\theta, d)| x] = \\int_{\\Omega} L(\\theta, d) \\pi(\\theta \\mid x) \\, d \\theta, em que \\pi(\\theta \\mid x) \\propto f(x \\mid \\theta)\\pi(\\theta) . O erro integrado \u00e9 definido como r(\\pi, \\delta) = \\mathbb{E}^{\\pi}[R(\\theta, \\delta)] = \\int_{\\Omega} \\int_{\\mathcal{X}} L(\\theta, \\delta(x)) f(x|\\theta) \\, dx \\, \\pi(\\theta) \\, d\\theta = \\int_{\\mathcal{X}} \\varrho(\\pi, \\delta(x) | x) m(x) \\, dx, em que a \u00faltima rela\u00e7\u00e3o \u00e9 uma aplica\u00e7\u00e3o do Teorema de Fubini dado que L \\ge 0 . Al\u00e9m do mais, para minimizar r(\\pi, \\delta) , para cada x , podemos tomar d = \\delta(x) que minimiza \\varrho(\\pi, d | x) , pela \u00faltima igualdade da express\u00e3o acima. Estimador de Bayes: Seja uma priori \\pi e uma perda L . O estimador de Bayes \u00e9 \\delta^{\\pi} que minimiza r(\\pi, \\delta) . Em particular, para cada x , temos que \\delta^{\\pi}(x) = \\arg \\min_d \\varrho(\\pi, d | x) . O risco bayesiano \u00e9 o valor r(\\pi) = r(\\pi, \\delta^{\\pi}) . Note que para perdas estritamente convexas, o estimador de Bayes \u00e9 \u00fanico. Maximalidade e admissibilidade Considere \\mathcal{D}^* o espa\u00e7o das distribui\u00e7\u00f5es de probabilidade em \\mathcal{D} . Um estimador aleatorizado \\delta^* significa tomar uma decis\u00e3o de acordo com a densidade de probabilidade \\delta^*(x, \\cdot) . A perda \u00e9 definida como L(\\theta, \\delta^*(x)) = \\int_{\\mathcal{D}} L(\\theta, a) \\delta^*(x, a) \\, da. Usar esse estimador n\u00e3o \u00e9 usual porque ele adiciona ru\u00eddo em um fen\u00f4meno para tomar uma decis\u00e3o sob incerteza. Al\u00e9m do mais, ele n\u00e3o obedece o Princ\u00edpio da Verossimilhan\u00e7a, dado que para o mesmo valor de x , podem existir v\u00e1rios valores estimados. \ud83d\udcdd Exemplo (Estimador randomizado) Podemos definir um estimador randomizado segundo \\delta^*(x_1, x_2)(t) = \\begin{cases} 1_{2t = x_1 + x_2} &\\text{se } x_1 \\neq x_2 \\\\ [1_{t = x_1 - 1} + 1_{t = x_1 + 1}]/2 &\\text{se } x_1 = x_2, \\end{cases} em que 1_{v} \u00e9 a massa de Dirac em v . Para toda priori \\pi , o risco de Bayes \u00e9 o mesmo no conjunto dos estimadores randomizados e n\u00e3o randomizados, isto \u00e9, \\inf_{\\delta \\in \\mathcal{D}} r(\\pi, \\delta) = \\inf_{\\delta^* \\in \\mathcal{D}^*} r(\\pi, \\delta^*) = r(\\pi). Como um procedimento randomizado \u00e9 a m\u00e9dia de riscos de estimadores n\u00e3o randomizados, ele n\u00e3o pode melhor\u00e1-los. Maximalidade Risco minimax : \\bar{R} = \\inf_{\\delta \\in \\mathcal{D}^*} \\sup_{\\theta} R(\\theta, \\delta) = \\inf_{\\delta \\in \\mathcal{D}^*} \\sup_{\\theta} \\mathbb{E}_{\\theta}[L(\\theta, \\delta(x))] . Um estimador minimax \u00e9 um estimador \\delta_0 que satisfaz \\sup_{\\theta} R(\\theta, \\delta_0) = \\bar{R} . Note que esse estimador, toma o pior caso para \\theta e ent\u00e3o minimiza para os procedimentos desse pior caso. Esse m\u00e9todo enxerga a natureza como um agente inimigo que tende a escolher o pior caso. O estimador minimax nem sempre existe. Para isso, condi\u00e7\u00f5es suficientes precisam ser estudadas. Se \\Omega \u00e9 finito e L \u00e9 cont\u00ednua, ent\u00e3o existe uma estrat\u00e9gia minimax. Outra proposta \u00e9 verificar que o conjunto das fun\u00e7\u00f5es de risco em \\mathcal{D} \u00e9 compacto em um espa\u00e7o maior em que \\mathcal{D} est\u00e1 inserido e que a perda \u00e9 constante. Teorema: Se \\mathcal{D} \\subseteq \\mathbb{R}^k \u00e9 um conjunto convexo compacto e L(\\theta, d) \u00e9 cont\u00ednua e convexa como fun\u00e7\u00e3o de d para \\theta fixado, ent\u00e3o existe um estimador minimax n\u00e3o randomizado. O estimador ser\u00e1 n\u00e3o randomizado pela desigualdade de Jensen . Esse resultado \u00e9 um caso particular do Teorema Rao-Blackwell . O risco de Bayes \u00e9 sempre menor do que o risco minimax , o que \u00e9 expresso matematicamente por \\underbar{R} = \\sup_{\\pi} r(\\pi) = \\sup_{\\pi} \\inf_{\\delta \\in \\mathcal{D}} r(\\pi, \\delta) \\le \\bar{R} = \\inf_{d \\in \\mathcal{D}^*} \\sup_{\\theta} R(\\theta, \\delta). A distribui\u00e7\u00e3o menos favor\u00e1vel \u00e9 \\pi^* tal que r(\\pi^*) = \\underbar{R} . O problema de estima\u00e7\u00e3o tem um valor quando \\underbar{R} = \\bar{R} . Um resultado interessante \u00e9 que se \\delta \u00e9 estimador de Bayes com respeito a \\pi e R(\\theta, \\delta) \\le r(\\pi) para todo \\theta \\in \\Omega , ent\u00e3o \\delta \u00e9 estimador minimax e \\pi \u00e9 a distribui\u00e7\u00e3o menos favor\u00e1vel. Teorema: Considere um problema estat\u00edstico que possua um valor, uma distribui\u00e7\u00e3o menos favor\u00e1vel \\pi_0 e um estimador minimax \\delta^{\\pi_0} . Ent\u00e3o se \\Omega \\subseteq \\mathbb{R} \u00e9 compacto e R(\\theta, \\delta^{\\pi_0}) \u00e9 fun\u00e7\u00e3o anal\u00edtica de \\theta , ent\u00e3o \\pi_0 tem suporte finito ou R(\\theta, \\delta^{\\pi_0}) \u00e9 constante. Esse teorema mostra que o minimax n\u00e3o \u00e9 um bom estimador do ponto de vista bayesiano, dado que (1) ele pode ser randomizado ou (2) ele pode levar a prioris n\u00e3o real\u00edsticas com suporte finito. Admissibilidade Admissibilidade: Um estimador \\delta_0 \u00e9 inadmiss\u00edvel se existe um estimador \\delta_1 que domina \\delta_0 , isto \u00e9, R(\\theta, \\delta_0) \\ge R(\\theta, \\delta_1) para todo \\theta \\in \\Omega , e pelo menos para um valor \\theta_0 , vale a desigualdade estrita. Caso contr\u00e1rio, o estimador \u00e9 admiss\u00edvel. Construir um estimador apenas considerando a admissibilidade n\u00e3o \u00e9 uma boa estrat\u00e9gia, afinal \\delta(x) = \\theta_0 \\in \\Omega \u00e9 um estimador que tem valor exato para \\theta = \\theta_0 . Logo, faz sentido considerar maximalidade simultaneamente. O interessante \u00e9 que se existe um \u00fanico estimador minimax, ent\u00e3o ele \u00e9 admiss\u00edvel. A rec\u00edproca \u00e9 falsa em geral, mas se \\delta_0 \u00e9 admiss\u00edvel com risco constante, ent\u00e3o ele \u00e9 o \u00fanico estimador minimax. A rela\u00e7\u00e3o de admissibilidade com estimadores de Bayes \u00e9 bem estrita: (1) Se a priori \\pi \u00e9 estritamente positiva em \\Omega , com risco de Bayes finito, e a fun\u00e7\u00e3o de risco \u00e9 cont\u00ednua em \\theta para todo \\delta , ent\u00e3o o estimador de Bayes \\delta^{\\pi} \u00e9 admiss\u00edvel. (2) Se o estimador de Bayes \\delta^{\\pi} \u00e9 \u00fanico, ent\u00e3o ele \u00e9 admiss\u00edvel. Perdas cl\u00e1ssicas Essas perdas s\u00e3o trat\u00e1veis matematicamente e bem documentadas, mesmo que n\u00e3o representem perfeitamente o problema em quest\u00e3o. Perda quadr\u00e1tica \u00c9 definida como L(\\theta, d) = (\\theta - d)^2 . Provavelmente a perda mais utilizada. Penalizada fortemente desvios altos. Mas, como a perda \u00e9 convexa e vale a desigualdade de Jensen mencionada mais acima, o que exclui estimadores randomizados. O interessante \u00e9 que, sob essa perda, o estimador de Bayes \u00e9 a m\u00e9dia a posteriori, um dos valores que pensar\u00edamos naturalmente, mesmo sem adicionar a carga da teoria da decis\u00e3o. Proposi\u00e7\u00e3o: O estimador de Bayes \\delta^{\\pi} associado com a perda quadr\u00e1tica L \u00e9 a esperan\u00e7a a posteriori \\delta^{\\pi}(x) = \\mathbb{E}^{\\pi}[\\theta | x] . O resultado imediato ocorre quando L(\\theta, \\delta) = w(\\theta)(\\theta - \\delta)^2 , como uma pondera\u00e7\u00e3o. Nesse caso, o estimador de Bayes \u00e9 \\delta^{\\pi}(x) = \\frac{\\mathbb{E}^{\\pi}[w(\\theta) \\theta | x]}{\\mathbb{E}^{\\pi}[w(\\theta)| x]}. Perda absoluta Uma alternativa \u00e0 perda quadr\u00e1tica \u00e9 L(\\theta, d) = |\\theta - d| , que pode ser generalizada para L_{k_1, k_2}(\\theta, \\delta) = \\begin{cases} k_2(\\theta - d) &\\text{se } \\theta > d \\\\ k_1(d - \\theta) &\\text{c.c.} \\end{cases} A penaliza\u00e7\u00e3o para desvios maiores \u00e9 menor, apesar de manter a convexidade. \u00e9 poss\u00edvel tamb\u00e9m propor uma perda como uma mistura dessas perdas. Em uma regi\u00e3o pr\u00f3xima de zero, usamos a perda quadr\u00e1tica. Depois, usamos a perda absoluta. Com essa perda, por exemplo, n\u00e3o existe estimador de Bayes em forma fechada. O estimador de Bayes associado a L_{k_1, k_2}(\\theta, \\delta) e a \\pi \u00e9 um quartil k_2/(k_1 + k_2) de \\pi(\\theta | x) . Em particular, quando k_1 = k_2 , o estimador \u00e9 a mediana a posteriori. Perda 0-1 Essa perda \u00e9 mais utilizada no contexto de teste de hip\u00f3teses. Ela \u00e9 definida como L(\\theta, \\delta) = 1 - 1_{\\theta = \\delta} . \ud83d\udcdd Exemplo (Teste de hip\u00f3teses) Seja o teste de hip\u00f3teses H_0 : \\theta \\in \\Omega_0 e H_1 : \\theta \\in \\Omega_1 . Ent\u00e3o \\mathcal{D} = \\{0, 1\\} em que 0 significa rejeitar H_0 . Logo queremos estimar a fun\u00e7\u00e3o 1_{\\theta \\in \\Omega_0} . O risco frequentista \u00e9 R(\\theta, \\delta) = \\begin{cases} \\Pr_{\\theta}(\\delta(x) = 0), &\\text{se } \\theta \\in \\Omega_0 \\\\ \\Pr_{\\theta}(\\delta(x) = 1), &\\text{c.c.,} \\end{cases} que s\u00e3o os erros do tipo 1 e do tipo 2, respectivamente. O estimador de Bayes \u00e9 dado por \\delta^{\\pi}(x) = \\begin{cases} 1 &\\text{se }\\Pr(\\theta \\in \\Omega_0 | x) > \\Pr(\\theta \\in \\Omega_1 | x) \\\\ 0, &\\text{c.c.} \\end{cases} Perdas intr\u00ednsecas \u00c0s vezes, estamos em uma situa\u00e7\u00e3o n\u00e3o informativa sobre a parametriza\u00e7\u00e3o natural e a escolha da fun\u00e7\u00e3o de perda. O estimador de Bayes n\u00e3o \u00e9 invariante por transforma\u00e7\u00f5es biun\u00edvocas em geral. Dessa forma, pode ser interessante obter perdas invariantes. Nesse caso, comparar f(\\cdot | \\delta) com f(\\cdot | \\theta) pode ser interessante, isto \u00e9, definir L(\\theta, \\delta) = d(f(\\cdot | \\theta), f(\\cdot | \\delta)). Duas dist\u00e2ncias usuais s\u00e3o: (1) entropia, Kullback\u2013Leibler divergence, ou (2) Hellinger. Elas resultam nas seguintes perdas: (1) L_e(\\theta, \\delta) = \\mathbb{E}_{\\theta}\\left[\\log\\left(\\frac{f(x|\\theta)}{f(x|\\delta)}\\right)\\right]. (2) L_H(\\theta, \\delta) = \\frac{1}{2}\\mathbb{E}_{\\theta}\\left[\\left(\\sqrt{\\frac{f(x|\\delta)}{f(x|\\theta)}} - 1\\right)^2\\right]. Links Optimal Statistical Decisions, Morris DeGroot : esse livro expande os resultados de teoria da decis\u00e3o apresentados no livro do Robert.","title":"Teoria da Decis\u00e3o"},{"location":"bayesian/decision-theory/#teoria-da-decisao","text":"Em geral, a estat\u00edstica se preocupa em propor uma decis\u00e3o frente a um problema apresentado. Nesse caso, a avalia\u00e7\u00e3o deve estar clara, como, por exemplo, com a descri\u00e7\u00e3o do procedimento e suas consequ\u00eancias. A Teoria da Decis\u00e3o entra para axiomatizar a estrutura de avaliar um estimador para algum par\u00e2metro. O crit\u00e9rio para avaliar uma tomada de decis\u00e3o \u00e9 usualmente atrav\u00e9s de uma fun\u00e7\u00e3o de perda . Alguns estat\u00edsticos discordam de us\u00e1-la, justamente porque defini-la para um problema pode levar a resultados inesperados. Seja \\mathcal{D} o espa\u00e7o das decis\u00f5es (por exemplo, uma estimativa \u00e9 uma decis\u00e3o) e \\Omega o espa\u00e7o dos par\u00e2metros. Uma fun\u00e7\u00e3o de perda \u00e9 uma fun\u00e7\u00e3o L : \\Omega \\times \\mathcal{D} \\to [0, +\\infty] e avalia uma penalidade L(\\theta, d) em tomar a decis\u00e3o d com respeito a \\theta . Quando \\mathcal{D} = h(\\Omega) , temos que L(\\theta, d) mede o erro em obter h(\\theta) por d . Escolher uma fun\u00e7\u00e3o de perda de maneira a considerar o problema em quest\u00e3o n\u00e3o \u00e9 uma tarefa f\u00e1cil. A complexidade envolvida em defini-la a partir de conceitos subjetivos leva ao uso de perdas matematicamente trat\u00e1veis, como a perda quadr\u00e1tica ou absoluta, por exemplo. Ent\u00e3o, em uma infer\u00eancia bayesiana, do ponto de vista da Teoria da Decis\u00e3o, os tr\u00eas fatores principais s\u00e3o: a fam\u00edlia param\u00e9trica de distribui\u00e7\u00f5es das observa\u00e7\u00f5es, a distribui\u00e7\u00e3o a priori dos par\u00e2metros, e a fun\u00e7\u00e3o de perda associada \u00e0s decis\u00f5es. Inclusive a subjetividade em definir a fun\u00e7\u00e3o de perda e a priori n\u00e3o pode ser separada, conforme destacado por Lindley (1985) . \ud83d\udcdd Exemplo (Fun\u00e7\u00e3o de perda) Considere o problema de estimar a \\mathbb{E}[x] = \\theta , em que x \\sim Normal(\\theta, \\sigma^2) , em que \\sigma^2 \u00e9 conhecido. Nesse caso, \\mathcal{D} = \\Omega = \\mathbb{R} . Uma ideia \u00e9 usar a perda da forma L\\left(\\frac{\\delta - \\theta}{\\sigma}\\right), \\delta \\in \\mathcal{D}, \\theta \\in \\Omega, em que o m\u00ednimo de L \u00e9 em 0. Al\u00e9m disso, a divis\u00e3o pelo desvio padr\u00e3o reduz o vi\u00e9s de vari\u00e2ncia grande, principalmente quando a dimens\u00e3o de \\theta aumenta. Usualmente L(t) = t^2 \u00e9 a perda escolhida.","title":"Teoria da Decis\u00e3o"},{"location":"bayesian/decision-theory/#funcao-utilidade","text":"Utilidade \u00e9 definida como o oposto de perda e \u00e9 utilizada quando se pretende ordenar consequ\u00eancias de decis\u00f5es. Ou seja, a utilidade sumariza os poss\u00edveis resultados de uma decis\u00e3o, como, por exemplo, o lucro da empresa. Seja \\mathcal{R} o espa\u00e7o das recompensas, que assumimos possuir uma ordena\u00e7\u00e3o total \\le de forma que para todo r_1, r_2 \\in R , tem-se que r_1 \\le r_2 ou r_2 \\le r_1 e r_1 \\le r_2 com r_2 \\le r_3 implica r_1 \\le r_3 . A primeira propriedade permite comparar qualquer duas recompensas, enquanto a segunda \u00e9 a transitividade e fixa uma no\u00e7\u00e3o que esperar\u00edamos de recompensas. Agora, vamos estender essa no\u00e7\u00e3o de ordem para \\mathcal{P} , o espa\u00e7o das distribui\u00e7\u00f5es de probabilidade em \\mathcal{R} . Assumimos que \\le est\u00e1 dispon\u00edvel em \\mathcal{P} e que satisfaz (H1) ordem total; (H2) transitividade. No caso, de certa forma, \\mathcal{R} \\subseteq \\mathcal{P} atrav\u00e9s das distribui\u00e7\u00f5es de Dirac com massa em um ponto r \\in \\mathcal{R} espec\u00edfico. Queremos construir uma fun\u00e7\u00e3o U em \\mathcal{R} que chamaremos de fun\u00e7\u00e3o de utilidade atrav\u00e9s da rela\u00e7\u00e3o \\le . Atrav\u00e9s da seguinte axiomatiza\u00e7\u00e3o, conseguimos assegurar tal exist\u00eancia. Observe que se a rela\u00e7\u00e3o \\mathbb{E}^{P_1}[U(r)] \\le \\mathbb{E}^{P_2}[U(r)] for equivalente a P_1 \\le P_2 , ent\u00e3o conseguimos determinar a exist\u00eancia dessa rela\u00e7\u00e3o em \\mathcal{P} , o que d\u00e1 uma esp\u00e9cie de rec\u00edproca do que queremos encontrar. Considere o conjunto das distribui\u00e7\u00f5es definidas em um suporte limitado \\mathcal{P}_{B} . Uma mistura \u00e9 definida como P = \\alpha P_1 + (1-\\alpha)P_2 , em que \\alpha \\in (0,1) . Assumimos que (H3) Se P_1 \\le P_2 , temos que \\forall P \\in \\mathcal{P} , \\alpha P_1 + (1-\\alpha) P \\le \\alpha P_2 + (1-\\alpha) P . (H4) Se P_1 \\le P_2 \\le P_3 , ent\u00e3o existem \\alpha, \\beta \\in (0,1) de forma que, \\alpha P_1 + (1-\\alpha) P_3 \\le P_2 \\le \\beta P_1 + (1-\\beta)P_3. Note que H4 implica que se r_1 \\le r \\le r_2 com r_1 < r_2 , ent\u00e3o existe um \u00fanico \\alpha \\in [0,1] de forma que r = \\alpha r_1 + (1-\\alpha)r_2 . Para demonstrar esse resultado, basta supor a n\u00e3o exist\u00eancia e usar um argumento de supremo e \u00ednfimo associado \u00e0 hip\u00f3tese de (H4). Com esse resultado, dados r_1 < r_2 , defina U da seguinte forma: \\begin{cases} U(r) = v, &\\text{se } r_1 \\le r \\le r_2 \\text{ e } r = vr_2 + (1-v)r_1 \\\\ U(r) = -\\frac{v}{1-v}, &\\text{se } r \\le r_1 \\text{ e } r_1 = vr_2 + (1-v)r \\\\ U(r) = \\frac{1}{v}, &\\text{se } r_2 \\le r \\text{ e } r_2 = vr + (1-v)r_1. \\end{cases} Com essa defini\u00e7\u00e3o, temos que U(r_1) = 1 e U(r_2) = 0 . Al\u00e9m do mais, U preserva rela\u00e7\u00e3o de ordem e se r = \\alpha r_1 + (1-\\alpha)r_2 , ent\u00e3o teremos que U(r) = \\alpha U(r_1) + (1-\\alpha) U(r_2) . Agora, precisamos extender a defini\u00e7\u00e3o de U para \\mathcal{P}_{B} , o que existe uma hip\u00f3tese adicional. Defina \\alpha(r) = \\frac{U(r) - U(r_1)}{U(r_2) - U(r_1)}, \\beta = \\int_{[r_1, r_2]} \\alpha(r) dP(r), em que P([r_1, r_2]) = 1 . Note que \\alpha \u00e9 obtido a partir das rela\u00e7\u00f5es do par\u00e1grafo anterior, isto \u00e9, sabemos que para cada r , existe \\alpha(r) , de forma que r = \\alpha(r) r_1 + (1-\\alpha(r))r_2 e obtemos \\alpha atrav\u00e9s da f\u00f3rmula U(r) = \\alpha(r) U(r_1) + (1-\\alpha(r)) U(r_2) . Al\u00e9m disso \\beta indica a probabilidade de selecionarmos r_1 quando escolhemos uma loteria r = \\alpha(r) r_1 + (1-\\alpha(r))r_2 segundo a distribui\u00e7\u00e3o de probabilidade P . Assumimos que (H5) P = \\beta \\delta_{r_2} + (1-\\beta) \\delta_{r_1} . Com isso, \u00e9 poss\u00edvel definir a fun\u00e7\u00e3o de utilidade em \\mathcal{P}_B . Dos resultados que se seguem, considere o seguinte teorema: Teorema: Sejam P_1, P_2 \\in \\mathcal{P}_B . Ent\u00e3o P_1 \\le P_2 se, e somente se, \\mathbb{E}^{P_1}[U(r)] \\le \\mathbb{E}^{P_2}[U(r)] . Al\u00e9m do mais, se outra fun\u00e7\u00e3o de utilidade U^* satisfaz a rela\u00e7\u00e3o de equival\u00eancia, ent\u00e3o existem constantes a > 0 e b de forma que U^*(\\cdot) = a U(\\cdot) + b . Com duas hip\u00f3teses adicionais, podemos extender esse resultado para \\mathcal{P}_E que \u00e9 o conjunto das distribui\u00e7\u00f5es P que tem \\mathbb{E}^P[U(r)] finita. Algumas cr\u00edticas ao formalismo incluem: \u00e9 imposs\u00edvel que um indiv\u00edduo consiga comparar quaisquer duas recompensas. Al\u00e9m do mais, a transitividade \u00e9 algo forte demais. \u00c0s vezes, resultados da vida real levam \u00e0 n\u00e3o transitividade. A extens\u00e3o de \\mathcal{R} para \\mathcal{P} tamb\u00e9m \u00e9 bastante problematizada, mas explica um pouco da rela\u00e7\u00e3o da priori com a escolha da fun\u00e7\u00e3o de perda, no sentido bayesiano. Um exemplo interessante \u00e9 o paradoxo de Saint Petersburg que argumenta que o valor esperado do pr\u00eamio \u00e9 infinito, mas a quantidade que os jogadores recebem \u00e9 em geral baixa. Uma solu\u00e7\u00e3o poss\u00edvel para esse paradoxo \u00e9 mudar a fun\u00e7\u00e3o de utilidade para uma limitada.","title":"Fun\u00e7\u00e3o utilidade"},{"location":"bayesian/decision-theory/#relacao-entre-utilidade-e-perda","text":"A Teoria da Decis\u00e3o assume que cada a\u00e7\u00e3o d \\in \\mathcal{D} pode ser avaliada e leva a uma recompensa r com utilidade U(r) . Seja U(\\theta, d) = \\mathbb{E}_{\\theta, d}[U(r)] . Temos que U mede uma proximidade entre d e h(\\theta) . Ap\u00f3s definir a fun\u00e7\u00e3o de utilidade, fazemos L(\\theta, d) = -U(\\theta, d) \\ge 0 como a fun\u00e7\u00e3o de perda. Note que essa desigualdade implica que U \u00e9 limitada superiormente por 0 . \u00c9 claro que \\min_d L(\\theta, d) , quando \\theta \u00e9 desconhecido, \u00e9 praticamente imposs\u00edvel, pois dever\u00edamos ter um resultado uniforme em \\Omega . Por isso, os frequentistas usam a no\u00e7\u00e3o de perda m\u00e9dia ou risco frequentista : R(\\theta, \\delta) = \\mathbb{E}_{\\theta}[L(\\theta, \\delta(x))] = \\int_{\\mathcal{X}} L(\\theta, \\delta(x)) f(x|\\theta) \\, dx, em que \\delta(x) \u00e9 a decis\u00e3o baseada em x quando x \\sim f(x|\\theta) . Chamamos \\delta de estimador , enquanto \\delta(x) de estimativa. No cen\u00e1rio frequentista, estimadores s\u00e3o comparados segundo a performance a longo-prazo, para todos os valores de \\theta . Note que R(\\theta, \\delta) \u00e9 uma perda m\u00e9dia ponderada sobre a distribui\u00e7\u00e3o de x . Logo, o dado observado n\u00e3o \u00e9 considerado nesse caso, o que \u00e9 uma cr\u00edtica ao m\u00e9todo. Al\u00e9m disso, existe uma controv\u00e9rsia sobre a ideia de repetir experimentos, conceito importante para o frequentismo. Por fim, para cada \\delta , temos que R(\\cdot, \\delta) \u00e9 uma fun\u00e7\u00e3o e, portanto, n\u00e3o induz uma ordem total no conjunto de procedimentos. No procedimento bayesiano, j\u00e1 integramos sobre o espa\u00e7o de \\Omega dos par\u00e2metros. Assim, usamos a perda esperada a posteriori : \\varrho(\\pi, d \\mid x) = \\mathbb{E}^{\\pi}[L(\\theta, d)| x] = \\int_{\\Omega} L(\\theta, d) \\pi(\\theta \\mid x) \\, d \\theta, em que \\pi(\\theta \\mid x) \\propto f(x \\mid \\theta)\\pi(\\theta) . O erro integrado \u00e9 definido como r(\\pi, \\delta) = \\mathbb{E}^{\\pi}[R(\\theta, \\delta)] = \\int_{\\Omega} \\int_{\\mathcal{X}} L(\\theta, \\delta(x)) f(x|\\theta) \\, dx \\, \\pi(\\theta) \\, d\\theta = \\int_{\\mathcal{X}} \\varrho(\\pi, \\delta(x) | x) m(x) \\, dx, em que a \u00faltima rela\u00e7\u00e3o \u00e9 uma aplica\u00e7\u00e3o do Teorema de Fubini dado que L \\ge 0 . Al\u00e9m do mais, para minimizar r(\\pi, \\delta) , para cada x , podemos tomar d = \\delta(x) que minimiza \\varrho(\\pi, d | x) , pela \u00faltima igualdade da express\u00e3o acima. Estimador de Bayes: Seja uma priori \\pi e uma perda L . O estimador de Bayes \u00e9 \\delta^{\\pi} que minimiza r(\\pi, \\delta) . Em particular, para cada x , temos que \\delta^{\\pi}(x) = \\arg \\min_d \\varrho(\\pi, d | x) . O risco bayesiano \u00e9 o valor r(\\pi) = r(\\pi, \\delta^{\\pi}) . Note que para perdas estritamente convexas, o estimador de Bayes \u00e9 \u00fanico.","title":"Rela\u00e7\u00e3o entre utilidade e perda"},{"location":"bayesian/decision-theory/#maximalidade-e-admissibilidade","text":"Considere \\mathcal{D}^* o espa\u00e7o das distribui\u00e7\u00f5es de probabilidade em \\mathcal{D} . Um estimador aleatorizado \\delta^* significa tomar uma decis\u00e3o de acordo com a densidade de probabilidade \\delta^*(x, \\cdot) . A perda \u00e9 definida como L(\\theta, \\delta^*(x)) = \\int_{\\mathcal{D}} L(\\theta, a) \\delta^*(x, a) \\, da. Usar esse estimador n\u00e3o \u00e9 usual porque ele adiciona ru\u00eddo em um fen\u00f4meno para tomar uma decis\u00e3o sob incerteza. Al\u00e9m do mais, ele n\u00e3o obedece o Princ\u00edpio da Verossimilhan\u00e7a, dado que para o mesmo valor de x , podem existir v\u00e1rios valores estimados. \ud83d\udcdd Exemplo (Estimador randomizado) Podemos definir um estimador randomizado segundo \\delta^*(x_1, x_2)(t) = \\begin{cases} 1_{2t = x_1 + x_2} &\\text{se } x_1 \\neq x_2 \\\\ [1_{t = x_1 - 1} + 1_{t = x_1 + 1}]/2 &\\text{se } x_1 = x_2, \\end{cases} em que 1_{v} \u00e9 a massa de Dirac em v . Para toda priori \\pi , o risco de Bayes \u00e9 o mesmo no conjunto dos estimadores randomizados e n\u00e3o randomizados, isto \u00e9, \\inf_{\\delta \\in \\mathcal{D}} r(\\pi, \\delta) = \\inf_{\\delta^* \\in \\mathcal{D}^*} r(\\pi, \\delta^*) = r(\\pi). Como um procedimento randomizado \u00e9 a m\u00e9dia de riscos de estimadores n\u00e3o randomizados, ele n\u00e3o pode melhor\u00e1-los.","title":"Maximalidade e admissibilidade"},{"location":"bayesian/decision-theory/#maximalidade","text":"Risco minimax : \\bar{R} = \\inf_{\\delta \\in \\mathcal{D}^*} \\sup_{\\theta} R(\\theta, \\delta) = \\inf_{\\delta \\in \\mathcal{D}^*} \\sup_{\\theta} \\mathbb{E}_{\\theta}[L(\\theta, \\delta(x))] . Um estimador minimax \u00e9 um estimador \\delta_0 que satisfaz \\sup_{\\theta} R(\\theta, \\delta_0) = \\bar{R} . Note que esse estimador, toma o pior caso para \\theta e ent\u00e3o minimiza para os procedimentos desse pior caso. Esse m\u00e9todo enxerga a natureza como um agente inimigo que tende a escolher o pior caso. O estimador minimax nem sempre existe. Para isso, condi\u00e7\u00f5es suficientes precisam ser estudadas. Se \\Omega \u00e9 finito e L \u00e9 cont\u00ednua, ent\u00e3o existe uma estrat\u00e9gia minimax. Outra proposta \u00e9 verificar que o conjunto das fun\u00e7\u00f5es de risco em \\mathcal{D} \u00e9 compacto em um espa\u00e7o maior em que \\mathcal{D} est\u00e1 inserido e que a perda \u00e9 constante. Teorema: Se \\mathcal{D} \\subseteq \\mathbb{R}^k \u00e9 um conjunto convexo compacto e L(\\theta, d) \u00e9 cont\u00ednua e convexa como fun\u00e7\u00e3o de d para \\theta fixado, ent\u00e3o existe um estimador minimax n\u00e3o randomizado. O estimador ser\u00e1 n\u00e3o randomizado pela desigualdade de Jensen . Esse resultado \u00e9 um caso particular do Teorema Rao-Blackwell . O risco de Bayes \u00e9 sempre menor do que o risco minimax , o que \u00e9 expresso matematicamente por \\underbar{R} = \\sup_{\\pi} r(\\pi) = \\sup_{\\pi} \\inf_{\\delta \\in \\mathcal{D}} r(\\pi, \\delta) \\le \\bar{R} = \\inf_{d \\in \\mathcal{D}^*} \\sup_{\\theta} R(\\theta, \\delta). A distribui\u00e7\u00e3o menos favor\u00e1vel \u00e9 \\pi^* tal que r(\\pi^*) = \\underbar{R} . O problema de estima\u00e7\u00e3o tem um valor quando \\underbar{R} = \\bar{R} . Um resultado interessante \u00e9 que se \\delta \u00e9 estimador de Bayes com respeito a \\pi e R(\\theta, \\delta) \\le r(\\pi) para todo \\theta \\in \\Omega , ent\u00e3o \\delta \u00e9 estimador minimax e \\pi \u00e9 a distribui\u00e7\u00e3o menos favor\u00e1vel. Teorema: Considere um problema estat\u00edstico que possua um valor, uma distribui\u00e7\u00e3o menos favor\u00e1vel \\pi_0 e um estimador minimax \\delta^{\\pi_0} . Ent\u00e3o se \\Omega \\subseteq \\mathbb{R} \u00e9 compacto e R(\\theta, \\delta^{\\pi_0}) \u00e9 fun\u00e7\u00e3o anal\u00edtica de \\theta , ent\u00e3o \\pi_0 tem suporte finito ou R(\\theta, \\delta^{\\pi_0}) \u00e9 constante. Esse teorema mostra que o minimax n\u00e3o \u00e9 um bom estimador do ponto de vista bayesiano, dado que (1) ele pode ser randomizado ou (2) ele pode levar a prioris n\u00e3o real\u00edsticas com suporte finito.","title":"Maximalidade"},{"location":"bayesian/decision-theory/#admissibilidade","text":"Admissibilidade: Um estimador \\delta_0 \u00e9 inadmiss\u00edvel se existe um estimador \\delta_1 que domina \\delta_0 , isto \u00e9, R(\\theta, \\delta_0) \\ge R(\\theta, \\delta_1) para todo \\theta \\in \\Omega , e pelo menos para um valor \\theta_0 , vale a desigualdade estrita. Caso contr\u00e1rio, o estimador \u00e9 admiss\u00edvel. Construir um estimador apenas considerando a admissibilidade n\u00e3o \u00e9 uma boa estrat\u00e9gia, afinal \\delta(x) = \\theta_0 \\in \\Omega \u00e9 um estimador que tem valor exato para \\theta = \\theta_0 . Logo, faz sentido considerar maximalidade simultaneamente. O interessante \u00e9 que se existe um \u00fanico estimador minimax, ent\u00e3o ele \u00e9 admiss\u00edvel. A rec\u00edproca \u00e9 falsa em geral, mas se \\delta_0 \u00e9 admiss\u00edvel com risco constante, ent\u00e3o ele \u00e9 o \u00fanico estimador minimax. A rela\u00e7\u00e3o de admissibilidade com estimadores de Bayes \u00e9 bem estrita: (1) Se a priori \\pi \u00e9 estritamente positiva em \\Omega , com risco de Bayes finito, e a fun\u00e7\u00e3o de risco \u00e9 cont\u00ednua em \\theta para todo \\delta , ent\u00e3o o estimador de Bayes \\delta^{\\pi} \u00e9 admiss\u00edvel. (2) Se o estimador de Bayes \\delta^{\\pi} \u00e9 \u00fanico, ent\u00e3o ele \u00e9 admiss\u00edvel.","title":"Admissibilidade"},{"location":"bayesian/decision-theory/#perdas-classicas","text":"Essas perdas s\u00e3o trat\u00e1veis matematicamente e bem documentadas, mesmo que n\u00e3o representem perfeitamente o problema em quest\u00e3o.","title":"Perdas cl\u00e1ssicas"},{"location":"bayesian/decision-theory/#perda-quadratica","text":"\u00c9 definida como L(\\theta, d) = (\\theta - d)^2 . Provavelmente a perda mais utilizada. Penalizada fortemente desvios altos. Mas, como a perda \u00e9 convexa e vale a desigualdade de Jensen mencionada mais acima, o que exclui estimadores randomizados. O interessante \u00e9 que, sob essa perda, o estimador de Bayes \u00e9 a m\u00e9dia a posteriori, um dos valores que pensar\u00edamos naturalmente, mesmo sem adicionar a carga da teoria da decis\u00e3o. Proposi\u00e7\u00e3o: O estimador de Bayes \\delta^{\\pi} associado com a perda quadr\u00e1tica L \u00e9 a esperan\u00e7a a posteriori \\delta^{\\pi}(x) = \\mathbb{E}^{\\pi}[\\theta | x] . O resultado imediato ocorre quando L(\\theta, \\delta) = w(\\theta)(\\theta - \\delta)^2 , como uma pondera\u00e7\u00e3o. Nesse caso, o estimador de Bayes \u00e9 \\delta^{\\pi}(x) = \\frac{\\mathbb{E}^{\\pi}[w(\\theta) \\theta | x]}{\\mathbb{E}^{\\pi}[w(\\theta)| x]}.","title":"Perda quadr\u00e1tica"},{"location":"bayesian/decision-theory/#perda-absoluta","text":"Uma alternativa \u00e0 perda quadr\u00e1tica \u00e9 L(\\theta, d) = |\\theta - d| , que pode ser generalizada para L_{k_1, k_2}(\\theta, \\delta) = \\begin{cases} k_2(\\theta - d) &\\text{se } \\theta > d \\\\ k_1(d - \\theta) &\\text{c.c.} \\end{cases} A penaliza\u00e7\u00e3o para desvios maiores \u00e9 menor, apesar de manter a convexidade. \u00e9 poss\u00edvel tamb\u00e9m propor uma perda como uma mistura dessas perdas. Em uma regi\u00e3o pr\u00f3xima de zero, usamos a perda quadr\u00e1tica. Depois, usamos a perda absoluta. Com essa perda, por exemplo, n\u00e3o existe estimador de Bayes em forma fechada. O estimador de Bayes associado a L_{k_1, k_2}(\\theta, \\delta) e a \\pi \u00e9 um quartil k_2/(k_1 + k_2) de \\pi(\\theta | x) . Em particular, quando k_1 = k_2 , o estimador \u00e9 a mediana a posteriori.","title":"Perda absoluta"},{"location":"bayesian/decision-theory/#perda-0-1","text":"Essa perda \u00e9 mais utilizada no contexto de teste de hip\u00f3teses. Ela \u00e9 definida como L(\\theta, \\delta) = 1 - 1_{\\theta = \\delta} . \ud83d\udcdd Exemplo (Teste de hip\u00f3teses) Seja o teste de hip\u00f3teses H_0 : \\theta \\in \\Omega_0 e H_1 : \\theta \\in \\Omega_1 . Ent\u00e3o \\mathcal{D} = \\{0, 1\\} em que 0 significa rejeitar H_0 . Logo queremos estimar a fun\u00e7\u00e3o 1_{\\theta \\in \\Omega_0} . O risco frequentista \u00e9 R(\\theta, \\delta) = \\begin{cases} \\Pr_{\\theta}(\\delta(x) = 0), &\\text{se } \\theta \\in \\Omega_0 \\\\ \\Pr_{\\theta}(\\delta(x) = 1), &\\text{c.c.,} \\end{cases} que s\u00e3o os erros do tipo 1 e do tipo 2, respectivamente. O estimador de Bayes \u00e9 dado por \\delta^{\\pi}(x) = \\begin{cases} 1 &\\text{se }\\Pr(\\theta \\in \\Omega_0 | x) > \\Pr(\\theta \\in \\Omega_1 | x) \\\\ 0, &\\text{c.c.} \\end{cases}","title":"Perda 0-1"},{"location":"bayesian/decision-theory/#perdas-intrinsecas","text":"\u00c0s vezes, estamos em uma situa\u00e7\u00e3o n\u00e3o informativa sobre a parametriza\u00e7\u00e3o natural e a escolha da fun\u00e7\u00e3o de perda. O estimador de Bayes n\u00e3o \u00e9 invariante por transforma\u00e7\u00f5es biun\u00edvocas em geral. Dessa forma, pode ser interessante obter perdas invariantes. Nesse caso, comparar f(\\cdot | \\delta) com f(\\cdot | \\theta) pode ser interessante, isto \u00e9, definir L(\\theta, \\delta) = d(f(\\cdot | \\theta), f(\\cdot | \\delta)). Duas dist\u00e2ncias usuais s\u00e3o: (1) entropia, Kullback\u2013Leibler divergence, ou (2) Hellinger. Elas resultam nas seguintes perdas: (1) L_e(\\theta, \\delta) = \\mathbb{E}_{\\theta}\\left[\\log\\left(\\frac{f(x|\\theta)}{f(x|\\delta)}\\right)\\right]. (2) L_H(\\theta, \\delta) = \\frac{1}{2}\\mathbb{E}_{\\theta}\\left[\\left(\\sqrt{\\frac{f(x|\\delta)}{f(x|\\theta)}} - 1\\right)^2\\right].","title":"Perdas intr\u00ednsecas"},{"location":"bayesian/decision-theory/#links","text":"Optimal Statistical Decisions, Morris DeGroot : esse livro expande os resultados de teoria da decis\u00e3o apresentados no livro do Robert.","title":"Links"},{"location":"bayesian/finetti/","text":"Teorema da Representa\u00e7\u00e3o de De Finetti Considere uma sequ\u00eancia X_1, \\dots, X_n de vari\u00e1veis aleat\u00f3rias indicadoras (assumem valor 0 ou 1 ). Em geral, assumimos que elas s\u00e3o independentes e identicamente distribu\u00eddas (IID), implicando em algo do tipo: \\Pr(X_n = x_n \\mid X_1 = x_1, \\dots, X_{n-1} = x_{n-1}) = \\Pr(X_n = x_n). Desta forma, assumindo essa propriedade e que \\Pr(X = 1) = \\Pr(X = 0) = 1/2 , mesmo que eu visualizasse um milh\u00e3o de vari\u00e1veis sendo 1 , eu acreditaria que a milion\u00e9sima primeira ainda teria a mesma probabilidade. Logo, observar um experimento n\u00e3o traria informa\u00e7\u00e3o para a moeda. Com isso, De Finetti introduziu um conceito mais fraco, o de permutabilidade. Em particular, ele demonstrou, para esse caso de vari\u00e1veis bin\u00e1rias, que existe uma vari\u00e1vel \\theta com distribui\u00e7\u00e3o \\mu_{\\theta} tal que \\Pr(X_1 = x_1, \\dots, X_n = x_n) = \\int_0^1 \\theta^s(1-\\theta)^{n-s} d\\mu_{\\theta}(\\theta), com s = x_1 + \\cdots + x_n . Portanto, sob a hip\u00f3tese de permutabilidade, existe um par\u00e2metro \\theta tal que esse par\u00e2metro \u00e9 uma fun\u00e7\u00e3o do limite das distribui\u00e7\u00f5es emp\u00edricas e, em particular, as vari\u00e1veis s\u00e3o IID condicionadas em \\theta . Isso permite a introdu\u00e7\u00e3o de modelos estat\u00edsticos bayesianos de uma forma natural. Permutabilidade Em uma an\u00e1lise estat\u00edstica, existem diversas quantidades sobre as quais somos incertos. Por exemplo, considere uma popula\u00e7\u00e3o e uma determinada doen\u00e7a. A priori, n\u00e3o sabemos se as pessoas t\u00eam ou n\u00e3o a doen\u00e7a. E mesmo que testemos algumas das pessoas, ainda n\u00e3o saberemos as respostas daqueles que n\u00e3o foram testados. Al\u00e9m do mais, o pr\u00f3prio teste \u00e9 uma quantidade incerta, visto que o seu resultado pode ser falso. Portanto, dever\u00edamos conseguir escrever a distribui\u00e7\u00e3o de probabilidade conjunta de todas as vari\u00e1veis de interesse. Nesse caso, elas se resumem \u00e0 indica\u00e7\u00e3o de doen\u00e7a de cada indiv\u00edduo. Outro exemplo \u00e9 a de jogar uma moeda n vezes. N\u00e3o temos informa\u00e7\u00e3o de que jogadas pr\u00f3ximas tenham algum tipo de rela\u00e7\u00e3o diferente da rela\u00e7\u00e3o para jogadas distantes. Por esse motivo, faz sentido assumirmos que as jogadas s\u00e3o sim\u00e9tricas, e em geral, assumimos que s\u00e3o independentes e identicamente distribu\u00eddas, imbuindo um modelo e um par\u00e2metro \\theta . Mas essa maquinaria pode ser simplificada para refletir a ideia de simetria que gostar\u00edamos de atingir. Por simetria, dizemos que para toda permuta\u00e7\u00e3o de resultados x_1, \\dots, x_n leva aos mesmos resultados. Em particular, apenas essa hip\u00f3tese vai implicar que existe uma medida para \\theta que satisfaz propriedades probabil\u00edsticas que almejamos. Permutabilidade: As vari\u00e1veis aleat\u00f3rias X_1, \\dots, X_n s\u00e3o permut\u00e1veis se toda permuta\u00e7\u00e3o de (X_1, \\dots, X_n) tem a mesma distribui\u00e7\u00e3o conjunta. Para uma cole\u00e7\u00e3o infinita, essa propriedade \u00e9 v\u00e1lida quando todo subconjunto finito \u00e9 permut\u00e1vel. Um resultado direto \u00e9 que distribui\u00e7\u00f5es marginais s\u00e3o iguais. Al\u00e9m disso, a distribui\u00e7\u00e3o conjunta de qualquer subconjunto de vari\u00e1veis de tamanho n s\u00f3 depende de n . Tamb\u00e9m podemos afirmar que IID implica permutabilidade . Todavia, a rec\u00edproca n\u00e3o \u00e9 verdadeira. Se X_1, X_2, \\dots s\u00e3o (condicionalmente) IID dado Y , as vari\u00e1veis s\u00e3o permut\u00e1veis, mas n\u00e3o IID necessariamente. De forma simples, a permutabilidade \u00e9 uma hip\u00f3tese fraca quando se deseja simetria entre as vari\u00e1veis, e que o \u00edndice delas seja \"irrelevante\". Observa\u00e7\u00e3o: probabilidade \u00e9 vista como um limite de frequ\u00eancias por uma grande parte dos estat\u00edsticos. Em particular, isso exige a tomada de uma quantidade infinita de vari\u00e1veis, o que \u00e9 imposs\u00edvel. O Teorema de Finetti a seguir esclarece que o limite de frequ\u00eancias de 1s na sequ\u00eancia Bernoulli, denominado \\theta \u00e9 apenas uma probabilidade condicional dada informa\u00e7\u00e3o desconhecida. Essa probabilidade tem vis\u00e3o subjetiva. Mesmo que o c\u00e1lculo de probabilidades seja diferente, se os indiv\u00edduos assumem permutabilidade das vari\u00e1veis aleat\u00f3rias, ent\u00e3o eles acreditam na exist\u00eancia de \\theta , de forma que condicionado em \\theta , as vari\u00e1veis s\u00e3o IID Bernoulli( \\theta ). Teorema de De Finetti O teorema de De Finetti conecta os conceitos de permutabilidade com o de independ\u00eancia e distribui\u00e7\u00e3o id\u00eantica condicionada em uma outra vari\u00e1vel aleat\u00f3ria. Por fim, ele relaciona essa vari\u00e1vel condicionadora com o que chamamos de par\u00e2metro, cerne dos modelos estat\u00edsticos param\u00e9tricos. Vari\u00e1veis aleat\u00f3rias Bernoulli Considere uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias \\{X_n\\}_{n=1}^{+\\infty} distribu\u00eddas conforme Bernoulli. A sequ\u00eancia \u00e9 permut\u00e1vel se, e somente se, existe uma vari\u00e1vel aleat\u00f3ria \\theta \\in [0,1] tal que \\{X_n \\mid \\theta\\}_{n=1}^{+\\infty} \u00e9 uma sequ\u00eancia IID Bernoulli( \\theta ). Al\u00e9m do mais, se a sequ\u00eancia \u00e9 permut\u00e1vel, a distribui\u00e7\u00e3o de \\theta \u00e9 \u00fanica e \\frac{1}{n}\\sum_{i=1}^n X_i \\to \\theta quase certamente. Vers\u00e3o Finita Suponha que X_1, \\dots, X_n s\u00e3o vari\u00e1veis aleat\u00f3rias em um espa\u00e7o de Borel (\\mathcal{X}, \\mathcal{B}) . Seja X = (X_1, \\dots, X_n) e para cada B \\in \\mathcal{B} , seja P_n(B) = \\sum_{i=1}^n 1_{X_i \\in B}/n a distribui\u00e7\u00e3o emp\u00edrica de X . As vari\u00e1veis aleat\u00f3rias s\u00e3o permut\u00e1veis se, e somente se, para toda k -tupla de distintos elementos de X , a distribui\u00e7\u00e3o conjunta de (X_{i_1}, \\dots, X_{i_k}) , condicionadas em P_n = P \u00e9 a de uma amostra aleat\u00f3ria simples sem reposi\u00e7\u00e3o da distribui\u00e7\u00e3o P . Nesse caso, tomamos k amostras sem reposi\u00e7\u00e3o de uma urna tal que cada bola tem a mesma probabilidade de ser retirada. Vers\u00e3o infinita Seja (S, \\mathcal{A}, \\mu) um espa\u00e7o de probabilidade e (\\mathcal{X}, \\mathcal{B}) um espa\u00e7o de Borel. Para cada n , seja X_n : S \\to \\mathcal{X} mensur\u00e1vel. A sequ\u00eancia \\{X_n\\}_{n \\in \\mathbb{N}} \u00e9 permut\u00e1vel se, e somente se, existe uma medida de probabilidade aleat\u00f3ria P em (\\mathcal{X}, \\mathcal{B}) tal que, condicionada em P , a sequ\u00eancia \u00e9 IID com distribui\u00e7\u00e3o P . Nesse caso, P \u00e9 \u00fanica e P_n(B) converge para P(B) quase certamente para cada B \\in \\mathcal{B} . Alguns pontos importantes: (1) Um espa\u00e7o de Borel: Seja (\\mathcal{X}, \\mathcal{B}) um espa\u00e7o mensur\u00e1vel. Se existe uma fun\u00e7\u00e3o bimensur\u00e1vel (mensur\u00e1vel bijetiva com inversa mensur\u00e1vel) \\phi : \\mathcal{X} \\to R \\subseteq \\mathbb{R} com R Borel, ent\u00e3o (\\mathcal{X}, \\mathcal{B}) \u00e9 espa\u00e7o de Borel. (2) Medida de probabilidade aleat\u00f3ria: Seja \\mathcal{P} o conjunto de todas as medidas de probabilidade em (\\mathcal{X}, \\mathcal{B}) e \\mathcal{C}_{\\mathcal{P}} uma \\sigma -\u00e1lgebra de subconjuntos \\mathcal{P} de forma que g_B : \\mathcal{P} \\to \\mathbb{R} sejam mensur\u00e1veis, com g_B(P) = P(B) . Em particular, a fun\u00e7\u00e3o P : S \\to \\mathcal{S} \u00e9 medida de probabilidade aleat\u00f3ria, em que (S, \\mathcal{A}, \\mu) \u00e9 espa\u00e7o de probabilidade. Esse teorema foi estendido para sequ\u00eancia de vari\u00e1veis aleat\u00f3rias n\u00e3o permut\u00e1veis. A demonstra\u00e7\u00e3o se encontra entre as p\u00e1ginas 33-49 do livro de Schervish. Links relevantes What is so cool about de Finetti's representation theorem? Exchangeability, Correlation, and Bayes' Effect, Ben O'Neill : esse paper discute a diferen\u00e7a da estat\u00edstica bayesiana e frequentista sobre os valores observados, relacionando com o conceito de permutabilidade.","title":"Teorema da Representa\u00e7\u00e3o de De Finetti"},{"location":"bayesian/finetti/#teorema-da-representacao-de-de-finetti","text":"Considere uma sequ\u00eancia X_1, \\dots, X_n de vari\u00e1veis aleat\u00f3rias indicadoras (assumem valor 0 ou 1 ). Em geral, assumimos que elas s\u00e3o independentes e identicamente distribu\u00eddas (IID), implicando em algo do tipo: \\Pr(X_n = x_n \\mid X_1 = x_1, \\dots, X_{n-1} = x_{n-1}) = \\Pr(X_n = x_n). Desta forma, assumindo essa propriedade e que \\Pr(X = 1) = \\Pr(X = 0) = 1/2 , mesmo que eu visualizasse um milh\u00e3o de vari\u00e1veis sendo 1 , eu acreditaria que a milion\u00e9sima primeira ainda teria a mesma probabilidade. Logo, observar um experimento n\u00e3o traria informa\u00e7\u00e3o para a moeda. Com isso, De Finetti introduziu um conceito mais fraco, o de permutabilidade. Em particular, ele demonstrou, para esse caso de vari\u00e1veis bin\u00e1rias, que existe uma vari\u00e1vel \\theta com distribui\u00e7\u00e3o \\mu_{\\theta} tal que \\Pr(X_1 = x_1, \\dots, X_n = x_n) = \\int_0^1 \\theta^s(1-\\theta)^{n-s} d\\mu_{\\theta}(\\theta), com s = x_1 + \\cdots + x_n . Portanto, sob a hip\u00f3tese de permutabilidade, existe um par\u00e2metro \\theta tal que esse par\u00e2metro \u00e9 uma fun\u00e7\u00e3o do limite das distribui\u00e7\u00f5es emp\u00edricas e, em particular, as vari\u00e1veis s\u00e3o IID condicionadas em \\theta . Isso permite a introdu\u00e7\u00e3o de modelos estat\u00edsticos bayesianos de uma forma natural.","title":"Teorema da Representa\u00e7\u00e3o de De Finetti"},{"location":"bayesian/finetti/#permutabilidade","text":"Em uma an\u00e1lise estat\u00edstica, existem diversas quantidades sobre as quais somos incertos. Por exemplo, considere uma popula\u00e7\u00e3o e uma determinada doen\u00e7a. A priori, n\u00e3o sabemos se as pessoas t\u00eam ou n\u00e3o a doen\u00e7a. E mesmo que testemos algumas das pessoas, ainda n\u00e3o saberemos as respostas daqueles que n\u00e3o foram testados. Al\u00e9m do mais, o pr\u00f3prio teste \u00e9 uma quantidade incerta, visto que o seu resultado pode ser falso. Portanto, dever\u00edamos conseguir escrever a distribui\u00e7\u00e3o de probabilidade conjunta de todas as vari\u00e1veis de interesse. Nesse caso, elas se resumem \u00e0 indica\u00e7\u00e3o de doen\u00e7a de cada indiv\u00edduo. Outro exemplo \u00e9 a de jogar uma moeda n vezes. N\u00e3o temos informa\u00e7\u00e3o de que jogadas pr\u00f3ximas tenham algum tipo de rela\u00e7\u00e3o diferente da rela\u00e7\u00e3o para jogadas distantes. Por esse motivo, faz sentido assumirmos que as jogadas s\u00e3o sim\u00e9tricas, e em geral, assumimos que s\u00e3o independentes e identicamente distribu\u00eddas, imbuindo um modelo e um par\u00e2metro \\theta . Mas essa maquinaria pode ser simplificada para refletir a ideia de simetria que gostar\u00edamos de atingir. Por simetria, dizemos que para toda permuta\u00e7\u00e3o de resultados x_1, \\dots, x_n leva aos mesmos resultados. Em particular, apenas essa hip\u00f3tese vai implicar que existe uma medida para \\theta que satisfaz propriedades probabil\u00edsticas que almejamos. Permutabilidade: As vari\u00e1veis aleat\u00f3rias X_1, \\dots, X_n s\u00e3o permut\u00e1veis se toda permuta\u00e7\u00e3o de (X_1, \\dots, X_n) tem a mesma distribui\u00e7\u00e3o conjunta. Para uma cole\u00e7\u00e3o infinita, essa propriedade \u00e9 v\u00e1lida quando todo subconjunto finito \u00e9 permut\u00e1vel. Um resultado direto \u00e9 que distribui\u00e7\u00f5es marginais s\u00e3o iguais. Al\u00e9m disso, a distribui\u00e7\u00e3o conjunta de qualquer subconjunto de vari\u00e1veis de tamanho n s\u00f3 depende de n . Tamb\u00e9m podemos afirmar que IID implica permutabilidade . Todavia, a rec\u00edproca n\u00e3o \u00e9 verdadeira. Se X_1, X_2, \\dots s\u00e3o (condicionalmente) IID dado Y , as vari\u00e1veis s\u00e3o permut\u00e1veis, mas n\u00e3o IID necessariamente. De forma simples, a permutabilidade \u00e9 uma hip\u00f3tese fraca quando se deseja simetria entre as vari\u00e1veis, e que o \u00edndice delas seja \"irrelevante\". Observa\u00e7\u00e3o: probabilidade \u00e9 vista como um limite de frequ\u00eancias por uma grande parte dos estat\u00edsticos. Em particular, isso exige a tomada de uma quantidade infinita de vari\u00e1veis, o que \u00e9 imposs\u00edvel. O Teorema de Finetti a seguir esclarece que o limite de frequ\u00eancias de 1s na sequ\u00eancia Bernoulli, denominado \\theta \u00e9 apenas uma probabilidade condicional dada informa\u00e7\u00e3o desconhecida. Essa probabilidade tem vis\u00e3o subjetiva. Mesmo que o c\u00e1lculo de probabilidades seja diferente, se os indiv\u00edduos assumem permutabilidade das vari\u00e1veis aleat\u00f3rias, ent\u00e3o eles acreditam na exist\u00eancia de \\theta , de forma que condicionado em \\theta , as vari\u00e1veis s\u00e3o IID Bernoulli( \\theta ).","title":"Permutabilidade"},{"location":"bayesian/finetti/#teorema-de-de-finetti","text":"O teorema de De Finetti conecta os conceitos de permutabilidade com o de independ\u00eancia e distribui\u00e7\u00e3o id\u00eantica condicionada em uma outra vari\u00e1vel aleat\u00f3ria. Por fim, ele relaciona essa vari\u00e1vel condicionadora com o que chamamos de par\u00e2metro, cerne dos modelos estat\u00edsticos param\u00e9tricos.","title":"Teorema de De Finetti"},{"location":"bayesian/finetti/#variaveis-aleatorias-bernoulli","text":"Considere uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias \\{X_n\\}_{n=1}^{+\\infty} distribu\u00eddas conforme Bernoulli. A sequ\u00eancia \u00e9 permut\u00e1vel se, e somente se, existe uma vari\u00e1vel aleat\u00f3ria \\theta \\in [0,1] tal que \\{X_n \\mid \\theta\\}_{n=1}^{+\\infty} \u00e9 uma sequ\u00eancia IID Bernoulli( \\theta ). Al\u00e9m do mais, se a sequ\u00eancia \u00e9 permut\u00e1vel, a distribui\u00e7\u00e3o de \\theta \u00e9 \u00fanica e \\frac{1}{n}\\sum_{i=1}^n X_i \\to \\theta quase certamente.","title":"Vari\u00e1veis aleat\u00f3rias Bernoulli"},{"location":"bayesian/finetti/#versao-finita","text":"Suponha que X_1, \\dots, X_n s\u00e3o vari\u00e1veis aleat\u00f3rias em um espa\u00e7o de Borel (\\mathcal{X}, \\mathcal{B}) . Seja X = (X_1, \\dots, X_n) e para cada B \\in \\mathcal{B} , seja P_n(B) = \\sum_{i=1}^n 1_{X_i \\in B}/n a distribui\u00e7\u00e3o emp\u00edrica de X . As vari\u00e1veis aleat\u00f3rias s\u00e3o permut\u00e1veis se, e somente se, para toda k -tupla de distintos elementos de X , a distribui\u00e7\u00e3o conjunta de (X_{i_1}, \\dots, X_{i_k}) , condicionadas em P_n = P \u00e9 a de uma amostra aleat\u00f3ria simples sem reposi\u00e7\u00e3o da distribui\u00e7\u00e3o P . Nesse caso, tomamos k amostras sem reposi\u00e7\u00e3o de uma urna tal que cada bola tem a mesma probabilidade de ser retirada.","title":"Vers\u00e3o Finita"},{"location":"bayesian/finetti/#versao-infinita","text":"Seja (S, \\mathcal{A}, \\mu) um espa\u00e7o de probabilidade e (\\mathcal{X}, \\mathcal{B}) um espa\u00e7o de Borel. Para cada n , seja X_n : S \\to \\mathcal{X} mensur\u00e1vel. A sequ\u00eancia \\{X_n\\}_{n \\in \\mathbb{N}} \u00e9 permut\u00e1vel se, e somente se, existe uma medida de probabilidade aleat\u00f3ria P em (\\mathcal{X}, \\mathcal{B}) tal que, condicionada em P , a sequ\u00eancia \u00e9 IID com distribui\u00e7\u00e3o P . Nesse caso, P \u00e9 \u00fanica e P_n(B) converge para P(B) quase certamente para cada B \\in \\mathcal{B} . Alguns pontos importantes: (1) Um espa\u00e7o de Borel: Seja (\\mathcal{X}, \\mathcal{B}) um espa\u00e7o mensur\u00e1vel. Se existe uma fun\u00e7\u00e3o bimensur\u00e1vel (mensur\u00e1vel bijetiva com inversa mensur\u00e1vel) \\phi : \\mathcal{X} \\to R \\subseteq \\mathbb{R} com R Borel, ent\u00e3o (\\mathcal{X}, \\mathcal{B}) \u00e9 espa\u00e7o de Borel. (2) Medida de probabilidade aleat\u00f3ria: Seja \\mathcal{P} o conjunto de todas as medidas de probabilidade em (\\mathcal{X}, \\mathcal{B}) e \\mathcal{C}_{\\mathcal{P}} uma \\sigma -\u00e1lgebra de subconjuntos \\mathcal{P} de forma que g_B : \\mathcal{P} \\to \\mathbb{R} sejam mensur\u00e1veis, com g_B(P) = P(B) . Em particular, a fun\u00e7\u00e3o P : S \\to \\mathcal{S} \u00e9 medida de probabilidade aleat\u00f3ria, em que (S, \\mathcal{A}, \\mu) \u00e9 espa\u00e7o de probabilidade. Esse teorema foi estendido para sequ\u00eancia de vari\u00e1veis aleat\u00f3rias n\u00e3o permut\u00e1veis. A demonstra\u00e7\u00e3o se encontra entre as p\u00e1ginas 33-49 do livro de Schervish.","title":"Vers\u00e3o infinita"},{"location":"bayesian/finetti/#links-relevantes","text":"What is so cool about de Finetti's representation theorem? Exchangeability, Correlation, and Bayes' Effect, Ben O'Neill : esse paper discute a diferen\u00e7a da estat\u00edstica bayesiana e frequentista sobre os valores observados, relacionando com o conceito de permutabilidade.","title":"Links relevantes"},{"location":"bayesian/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de Estat\u00edstica Bayesiana do Doutorado de Matem\u00e1tica Aplicada correspondente ao per\u00edodo de 2022.3. Os livros utilizados s\u00e3o: The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation, Christian Robert A First Course in Bayesian Statistical Methods, Peter D Hoff Theory of Statistics, Mark J. Schervish T\u00f3picos Conceitos Introdu\u00e7\u00e3o Teorema de De Finetti Teoria da decis\u00e3o Distribui\u00e7\u00f5es a priori Bayesian Point Estimation Testes e regi\u00f5es de confian\u00e7a Escolha de modelos Notas Monitoria Itens discutidos Arquivo 28/03/2022 Princ\u00edpio de Verossimilhan\u00e7a, Prioris impr\u00f3prias e material adicional Visualizar 08/04/2022 Teoria da decis\u00e3o e lista 2 Monitoria presencial 18/04/2022 Probabilidade Subjetiva Visualizar 22/04/2022 Fam\u00edlia exponencial Indispon\u00edvel Listas N\u00famero Exerc\u00edcios Solu\u00e7\u00e3o Extras 1 Cap\u00edtulo 1 The Bayesian Choice: 1.17, 1.26, 1.32, 1.41 1 1.16, 1.25, 1.39, 1.43, 1.47, 1.50 2 Cap\u00edtulo 2 The Bayesian Choice: 2.13, 2.25, 2.28, 2.36, 2.42 2 2.11, 2.18, 2.19, 2.23, 2.51 3 Livro Optimal Statistical Decisions, Cap\u00edtulo 6 3 - 4 Cap\u00edtulo 3 The Bayesian Choice: 3.15, 3.21, 3.35, 3.37 4 3.2, 3.6, 3.30, 3.53, 3.59 5 Cap\u00edtulo 3 The Bayesian Choice: 3.44, 3.51, 3.57, 3.58, 3.63 5 - 6 Cap\u00edtulo 4 The Bayesian Choice: 4.5, 4.17, 4.40, 4.44 6 - 7 Cap\u00edtulo 5 The Bayesian Choice: 5.2, 5.6, 5.12, 5.42 7 5.11 8 Cap\u00edtulo 7 The Bayesian Choice: 7.1, 7.5, 7.12, 7,20 8 - 9 Cap\u00edtulo 10 The Bayesian Choice: 10.6, 10.9, 10.14, 10.17 9 - Material adicional WHAT IS BAYESIAN/FREQUENTIST INFERENCE? Github da disciplina de 2021","title":"Estat\u00edstica Bayesiana"},{"location":"bayesian/info/#informacoes-gerais","text":"Monitoria de Estat\u00edstica Bayesiana do Doutorado de Matem\u00e1tica Aplicada correspondente ao per\u00edodo de 2022.3. Os livros utilizados s\u00e3o: The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation, Christian Robert A First Course in Bayesian Statistical Methods, Peter D Hoff Theory of Statistics, Mark J. Schervish","title":"Informa\u00e7\u00f5es Gerais"},{"location":"bayesian/info/#topicos","text":"Conceitos Introdu\u00e7\u00e3o Teorema de De Finetti Teoria da decis\u00e3o Distribui\u00e7\u00f5es a priori Bayesian Point Estimation Testes e regi\u00f5es de confian\u00e7a Escolha de modelos","title":"T\u00f3picos"},{"location":"bayesian/info/#notas","text":"Monitoria Itens discutidos Arquivo 28/03/2022 Princ\u00edpio de Verossimilhan\u00e7a, Prioris impr\u00f3prias e material adicional Visualizar 08/04/2022 Teoria da decis\u00e3o e lista 2 Monitoria presencial 18/04/2022 Probabilidade Subjetiva Visualizar 22/04/2022 Fam\u00edlia exponencial Indispon\u00edvel","title":"Notas"},{"location":"bayesian/info/#listas","text":"N\u00famero Exerc\u00edcios Solu\u00e7\u00e3o Extras 1 Cap\u00edtulo 1 The Bayesian Choice: 1.17, 1.26, 1.32, 1.41 1 1.16, 1.25, 1.39, 1.43, 1.47, 1.50 2 Cap\u00edtulo 2 The Bayesian Choice: 2.13, 2.25, 2.28, 2.36, 2.42 2 2.11, 2.18, 2.19, 2.23, 2.51 3 Livro Optimal Statistical Decisions, Cap\u00edtulo 6 3 - 4 Cap\u00edtulo 3 The Bayesian Choice: 3.15, 3.21, 3.35, 3.37 4 3.2, 3.6, 3.30, 3.53, 3.59 5 Cap\u00edtulo 3 The Bayesian Choice: 3.44, 3.51, 3.57, 3.58, 3.63 5 - 6 Cap\u00edtulo 4 The Bayesian Choice: 4.5, 4.17, 4.40, 4.44 6 - 7 Cap\u00edtulo 5 The Bayesian Choice: 5.2, 5.6, 5.12, 5.42 7 5.11 8 Cap\u00edtulo 7 The Bayesian Choice: 7.1, 7.5, 7.12, 7,20 8 - 9 Cap\u00edtulo 10 The Bayesian Choice: 10.6, 10.9, 10.14, 10.17 9 -","title":"Listas"},{"location":"bayesian/info/#material-adicional","text":"WHAT IS BAYESIAN/FREQUENTIST INFERENCE? Github da disciplina de 2021","title":"Material adicional"},{"location":"bayesian/intro/","text":"Introdu\u00e7\u00e3o Teoria Estat\u00edstica : objetiva obter uma infer\u00eancia sobre a distribui\u00e7\u00e3o de probabilidade de um fen\u00f4meno a partir de observa\u00e7\u00f5es. A base te\u00f3rica do livro The Bayesian Choice \u00e9 constru\u00edda sob o ponto de vista da Teoria da Decis\u00e3o. Isso se deve a dois fatores: (I) a infer\u00eancia tem algum objetivo, isto \u00e9, alguma decis\u00e3o \u00e9 tomada baseada em uma previs\u00e3o ou an\u00e1lise, e ela tem consequ\u00eancias mensur\u00e1veis; e (II) a decis\u00e3o clarifica a prefer\u00eancia do estat\u00edstico. Estat\u00edstica \u00e9 mais sobre a interpreta\u00e7\u00e3o de fen\u00f4menos naturais do que a explica\u00e7\u00e3o sobre eles. Al\u00e9m disso, ela tem um passo de formaliza\u00e7\u00e3o redutiva . A modelagem estat\u00edstica, atrav\u00e9s probabilidade, possibilita incluir a informa\u00e7\u00e3o dispon\u00edvel sobre o fen\u00f4meno e a incerteza sobre essa informa\u00e7\u00e3o. Uma cr\u00edtica \u00e0 abordagem probabil\u00edstica \u00e9 a dificuldade em saber exatamente a distribui\u00e7\u00e3o de probabilidade do fen\u00f4meno. \ud83d\udcdd Exemplo (modelo capture-recapture) Suponha que queremos estimar o n\u00famero de \u00f4nibus N em uma cidade. Uma forma de fazer isso \u00e9 a seguinte: contamos a quantidade vista de \u00f4nibus em um dia ( q_1 ) e armazenamos a identifica\u00e7\u00e3o de cada um. No dia seguinte, fazemos a mesma coisa e obtemos ( q_2 ). Seja n o n\u00famero de \u00f4nibus que vimos nos dois dias. Qual a distribui\u00e7\u00e3o de n ? Olhando para o segundo dia, em uma popula\u00e7\u00e3o de tamanho N , t\u00ednhamos q_1 \u00f4nibus de interesse para recontar. Nossa amostra \u00e9 de tamanho q_2 . Isso define a distribui\u00e7\u00e3o hipergeom\u00e9trica , pois a amostragem do segundo dia \u00e9 sem reposi\u00e7\u00e3o (note que simplificamos que s\u00f3 podemos ver o mesmo \u00f4nibus uma vez). Logo n \\sim Hypergeometric(N, q_1, q_2). Sabemos que \\mathbb{E}[n] = (q_1/N) \\cdot q_2 \\implies \\hat{N} = q_1 \\cdot q_2 / n \u00e9 um poss\u00edvel estimador para N . Note que esse estimador n\u00e3o \u00e9 necessariamente n\u00e3o enviesado, pois \\mathbb{E}[1/n] \\neq 1/\\mathbb{E}[n] . Para aproximar uma distribui\u00e7\u00e3o de probabilidade de um fen\u00f4meno, duas abordagens estat\u00edsticas s\u00e3o usadas: n\u00e3o param\u00e9trica e param\u00e9trica . Na primeira, a estimativa procura assumir o m\u00ednimo de hip\u00f3teses poss\u00edvel, procurando uma estima\u00e7\u00e3o funcional. J\u00e1 a segunda vem de uma densidade parametrizada, em que o par\u00e2metro de dimens\u00e3o finita \u00e9 desconhecido. Um modelo estat\u00edstico param\u00e9trico consiste de observa\u00e7\u00f5es de uma vari\u00e1vel aleat\u00f3ria x \\in \\mathcal{X} (espa\u00e7o de estados) com distribui\u00e7\u00e3o cuja densidade \u00e9 f(x | \\theta) e \\theta \\in \\Omega (espa\u00e7o dos par\u00e2metros) \u00e9 desconhecido com dimens\u00e3o finita. Logo, m\u00e9todos estat\u00edsticos permitem fazer infer\u00eancia sobre \\theta a partir de x , enquanto a modelagem probabil\u00edstica caracteriza o comportamento de observa\u00e7\u00f5es futuras condicionadas em \\theta . Modelo param\u00e9trico Schervish apresenta uma defini\u00e7\u00e3o formalizada de modelo param\u00e9trico, a qual eu apresento a seguir. Seja (S, \\mathcal{A}, \\mu) um espa\u00e7o de probabilidade, e (\\mathcal{X}, \\mathcal{B}) e (\\Omega, \\tau) espa\u00e7os de Borel (espa\u00e7o mensur\u00e1vel isomorfo a um subconjunto mensur\u00e1vel dos reais. Em geral, veremos que esses espa\u00e7os s\u00e3o subconjuntos mensur\u00e1veis dos reais). Seja X : S \\to \\mathcal{X} e \\Theta : S \\to \\Omega fun\u00e7\u00f5es mensur\u00e1veis. Chamamos \\Theta de par\u00e2metro e \\Omega de espa\u00e7o de par\u00e2metros. A distribui\u00e7\u00e3o de X | \\Theta = \\theta \u00e9 fam\u00edlia param\u00e9trica de distribui\u00e7\u00f5es de X dada por P_{\\theta} . A medida de probabilidade \\mu_{\\Theta} sobre (\\Omega, \\tau) induzida por \\Theta a partir de \\mu \u00e9 chamada de distribui\u00e7\u00e3o a priori ( \\mu_{\\Theta}(B) = \\mu(\\Theta \\in B), B \\in \\tau ). A densidade de P_{\\theta} (que \u00e9 absolutamente cont\u00ednua) com respeito \u00e0 uma medida \\nu \u00e9 dada por f_{X|\\Theta}(x|\\theta) = \\frac{dP_{\\theta}}{d\\nu}(x), a derivada de Radon-Nikodym. Paradigma bayesiano No paradigma bayesiano, as quantidades desconhecidas s\u00e3o tratadas como vari\u00e1veis aleat\u00f3rias, incluindo o par\u00e2metro \\theta . Na p\u00e1gina 12 de seu livro, Shervish apresenta uma justificativa matem\u00e1tica para esse fato. Assim, se temos um modelo P_{\\theta} para x , precisamos de uma distribui\u00e7\u00e3o para \\theta , a qual chamamos de distribui\u00e7\u00e3o a priori . Com elas, constru\u00edmos uma distribui\u00e7\u00e3o em \\mathcal{X} \\times \\Omega . Em particular, \\Pr((x,\\theta) \\in B) = \\int_{\\Omega} \\int_{\\mathcal{X}} I_B(u, v) f_{x | \\theta}(u|v)f_{\\theta}(v) \\, du \\, dv, se f_{\\theta} for a densidade da distribui\u00e7\u00e3o de \\theta . Para isso, basta exigir que f_{X | \\Theta} seja mensur\u00e1vel em \\mathcal{B} \\otimes \\tau .Matematicamente, probabilidades podem representar cren\u00e7as numericamente, relacionando informa\u00e7\u00e3o com probabilidade. A Regra de Bayes prov\u00ea um m\u00e9todo racional para atualizar essas cren\u00e7as frente a novas informa\u00e7\u00f5es. O processo indutivo de atualizar cren\u00e7as com Bayes \u00e9 chamada de infer\u00eancia bayesiana. Teorema de Bayes Se E \u00e9 um evento com probabilidade positiva e A \u00e9 um outro evento, temos que \\Pr(A | E) = \\frac{\\Pr(E |A) \\Pr(A)}{\\Pr(E)}. Podemos expressar atrav\u00e9s de densidades, com p(\\theta | x) = \\frac{f(x|\\theta)\\pi(\\theta)}{\\int_{\\Omega} f(x|t)\\pi(t) \\, dt}, em que \\pi(\\theta) \u00e9 a densidade da priori do par\u00e2metro \\theta e p(\\theta | x) \u00e9 chamada de distribui\u00e7\u00e3o a posteriori de \\theta (para uma demonstra\u00e7\u00e3o detalhada do Teorema de Bayes, vale conferir a p\u00e1gina 16 do livro de Schervish). Al\u00e9m disso, como o par\u00e2metro \\theta \u00e9 desconhecido, tamb\u00e9m denotamos a densidade de x condicionada em \\theta como uma fun\u00e7\u00e3o de \\theta , ap\u00f3s observar x . Nesse caso, chamamos de fun\u00e7\u00e3o de verossimilhan\u00e7a l(\\theta | x) = f(x | \\theta) para cada \\theta \\in \\Omega e x observado. O denominador da express\u00e3o acima \u00e9 chamada de densidade preditiva a priori e n\u00e3o depende de \\theta . \u00c9 a marginal no espa\u00e7o de estados. A distribui\u00e7\u00e3o a priori encapsula a informa\u00e7\u00e3o dispon\u00edvel sobre o par\u00e2metro \\theta antes do experimento, incluindo a incerteza residual. Se a distribui\u00e7\u00e3o a priori e a distribui\u00e7\u00e3o dos dados representam cren\u00e7as racionais, a regra de Bayes \u00e9 o m\u00e9todo \u00f3timo para atualizar essas cren\u00e7as sobre o par\u00e2metro dada nova informa\u00e7\u00e3o. Claro que, em geral, n\u00e3o conseguimos explorar as cren\u00e7as de modo perfeito e a posteriori n\u00e3o vai ser \u00f3tima, nesse sentido. \ud83d\udcdd Exemplo (Bayes, 1974) Uma bola de sinuca O \u00e9 rolada em uma linha de comprimento 1. \u00c9 natural assumir que ela tem uma distribui\u00e7\u00e3o uniforme de parar em qualquer lugar, s\u00f3 dependendo da for\u00e7a exercida sobre ela. Seja p o ponto de parada. Em seguida, rolamos uma outra bola n vezes e contamos a quantidade de vezes ( X ) que ela parou antes de B . Nesse caso, observado X=x , queremos inferir p . Se p fosse conhecido, qual seria distribui\u00e7\u00e3o de X ? Veja que temos n experimentos independentes e id\u00eanticos de sucesso ou falha, com probabilidade de sucesso p (lembrando da distribui\u00e7\u00e3o uniforme!). Nesse caso X|p \\sim Bernoulli(n, p) . Com essas configura\u00e7\u00f5es, podemos verificar que \\Pr(a < p < b | X=x) = \\frac{\\int_a^b p^x(1-p)^{n-x} \\, dp}{B(x+1, n-x+1)}, em que B \u00e9 a fun\u00e7\u00e3o Beta . Enquanto a estat\u00edstica cl\u00e1ssica \u00e9 dirigida por princ\u00edpios justificados axiomaticamente, a abordagem bayesiana incorpora esses princ\u00edpios sem a restri\u00e7\u00e3o sobre os procedimentos e tamb\u00e9m rejeita outros princ\u00edpios. Por exemplo, o conceito de estimadores n\u00e3o enviesados, em geral preferidos na estat\u00edstica cl\u00e1ssica, imp\u00f5e restri\u00e7\u00f5es fortes sobre os procedimentos adotados e leva a performances ineficientes (ver exemplo de Stein ). Isso acontece, pois a justificativa \u00e9 assint\u00f3tica, j\u00e1 que em m\u00e9dia o estimador \u00e9 correto. Por fim, em estat\u00edstica bayesiana, TODAS AS INFER\u00caNCIAS S\u00c3O BASEADAS NA DISTRIBUI\u00c7\u00c3O A POSTERIORI . Um pouco de hist\u00f3ria Em 1763, \u00e9 publicado An Essay towards Solving a Problem in the Doctrine of Chances , paper atribu\u00eddo a Thomas Bayes e publicado por Richard Price. O principal objeto desse trabalho, al\u00e9m do exemplo acima, \u00e9 o que conhecemos como Teorema de Bayes, mas com a priori sendo uniforme. Laplace, em Memoir on the Probability of the Causes of Events foi quem descreveu em uma forma mais geral, apesar de ainda ser discreta. Do ponto de vista probabil\u00edstico, o Teorema de Bayes \u00e9 apenas uma forma de mensurar a incerteza. A controv\u00e9rsia adv\u00e9m da interpreta\u00e7\u00e3o da probabilidade e se ele deveria ser considerado ponto central no processo de aprendizado. Vis\u00e3o subjetiva da probabilidade A vis\u00e3o de que o mundo \u00e9 determin\u00edstico ou n\u00e3o, como a discuss\u00e3o do Dem\u00f4nio de Laplace , \u00e9 pouco importante na verdade para a estat\u00edstica. O que importa \u00e9 a incerteza que temos sobre as quantidades. No pref\u00e1cio de seu livro Theory of Probability , Bruno de Finetti argumenta que \"probabilidade n\u00e3o existe\" no sentido objetivo . A \u00fanica exig\u00eancia \u00e9 que exista consist\u00eancia em nossas cren\u00e7as e na rela\u00e7\u00e3o com dados objetivos. Basicamente, a defini\u00e7\u00e3o de probabilidade \u00e9 subjetiva: a taxa em que o indiv\u00edduo est\u00e1 disposto a apostar na ocorr\u00eancia de um evento. Considere um dado normal de seis lados. Um frequentista afirmaria que, por simetria, cada lado tem igual chance de ocorrer. Evid\u00eancia emp\u00edrica passada suportaria sua afirma\u00e7\u00e3o. Um subjetivista ouviria os argumentos, mas o que realmente iria considerar seria sua cren\u00e7a sobre o que acontecer\u00e1 em uma jogada de dados, isto \u00e9, quanto seria apostado em cada lado, dada a informa\u00e7\u00e3o presente. Logo, no trabalho de De Finetti, Probabilidade e Pre\u00e7o s\u00e3o equivalentes. Para uma discuss\u00e3o mais detalhada, consulte esse trabalho . Princ\u00edpio da Verossimilhan\u00e7a e Princ\u00edpio da Sufici\u00eancia Sufici\u00eancia Seja x \\sim f(x | \\theta) . Uma fun\u00e7\u00e3o/estat\u00edstica T: \\mathcal{X} \\to \\mathbb{R}^k (a imagem de T , juntamente com o conjunto de seus singletons, pode ser qualquer espa\u00e7o mensur\u00e1vel) \u00e9 suficiente se a distribui\u00e7\u00e3o de x condicionada em T(x) n\u00e3o depende de \\theta . Para mais detalhes, ver aqui . De forma simplificada, T(x) traz toda a informa\u00e7\u00e3o sobre \\theta advinda de x . Schervish (p\u00e1gina 84) adiciona o fato de que para qualquer priori \\pi(theta) , a distribui\u00e7\u00e3o a posteriori de \\theta condicionada em x e a posteriori condicionada em T(x) s\u00e3o iguais quase certamente. Como demonstrado no Teorema 2.14 do mesmo livro, essas defini\u00e7\u00f5es s\u00e3o equivalentes dadas algumas hip\u00f3teses de regularidade. O Teorema da fatora\u00e7\u00e3o Fisher-Neyman mostra que se a densidade de x \u00e9 a derivada de Radon-Nikodym para alguma medida de probabilidade ( \\sigma - finita) cuja distribui\u00e7\u00e3o seja absolutamente cont\u00ednua, ent\u00e3o vale que T(x) \u00e9 suficiente para \\theta se, e somente se, existem fun\u00e7\u00f5es g e h n\u00e3o-negativas tal que f(x|\\theta) = g(T(x)|\\theta)h(x|T(x)). O conceito de sufici\u00eancia foi introduzido por Fisher e est\u00e1 associado com o seguinte princ\u00edpio: Princ\u00edpio da Sufici\u00eancia (PS): Se duas observa\u00e7\u00f5es x e y s\u00e3o tais que T(x) = T(y) para alguma estat\u00edstica suficiente T , ent\u00e3o elas devem levar \u00e0 mesma infer\u00eancia sobre o par\u00e2metro. \ud83d\udcdd Exemplo Suponha que observamos x = (x_1, \\dots, x_n) \\overset{iid}{\\sim} Exponential(\\lambda) . Uma estat\u00edstica suficiente para \\lambda \u00e9 a m\u00e9dia amostral T(x) = \\bar{x} , em particular, f(x | \\lambda) = \\lambda^n e^{-\\lambda n \\bar{x}}. Logo, infer\u00eancias sobre \\lambda s\u00f3 devem se basear em \\bar{x} , segundo o Princ\u00edpio da sufici\u00eancia. Princ\u00edpio da Verossimilhan\u00e7a Esse conceito \u00e9 tamb\u00e9m atribu\u00eddo a Fisher , mas a sua formaliza\u00e7\u00e3o se deve a Birnbaum (1962) . Princ\u00edpio da Verossimilhan\u00e7a (PV): a informa\u00e7\u00e3o trazida por uma observa\u00e7\u00e3o x sobre \\theta \u00e9 inteiramente contida na fun\u00e7\u00e3o de verossimilhan\u00e7a l(\\theta|x) . Al\u00e9m do mais, se duas observa\u00e7\u00f5es x e y dependem de \\theta , de forma que l_1(\\theta|x) = cl_2(\\theta|y), \\, \\forall \\theta \\in \\Omega para alguma constante c , ent\u00e3o elas levam \u00e0 mesma infer\u00eancia sobre \\theta . Uma outra forma de expressar esse princ\u00edpio \u00e9 o seguinte: Se E e E' s\u00e3o experimentos definidos em \\Omega , representados pelas densidades f(x, \\theta) e g(y, \\theta) , e x e y s\u00e3o observa\u00e7\u00f5es determinando a mesma fun\u00e7\u00e3o de verossimilhan\u00e7a, ent\u00e3o a evid\u00eancia trazida por ambos os experimentos \u00e9 a mesma vista a partir dessas observa\u00e7\u00f5es, isto \u00e9, o resultado x de qualquer experimento E \u00e9 caracterizado somente pela verossimilhan\u00e7a at\u00e9 uma constante. \ud83d\udcdd Exemplo Seja \\theta \\in [0,1] a propor\u00e7\u00e3o de doentes em uma popula\u00e7\u00e3o. Um examinador encontrou nove pessoas saud\u00e1veis e tr\u00eas doentes. Se nenhuma informa\u00e7\u00e3o adicional \u00e9 obtida, podemos propor dois modelos diferentes para esse fen\u00f4meno: (1) O examinador testou 12 pessoas e observou x \\sim Binomial(12, \\theta) com x = 3 . (2) Ele questionou N = 12 pessoas at\u00e9 encontrar 3 doentes. Nesse caso, N \\sim NegativeBinomial(3, \\theta) Apesar do dado ser diferente em ambos os experimentos, as verossimilhan\u00e7as s\u00e3o proporcionais. Portanto, as infer\u00eancias devem ser as mesmas sobre \\theta . Como as infer\u00eancias s\u00e3o baseadas na posteriori, a abordagem bayesiana satisfaz o Princ\u00edpio da Verossimilhan\u00e7a. Por\u00e9m, na abordagem frequentista, isso n\u00e3o \u00e9 verdade, j\u00e1 que \u00e9 baseada no comportamento m\u00e9dio dos procedimentos. O estimador de m\u00e1xima verossimilhan\u00e7a tamb\u00e9m satisfaz. Derivando o princ\u00edpio da verossimilhan\u00e7a Princ\u00edpio da Condicionalidade (PC): Se dois experimentos E e E' sobre \\Omega est\u00e3o dispon\u00edveis e um deles \u00e9 selecionado com probabilidade p , a infer\u00eancia resultante sobre \\theta s\u00f3 deveria depender do experimento selecionado. O fato que Birnbaum demonstrou \u00e9 que PS + PC = PL. Isso \u00e9 interessante, pois, para muitos estat\u00edsticos, PS e PC s\u00e3o aceit\u00e1veis, mais PL n\u00e3o. Isso faz com que os resultados cient\u00edficos, para serem coerentes, devessem ser descritos atrav\u00e9s da fun\u00e7\u00e3o de verossimilhan\u00e7a, e n\u00e3o por n\u00edveis de signific\u00e2ncia e estimativas intervalares. Evans, 2013 utiliza teoria dos conjuntos para mostrar que a demonstra\u00e7\u00e3o de Birnbaum tem falhas, j\u00e1 que ignora uma hip\u00f3tese chave. Gandenberger, 2015 ofereceu uma nova demonstra\u00e7\u00e3o para o Princ\u00edpio da Verossimilhan\u00e7a, mas com hip\u00f3teses diferentes. Aqui temos um breve resumo em formato de slides. Distribui\u00e7\u00f5es a priori e a posteriori Dada a distribui\u00e7\u00e3o de x dada por f(x|\\theta) e a distribui\u00e7\u00e3o a priori \\pi(\\theta) , podemos derivar as seguintes distribui\u00e7\u00f5es: (a) a distribui\u00e7\u00e3o conjunta \\varphi(x, \\theta) = f(x |\\theta) \\pi(\\theta); (b) a distribui\u00e7\u00e3o marginal de x m(x) = \\int f(x|\\theta) \\pi(\\theta) \\, d\\theta; (c) a distribui\u00e7\u00e3o a posteriori de \\theta p(\\theta|x) = \\frac{f(x|\\theta)\\pi(\\theta)}{m(x)}; (d) a distribui\u00e7\u00e3o preditiva de y , quando y \\sim g(y|x, \\theta), g(y|x) = \\int g(y|\\theta, x) \\pi(\\theta | x) \\, d\\theta. Distribui\u00e7\u00f5es a priori impr\u00f3prias Para a especifica\u00e7\u00e3o de um modelo (param\u00e9trico) segundo o preceito bayesiano, \u00e9 preciso definir uma fam\u00edlia param\u00e9trica para as observa\u00e7\u00f5es X e uma distribui\u00e7\u00e3o a priori para \\theta . \u00c9 importante destacar que ambas s\u00e3o escolhas que introduzem subjetividade. Para especificar uma priori, traduzimos conhecimento pr\u00e9vio em uma distribui\u00e7\u00e3o de probabilidade. Nem sempre, temos uma informa\u00e7\u00e3o suficiente para tal. Uma maneira usual de contornar essa situa\u00e7\u00e3o \u00e9 construir uma sequ\u00eancia de distribui\u00e7\u00f5es no espa\u00e7o de par\u00e2metros e tomar \\pi como a distribui\u00e7\u00e3o limite. Todavia, ela poder\u00e1 sofrer com a propriedade que \\int_{\\Omega} \\pi(\\theta) \\, d\\theta = + \\infty. Nesse caso temos uma distribui\u00e7\u00e3o impr\u00f3pria ou generalizada. \ud83d\udcdd Exemplo Suponha que tenhamos X \\sim Normal(\\theta, 1) , em que \\theta \\in \\mathbb{R} . Queremos uma priori normal em \\theta , mas n\u00e3o temos muita certeza de sua localiza\u00e7\u00e3o. Logo, uma vari\u00e2ncia baixa seria uma p\u00e9ssima escolha. Seja \\theta \\sim Normal(0, n^2) . Qual a vari\u00e2ncia que podemos dizer que n\u00e3o \u00e9 baixa? Se fizermos n \\to +\\infty , n\u00e3o teremos mais uma distribui\u00e7\u00e3o normal. Nesses casos, escolhemos uma medida sobre \\Omega , cuja posteriori calculada exista, com respeito a \\lambda . Nesse caso, basta verificar se \\int_{\\Omega} f_{X|\\Theta}(x|t) \\, d\\lambda(t) \u00e9 finita e positiva. Nesse caso, definimos a posteriori segundo a sua f\u00f3rmula pelo Teorema de Bayes. Para definir matematicamente, de forma precisa, a distribui\u00e7\u00e3o impr\u00f3pria, existem algumas tentativas. (1) Remover a restri\u00e7\u00e3o de que a probabilidade do espa\u00e7o \u00e9 1. (2) Probabilidades s\u00e3o finitamente aditivas, e n\u00e3o contavelmente aditivas. Em ambos os casos, muitos resultados de probabilidade falham. Alguns coment\u00e1rios sobre prioris impr\u00f3prias destacados por Robert (p\u00e1ginas 27-31): Muitas vezes, a defini\u00e7\u00e3o de prioris impr\u00f3prias s\u00e3o derivadas por m\u00e9todos autom\u00e1ticos, quando tem-se pouca ou nenhuma informa\u00e7\u00e3o sobre o par\u00e2metro antes de observar o dado (nenhuma informa\u00e7\u00e3o \u00e9 uma express\u00e3o exagerada sujeita \u00e0 cr\u00edtica, tamb\u00e9m). Se a posteriori estiver bem definida, e os estimadores resultantes tiverem boa performance, j\u00e1 temos uma boa justificativa para usar a priori. Distribui\u00e7\u00f5es impr\u00f3prias podem muitas vezes serem vistas como limite de distribui\u00e7\u00f5es pr\u00f3prias, isto \u00e9, s\u00e3o casos extremos onde a informa\u00e7\u00e3o a priori n\u00e3o \u00e9 confi\u00e1vel. Essas distribui\u00e7\u00f5es t\u00eam justificativas frequentistas, em geral. Prioris impr\u00f3prias n\u00e3o tem interpreta\u00e7\u00e3o probabil\u00edstica. A posteriori precisa ser bem-definida. Computacionalmente, amostrar da posteriori se ela for mal-definida vai levar a problemas de estima\u00e7\u00e3o. Logo, \u00e9 essencial verificar se o denominador \u00e9 finito e positivo.","title":"Introdu\u00e7\u00e3o"},{"location":"bayesian/intro/#introducao","text":"Teoria Estat\u00edstica : objetiva obter uma infer\u00eancia sobre a distribui\u00e7\u00e3o de probabilidade de um fen\u00f4meno a partir de observa\u00e7\u00f5es. A base te\u00f3rica do livro The Bayesian Choice \u00e9 constru\u00edda sob o ponto de vista da Teoria da Decis\u00e3o. Isso se deve a dois fatores: (I) a infer\u00eancia tem algum objetivo, isto \u00e9, alguma decis\u00e3o \u00e9 tomada baseada em uma previs\u00e3o ou an\u00e1lise, e ela tem consequ\u00eancias mensur\u00e1veis; e (II) a decis\u00e3o clarifica a prefer\u00eancia do estat\u00edstico. Estat\u00edstica \u00e9 mais sobre a interpreta\u00e7\u00e3o de fen\u00f4menos naturais do que a explica\u00e7\u00e3o sobre eles. Al\u00e9m disso, ela tem um passo de formaliza\u00e7\u00e3o redutiva . A modelagem estat\u00edstica, atrav\u00e9s probabilidade, possibilita incluir a informa\u00e7\u00e3o dispon\u00edvel sobre o fen\u00f4meno e a incerteza sobre essa informa\u00e7\u00e3o. Uma cr\u00edtica \u00e0 abordagem probabil\u00edstica \u00e9 a dificuldade em saber exatamente a distribui\u00e7\u00e3o de probabilidade do fen\u00f4meno. \ud83d\udcdd Exemplo (modelo capture-recapture) Suponha que queremos estimar o n\u00famero de \u00f4nibus N em uma cidade. Uma forma de fazer isso \u00e9 a seguinte: contamos a quantidade vista de \u00f4nibus em um dia ( q_1 ) e armazenamos a identifica\u00e7\u00e3o de cada um. No dia seguinte, fazemos a mesma coisa e obtemos ( q_2 ). Seja n o n\u00famero de \u00f4nibus que vimos nos dois dias. Qual a distribui\u00e7\u00e3o de n ? Olhando para o segundo dia, em uma popula\u00e7\u00e3o de tamanho N , t\u00ednhamos q_1 \u00f4nibus de interesse para recontar. Nossa amostra \u00e9 de tamanho q_2 . Isso define a distribui\u00e7\u00e3o hipergeom\u00e9trica , pois a amostragem do segundo dia \u00e9 sem reposi\u00e7\u00e3o (note que simplificamos que s\u00f3 podemos ver o mesmo \u00f4nibus uma vez). Logo n \\sim Hypergeometric(N, q_1, q_2). Sabemos que \\mathbb{E}[n] = (q_1/N) \\cdot q_2 \\implies \\hat{N} = q_1 \\cdot q_2 / n \u00e9 um poss\u00edvel estimador para N . Note que esse estimador n\u00e3o \u00e9 necessariamente n\u00e3o enviesado, pois \\mathbb{E}[1/n] \\neq 1/\\mathbb{E}[n] . Para aproximar uma distribui\u00e7\u00e3o de probabilidade de um fen\u00f4meno, duas abordagens estat\u00edsticas s\u00e3o usadas: n\u00e3o param\u00e9trica e param\u00e9trica . Na primeira, a estimativa procura assumir o m\u00ednimo de hip\u00f3teses poss\u00edvel, procurando uma estima\u00e7\u00e3o funcional. J\u00e1 a segunda vem de uma densidade parametrizada, em que o par\u00e2metro de dimens\u00e3o finita \u00e9 desconhecido. Um modelo estat\u00edstico param\u00e9trico consiste de observa\u00e7\u00f5es de uma vari\u00e1vel aleat\u00f3ria x \\in \\mathcal{X} (espa\u00e7o de estados) com distribui\u00e7\u00e3o cuja densidade \u00e9 f(x | \\theta) e \\theta \\in \\Omega (espa\u00e7o dos par\u00e2metros) \u00e9 desconhecido com dimens\u00e3o finita. Logo, m\u00e9todos estat\u00edsticos permitem fazer infer\u00eancia sobre \\theta a partir de x , enquanto a modelagem probabil\u00edstica caracteriza o comportamento de observa\u00e7\u00f5es futuras condicionadas em \\theta .","title":"Introdu\u00e7\u00e3o"},{"location":"bayesian/intro/#modelo-parametrico","text":"Schervish apresenta uma defini\u00e7\u00e3o formalizada de modelo param\u00e9trico, a qual eu apresento a seguir. Seja (S, \\mathcal{A}, \\mu) um espa\u00e7o de probabilidade, e (\\mathcal{X}, \\mathcal{B}) e (\\Omega, \\tau) espa\u00e7os de Borel (espa\u00e7o mensur\u00e1vel isomorfo a um subconjunto mensur\u00e1vel dos reais. Em geral, veremos que esses espa\u00e7os s\u00e3o subconjuntos mensur\u00e1veis dos reais). Seja X : S \\to \\mathcal{X} e \\Theta : S \\to \\Omega fun\u00e7\u00f5es mensur\u00e1veis. Chamamos \\Theta de par\u00e2metro e \\Omega de espa\u00e7o de par\u00e2metros. A distribui\u00e7\u00e3o de X | \\Theta = \\theta \u00e9 fam\u00edlia param\u00e9trica de distribui\u00e7\u00f5es de X dada por P_{\\theta} . A medida de probabilidade \\mu_{\\Theta} sobre (\\Omega, \\tau) induzida por \\Theta a partir de \\mu \u00e9 chamada de distribui\u00e7\u00e3o a priori ( \\mu_{\\Theta}(B) = \\mu(\\Theta \\in B), B \\in \\tau ). A densidade de P_{\\theta} (que \u00e9 absolutamente cont\u00ednua) com respeito \u00e0 uma medida \\nu \u00e9 dada por f_{X|\\Theta}(x|\\theta) = \\frac{dP_{\\theta}}{d\\nu}(x), a derivada de Radon-Nikodym.","title":"Modelo param\u00e9trico"},{"location":"bayesian/intro/#paradigma-bayesiano","text":"No paradigma bayesiano, as quantidades desconhecidas s\u00e3o tratadas como vari\u00e1veis aleat\u00f3rias, incluindo o par\u00e2metro \\theta . Na p\u00e1gina 12 de seu livro, Shervish apresenta uma justificativa matem\u00e1tica para esse fato. Assim, se temos um modelo P_{\\theta} para x , precisamos de uma distribui\u00e7\u00e3o para \\theta , a qual chamamos de distribui\u00e7\u00e3o a priori . Com elas, constru\u00edmos uma distribui\u00e7\u00e3o em \\mathcal{X} \\times \\Omega . Em particular, \\Pr((x,\\theta) \\in B) = \\int_{\\Omega} \\int_{\\mathcal{X}} I_B(u, v) f_{x | \\theta}(u|v)f_{\\theta}(v) \\, du \\, dv, se f_{\\theta} for a densidade da distribui\u00e7\u00e3o de \\theta . Para isso, basta exigir que f_{X | \\Theta} seja mensur\u00e1vel em \\mathcal{B} \\otimes \\tau .Matematicamente, probabilidades podem representar cren\u00e7as numericamente, relacionando informa\u00e7\u00e3o com probabilidade. A Regra de Bayes prov\u00ea um m\u00e9todo racional para atualizar essas cren\u00e7as frente a novas informa\u00e7\u00f5es. O processo indutivo de atualizar cren\u00e7as com Bayes \u00e9 chamada de infer\u00eancia bayesiana.","title":"Paradigma bayesiano"},{"location":"bayesian/intro/#teorema-de-bayes","text":"Se E \u00e9 um evento com probabilidade positiva e A \u00e9 um outro evento, temos que \\Pr(A | E) = \\frac{\\Pr(E |A) \\Pr(A)}{\\Pr(E)}. Podemos expressar atrav\u00e9s de densidades, com p(\\theta | x) = \\frac{f(x|\\theta)\\pi(\\theta)}{\\int_{\\Omega} f(x|t)\\pi(t) \\, dt}, em que \\pi(\\theta) \u00e9 a densidade da priori do par\u00e2metro \\theta e p(\\theta | x) \u00e9 chamada de distribui\u00e7\u00e3o a posteriori de \\theta (para uma demonstra\u00e7\u00e3o detalhada do Teorema de Bayes, vale conferir a p\u00e1gina 16 do livro de Schervish). Al\u00e9m disso, como o par\u00e2metro \\theta \u00e9 desconhecido, tamb\u00e9m denotamos a densidade de x condicionada em \\theta como uma fun\u00e7\u00e3o de \\theta , ap\u00f3s observar x . Nesse caso, chamamos de fun\u00e7\u00e3o de verossimilhan\u00e7a l(\\theta | x) = f(x | \\theta) para cada \\theta \\in \\Omega e x observado. O denominador da express\u00e3o acima \u00e9 chamada de densidade preditiva a priori e n\u00e3o depende de \\theta . \u00c9 a marginal no espa\u00e7o de estados. A distribui\u00e7\u00e3o a priori encapsula a informa\u00e7\u00e3o dispon\u00edvel sobre o par\u00e2metro \\theta antes do experimento, incluindo a incerteza residual. Se a distribui\u00e7\u00e3o a priori e a distribui\u00e7\u00e3o dos dados representam cren\u00e7as racionais, a regra de Bayes \u00e9 o m\u00e9todo \u00f3timo para atualizar essas cren\u00e7as sobre o par\u00e2metro dada nova informa\u00e7\u00e3o. Claro que, em geral, n\u00e3o conseguimos explorar as cren\u00e7as de modo perfeito e a posteriori n\u00e3o vai ser \u00f3tima, nesse sentido. \ud83d\udcdd Exemplo (Bayes, 1974) Uma bola de sinuca O \u00e9 rolada em uma linha de comprimento 1. \u00c9 natural assumir que ela tem uma distribui\u00e7\u00e3o uniforme de parar em qualquer lugar, s\u00f3 dependendo da for\u00e7a exercida sobre ela. Seja p o ponto de parada. Em seguida, rolamos uma outra bola n vezes e contamos a quantidade de vezes ( X ) que ela parou antes de B . Nesse caso, observado X=x , queremos inferir p . Se p fosse conhecido, qual seria distribui\u00e7\u00e3o de X ? Veja que temos n experimentos independentes e id\u00eanticos de sucesso ou falha, com probabilidade de sucesso p (lembrando da distribui\u00e7\u00e3o uniforme!). Nesse caso X|p \\sim Bernoulli(n, p) . Com essas configura\u00e7\u00f5es, podemos verificar que \\Pr(a < p < b | X=x) = \\frac{\\int_a^b p^x(1-p)^{n-x} \\, dp}{B(x+1, n-x+1)}, em que B \u00e9 a fun\u00e7\u00e3o Beta . Enquanto a estat\u00edstica cl\u00e1ssica \u00e9 dirigida por princ\u00edpios justificados axiomaticamente, a abordagem bayesiana incorpora esses princ\u00edpios sem a restri\u00e7\u00e3o sobre os procedimentos e tamb\u00e9m rejeita outros princ\u00edpios. Por exemplo, o conceito de estimadores n\u00e3o enviesados, em geral preferidos na estat\u00edstica cl\u00e1ssica, imp\u00f5e restri\u00e7\u00f5es fortes sobre os procedimentos adotados e leva a performances ineficientes (ver exemplo de Stein ). Isso acontece, pois a justificativa \u00e9 assint\u00f3tica, j\u00e1 que em m\u00e9dia o estimador \u00e9 correto. Por fim, em estat\u00edstica bayesiana, TODAS AS INFER\u00caNCIAS S\u00c3O BASEADAS NA DISTRIBUI\u00c7\u00c3O A POSTERIORI .","title":"Teorema de Bayes"},{"location":"bayesian/intro/#um-pouco-de-historia","text":"Em 1763, \u00e9 publicado An Essay towards Solving a Problem in the Doctrine of Chances , paper atribu\u00eddo a Thomas Bayes e publicado por Richard Price. O principal objeto desse trabalho, al\u00e9m do exemplo acima, \u00e9 o que conhecemos como Teorema de Bayes, mas com a priori sendo uniforme. Laplace, em Memoir on the Probability of the Causes of Events foi quem descreveu em uma forma mais geral, apesar de ainda ser discreta. Do ponto de vista probabil\u00edstico, o Teorema de Bayes \u00e9 apenas uma forma de mensurar a incerteza. A controv\u00e9rsia adv\u00e9m da interpreta\u00e7\u00e3o da probabilidade e se ele deveria ser considerado ponto central no processo de aprendizado.","title":"Um pouco de hist\u00f3ria"},{"location":"bayesian/intro/#visao-subjetiva-da-probabilidade","text":"A vis\u00e3o de que o mundo \u00e9 determin\u00edstico ou n\u00e3o, como a discuss\u00e3o do Dem\u00f4nio de Laplace , \u00e9 pouco importante na verdade para a estat\u00edstica. O que importa \u00e9 a incerteza que temos sobre as quantidades. No pref\u00e1cio de seu livro Theory of Probability , Bruno de Finetti argumenta que \"probabilidade n\u00e3o existe\" no sentido objetivo . A \u00fanica exig\u00eancia \u00e9 que exista consist\u00eancia em nossas cren\u00e7as e na rela\u00e7\u00e3o com dados objetivos. Basicamente, a defini\u00e7\u00e3o de probabilidade \u00e9 subjetiva: a taxa em que o indiv\u00edduo est\u00e1 disposto a apostar na ocorr\u00eancia de um evento. Considere um dado normal de seis lados. Um frequentista afirmaria que, por simetria, cada lado tem igual chance de ocorrer. Evid\u00eancia emp\u00edrica passada suportaria sua afirma\u00e7\u00e3o. Um subjetivista ouviria os argumentos, mas o que realmente iria considerar seria sua cren\u00e7a sobre o que acontecer\u00e1 em uma jogada de dados, isto \u00e9, quanto seria apostado em cada lado, dada a informa\u00e7\u00e3o presente. Logo, no trabalho de De Finetti, Probabilidade e Pre\u00e7o s\u00e3o equivalentes. Para uma discuss\u00e3o mais detalhada, consulte esse trabalho .","title":"Vis\u00e3o subjetiva da probabilidade"},{"location":"bayesian/intro/#principio-da-verossimilhanca-e-principio-da-suficiencia","text":"","title":"Princ\u00edpio da Verossimilhan\u00e7a e Princ\u00edpio da Sufici\u00eancia"},{"location":"bayesian/intro/#suficiencia","text":"Seja x \\sim f(x | \\theta) . Uma fun\u00e7\u00e3o/estat\u00edstica T: \\mathcal{X} \\to \\mathbb{R}^k (a imagem de T , juntamente com o conjunto de seus singletons, pode ser qualquer espa\u00e7o mensur\u00e1vel) \u00e9 suficiente se a distribui\u00e7\u00e3o de x condicionada em T(x) n\u00e3o depende de \\theta . Para mais detalhes, ver aqui . De forma simplificada, T(x) traz toda a informa\u00e7\u00e3o sobre \\theta advinda de x . Schervish (p\u00e1gina 84) adiciona o fato de que para qualquer priori \\pi(theta) , a distribui\u00e7\u00e3o a posteriori de \\theta condicionada em x e a posteriori condicionada em T(x) s\u00e3o iguais quase certamente. Como demonstrado no Teorema 2.14 do mesmo livro, essas defini\u00e7\u00f5es s\u00e3o equivalentes dadas algumas hip\u00f3teses de regularidade. O Teorema da fatora\u00e7\u00e3o Fisher-Neyman mostra que se a densidade de x \u00e9 a derivada de Radon-Nikodym para alguma medida de probabilidade ( \\sigma - finita) cuja distribui\u00e7\u00e3o seja absolutamente cont\u00ednua, ent\u00e3o vale que T(x) \u00e9 suficiente para \\theta se, e somente se, existem fun\u00e7\u00f5es g e h n\u00e3o-negativas tal que f(x|\\theta) = g(T(x)|\\theta)h(x|T(x)). O conceito de sufici\u00eancia foi introduzido por Fisher e est\u00e1 associado com o seguinte princ\u00edpio: Princ\u00edpio da Sufici\u00eancia (PS): Se duas observa\u00e7\u00f5es x e y s\u00e3o tais que T(x) = T(y) para alguma estat\u00edstica suficiente T , ent\u00e3o elas devem levar \u00e0 mesma infer\u00eancia sobre o par\u00e2metro. \ud83d\udcdd Exemplo Suponha que observamos x = (x_1, \\dots, x_n) \\overset{iid}{\\sim} Exponential(\\lambda) . Uma estat\u00edstica suficiente para \\lambda \u00e9 a m\u00e9dia amostral T(x) = \\bar{x} , em particular, f(x | \\lambda) = \\lambda^n e^{-\\lambda n \\bar{x}}. Logo, infer\u00eancias sobre \\lambda s\u00f3 devem se basear em \\bar{x} , segundo o Princ\u00edpio da sufici\u00eancia.","title":"Sufici\u00eancia"},{"location":"bayesian/intro/#principio-da-verossimilhanca","text":"Esse conceito \u00e9 tamb\u00e9m atribu\u00eddo a Fisher , mas a sua formaliza\u00e7\u00e3o se deve a Birnbaum (1962) . Princ\u00edpio da Verossimilhan\u00e7a (PV): a informa\u00e7\u00e3o trazida por uma observa\u00e7\u00e3o x sobre \\theta \u00e9 inteiramente contida na fun\u00e7\u00e3o de verossimilhan\u00e7a l(\\theta|x) . Al\u00e9m do mais, se duas observa\u00e7\u00f5es x e y dependem de \\theta , de forma que l_1(\\theta|x) = cl_2(\\theta|y), \\, \\forall \\theta \\in \\Omega para alguma constante c , ent\u00e3o elas levam \u00e0 mesma infer\u00eancia sobre \\theta . Uma outra forma de expressar esse princ\u00edpio \u00e9 o seguinte: Se E e E' s\u00e3o experimentos definidos em \\Omega , representados pelas densidades f(x, \\theta) e g(y, \\theta) , e x e y s\u00e3o observa\u00e7\u00f5es determinando a mesma fun\u00e7\u00e3o de verossimilhan\u00e7a, ent\u00e3o a evid\u00eancia trazida por ambos os experimentos \u00e9 a mesma vista a partir dessas observa\u00e7\u00f5es, isto \u00e9, o resultado x de qualquer experimento E \u00e9 caracterizado somente pela verossimilhan\u00e7a at\u00e9 uma constante. \ud83d\udcdd Exemplo Seja \\theta \\in [0,1] a propor\u00e7\u00e3o de doentes em uma popula\u00e7\u00e3o. Um examinador encontrou nove pessoas saud\u00e1veis e tr\u00eas doentes. Se nenhuma informa\u00e7\u00e3o adicional \u00e9 obtida, podemos propor dois modelos diferentes para esse fen\u00f4meno: (1) O examinador testou 12 pessoas e observou x \\sim Binomial(12, \\theta) com x = 3 . (2) Ele questionou N = 12 pessoas at\u00e9 encontrar 3 doentes. Nesse caso, N \\sim NegativeBinomial(3, \\theta) Apesar do dado ser diferente em ambos os experimentos, as verossimilhan\u00e7as s\u00e3o proporcionais. Portanto, as infer\u00eancias devem ser as mesmas sobre \\theta . Como as infer\u00eancias s\u00e3o baseadas na posteriori, a abordagem bayesiana satisfaz o Princ\u00edpio da Verossimilhan\u00e7a. Por\u00e9m, na abordagem frequentista, isso n\u00e3o \u00e9 verdade, j\u00e1 que \u00e9 baseada no comportamento m\u00e9dio dos procedimentos. O estimador de m\u00e1xima verossimilhan\u00e7a tamb\u00e9m satisfaz.","title":"Princ\u00edpio da Verossimilhan\u00e7a"},{"location":"bayesian/intro/#derivando-o-principio-da-verossimilhanca","text":"Princ\u00edpio da Condicionalidade (PC): Se dois experimentos E e E' sobre \\Omega est\u00e3o dispon\u00edveis e um deles \u00e9 selecionado com probabilidade p , a infer\u00eancia resultante sobre \\theta s\u00f3 deveria depender do experimento selecionado. O fato que Birnbaum demonstrou \u00e9 que PS + PC = PL. Isso \u00e9 interessante, pois, para muitos estat\u00edsticos, PS e PC s\u00e3o aceit\u00e1veis, mais PL n\u00e3o. Isso faz com que os resultados cient\u00edficos, para serem coerentes, devessem ser descritos atrav\u00e9s da fun\u00e7\u00e3o de verossimilhan\u00e7a, e n\u00e3o por n\u00edveis de signific\u00e2ncia e estimativas intervalares. Evans, 2013 utiliza teoria dos conjuntos para mostrar que a demonstra\u00e7\u00e3o de Birnbaum tem falhas, j\u00e1 que ignora uma hip\u00f3tese chave. Gandenberger, 2015 ofereceu uma nova demonstra\u00e7\u00e3o para o Princ\u00edpio da Verossimilhan\u00e7a, mas com hip\u00f3teses diferentes. Aqui temos um breve resumo em formato de slides.","title":"Derivando o princ\u00edpio da verossimilhan\u00e7a"},{"location":"bayesian/intro/#distribuicoes-a-priori-e-a-posteriori","text":"Dada a distribui\u00e7\u00e3o de x dada por f(x|\\theta) e a distribui\u00e7\u00e3o a priori \\pi(\\theta) , podemos derivar as seguintes distribui\u00e7\u00f5es: (a) a distribui\u00e7\u00e3o conjunta \\varphi(x, \\theta) = f(x |\\theta) \\pi(\\theta); (b) a distribui\u00e7\u00e3o marginal de x m(x) = \\int f(x|\\theta) \\pi(\\theta) \\, d\\theta; (c) a distribui\u00e7\u00e3o a posteriori de \\theta p(\\theta|x) = \\frac{f(x|\\theta)\\pi(\\theta)}{m(x)}; (d) a distribui\u00e7\u00e3o preditiva de y , quando y \\sim g(y|x, \\theta), g(y|x) = \\int g(y|\\theta, x) \\pi(\\theta | x) \\, d\\theta.","title":"Distribui\u00e7\u00f5es a priori e a posteriori"},{"location":"bayesian/intro/#distribuicoes-a-priori-improprias","text":"Para a especifica\u00e7\u00e3o de um modelo (param\u00e9trico) segundo o preceito bayesiano, \u00e9 preciso definir uma fam\u00edlia param\u00e9trica para as observa\u00e7\u00f5es X e uma distribui\u00e7\u00e3o a priori para \\theta . \u00c9 importante destacar que ambas s\u00e3o escolhas que introduzem subjetividade. Para especificar uma priori, traduzimos conhecimento pr\u00e9vio em uma distribui\u00e7\u00e3o de probabilidade. Nem sempre, temos uma informa\u00e7\u00e3o suficiente para tal. Uma maneira usual de contornar essa situa\u00e7\u00e3o \u00e9 construir uma sequ\u00eancia de distribui\u00e7\u00f5es no espa\u00e7o de par\u00e2metros e tomar \\pi como a distribui\u00e7\u00e3o limite. Todavia, ela poder\u00e1 sofrer com a propriedade que \\int_{\\Omega} \\pi(\\theta) \\, d\\theta = + \\infty. Nesse caso temos uma distribui\u00e7\u00e3o impr\u00f3pria ou generalizada. \ud83d\udcdd Exemplo Suponha que tenhamos X \\sim Normal(\\theta, 1) , em que \\theta \\in \\mathbb{R} . Queremos uma priori normal em \\theta , mas n\u00e3o temos muita certeza de sua localiza\u00e7\u00e3o. Logo, uma vari\u00e2ncia baixa seria uma p\u00e9ssima escolha. Seja \\theta \\sim Normal(0, n^2) . Qual a vari\u00e2ncia que podemos dizer que n\u00e3o \u00e9 baixa? Se fizermos n \\to +\\infty , n\u00e3o teremos mais uma distribui\u00e7\u00e3o normal. Nesses casos, escolhemos uma medida sobre \\Omega , cuja posteriori calculada exista, com respeito a \\lambda . Nesse caso, basta verificar se \\int_{\\Omega} f_{X|\\Theta}(x|t) \\, d\\lambda(t) \u00e9 finita e positiva. Nesse caso, definimos a posteriori segundo a sua f\u00f3rmula pelo Teorema de Bayes. Para definir matematicamente, de forma precisa, a distribui\u00e7\u00e3o impr\u00f3pria, existem algumas tentativas. (1) Remover a restri\u00e7\u00e3o de que a probabilidade do espa\u00e7o \u00e9 1. (2) Probabilidades s\u00e3o finitamente aditivas, e n\u00e3o contavelmente aditivas. Em ambos os casos, muitos resultados de probabilidade falham. Alguns coment\u00e1rios sobre prioris impr\u00f3prias destacados por Robert (p\u00e1ginas 27-31): Muitas vezes, a defini\u00e7\u00e3o de prioris impr\u00f3prias s\u00e3o derivadas por m\u00e9todos autom\u00e1ticos, quando tem-se pouca ou nenhuma informa\u00e7\u00e3o sobre o par\u00e2metro antes de observar o dado (nenhuma informa\u00e7\u00e3o \u00e9 uma express\u00e3o exagerada sujeita \u00e0 cr\u00edtica, tamb\u00e9m). Se a posteriori estiver bem definida, e os estimadores resultantes tiverem boa performance, j\u00e1 temos uma boa justificativa para usar a priori. Distribui\u00e7\u00f5es impr\u00f3prias podem muitas vezes serem vistas como limite de distribui\u00e7\u00f5es pr\u00f3prias, isto \u00e9, s\u00e3o casos extremos onde a informa\u00e7\u00e3o a priori n\u00e3o \u00e9 confi\u00e1vel. Essas distribui\u00e7\u00f5es t\u00eam justificativas frequentistas, em geral. Prioris impr\u00f3prias n\u00e3o tem interpreta\u00e7\u00e3o probabil\u00edstica. A posteriori precisa ser bem-definida. Computacionalmente, amostrar da posteriori se ela for mal-definida vai levar a problemas de estima\u00e7\u00e3o. Logo, \u00e9 essencial verificar se o denominador \u00e9 finito e positivo.","title":"Distribui\u00e7\u00f5es a priori impr\u00f3prias"},{"location":"bayesian/model-choice/","text":"Escolha de modelos Podemos considerar a escolha de modelos como um caso especial de testagem, afinal estamos testando qual modelo usar. Todavia, cuidado adicional deve ser tomado porque estamos lidando com modelos potencialmente bastante diferentes, diferente de apenas verificarmos se o par\u00e2metro de um modelo espec\u00edfico mora em uma regi\u00e3o do espa\u00e7o de par\u00e2metros. Estamos agora considerando que a distribui\u00e7\u00e3o dos dados f \u00e9 desconhecida, o que torna mais dif\u00edcil condicionar em x . Isso tamb\u00e9m levanta a pergunta se f pertence mesmo \u00e0 fam\u00edlia considerada \\{f_{\\theta} : \\theta \\in \\Theta\\} e, de forma mais pronfunda, se um modelo verdadeiro de fato existe. Considere inicialmente uma situa\u00e7\u00e3o que temos modelos param\u00e9tricos competindo: \\mathcal{M}_i : x \\sim f_i(x|\\theta_i). \\theta_i \\in \\Theta_i, i \\in I. Do ponto de vista bayesiano, poder\u00edamos contruir uma distribui\u00e7\u00e3o a priori para I , e todas as infer\u00eancias deveriam ser baseadas na posteriori definida em I . Em geral, I \u00e9 um conjunto pequeno, com distribui\u00e7\u00f5es conhecidas. \ud83d\udcdd Exemplo Em problemas de contagem, por exemplo o n\u00famero de acidentes de carro em uma rodovia em um per\u00edodo de tempo, estamos modelando N . Podemos atribuir duas distribui\u00e7\u00f5es distitas: \\mathcal{M}_1 : N \\sim \\operatorname{Poi}(\\lambda) ou \\mathcal{M}_2 : N \\sim \\operatorname{NegBin}(m,p) . Note que a dimens\u00e3o dos par\u00e2metros \u00e9 completamente diferente e cada distribui\u00e7\u00e3o tem suas particularidades. Podemos tamb\u00e9m atribuir modelos n\u00e3o param\u00e9tricos, quando pouca informa\u00e7\u00e3o sobre o processo gerador \u00e9 obtido. Nesse caso, I \u00e9 infinito e possivelmente n\u00e3o enumer\u00e1vel. Outro problema que precisamos enfrentar, \u00e9 que modelos diferentes podem ter resultados similares e serem apropriados, mesmo que n\u00e3o sejam os verdadeiros (se \u00e9 que isso existe!). Por fim, existe a situa\u00e7\u00e3o de compararmos modelos em que um \u00e9 submodelo do outro. Nesse caso, em geral o modelo maior vai apresentar uma perda quadr\u00e1tica menor, por exemplo, mas mais par\u00e2metros s\u00e3o estimados a partir da mesma amostra. O cl\u00e1ssico exemplo dessa situa\u00e7\u00e3o \u00e9 a escolha das vari\u00e1veis que v\u00e3o compor uma regress\u00e3o linear. Framework padr\u00e3o Modelagem a priori Podemos extender o espa\u00e7o dos par\u00e2metros para \\boldsymbol{\\Theta} = \\cup_{i\\in I} \\{i\\}\\times \\Theta_i , em que \\mu \\in I \u00e9 tamb\u00e9m um par\u00e2metro. Assim, podemos definir p_{i} como a probabilidade a priori para o modelo \\mathcal{M}_i . Com isso, o Teorema de Bayes diz que p(\\mathcal{M}_i | x) \\propto p_i\\int_{\\Theta_i} f_i(x|\\theta_i) \\pi_i(\\theta_i) \\, d\\theta_i. Quando I \u00e9 infinito, a constru\u00e7\u00e3o da priori (\\pi_i, p_i) para cada i \\in I \u00e9 delicada. Al\u00e9m do mais, quando um modelo i \u00e9 submodelo de outro j , deveria haver uma coer\u00eancia entre \\pi_i e \\pi_j e, talvez, entre p_i e p_j . Um outro ponto importante \u00e9 que par\u00e2metros comuns a modelos diferentes devem ser tratados como entidades separadas. Exce\u00e7\u00f5es devem ser consideradas caso a caso. Fator de Bayes O fator de Bayes B_{12} = \\frac{P(\\mathcal{M}_1|x)}{P(\\mathcal{M}_2|x)} \\bigg/ \\frac{P(\\mathcal{M}_1)}{P(\\mathcal{M}_2)} \u00e9 usado para comparar os modelos \\mathcal{M}_1 e \\mathcal{M}_2 . O problema acontece quando queremos comparar muitos modelos. Crit\u00e9rio de Schwartz Considere a expans\u00e3o de Laplace \\int_{\\Theta} \\exp\\{n h(\\theta)\\} \\, d\\theta = \\exp\\{n h(\\hat\\theta)\\} (2\\pi)^{p/2} n^{-p/2} |H^{-1}(\\hat\\theta)| + O(n^{-1}), em que \\hat{\\theta} \u00e9 o argumento m\u00e1ximo de h e H \u00e9 a Hessiana de h . Aplicando essa aproxima\u00e7\u00e3o ao fator de Bayes, obtemos B_{12} \\approx \\frac{L_{1,n}(\\hat\\theta_{1,n})}{L_{2,n}(\\hat\\theta_{2,n})}\\bigg|\\frac{H_1^{-1}(\\hat\\theta_{1,n})}{H_2^{-1}(\\hat\\theta_{2,n})}\\bigg|\\left(\\frac{n}{2\\pi}\\right)^{(p_1-p_2)/2}, em que L_{i,n} \u00e9 a verossimilhan\u00e7a do modelo i para n observa\u00e7\u00f5es e \\hat{\\theta}_{i,n} o respectivo argumento m\u00e1ximo. Portanto, \\log(B_{12}) \\approx \\log\\lambda_n + \\frac{p_2-p_1}{2}\\log(n) + K(\\hat\\theta_{1,2}, \\hat\\theta_{2,n}), em que \\lambda_n = L_{1,n}(\\hat\\theta_{1,n}) / L_{2,n}(\\hat\\theta_{2,n}) O crit\u00e9rio de Schwartz \u00e9 dado por S = -\\log \\lambda_n - \\frac{p_2-p_1}{2}\\log(n) quando \\mathcal{M}_1 \\subset \\mathcal{M}_2 e o termo restante \u00e9 negligenci\u00e1vel. Esse crit\u00e9rio tamb\u00e9m \u00e9 conhecido como Crit\u00e9rio de Informa\u00e7\u00e3o de Bayes (BIC). Substituindo 1/2 por \\log(2) , e temos o primeiro termo multiplicado por 2, temos o Crit\u00e9rio de Informa\u00e7\u00e3o Akaike (AIC). Apesar de ser uma aproxima\u00e7\u00e3o de primeira ordem para o fator de Bayes, a depend\u00eancia na priori desaparece, e a compara\u00e7\u00e3o s\u00f3 \u00e9 v\u00e1lida para modelos regulares, logo a relev\u00e2ncia em Infer\u00eancia Bayesiana \u00e9 menor. Desvio bayesiano Uma alternativa ao AIC e ao BIC \u00e9 o Crit\u00e9rio de Informa\u00e7\u00e3o de Desvio (DIC), definido como \\mathbb{E}[D(\\theta)|x] + p_D = \\mathbb{E}[D(\\theta)|x] + (\\mathbb{E}[D(\\theta)|x] - D(\\mathbb{E}[\\theta|x])), em que D(\\theta) = -2\\log(f(x|\\theta)) \u00e9 uma medida de desvio. e p_D \u00e9 uma penaliza\u00e7\u00e3o. Assim, quanto menor o valor de DIC , melhor o modelo. Ideias adicionais Modelo m\u00e9dio: Uma forma de lidar com a escolha de modelos \u00e9 n\u00e3o escolher um de fato, mas sim, incluir todos os modelos para lidar com a incerteza do modelo propriamente. Isso nem sempre \u00e9 poss\u00edvel em quest\u00f5es cient\u00edficas, j\u00e1 que a escolha de um modelo explicativo pode ser relevante. Al\u00e9m do mais, essa maneira parece infringir a parcim\u00f4nia. Proje\u00e7\u00e3o de modelos: essa abordagem \u00e9 baseada na ideia de projetar um modelo f(y|\\theta) em submodelos atrav\u00e9s de restri\u00e7\u00f5es em \\theta . Isso permite a constru\u00e7\u00e3o de uma \u00fanica priori para \\theta e, portanto, acomoda bem prioris impr\u00f3prias. Dada uma restri\u00e7\u00e3o \\Theta_0 , uma ideia \u00e9 considerar uma restri\u00e7\u00e3o aceit\u00e1vel se d(f(\\cdot|\\theta). \\Theta_0)< \\epsilon , em que d \u00e9 uma medida de diverg\u00eancia, tal como a pseudo-dist\u00e2ncia de Kullback-Leibler.","title":"Escolha de modelos"},{"location":"bayesian/model-choice/#escolha-de-modelos","text":"Podemos considerar a escolha de modelos como um caso especial de testagem, afinal estamos testando qual modelo usar. Todavia, cuidado adicional deve ser tomado porque estamos lidando com modelos potencialmente bastante diferentes, diferente de apenas verificarmos se o par\u00e2metro de um modelo espec\u00edfico mora em uma regi\u00e3o do espa\u00e7o de par\u00e2metros. Estamos agora considerando que a distribui\u00e7\u00e3o dos dados f \u00e9 desconhecida, o que torna mais dif\u00edcil condicionar em x . Isso tamb\u00e9m levanta a pergunta se f pertence mesmo \u00e0 fam\u00edlia considerada \\{f_{\\theta} : \\theta \\in \\Theta\\} e, de forma mais pronfunda, se um modelo verdadeiro de fato existe. Considere inicialmente uma situa\u00e7\u00e3o que temos modelos param\u00e9tricos competindo: \\mathcal{M}_i : x \\sim f_i(x|\\theta_i). \\theta_i \\in \\Theta_i, i \\in I. Do ponto de vista bayesiano, poder\u00edamos contruir uma distribui\u00e7\u00e3o a priori para I , e todas as infer\u00eancias deveriam ser baseadas na posteriori definida em I . Em geral, I \u00e9 um conjunto pequeno, com distribui\u00e7\u00f5es conhecidas. \ud83d\udcdd Exemplo Em problemas de contagem, por exemplo o n\u00famero de acidentes de carro em uma rodovia em um per\u00edodo de tempo, estamos modelando N . Podemos atribuir duas distribui\u00e7\u00f5es distitas: \\mathcal{M}_1 : N \\sim \\operatorname{Poi}(\\lambda) ou \\mathcal{M}_2 : N \\sim \\operatorname{NegBin}(m,p) . Note que a dimens\u00e3o dos par\u00e2metros \u00e9 completamente diferente e cada distribui\u00e7\u00e3o tem suas particularidades. Podemos tamb\u00e9m atribuir modelos n\u00e3o param\u00e9tricos, quando pouca informa\u00e7\u00e3o sobre o processo gerador \u00e9 obtido. Nesse caso, I \u00e9 infinito e possivelmente n\u00e3o enumer\u00e1vel. Outro problema que precisamos enfrentar, \u00e9 que modelos diferentes podem ter resultados similares e serem apropriados, mesmo que n\u00e3o sejam os verdadeiros (se \u00e9 que isso existe!). Por fim, existe a situa\u00e7\u00e3o de compararmos modelos em que um \u00e9 submodelo do outro. Nesse caso, em geral o modelo maior vai apresentar uma perda quadr\u00e1tica menor, por exemplo, mas mais par\u00e2metros s\u00e3o estimados a partir da mesma amostra. O cl\u00e1ssico exemplo dessa situa\u00e7\u00e3o \u00e9 a escolha das vari\u00e1veis que v\u00e3o compor uma regress\u00e3o linear.","title":"Escolha de modelos"},{"location":"bayesian/model-choice/#framework-padrao","text":"","title":"Framework padr\u00e3o"},{"location":"bayesian/model-choice/#modelagem-a-priori","text":"Podemos extender o espa\u00e7o dos par\u00e2metros para \\boldsymbol{\\Theta} = \\cup_{i\\in I} \\{i\\}\\times \\Theta_i , em que \\mu \\in I \u00e9 tamb\u00e9m um par\u00e2metro. Assim, podemos definir p_{i} como a probabilidade a priori para o modelo \\mathcal{M}_i . Com isso, o Teorema de Bayes diz que p(\\mathcal{M}_i | x) \\propto p_i\\int_{\\Theta_i} f_i(x|\\theta_i) \\pi_i(\\theta_i) \\, d\\theta_i. Quando I \u00e9 infinito, a constru\u00e7\u00e3o da priori (\\pi_i, p_i) para cada i \\in I \u00e9 delicada. Al\u00e9m do mais, quando um modelo i \u00e9 submodelo de outro j , deveria haver uma coer\u00eancia entre \\pi_i e \\pi_j e, talvez, entre p_i e p_j . Um outro ponto importante \u00e9 que par\u00e2metros comuns a modelos diferentes devem ser tratados como entidades separadas. Exce\u00e7\u00f5es devem ser consideradas caso a caso.","title":"Modelagem a priori"},{"location":"bayesian/model-choice/#fator-de-bayes","text":"O fator de Bayes B_{12} = \\frac{P(\\mathcal{M}_1|x)}{P(\\mathcal{M}_2|x)} \\bigg/ \\frac{P(\\mathcal{M}_1)}{P(\\mathcal{M}_2)} \u00e9 usado para comparar os modelos \\mathcal{M}_1 e \\mathcal{M}_2 . O problema acontece quando queremos comparar muitos modelos.","title":"Fator de Bayes"},{"location":"bayesian/model-choice/#criterio-de-schwartz","text":"Considere a expans\u00e3o de Laplace \\int_{\\Theta} \\exp\\{n h(\\theta)\\} \\, d\\theta = \\exp\\{n h(\\hat\\theta)\\} (2\\pi)^{p/2} n^{-p/2} |H^{-1}(\\hat\\theta)| + O(n^{-1}), em que \\hat{\\theta} \u00e9 o argumento m\u00e1ximo de h e H \u00e9 a Hessiana de h . Aplicando essa aproxima\u00e7\u00e3o ao fator de Bayes, obtemos B_{12} \\approx \\frac{L_{1,n}(\\hat\\theta_{1,n})}{L_{2,n}(\\hat\\theta_{2,n})}\\bigg|\\frac{H_1^{-1}(\\hat\\theta_{1,n})}{H_2^{-1}(\\hat\\theta_{2,n})}\\bigg|\\left(\\frac{n}{2\\pi}\\right)^{(p_1-p_2)/2}, em que L_{i,n} \u00e9 a verossimilhan\u00e7a do modelo i para n observa\u00e7\u00f5es e \\hat{\\theta}_{i,n} o respectivo argumento m\u00e1ximo. Portanto, \\log(B_{12}) \\approx \\log\\lambda_n + \\frac{p_2-p_1}{2}\\log(n) + K(\\hat\\theta_{1,2}, \\hat\\theta_{2,n}), em que \\lambda_n = L_{1,n}(\\hat\\theta_{1,n}) / L_{2,n}(\\hat\\theta_{2,n}) O crit\u00e9rio de Schwartz \u00e9 dado por S = -\\log \\lambda_n - \\frac{p_2-p_1}{2}\\log(n) quando \\mathcal{M}_1 \\subset \\mathcal{M}_2 e o termo restante \u00e9 negligenci\u00e1vel. Esse crit\u00e9rio tamb\u00e9m \u00e9 conhecido como Crit\u00e9rio de Informa\u00e7\u00e3o de Bayes (BIC). Substituindo 1/2 por \\log(2) , e temos o primeiro termo multiplicado por 2, temos o Crit\u00e9rio de Informa\u00e7\u00e3o Akaike (AIC). Apesar de ser uma aproxima\u00e7\u00e3o de primeira ordem para o fator de Bayes, a depend\u00eancia na priori desaparece, e a compara\u00e7\u00e3o s\u00f3 \u00e9 v\u00e1lida para modelos regulares, logo a relev\u00e2ncia em Infer\u00eancia Bayesiana \u00e9 menor.","title":"Crit\u00e9rio de Schwartz"},{"location":"bayesian/model-choice/#desvio-bayesiano","text":"Uma alternativa ao AIC e ao BIC \u00e9 o Crit\u00e9rio de Informa\u00e7\u00e3o de Desvio (DIC), definido como \\mathbb{E}[D(\\theta)|x] + p_D = \\mathbb{E}[D(\\theta)|x] + (\\mathbb{E}[D(\\theta)|x] - D(\\mathbb{E}[\\theta|x])), em que D(\\theta) = -2\\log(f(x|\\theta)) \u00e9 uma medida de desvio. e p_D \u00e9 uma penaliza\u00e7\u00e3o. Assim, quanto menor o valor de DIC , melhor o modelo.","title":"Desvio bayesiano"},{"location":"bayesian/model-choice/#ideias-adicionais","text":"Modelo m\u00e9dio: Uma forma de lidar com a escolha de modelos \u00e9 n\u00e3o escolher um de fato, mas sim, incluir todos os modelos para lidar com a incerteza do modelo propriamente. Isso nem sempre \u00e9 poss\u00edvel em quest\u00f5es cient\u00edficas, j\u00e1 que a escolha de um modelo explicativo pode ser relevante. Al\u00e9m do mais, essa maneira parece infringir a parcim\u00f4nia. Proje\u00e7\u00e3o de modelos: essa abordagem \u00e9 baseada na ideia de projetar um modelo f(y|\\theta) em submodelos atrav\u00e9s de restri\u00e7\u00f5es em \\theta . Isso permite a constru\u00e7\u00e3o de uma \u00fanica priori para \\theta e, portanto, acomoda bem prioris impr\u00f3prias. Dada uma restri\u00e7\u00e3o \\Theta_0 , uma ideia \u00e9 considerar uma restri\u00e7\u00e3o aceit\u00e1vel se d(f(\\cdot|\\theta). \\Theta_0)< \\epsilon , em que d \u00e9 uma medida de diverg\u00eancia, tal como a pseudo-dist\u00e2ncia de Kullback-Leibler.","title":"Ideias adicionais"},{"location":"bayesian/point-estimation/","text":"Estima\u00e7\u00e3o pontual em infer\u00eancia bayesiana Infer\u00eancia bayesiana sobre um par\u00e2metro de interesse \\theta ap\u00f3s observar os dados nos d\u00e1 uma distribui\u00e7\u00e3o de probabilidade p(\\theta | x) , a distribui\u00e7\u00e3o a posteriori. Resumir essa informa\u00e7\u00e3o pontualmente em um estimador \u00e9 de interesse do agente tomador de decis\u00e3o. Por exemplo, se a quantidade de interesse \u00e9 h(\\theta) , a m\u00e9dia a posteriori de h(\\theta) sob a distribui\u00e7\u00e3o a posteriori p(\\cdot|x) \u00e9 um poss\u00edvel estimador. Sem definir uma perda, n\u00e3o h\u00e1 como selecionar um melhor estimador. Um poss\u00edvel estimador \u00e9 o m\u00e1ximo a posteriori (MAP) que \u00e9 definido como a moda da distribui\u00e7\u00e3o a posteriori, isto \u00e9, o valor de \\theta cuja densidade \u00e9 a mais alta. Se \\theta \\in \\{0,1\\} , a perda associada a esse estimador \u00e9 a 0-1. Para o caso cont\u00ednuo, defina L_{\\epsilon}(\\theta, d) = 1_{\\|\\theta - d\\| \\ge \\epsilon} . O estimador que minimizar L_{\\epsilon} quando \\epsilon \\to 0 \u00e9 o MAP. Como maximizar p(\\theta | x) em \\theta \u00e9 equivalente a maximizar \\log l(\\theta|x) + \\log\\pi(\\theta) , o MAP pode ser visto como um estimador de m\u00e1xima verossimilhan\u00e7a com penaliza\u00e7\u00e3o. Al\u00e9m do mais, sob algumas condi\u00e7\u00f5es de regularidade, as propriedades assint\u00f3ticas do MLE cl\u00e1ssico s\u00e3o preservadas, o que faz sentido dado que quando n aumenta, a informa\u00e7\u00e3o da verossimilhan\u00e7a \u00e9 predominante se comparada com a informa\u00e7\u00e3o da priori, que \u00e9 fixa. Quando atribu\u00edmos um estimador \\delta^{\\pi}(x) para h(\\theta) , tamb\u00e9m podemos medir sua precis\u00e3o atrav\u00e9s de alguma esperan\u00e7a, por exemplo o erro quadrado a posteriori \\mathbb{E}^{p}[(\\delta^{\\pi}(x) - h(\\theta))^2 | x], que \u00e9, em particular, a vari\u00e2ncia sob p de h(\\theta) condicionada em x quando o estimador \u00e9 a m\u00e9dia a posteriori de h(\\theta) . Teoria da decis\u00e3o em infer\u00eancia bayesiana Lembramos que dada uma perda L(\\theta, \\delta) e uma distribui\u00e7\u00e3o a priori \\pi , o estimador de Bayes \\delta^{\\pi}(x) \u00e9 solu\u00e7\u00e3o de \\min \\mathbb{E}^p[L(\\theta, \\delta)|x] Lema: Seja f(x|\\theta) = h(x)e^{\\theta\\cdot x - \\psi(\\theta)} uma distribui\u00e7\u00e3o da fam\u00edlia exponencial. Assim, a m\u00e9dia a posteriori de \\theta \u00e9 dada por \\delta^{\\pi}(x) = \\nabla \\log m(x) - \\nabla \\log h(x), em que m(x) \u00e9 a distribui\u00e7\u00e3o marginal do dado. Esse resultado \u00e9 como se fosse o dual da rela\u00e7\u00e3o dos momentos de f com \\psi . Dada uma perda L(\\theta, \\delta) e o estimador de Bayes associado \\delta^{\\pi}(x) , podemos estimar a performance dele atrav\u00e9s da estimativa da perda L(\\theta, \\delta^{\\pi}(x)) atrav\u00e9s de \\tilde{L}(\\theta, \\delta^{\\pi}, \\gamma) = [\\gamma(x) - L(\\theta, \\delta^{\\pi}(x))]^2. O estimador de Bayes para L(\\theta, \\delta^{\\pi}(x)) segundo essa perda \u00e9 \\gamma^{\\pi}(x) = \\mathbb{E}[L(\\theta, \\delta^{\\pi}(x))|x]. Quando L \u00e9 a perda quadr\u00e1tica, note que \\gamma^{\\pi}(x) = \\operatorname{Var}^{\\pi}(\\theta | x) .","title":"Estima\u00e7\u00e3o pontual em infer\u00eancia bayesiana"},{"location":"bayesian/point-estimation/#estimacao-pontual-em-inferencia-bayesiana","text":"Infer\u00eancia bayesiana sobre um par\u00e2metro de interesse \\theta ap\u00f3s observar os dados nos d\u00e1 uma distribui\u00e7\u00e3o de probabilidade p(\\theta | x) , a distribui\u00e7\u00e3o a posteriori. Resumir essa informa\u00e7\u00e3o pontualmente em um estimador \u00e9 de interesse do agente tomador de decis\u00e3o. Por exemplo, se a quantidade de interesse \u00e9 h(\\theta) , a m\u00e9dia a posteriori de h(\\theta) sob a distribui\u00e7\u00e3o a posteriori p(\\cdot|x) \u00e9 um poss\u00edvel estimador. Sem definir uma perda, n\u00e3o h\u00e1 como selecionar um melhor estimador. Um poss\u00edvel estimador \u00e9 o m\u00e1ximo a posteriori (MAP) que \u00e9 definido como a moda da distribui\u00e7\u00e3o a posteriori, isto \u00e9, o valor de \\theta cuja densidade \u00e9 a mais alta. Se \\theta \\in \\{0,1\\} , a perda associada a esse estimador \u00e9 a 0-1. Para o caso cont\u00ednuo, defina L_{\\epsilon}(\\theta, d) = 1_{\\|\\theta - d\\| \\ge \\epsilon} . O estimador que minimizar L_{\\epsilon} quando \\epsilon \\to 0 \u00e9 o MAP. Como maximizar p(\\theta | x) em \\theta \u00e9 equivalente a maximizar \\log l(\\theta|x) + \\log\\pi(\\theta) , o MAP pode ser visto como um estimador de m\u00e1xima verossimilhan\u00e7a com penaliza\u00e7\u00e3o. Al\u00e9m do mais, sob algumas condi\u00e7\u00f5es de regularidade, as propriedades assint\u00f3ticas do MLE cl\u00e1ssico s\u00e3o preservadas, o que faz sentido dado que quando n aumenta, a informa\u00e7\u00e3o da verossimilhan\u00e7a \u00e9 predominante se comparada com a informa\u00e7\u00e3o da priori, que \u00e9 fixa. Quando atribu\u00edmos um estimador \\delta^{\\pi}(x) para h(\\theta) , tamb\u00e9m podemos medir sua precis\u00e3o atrav\u00e9s de alguma esperan\u00e7a, por exemplo o erro quadrado a posteriori \\mathbb{E}^{p}[(\\delta^{\\pi}(x) - h(\\theta))^2 | x], que \u00e9, em particular, a vari\u00e2ncia sob p de h(\\theta) condicionada em x quando o estimador \u00e9 a m\u00e9dia a posteriori de h(\\theta) .","title":"Estima\u00e7\u00e3o pontual em infer\u00eancia bayesiana"},{"location":"bayesian/point-estimation/#teoria-da-decisao-em-inferencia-bayesiana","text":"Lembramos que dada uma perda L(\\theta, \\delta) e uma distribui\u00e7\u00e3o a priori \\pi , o estimador de Bayes \\delta^{\\pi}(x) \u00e9 solu\u00e7\u00e3o de \\min \\mathbb{E}^p[L(\\theta, \\delta)|x] Lema: Seja f(x|\\theta) = h(x)e^{\\theta\\cdot x - \\psi(\\theta)} uma distribui\u00e7\u00e3o da fam\u00edlia exponencial. Assim, a m\u00e9dia a posteriori de \\theta \u00e9 dada por \\delta^{\\pi}(x) = \\nabla \\log m(x) - \\nabla \\log h(x), em que m(x) \u00e9 a distribui\u00e7\u00e3o marginal do dado. Esse resultado \u00e9 como se fosse o dual da rela\u00e7\u00e3o dos momentos de f com \\psi . Dada uma perda L(\\theta, \\delta) e o estimador de Bayes associado \\delta^{\\pi}(x) , podemos estimar a performance dele atrav\u00e9s da estimativa da perda L(\\theta, \\delta^{\\pi}(x)) atrav\u00e9s de \\tilde{L}(\\theta, \\delta^{\\pi}, \\gamma) = [\\gamma(x) - L(\\theta, \\delta^{\\pi}(x))]^2. O estimador de Bayes para L(\\theta, \\delta^{\\pi}(x)) segundo essa perda \u00e9 \\gamma^{\\pi}(x) = \\mathbb{E}[L(\\theta, \\delta^{\\pi}(x))|x]. Quando L \u00e9 a perda quadr\u00e1tica, note que \\gamma^{\\pi}(x) = \\operatorname{Var}^{\\pi}(\\theta | x) .","title":"Teoria da decis\u00e3o em infer\u00eancia bayesiana"},{"location":"bayesian/priors/","text":"Distribui\u00e7\u00f5es a priori A determina\u00e7\u00e3o da distribui\u00e7\u00e3o a priori para a quantidade de interesse \u00e9 um ponto-chave da infer\u00eancia bayesiana e, simultaneamente, \u00e9 alvo de cr\u00edticas. De forma geral, queremos codificar a informa\u00e7\u00e3o sobre um par\u00e2metro \\theta antes de realizarmos um determinado experimento. Dessa forma, a partir de uma informa\u00e7\u00e3o a priori, queremos definir uma distribui\u00e7\u00e3o a priori. \u00c9 claro que raramente conseguimos fazer esse processo de forma exata e \u00fanica. Quando a informa\u00e7\u00e3o \u00e9 insuficiente para definir uma distribui\u00e7\u00e3o de probabilidade, o estat\u00edstico deve colocar informa\u00e7\u00e3o subjetiva para, ent\u00e3o, obter uma priori que fa\u00e7a sentido. Como a distribui\u00e7\u00e3o a priori influencia as infer\u00eancias a posteriori o tanto quanto se queira (no limite, podemos colocar a massa de probabilidade em um \u00fanico ponto), an\u00e1lises de robustez e de sensibilidade s\u00e3o essenciais no dia-a-dia bayesiano. Determina\u00e7\u00e3o de uma priori Queremos que a priori \\pi resuma a informa\u00e7\u00e3o dispon\u00edvel sobre o fen\u00f4meno e a incerteza que temos sobre essa informa\u00e7\u00e3o, mesmo quando \\theta n\u00e3o adv\u00e9m de um processo aleat\u00f3rio. Usando o conceito de George Box de que todos os modelos s\u00e3o errados , n\u00e3o existe uma priori verdadeira e, portanto, buscamos aproxima\u00e7\u00f5es para representar \\pi . \ud83d\udcdd Exemplo (Distribui\u00e7\u00e3o normal) Suponha que observamos x_1, \\dots, x_n \\sim N(\\theta, 1) e assumimos que \\theta \\sim N(\\mu, \\tau) . A m\u00e9dia a posteriori de \\theta \u00e9 dada por \\mathbb{E}[\\theta|x] = \\frac{\\bar{x}\\tau + \\mu/n}{\\tau + 1/n}. Nesse caso, note que estamos fazendo uma m\u00e9dia ponderada de \\bar{x} e \\mu com pesos \\tau e 1/n , respectivamente. Dessa forma, poder\u00edamos pensar que a priori, ter\u00edamos virtualmente uma amostra de tamanho \\tau^{-1} e m\u00e9dia \\mu . Podemos construir uma medida de probabilidade para \\theta atrav\u00e9s de uma rela\u00e7\u00e3o \\preceq que ordena o qu\u00e3o prov\u00e1vel \u00e9 um evento definido pela vari\u00e1vel aleat\u00f3ria \\theta , como resumido aqui . Quando n\u00e3o existe uma informa\u00e7\u00e3o direta sobre \\theta , uma alternativa \u00e9 usar a distribui\u00e7\u00e3o marginal de x dada por m(x) = \\int_{\\Theta} f(x|\\theta) \\pi(\\theta) \\, d\\theta. Por exemplo, se \\theta \u00e9 a produ\u00e7\u00e3o di\u00e1ria m\u00e9dia de leite, informa\u00e7\u00e3o sobre \\theta pode ser obtida a partir da informa\u00e7\u00e3o que j\u00e1 temos sobre o rebanho, o que \u00e9 informa\u00e7\u00e3o sobre a marginal de x . Prioris de entropia m\u00e1xima Assuma que temos \\mathbb{E}^{\\pi}[g_k(\\theta)] = \\omega_k para k=1,\\dots, K . Podemos selecionar \\pi que satisfa\u00e7a essas K rela\u00e7\u00f5es e maximize a entropia, que mede o n\u00edvel de desordem em um sistema. Se \\Theta \u00e9 finito, temos que \\mathcal{E}(\\pi) = \\sum_{\\theta_i \\in \\Theta} \\pi(\\theta_i) \\log(\\pi(\\theta_i)) \u00e9 a entropia de \\pi . Essa quantidade \u00e9 uma medida de incerteza dada por \\pi . Dessa forma, estamos minimizando a informa\u00e7\u00e3o de \\pi trazida a priori. Sendo \\lambda_k o multiplicador de Lagrange associado a \\mathbb{E}^{\\pi}[g_k(\\theta)] = \\omega_k , temos que a defini\u00e7\u00e3o da priori \u00e9 \\pi^*(\\theta_i) \\propto \\exp\\left\\{\\sum_{k=1}^K \\lambda_k g_k(\\theta_i)\\right\\}. No caso cont\u00ednuo, as contas se complicam um pouco e precisamos de uma medida de refer\u00eancia \\pi_0 , que pode ser vista como a distribui\u00e7\u00e3o a priori sem restri\u00e7\u00e3o de momentos. Quando existe uma estrutura de grupo, a medida de Haar invariante \u00e0 direita \u00e9 a escolha natural para \\pi_0 . Nesse caso, a entropia \u00e9 definida como \\mathcal{E}(\\pi) = \\int \\log\\left(\\frac{\\pi(\\theta)}{\\pi_0(\\theta)}\\right) \\, \\pi_0(d\\theta), que \u00e9 a dist\u00e2ncia de Kullback-Leibler entre \\pi e \\pi_0 e a distribui\u00e7\u00e3o a priori que maximiza \\mathcal{E} \u00e9 \\pi^*(\\theta) \\propto \\exp\\left\\{\\sum_{k=1}^K \\lambda_k g_k(\\theta_i)\\right\\}\\pi_0(\\theta) Aproxima\u00e7\u00f5es param\u00e9tricas A forma mais utilizada \u00e9 provavelmente essa. Definimos uma fam\u00edlia param\u00e9trica para a distribui\u00e7\u00e3o a priori e buscamos definir os par\u00e2metros atrav\u00e9s dos momentos ou quartis da distribui\u00e7\u00e3o. A base \u00e9 mais a tratabilidade matem\u00e1tica do que a subjetividade. Outro ponto \u00e9 que distribui\u00e7\u00f5es com caudas muito diferentes para \\Theta infinito levam a infer\u00eancias bastante distintas. Prioris conjugadas Prioris conjugadas s\u00e3o baseadas na verossimilhan\u00e7a e, portanto, j\u00e1 definem a fam\u00edlia param\u00e9trica da distribui\u00e7\u00e3o a priori, restando a defini\u00e7\u00e3o dos par\u00e2metros. Isso limita a informa\u00e7\u00e3o a priori que deve ser obtida a fim de definir uma distribui\u00e7\u00e3o de probabilidade. Ela tamb\u00e9m auxilia na computa\u00e7\u00e3o, como veremos na defini\u00e7\u00e3o: Fam\u00edlia conjugada: Uma fam\u00edlia de distribui\u00e7\u00f5es \\mathcal{F} sobre \\Theta \u00e9 conjugada para a verossimilhan\u00e7a f(x|\\theta) se para toda priori \\pi \\in \\mathcal{F} , a posteriori p(\\cdot \\mid x) \\in \\mathcal{F} . Essa fam\u00edlia se torna interessante quando ela \u00e9 a menor poss\u00edvel (\u00e9 imposs\u00edvel encontrar uma m\u00ednima propriamente dita, mas a ideia \u00e9 que ela tenha dimens\u00e3o de par\u00e2metros baixa) e parametrizada. Logo, atualizar a informa\u00e7\u00e3o por f(x|\\theta) \u00e9 equivalente a atualizar os par\u00e2metros da distribui\u00e7\u00e3o. Nas fam\u00edlias conjugadas, podemos interpretar os hiperpar\u00e2metros como observa\u00e7\u00f5es passadas virtuais, o que \u00e9 considerado um ponto positivo. Fam\u00edlia exponencial e distribui\u00e7\u00f5es conjugadas Seja \\mu uma medida \\sigma -finita em \\mathcal{X} e \\Theta o espa\u00e7o dos par\u00e2metros. Sejam C : \\mathcal{X} \\to \\mathbb{R}_+ , h : \\mathcal{X} \\to \\mathbb{R}_+ , R : \\Theta \\to \\mathbb{R}^k e T : \\mathcal{X} \\to \\mathbb{R}^k fun\u00e7\u00f5es. A fam\u00edlia de distribui\u00e7\u00f5es com densidade com respeito a \\mu f(x|\\theta) = C(\\theta) h(x) \\exp\\{R(\\theta) \\cdot T(x)\\} \u00e9 chamada de fam\u00edlia exponencial de dimens\u00e3o k . Quando R(\\theta) = \\theta e T(x) = x , a fam\u00edlia \u00e9 dita natural . Lema de Pitman\u2013Koopman : Se uma fam\u00edlia de distribui\u00e7\u00f5es f(\\cdot | \\theta) \u00e9 tal que para n suficientemente grande, existe uma estat\u00edstica suficiente de dimens\u00e3o constante, ela \u00e9 exponencial se o suporte n\u00e3o depende de \\theta (essa condi\u00e7\u00e3o final exclui a distribui\u00e7\u00e3o U[-\\theta, \\theta] , por exemplo). Al\u00e9m disso, para toda amostra de f , existe uma estat\u00edstica suficiente de dimens\u00e3o constante. O espa\u00e7o natural de par\u00e2metros \u00e9 denotado por N = \\left\\{\\theta \\in \\Theta \\mid \\int_{\\mathcal{X}} e^{\\theta\\cdot x} h(x) \\, d\\mu(x) < +\\infty \\right\\}. Ela \u00e9 regular se N \u00e9 aberto e m\u00ednimo se dim(N) = dim(C(\\mu)) = K em que C(\\mu) \u00e9 o menor conjunto convexo fechado que cont\u00e9m o suporte de \\mu . Reescreva a densidade como f(x|\\theta) = h(x) e^{\\theta \\cdot x - \\psi(\\theta)}, em que \\psi \u00e9 a fun\u00e7\u00e3o geradora cumulativa , pois \\mathbb{E}_{\\theta}[X] = \\nabla \\psi(\\theta), \\operatorname{Cov}(X_i, X_j) = \\psi_{\\theta_i \\theta_j}(\\theta), supondo \\psi de classe C^2 e \\theta \\in int(N) . Priori conjugada: Uma fam\u00edlia conjugada para f \u00e9 dada pela densidade \\pi(\\theta | \\mu, \\lambda) = K(\\mu, \\lambda) e^{\\theta\\cdot \\mu - \\lambda \\psi(\\theta)}, cuja posteriori \u00e9 dada por \\pi(\\theta | \\mu + x, \\lambda + 1) . Se \\lambda > 0 e \\mu/\\lambda pertence ao interior de C(\\mu) , \\pi define uma distribui\u00e7\u00e3o de probabilidade [Diaconis, Ylvisaker; 1978] . Para uma tabela completa, consulte aqui . Outro resultado verificado no artigo acima \u00e9 que a esperan\u00e7a a posteriori de \\theta \u00e9 dada por \\frac{\\mu + n\\bar{x}}{\\lambda + n}, quando \\mu \\in \\mathcal{X} . Extens\u00f5es Considere o seguinte exemplo \ud83d\udcdd Exemplo [Diaconis, Ylvisaker; 1985] Quando a moeda \u00e9 girada pela borda e observamos o resultado quando ela cai, temos um vi\u00e9s maior para um lado do que para o outro devido a irregularidades. Seja x \\sim Binomial(n, p) o n\u00famero de caras nesse experimento ap\u00f3s n jogadas. Como sabemos que existe uma irregularidade, poder\u00edamos pensar em uma priori para p que tivesse uma cara bimodal, dado peso para as duas possibilidades. Isso n\u00e3o \u00e9 poss\u00edvel com a fam\u00edlia conjugada beta. Uma forma de fazer isso \u00e9, portanto, atrav\u00e9s de misturas de distribui\u00e7\u00f5es beta. Com esse exemplo, podemos ver que misturas de distribui\u00e7\u00f5es conjugadas definem uma fam\u00edlia conjugada maior que d\u00e1 maior flexibilidade para o formato de uma distribui\u00e7\u00e3o a priori. Al\u00e9m disso, podemos verificar que a mistura de distribui\u00e7\u00f5es pode aproximar qualquer distribui\u00e7\u00e3o a priori sob a dist\u00e2ncia de Prokhorov . Distribui\u00e7\u00f5es a priori n\u00e3o informativas Quando pouca (ou nenhuma) informa\u00e7\u00e3o sobre \\theta est\u00e1 dispon\u00edvel, \u00e9 dif\u00edcil justificar a escolha com base subjetiva. Nesse caso, uma alternativa \u00e9 usar a distribui\u00e7\u00e3o dos dados para, a partir dela, definir uma distribui\u00e7\u00e3o a priori. Chamamos essas prioris de n\u00e3o informativas . Mas devemos que lembrar que uma distribui\u00e7\u00e3o ser n\u00e3o informativa n\u00e3o significa que ignor\u00e2ncia total est\u00e1 sendo representada probabilisticamente. Elas podem ser usadas como prioris de refer\u00eancia, todavia. Priori de Laplace Laplace sugeriu construir a distribui\u00e7\u00e3o a priori baseado no Princ\u00edpio da Raz\u00e3o Insuficiente , em que eventos elementares s\u00e3o equiprov\u00e1veis. Nesse caso, adotamos a priori uniforme. Se o espa\u00e7o de par\u00e2metros n\u00e3o for compacto, isso nos leva \u00e0 uma distribui\u00e7\u00e3o impr\u00f3pria, por consequ\u00eancia. Al\u00e9m disso, se fizermos uma transforma\u00e7\u00e3o biun\u00edvoca \\eta = g(\\theta) , a priori para \\eta n\u00e3o ser\u00e1 uniforme pela F\u00f3rmula da Mudan\u00e7a de Vari\u00e1veis, isto \u00e9, a informa\u00e7\u00e3o sobre \\theta n\u00e3o foi criada a partir dessa transforma\u00e7\u00e3o, mas a distribui\u00e7\u00e3o n\u00e3o \u00e9 uniforme. Prioris invariantes O conceito de invari\u00e2ncia \u00e9 bem profundo na matem\u00e1tica e, com base nesse conceito, podemos construir distribui\u00e7\u00f5es invariantes a reparametriza\u00e7\u00e3o. Por exemplo, a fam\u00edlia f(x - \\theta) \u00e9 invariante \u00e0 transla\u00e7\u00e3o, isto \u00e9, y = x - x_0 tem distribui\u00e7\u00e3o da mesma fam\u00edlia para todo x_0 , f(x - (\\theta - x_0)) . Chamamos \\theta de par\u00e2metro de loca\u00e7\u00e3o . Priori de Jeffreys A Priori de Jeffreys \u00e9 baseada na Informa\u00e7\u00e3o de Fisher dada pela express\u00e3o: I(\\theta) = \\mathbb{E}_{\\theta}\\left[\\left(\\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta}\\right)^2\\right], no caso unidimensional. A distribui\u00e7\u00e3o de Jeffreys \u00e9 definida por \\pi^*(\\theta) \\propto I^{1/2}(\\theta). Note que I(\\theta) = I(h(\\theta))(h'(\\theta))^2 se h \u00e9 uma transforma\u00e7\u00e3o biun\u00edvoca. Nesse caso, \\pi^*(h(\\theta)) \\propto I^{1/2}(h(\\theta)) = I^{1/2}(\\theta)/|h'(\\theta)| , exatamente o que esper\u00e1vamos usando a mudan\u00e7a de vari\u00e1veis. Logo, essa priori \u00e9 invariante \u00e0 reparametriza\u00e7\u00e3o. Al\u00e9m disso, I(\\theta) \u00e9 uma medida da quantidade de informa\u00e7\u00e3o trazida pelo modelo sobre \\theta . Na pr\u00e1tica, prioris de Jeffreys s\u00e3o usualmente impr\u00f3prias. No caso multidimensional, \\pi^*(\\theta) \\propto [det(I(\\theta))]^{1/2} \u00e9 a priori de Jeffreys. Todavia, usar esse procedimento para construir prioris leva a paradoxos e incoer\u00eancias. Observa\u00e7\u00e3o: A priori de Jeffreys para fam\u00edlia de loca\u00e7\u00e3o \u00e9 constante. \ud83d\udcdd Exemplo (caso normal) Suponha que x \\sim N(\\mu, \\sigma^2) com \\theta = (\\mu, \\sigma^2) desconhecido. Nesse caso, podemos calcular que I(\\theta) = \\begin{bmatrix} 1/\\sigma^2 & 0 \\\\ 0 & 2/\\sigma^2 \\end{bmatrix}, e, portanto, \\pi(\\theta) \\propto 1/\\sigma^2 . Se assumirmos que \\mu e \\sigma s\u00e3o independentes a priori, todavia, teremos que \\pi(\\mu, \\sigma) = 1/\\sigma como priori n\u00e3o informativa e tamb\u00e9m a medida de Haar invariante ao modelo loca\u00e7\u00e3o-scala. Essa priori, todavia, n\u00e3o obedece ao Princ\u00edpio da Verossimilhan\u00e7a, dado que a informa\u00e7\u00e3o de Fisher de dois modelos diferentes podem ser diferentes, mesmo que as verossimilhan\u00e7as sejam proporcionais. Confira essa resposta. . Prioris de refer\u00eancia [Bernardo; 1979] prop\u00f4s um procedimento para construir prioris n\u00e3o informativas, ou como ele destaca, um procedimento para obter prioris de forma que a posteriori aproxime aquela que adviria de uma informa\u00e7\u00e3o a priori vaga. Seja x \\sim f(x | \\theta) e \\theta = (\\theta_1, \\theta_2) , em que \\theta_1 \u00e9 o par\u00e2metro de interesse. A priori de refer\u00eancia primeiro define \\pi(\\theta_2 | \\theta_1) usando a priori de Jeffreys atrav\u00e9s f(x|\\theta) quando \\theta_1 \u00e9 fixado. Depois, ele calcula \\tilde{f}(x|\\theta_1) = \\int f(x|\\theta) \\pi(\\theta_2|\\theta_1) \\, d\\theta_2 e calcula a priori de Jeffreys para \\theta_1 baseando-se em \\tilde{f} . Assim, o processo elimina os par\u00e2metros que n\u00e3o s\u00e3o de interesse. O algoritmo se generaliza para mais camadas de par\u00e2metros. Prioris matching Uma forma, no m\u00ednimo curiosa, de construir prioris n\u00e3o informativas \u00e9 baseada em propriedades frequentistas, isto \u00e9, em que propriedades funcionem em m\u00e9dia considerando x . Seja C_x um conjunto de confian\u00e7a a posteriori n\u00edvel \\alpha , isto \u00e9, p(g(\\theta) \\in C_x | x) = 1-\\alpha. Esse conjunto define tem cobertura frequentista \\Pr_{\\theta}(g(\\theta) \\in C_x) , em que nesse caso C_x \u00e9 a vari\u00e1vel aleat\u00f3ria. Em geral, quando C_x = (-\\infty, k_{\\alpha}(x)) , ent\u00e3o \\Pr_{\\theta}(\\theta \\le k_{\\alpha}(x)) = 1 - \\alpha + O(n^{-1/2}) . No caso da priori de Jeffreys, isso se torna O(n^{-1}) que decresce mais rapidamente. Para mais detalhes, consulte esse link . Pontua\u00e7\u00f5es finais Informa\u00e7\u00e3o a priori n\u00e3o consegue ser traduzida em uma distribui\u00e7\u00e3o de probabilidade \u00fanica. Al\u00e9m disso, ela \u00e9 bastante complicada, principalmente em se tratando de definir caudas de forma subjetiva. Se alguma informa\u00e7\u00e3o \u00e9 dispon\u00edvel, us\u00e1-la favorece infer\u00eancias quando comparado a abordagens n\u00e3o informativas. An\u00e1lise de sensibilidade \u00e9 um t\u00f3pico importante, que verifica a influ\u00eancia da escolha da priori nas infer\u00eancias. Para essa an\u00e1lise, assumimos que a priori \\pi mora em alguma classe de distribui\u00e7\u00f5es, que mensura a incerteza sobre \\pi . Essas classes podem ser: (i) prioris conjugadas: convenientes matematicamente; (ii) momentos definidos: a classe de distribui\u00e7\u00f5es que tem determinados momentos limitados imp\u00f5e condi\u00e7\u00f5es fortes sobre a cauda e inclui muitas distribui\u00e7\u00f5es imposs\u00edveis; (iii) classes de vizinhan\u00e7a: pondera por um \\epsilon uma outra classe de distribui\u00e7\u00f5es Q a ser escolhida; entre outras.","title":"Distribui\u00e7\u00f5es a priori"},{"location":"bayesian/priors/#distribuicoes-a-priori","text":"A determina\u00e7\u00e3o da distribui\u00e7\u00e3o a priori para a quantidade de interesse \u00e9 um ponto-chave da infer\u00eancia bayesiana e, simultaneamente, \u00e9 alvo de cr\u00edticas. De forma geral, queremos codificar a informa\u00e7\u00e3o sobre um par\u00e2metro \\theta antes de realizarmos um determinado experimento. Dessa forma, a partir de uma informa\u00e7\u00e3o a priori, queremos definir uma distribui\u00e7\u00e3o a priori. \u00c9 claro que raramente conseguimos fazer esse processo de forma exata e \u00fanica. Quando a informa\u00e7\u00e3o \u00e9 insuficiente para definir uma distribui\u00e7\u00e3o de probabilidade, o estat\u00edstico deve colocar informa\u00e7\u00e3o subjetiva para, ent\u00e3o, obter uma priori que fa\u00e7a sentido. Como a distribui\u00e7\u00e3o a priori influencia as infer\u00eancias a posteriori o tanto quanto se queira (no limite, podemos colocar a massa de probabilidade em um \u00fanico ponto), an\u00e1lises de robustez e de sensibilidade s\u00e3o essenciais no dia-a-dia bayesiano.","title":"Distribui\u00e7\u00f5es a priori"},{"location":"bayesian/priors/#determinacao-de-uma-priori","text":"Queremos que a priori \\pi resuma a informa\u00e7\u00e3o dispon\u00edvel sobre o fen\u00f4meno e a incerteza que temos sobre essa informa\u00e7\u00e3o, mesmo quando \\theta n\u00e3o adv\u00e9m de um processo aleat\u00f3rio. Usando o conceito de George Box de que todos os modelos s\u00e3o errados , n\u00e3o existe uma priori verdadeira e, portanto, buscamos aproxima\u00e7\u00f5es para representar \\pi . \ud83d\udcdd Exemplo (Distribui\u00e7\u00e3o normal) Suponha que observamos x_1, \\dots, x_n \\sim N(\\theta, 1) e assumimos que \\theta \\sim N(\\mu, \\tau) . A m\u00e9dia a posteriori de \\theta \u00e9 dada por \\mathbb{E}[\\theta|x] = \\frac{\\bar{x}\\tau + \\mu/n}{\\tau + 1/n}. Nesse caso, note que estamos fazendo uma m\u00e9dia ponderada de \\bar{x} e \\mu com pesos \\tau e 1/n , respectivamente. Dessa forma, poder\u00edamos pensar que a priori, ter\u00edamos virtualmente uma amostra de tamanho \\tau^{-1} e m\u00e9dia \\mu . Podemos construir uma medida de probabilidade para \\theta atrav\u00e9s de uma rela\u00e7\u00e3o \\preceq que ordena o qu\u00e3o prov\u00e1vel \u00e9 um evento definido pela vari\u00e1vel aleat\u00f3ria \\theta , como resumido aqui . Quando n\u00e3o existe uma informa\u00e7\u00e3o direta sobre \\theta , uma alternativa \u00e9 usar a distribui\u00e7\u00e3o marginal de x dada por m(x) = \\int_{\\Theta} f(x|\\theta) \\pi(\\theta) \\, d\\theta. Por exemplo, se \\theta \u00e9 a produ\u00e7\u00e3o di\u00e1ria m\u00e9dia de leite, informa\u00e7\u00e3o sobre \\theta pode ser obtida a partir da informa\u00e7\u00e3o que j\u00e1 temos sobre o rebanho, o que \u00e9 informa\u00e7\u00e3o sobre a marginal de x .","title":"Determina\u00e7\u00e3o de uma priori"},{"location":"bayesian/priors/#prioris-de-entropia-maxima","text":"Assuma que temos \\mathbb{E}^{\\pi}[g_k(\\theta)] = \\omega_k para k=1,\\dots, K . Podemos selecionar \\pi que satisfa\u00e7a essas K rela\u00e7\u00f5es e maximize a entropia, que mede o n\u00edvel de desordem em um sistema. Se \\Theta \u00e9 finito, temos que \\mathcal{E}(\\pi) = \\sum_{\\theta_i \\in \\Theta} \\pi(\\theta_i) \\log(\\pi(\\theta_i)) \u00e9 a entropia de \\pi . Essa quantidade \u00e9 uma medida de incerteza dada por \\pi . Dessa forma, estamos minimizando a informa\u00e7\u00e3o de \\pi trazida a priori. Sendo \\lambda_k o multiplicador de Lagrange associado a \\mathbb{E}^{\\pi}[g_k(\\theta)] = \\omega_k , temos que a defini\u00e7\u00e3o da priori \u00e9 \\pi^*(\\theta_i) \\propto \\exp\\left\\{\\sum_{k=1}^K \\lambda_k g_k(\\theta_i)\\right\\}. No caso cont\u00ednuo, as contas se complicam um pouco e precisamos de uma medida de refer\u00eancia \\pi_0 , que pode ser vista como a distribui\u00e7\u00e3o a priori sem restri\u00e7\u00e3o de momentos. Quando existe uma estrutura de grupo, a medida de Haar invariante \u00e0 direita \u00e9 a escolha natural para \\pi_0 . Nesse caso, a entropia \u00e9 definida como \\mathcal{E}(\\pi) = \\int \\log\\left(\\frac{\\pi(\\theta)}{\\pi_0(\\theta)}\\right) \\, \\pi_0(d\\theta), que \u00e9 a dist\u00e2ncia de Kullback-Leibler entre \\pi e \\pi_0 e a distribui\u00e7\u00e3o a priori que maximiza \\mathcal{E} \u00e9 \\pi^*(\\theta) \\propto \\exp\\left\\{\\sum_{k=1}^K \\lambda_k g_k(\\theta_i)\\right\\}\\pi_0(\\theta)","title":"Prioris de entropia m\u00e1xima"},{"location":"bayesian/priors/#aproximacoes-parametricas","text":"A forma mais utilizada \u00e9 provavelmente essa. Definimos uma fam\u00edlia param\u00e9trica para a distribui\u00e7\u00e3o a priori e buscamos definir os par\u00e2metros atrav\u00e9s dos momentos ou quartis da distribui\u00e7\u00e3o. A base \u00e9 mais a tratabilidade matem\u00e1tica do que a subjetividade. Outro ponto \u00e9 que distribui\u00e7\u00f5es com caudas muito diferentes para \\Theta infinito levam a infer\u00eancias bastante distintas.","title":"Aproxima\u00e7\u00f5es param\u00e9tricas"},{"location":"bayesian/priors/#prioris-conjugadas","text":"Prioris conjugadas s\u00e3o baseadas na verossimilhan\u00e7a e, portanto, j\u00e1 definem a fam\u00edlia param\u00e9trica da distribui\u00e7\u00e3o a priori, restando a defini\u00e7\u00e3o dos par\u00e2metros. Isso limita a informa\u00e7\u00e3o a priori que deve ser obtida a fim de definir uma distribui\u00e7\u00e3o de probabilidade. Ela tamb\u00e9m auxilia na computa\u00e7\u00e3o, como veremos na defini\u00e7\u00e3o: Fam\u00edlia conjugada: Uma fam\u00edlia de distribui\u00e7\u00f5es \\mathcal{F} sobre \\Theta \u00e9 conjugada para a verossimilhan\u00e7a f(x|\\theta) se para toda priori \\pi \\in \\mathcal{F} , a posteriori p(\\cdot \\mid x) \\in \\mathcal{F} . Essa fam\u00edlia se torna interessante quando ela \u00e9 a menor poss\u00edvel (\u00e9 imposs\u00edvel encontrar uma m\u00ednima propriamente dita, mas a ideia \u00e9 que ela tenha dimens\u00e3o de par\u00e2metros baixa) e parametrizada. Logo, atualizar a informa\u00e7\u00e3o por f(x|\\theta) \u00e9 equivalente a atualizar os par\u00e2metros da distribui\u00e7\u00e3o. Nas fam\u00edlias conjugadas, podemos interpretar os hiperpar\u00e2metros como observa\u00e7\u00f5es passadas virtuais, o que \u00e9 considerado um ponto positivo.","title":"Prioris conjugadas"},{"location":"bayesian/priors/#familia-exponencial-e-distribuicoes-conjugadas","text":"Seja \\mu uma medida \\sigma -finita em \\mathcal{X} e \\Theta o espa\u00e7o dos par\u00e2metros. Sejam C : \\mathcal{X} \\to \\mathbb{R}_+ , h : \\mathcal{X} \\to \\mathbb{R}_+ , R : \\Theta \\to \\mathbb{R}^k e T : \\mathcal{X} \\to \\mathbb{R}^k fun\u00e7\u00f5es. A fam\u00edlia de distribui\u00e7\u00f5es com densidade com respeito a \\mu f(x|\\theta) = C(\\theta) h(x) \\exp\\{R(\\theta) \\cdot T(x)\\} \u00e9 chamada de fam\u00edlia exponencial de dimens\u00e3o k . Quando R(\\theta) = \\theta e T(x) = x , a fam\u00edlia \u00e9 dita natural . Lema de Pitman\u2013Koopman : Se uma fam\u00edlia de distribui\u00e7\u00f5es f(\\cdot | \\theta) \u00e9 tal que para n suficientemente grande, existe uma estat\u00edstica suficiente de dimens\u00e3o constante, ela \u00e9 exponencial se o suporte n\u00e3o depende de \\theta (essa condi\u00e7\u00e3o final exclui a distribui\u00e7\u00e3o U[-\\theta, \\theta] , por exemplo). Al\u00e9m disso, para toda amostra de f , existe uma estat\u00edstica suficiente de dimens\u00e3o constante. O espa\u00e7o natural de par\u00e2metros \u00e9 denotado por N = \\left\\{\\theta \\in \\Theta \\mid \\int_{\\mathcal{X}} e^{\\theta\\cdot x} h(x) \\, d\\mu(x) < +\\infty \\right\\}. Ela \u00e9 regular se N \u00e9 aberto e m\u00ednimo se dim(N) = dim(C(\\mu)) = K em que C(\\mu) \u00e9 o menor conjunto convexo fechado que cont\u00e9m o suporte de \\mu . Reescreva a densidade como f(x|\\theta) = h(x) e^{\\theta \\cdot x - \\psi(\\theta)}, em que \\psi \u00e9 a fun\u00e7\u00e3o geradora cumulativa , pois \\mathbb{E}_{\\theta}[X] = \\nabla \\psi(\\theta), \\operatorname{Cov}(X_i, X_j) = \\psi_{\\theta_i \\theta_j}(\\theta), supondo \\psi de classe C^2 e \\theta \\in int(N) . Priori conjugada: Uma fam\u00edlia conjugada para f \u00e9 dada pela densidade \\pi(\\theta | \\mu, \\lambda) = K(\\mu, \\lambda) e^{\\theta\\cdot \\mu - \\lambda \\psi(\\theta)}, cuja posteriori \u00e9 dada por \\pi(\\theta | \\mu + x, \\lambda + 1) . Se \\lambda > 0 e \\mu/\\lambda pertence ao interior de C(\\mu) , \\pi define uma distribui\u00e7\u00e3o de probabilidade [Diaconis, Ylvisaker; 1978] . Para uma tabela completa, consulte aqui . Outro resultado verificado no artigo acima \u00e9 que a esperan\u00e7a a posteriori de \\theta \u00e9 dada por \\frac{\\mu + n\\bar{x}}{\\lambda + n}, quando \\mu \\in \\mathcal{X} .","title":"Fam\u00edlia exponencial e distribui\u00e7\u00f5es conjugadas"},{"location":"bayesian/priors/#extensoes","text":"Considere o seguinte exemplo \ud83d\udcdd Exemplo [Diaconis, Ylvisaker; 1985] Quando a moeda \u00e9 girada pela borda e observamos o resultado quando ela cai, temos um vi\u00e9s maior para um lado do que para o outro devido a irregularidades. Seja x \\sim Binomial(n, p) o n\u00famero de caras nesse experimento ap\u00f3s n jogadas. Como sabemos que existe uma irregularidade, poder\u00edamos pensar em uma priori para p que tivesse uma cara bimodal, dado peso para as duas possibilidades. Isso n\u00e3o \u00e9 poss\u00edvel com a fam\u00edlia conjugada beta. Uma forma de fazer isso \u00e9, portanto, atrav\u00e9s de misturas de distribui\u00e7\u00f5es beta. Com esse exemplo, podemos ver que misturas de distribui\u00e7\u00f5es conjugadas definem uma fam\u00edlia conjugada maior que d\u00e1 maior flexibilidade para o formato de uma distribui\u00e7\u00e3o a priori. Al\u00e9m disso, podemos verificar que a mistura de distribui\u00e7\u00f5es pode aproximar qualquer distribui\u00e7\u00e3o a priori sob a dist\u00e2ncia de Prokhorov .","title":"Extens\u00f5es"},{"location":"bayesian/priors/#distribuicoes-a-priori-nao-informativas","text":"Quando pouca (ou nenhuma) informa\u00e7\u00e3o sobre \\theta est\u00e1 dispon\u00edvel, \u00e9 dif\u00edcil justificar a escolha com base subjetiva. Nesse caso, uma alternativa \u00e9 usar a distribui\u00e7\u00e3o dos dados para, a partir dela, definir uma distribui\u00e7\u00e3o a priori. Chamamos essas prioris de n\u00e3o informativas . Mas devemos que lembrar que uma distribui\u00e7\u00e3o ser n\u00e3o informativa n\u00e3o significa que ignor\u00e2ncia total est\u00e1 sendo representada probabilisticamente. Elas podem ser usadas como prioris de refer\u00eancia, todavia.","title":"Distribui\u00e7\u00f5es a priori n\u00e3o informativas"},{"location":"bayesian/priors/#priori-de-laplace","text":"Laplace sugeriu construir a distribui\u00e7\u00e3o a priori baseado no Princ\u00edpio da Raz\u00e3o Insuficiente , em que eventos elementares s\u00e3o equiprov\u00e1veis. Nesse caso, adotamos a priori uniforme. Se o espa\u00e7o de par\u00e2metros n\u00e3o for compacto, isso nos leva \u00e0 uma distribui\u00e7\u00e3o impr\u00f3pria, por consequ\u00eancia. Al\u00e9m disso, se fizermos uma transforma\u00e7\u00e3o biun\u00edvoca \\eta = g(\\theta) , a priori para \\eta n\u00e3o ser\u00e1 uniforme pela F\u00f3rmula da Mudan\u00e7a de Vari\u00e1veis, isto \u00e9, a informa\u00e7\u00e3o sobre \\theta n\u00e3o foi criada a partir dessa transforma\u00e7\u00e3o, mas a distribui\u00e7\u00e3o n\u00e3o \u00e9 uniforme.","title":"Priori de Laplace"},{"location":"bayesian/priors/#prioris-invariantes","text":"O conceito de invari\u00e2ncia \u00e9 bem profundo na matem\u00e1tica e, com base nesse conceito, podemos construir distribui\u00e7\u00f5es invariantes a reparametriza\u00e7\u00e3o. Por exemplo, a fam\u00edlia f(x - \\theta) \u00e9 invariante \u00e0 transla\u00e7\u00e3o, isto \u00e9, y = x - x_0 tem distribui\u00e7\u00e3o da mesma fam\u00edlia para todo x_0 , f(x - (\\theta - x_0)) . Chamamos \\theta de par\u00e2metro de loca\u00e7\u00e3o .","title":"Prioris invariantes"},{"location":"bayesian/priors/#priori-de-jeffreys","text":"A Priori de Jeffreys \u00e9 baseada na Informa\u00e7\u00e3o de Fisher dada pela express\u00e3o: I(\\theta) = \\mathbb{E}_{\\theta}\\left[\\left(\\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta}\\right)^2\\right], no caso unidimensional. A distribui\u00e7\u00e3o de Jeffreys \u00e9 definida por \\pi^*(\\theta) \\propto I^{1/2}(\\theta). Note que I(\\theta) = I(h(\\theta))(h'(\\theta))^2 se h \u00e9 uma transforma\u00e7\u00e3o biun\u00edvoca. Nesse caso, \\pi^*(h(\\theta)) \\propto I^{1/2}(h(\\theta)) = I^{1/2}(\\theta)/|h'(\\theta)| , exatamente o que esper\u00e1vamos usando a mudan\u00e7a de vari\u00e1veis. Logo, essa priori \u00e9 invariante \u00e0 reparametriza\u00e7\u00e3o. Al\u00e9m disso, I(\\theta) \u00e9 uma medida da quantidade de informa\u00e7\u00e3o trazida pelo modelo sobre \\theta . Na pr\u00e1tica, prioris de Jeffreys s\u00e3o usualmente impr\u00f3prias. No caso multidimensional, \\pi^*(\\theta) \\propto [det(I(\\theta))]^{1/2} \u00e9 a priori de Jeffreys. Todavia, usar esse procedimento para construir prioris leva a paradoxos e incoer\u00eancias. Observa\u00e7\u00e3o: A priori de Jeffreys para fam\u00edlia de loca\u00e7\u00e3o \u00e9 constante. \ud83d\udcdd Exemplo (caso normal) Suponha que x \\sim N(\\mu, \\sigma^2) com \\theta = (\\mu, \\sigma^2) desconhecido. Nesse caso, podemos calcular que I(\\theta) = \\begin{bmatrix} 1/\\sigma^2 & 0 \\\\ 0 & 2/\\sigma^2 \\end{bmatrix}, e, portanto, \\pi(\\theta) \\propto 1/\\sigma^2 . Se assumirmos que \\mu e \\sigma s\u00e3o independentes a priori, todavia, teremos que \\pi(\\mu, \\sigma) = 1/\\sigma como priori n\u00e3o informativa e tamb\u00e9m a medida de Haar invariante ao modelo loca\u00e7\u00e3o-scala. Essa priori, todavia, n\u00e3o obedece ao Princ\u00edpio da Verossimilhan\u00e7a, dado que a informa\u00e7\u00e3o de Fisher de dois modelos diferentes podem ser diferentes, mesmo que as verossimilhan\u00e7as sejam proporcionais. Confira essa resposta. .","title":"Priori de Jeffreys"},{"location":"bayesian/priors/#prioris-de-referencia","text":"[Bernardo; 1979] prop\u00f4s um procedimento para construir prioris n\u00e3o informativas, ou como ele destaca, um procedimento para obter prioris de forma que a posteriori aproxime aquela que adviria de uma informa\u00e7\u00e3o a priori vaga. Seja x \\sim f(x | \\theta) e \\theta = (\\theta_1, \\theta_2) , em que \\theta_1 \u00e9 o par\u00e2metro de interesse. A priori de refer\u00eancia primeiro define \\pi(\\theta_2 | \\theta_1) usando a priori de Jeffreys atrav\u00e9s f(x|\\theta) quando \\theta_1 \u00e9 fixado. Depois, ele calcula \\tilde{f}(x|\\theta_1) = \\int f(x|\\theta) \\pi(\\theta_2|\\theta_1) \\, d\\theta_2 e calcula a priori de Jeffreys para \\theta_1 baseando-se em \\tilde{f} . Assim, o processo elimina os par\u00e2metros que n\u00e3o s\u00e3o de interesse. O algoritmo se generaliza para mais camadas de par\u00e2metros.","title":"Prioris de refer\u00eancia"},{"location":"bayesian/priors/#prioris-matching","text":"Uma forma, no m\u00ednimo curiosa, de construir prioris n\u00e3o informativas \u00e9 baseada em propriedades frequentistas, isto \u00e9, em que propriedades funcionem em m\u00e9dia considerando x . Seja C_x um conjunto de confian\u00e7a a posteriori n\u00edvel \\alpha , isto \u00e9, p(g(\\theta) \\in C_x | x) = 1-\\alpha. Esse conjunto define tem cobertura frequentista \\Pr_{\\theta}(g(\\theta) \\in C_x) , em que nesse caso C_x \u00e9 a vari\u00e1vel aleat\u00f3ria. Em geral, quando C_x = (-\\infty, k_{\\alpha}(x)) , ent\u00e3o \\Pr_{\\theta}(\\theta \\le k_{\\alpha}(x)) = 1 - \\alpha + O(n^{-1/2}) . No caso da priori de Jeffreys, isso se torna O(n^{-1}) que decresce mais rapidamente. Para mais detalhes, consulte esse link .","title":"Prioris matching"},{"location":"bayesian/priors/#pontuacoes-finais","text":"Informa\u00e7\u00e3o a priori n\u00e3o consegue ser traduzida em uma distribui\u00e7\u00e3o de probabilidade \u00fanica. Al\u00e9m disso, ela \u00e9 bastante complicada, principalmente em se tratando de definir caudas de forma subjetiva. Se alguma informa\u00e7\u00e3o \u00e9 dispon\u00edvel, us\u00e1-la favorece infer\u00eancias quando comparado a abordagens n\u00e3o informativas. An\u00e1lise de sensibilidade \u00e9 um t\u00f3pico importante, que verifica a influ\u00eancia da escolha da priori nas infer\u00eancias. Para essa an\u00e1lise, assumimos que a priori \\pi mora em alguma classe de distribui\u00e7\u00f5es, que mensura a incerteza sobre \\pi . Essas classes podem ser: (i) prioris conjugadas: convenientes matematicamente; (ii) momentos definidos: a classe de distribui\u00e7\u00f5es que tem determinados momentos limitados imp\u00f5e condi\u00e7\u00f5es fortes sobre a cauda e inclui muitas distribui\u00e7\u00f5es imposs\u00edveis; (iii) classes de vizinhan\u00e7a: pondera por um \\epsilon uma outra classe de distribui\u00e7\u00f5es Q a ser escolhida; entre outras.","title":"Pontua\u00e7\u00f5es finais"},{"location":"bayesian/tests-regions-confidence/","text":"Testes e regi\u00f5es de confian\u00e7a Com a abordagem bayesiana, podemos fazer afirma\u00e7\u00f5es do tipo \\pi(\\theta \\in \\Theta_0 | x) , o que \u00e9 difere da proposta frequentista, j\u00e1 que l\u00e1 o par\u00e2metro \u00e9 fixo, apesar de desconhecido. Alguns bayesianos acreditam que o teste, em especial o teste com hip\u00f3tese nula pontual, n\u00e3o deveria ser realizado por diversos motivos: representa\u00e7\u00e3o redutiva do modelo; priori modificada de forma n\u00e3o natural para representar a quest\u00e3o pontual; falta de estrutura baseada em teoria da decis\u00e3o; impossibilidade do uso de prioris impr\u00f3prias. Uma primeira abordagem Considere um modelo f(x|\\theta) com \\theta \\in \\Theta . Queremos verificar H_0 : \\theta \\in \\Theta_0 com \\Theta_0 \\subseteq \\Theta sendo um subconjunto de interesse. Chamamos H_0 de hip\u00f3tese nula . Em contrapartida, temos a hip\u00f3tese alternativa H_1 : \\theta \\in \\Theta_1 com \\Theta_0 \\cap \\Theta_1 = \\emptyset e \\theta pertencendo a um dos dois conjuntos. A perda proposta por Neyman e Pearson \u00e9 L(\\theta, \\varphi) = \\begin{cases} 1 &\\text{se } \\varphi \\neq 1_{\\Theta_0}(\\theta), \\\\ 0 &\\text{c.c.} \\end{cases} Para essa perda, o estimador de Bayes \u00e9 \\varphi^{\\pi}(x) = \\begin{cases} 1 &\\text{se } \\Pr(\\theta \\in \\Theta_0 | x) > \\Pr(\\theta \\in \\Theta_1 | x), \\\\ 0 &\\text{c.c.} \\end{cases} Podemos generalizar essa perda para L(\\theta, \\varphi) = \\begin{cases} 0 &\\text{se } \\varphi = 1_{\\Theta_0}(\\theta), \\\\ a_0 &\\text{se } \\theta \\in \\Theta_0 \\text{ e } \\varphi = 0, \\\\ a_1 &\\text{se } \\theta \\in \\Theta_1 \\text{ e } \\varphi = 1, \\end{cases} cujo estimador de Bayes \u00e9 \\varphi^{\\pi}(x) = 1 se \\Pr(\\theta \\in \\Theta_0 | x) > a_1/(a_0 + a_1) , sendo essa fra\u00e7\u00e3o o n\u00edvel de aceita\u00e7\u00e3o . Fator de Bayes Definimos o fator de Bayes como a raz\u00e3o das raz\u00f5es da posteriori e da priori: B_{01}^{\\pi}(x) = \\frac{\\Pr(\\theta \\in \\Theta_0 | x)}{\\Pr(\\theta \\in \\Theta_1 | x)}\\Big /\\frac{\\Pr(\\theta \\in \\Theta_0)}{\\Pr(\\theta \\in \\Theta_1)} = \\frac{\\int_{\\Theta_0} f(x|\\theta)\\pi_0(\\theta) \\, d\\theta}{\\int_{\\Theta_1} f(x|\\theta)\\pi_1(\\theta) \\, d\\theta}, que avalia a modifica\u00e7\u00e3o das chances de \\Theta_0 contra \\Theta_1 . Observe que \\pi_i \u00e9 a distribui\u00e7\u00e3o a priori sob H_i . Jeffreys desenvolveu uma escala (fora da uma configura\u00e7\u00e3o baseada em teoria da decis\u00e3o) para avaliar a evid\u00eancia trazida por x contra H_0 , considerando o valor \\log_{10}(B_{10}^{\\pi}) : 0 < \\text{ pobre } < 0.5 < \\text{ substancial } < 1 < \\text{ forte } < 2 < \\text{ decisiva } < +\\infty. \ud83d\udcdd Exemplo (Basquete) Um jogador de basquete tem \"m\u00e3o quente\" quando tem bons e maus dias, ao inv\u00e9s de uma probabilidade fixa de vencer em um lan\u00e7amento. O modelo base \u00e9 H_0: y_i \\sim B(n_i, p) para cada jogo i=1,\\dots, G ( p \u00e9 fixo). A Alternativa \u00e9 que p varia com o dia. Considere a priori p_i \\sim Beta(\\xi/\\omega, (1-\\xi)/\\omega) , de forma que \\xi = \\mathbb{E}[p_i|\\xi] \\sim Unif(0,1) . Isto \u00e9, estamos fixando as m\u00e9dias das jogadas dos dias e afirmando que alguns dias teremos p_i > \\xi e outros p_i < \\xi , \u00e9 claro com alguma probabilidade. Al\u00e9m do mais, consideramos p \\sim Unif(0,1) . O fator de Bayes \u00e9 B_{10} = \\frac{\\int_0^1 \\left\\{\\prod_{i=1}^G \\int_0^1 p_i^{y_i}(1-p_i)^{n_i-y_i}p_i^{\\alpha-1}(1-p_i)^{\\beta-1} \\, dp_i \\right\\}\\left(\\Gamma(1/\\omega)/\\Gamma(\\xi/\\omega)\\Gamma((1-\\xi)/\\omega)\\right)^G \\, d\\xi}{\\int_0^1 p^{\\sum y_i}(1-p)^{\\sum (n_i-y_i)} \\, dp} No paper Kass, Raftery (1995) , verificou-se que para \\omega = 0.005 , temos B_{10} = 0.16 , que d\u00e1 baixa evid\u00eancia a favor de H_1 , isto \u00e9, que existem bons e maus dias. Escolhemos \\omega olhando para Var(p_i | \\xi) = \\xi(1-\\xi)/(1+\\omega^{-1}) \\implies \\omega^{-1} = \\xi(1-\\xi)/Var(p_i) - 1. Note que se H_0 for pontual, n\u00e3o conseguimos usar o Fator de Bayes. Para esses casos, precisamos definir \\pi(\\theta) = \\Pr(\\theta \\in \\Theta_0) \\pi_{0}(\\theta) + \\Pr(\\theta \\in \\Theta_1) \\pi_{1}(\\theta) = \\varrho_0 \\pi_0(\\theta) + \\varrho_1 \\pi_1(\\theta). Apesar de hip\u00f3teses pontuais serem problem\u00e1ticas, elas t\u00eam muita utilidade na pr\u00e1tica. Assim, teremos a priori \\pi(\\theta) = \\varrho_0 1_{\\theta = \\theta_0} + (1-\\varrho_0)g_1(\\theta) com posteriori \\pi(\\Theta_0 | x) = \\frac{f(x|\\theta_0) \\varrho_0}{f(x|\\theta_0)\\varrho_0 + (1-\\varrho_0)m_1(x))}, em que m_1 \u00e9 a marginal dos dados sob H_1 . Em particular, o Fator de Bayes \u00e9 B_{01}^{\\pi}(x) = \\frac{f(x|\\theta_0)}{m_1(x)}, levando \u00e0 rela\u00e7\u00e3o \\pi(\\Theta_0 | x) = \\left[1 + \\frac{1-\\varrho_0}{\\varrho_0} \\frac{1}{B_{01}(x)}\\right]^{-1}. \ud83d\udcdd Exemplo (Distribui\u00e7\u00e3o normal) Considere x \\sim \\operatorname{Normal}(\\theta, 1) e H_0: \\theta = 0 . Se usarmos a priori impr\u00f3pria \\pi(\\theta) = \\frac{1}{2}1_{\\theta = 0} + \\frac{1}{2} , teremos que a posteriori de H_0 ser\u00e1 \\pi(\\theta = 0 | x) = \\frac{e^{-x^2/2}}{e^{-x^2/2} + \\int e^{-(x-\\theta)^2/2} \\, d\\theta} = \\frac{1}{1 + \\sqrt{2\\pi}e^{x^2/2}} \\le 0.285, isto \u00e9, a posteriori \u00e9 limitada superiormente por um valor baixo. Fen\u00f4meno parecido acontece quando \\Theta_0 \u00e9 um conjunto compacto. Uma quest\u00e3o interessante \u00e9 que para os n\u00edveis tradicionais, que ocorrem quando x = 1.65, 1.96 e 2.58 (respectivamente n\u00edveis 0.1, 0.05, 0.01 ), a posteriori de H_0 \u00e9 pr\u00f3xima a esses valores (quando a priori \u00e9 a medida de Lebesgue). Em particular, quando H_0 : \\theta \\le 0 para \\pi(\\theta) = 1 , temos que \\pi(\\theta \\le 0 | x) = \\Phi(-x), que \u00e9 o p-valor desse mesmo teste. Compara\u00e7\u00e3o com a abordagem cl\u00e1ssica A abordagem cl\u00e1ssica da teoria de testagem \u00e9 de Neyman-Pearson. Com isso, consideramos a seguinte defini\u00e7\u00e3o: Poder: O poder de um procedimento de teste \\varphi \u00e9 a probabilidade de rejeitar H_0 sob a hip\u00f3tese alternativa, isto \u00e9, \\beta(\\theta) = 1 - \\mathbb{E}_{\\theta}[\\varphi(x)] quando \\theta \\in \\Theta_1 . A quantidade 1- \\beta(\\theta) \u00e9 o erro do tipo II , enquanto o erro do tipo I \u00e9 1-\\mathbb{E}_{\\theta}[\\varphi(x)] quando \\theta \\in \\Theta_0 . Em resumo, \\theta \\in \\Theta_0 \\implies 1-\\mathbb{E}_{\\theta}[\\varphi(x)] \u00e9 erro do tipo I. \\theta \\in \\Theta_1 \\implies \\mathbb{E}_{\\theta}[\\varphi(x)] \u00e9 erro do tipo II. Testes UMP Testes frequentistas buscam minimizar o risco frequentista \\mathbb{E}_{\\theta}[L(\\theta, \\varphi(x))] sob H_1 . Em particular, minimiza-se na classe de procesdimentos C_{\\alpha} , em que \\alpha \\in (0,1) e C_{\\alpha} = \\{\\varphi : \\sup_{\\theta \\in \\Theta_0} \\mathbb{E}_{\\theta}[L(\\theta, \\varphi(x))] = \\sup_{\\theta \\in \\Theta_0} \\Pr_{\\theta}(\\varphi(x) = 0) \\le \\alpha \\}. Um teste \\varphi \u00e9 dito Uniformemente Mais Poderoso (UMP) a n\u00edvel \\alpha se \\varphi \\in C_{\\alpha} e se ele minimiza o risco frequentista uniformemente em \\Theta_1 em C_{\\alpha} . Esse m\u00e9todo \u00e9 desbalanceado com respeito \u00e0s hip\u00f3teses, porque o erro do tipo II pode ser muito grande, mesmo quando \\Theta_0 \u00e9 uma transforma\u00e7\u00e3o cont\u00ednua de \\Theta_1 . Proposi\u00e7\u00e3o: Seja f(x|\\theta) uma distribui\u00e7\u00e3o que possua raz\u00e3o de verossimilhan\u00e7a crescente em T(x) , isto \u00e9, para \\theta_1 > \\theta_2 , a fun\u00e7\u00e3o f(x|\\theta_1) / f(x|\\theta_2) cresce com T(x) . Se queremos testar H_0 : \\theta \\le \\theta_0 , existe um teste UMP da forma \\varphi(x) = \\begin{cases} 1 &\\text{se } T(x) < c, \\\\ \\gamma &\\text{se } T(x) = c, \\\\ 0 &\\text{c.c.}, \\end{cases} de forma que \\mathbb{E}_{\\theta_0}[\\varphi(x)] = \\alpha . \ud83d\udcdd Exemplo (Fam\u00edlia exponencial) Seja f(x|\\theta) da fam\u00edlia exponencial, isto \u00e9, \\log \\frac{f(x|\\theta_1)}{f(x|\\theta_2)} = \\theta_1\\cdot x - \\psi(\\theta_1) - \\theta_2 \\cdot x + \\psi(\\theta_2) = (\\theta_1 - \\theta_2)\\cdot x - (\\psi(\\theta_1) - \\psi(\\theta_2)), que \u00e9 crescente em T(x) = x . Podemos construir uma proposi\u00e7\u00e3o no mesmo sentido da anterior, s\u00f3 que quando H_0 \u00e9 do tipo \\theta \\not\\in (\\theta_1, \\theta_2) e a densidade do dado pertence \u00e0 fam\u00edlia exponencial. Nesse caso o teste UMP \u00e9 da forma \\varphi(x) = 1 se T(x) \\not \\in [c_1, c_2] e \\varphi(x) = \\gamma_i se T(x) = c_i , em que c_i e \\gamma_i s\u00e3o escolhidos a partir da condi\u00e7\u00e3o de que \\mathbb{E}_{\\theta_i}[\\varphi(x)] = \\alpha . No caso em que o papel de H_0 \u00e9 trocado com o de H_1 , n\u00e3o existe teste UMP. Nessas situa\u00e7\u00f5es, podemos restringir a classe de testes para os n\u00e3o enviesados, em que \\sup_{\\Theta_0} \\Pr_{\\theta}(\\varphi(x) = 0) \\le \\inf_{\\Theta_1} \\Pr_{\\theta}(\\varphi(x) = 0). Isso leva a no\u00e7\u00e3o de teste Uniformemente Mais Poderoso n\u00e3o enviesado (UMPU) . Outra forma de construir testes \u00e9 baseada na distribui\u00e7\u00e3o (em geral assint\u00f3tica) de \\frac{\\sup_{\\Theta_0} f(x|\\theta)}{\\sup_{\\Theta_1} f(x|\\theta)}. p-valores O p-valor associado a um teste \u00e9 o menor n\u00edvel de signific\u00e2ncia \\alpha para o qual a hip\u00f3tese nula \u00e9 rejeitada. Para a hip\u00f3tese nula pontual, uma defini\u00e7\u00e3o mais geral considera como p-valor qualquer estat\u00edstica com distribui\u00e7\u00e3o uniforme sob a hip\u00f3tese nula. Algumas cr\u00edticas: O p-valor contradiz o Princ\u00edpio da Verossimilhan\u00e7a, pois envolve a distribui\u00e7\u00e3o inteira da observa\u00e7\u00e3o; N\u00e3o s\u00e3o avaliados sob nenhum fun\u00e7\u00e3o de perda, loho n\u00e3o tem otimalidade intr\u00ednseca; O p-valor \u00e9 frequentemente visto como uma aproxima\u00e7\u00e3o para \\Pr(\\theta \\in \\Theta_0|x) , apesar de isso ser insignificante em uma configura\u00e7\u00e3o frequentista. p-valores n\u00e3o sumarizam toda a informa\u00e7\u00e3o do teste, afinal o erro do tipo II \u00e9 omitido. Isso \u00e9 perigoso, afinal na pr\u00e1tica o p-valor \u00e9 visto como o procedimento de teste. Uma segunda abordagem A ideia \u00e9 construir uma alternativa a Newyman-Pearson que justifique o uso de probabilidades a posteriori como medidas para testes. A primeira modifica\u00e7\u00e3o \u00e9 alterar o espa\u00e7o de decis\u00f5es \\mathcal{D} = \\{0,1\\} para o intervalo [0,1] . Queremos que eles indiquem o grau de evid\u00eancia a favor de H_0 . Proposi\u00e7\u00e3o: Sob a perda L_2(\\theta, \\delta) = (\\delta - I_{\\Theta_0}(\\theta))^2 , o estimador de Bayes associado a \\pi \u00e9 a probabilidade a posteriori \\varphi^{\\pi}(x) = \\Pr^{\\pi}(\\theta \\in \\Theta_0|x) . Essa proposi\u00e7\u00e3o \u00e9 consequ\u00eancia direta de o estimador de Bayes ser a m\u00e9dia a posteriori de I_{\\Theta_0}(\\theta) . Consideremos a fam\u00edlia exponencial natural f(x|\\theta) = e^{\\theta x - \\psi(\\theta)}. Para a hip\u00f3tese nula unilateral H_0 : \\theta \\le \\theta_0 , um intervalo [t_1, t_2] \u00e9 dito conjunto trunca\u00e7\u00e3o para \\varphi quando \\varphi(t) = 1 se t < t_1 e \\varphi(t) = 0 para t > t_2 . No caso de uma hip\u00f3tese bilateral, vale quando \\varphi(t) = 0 para t \\not \\in [t_1, t_2] . Para a hip\u00f3tese bilateral com H_0 : \\theta \\in [\\theta_1, \\theta_2] , um estimador \\varphi com conjunto trunca\u00e7\u00e3o [t_1, t_2] \u00e9 admiss\u00edvel se existe uma medida de probabilidade \\pi_0 em [\\theta_1, \\theta_2] e uma medida \\sigma -finita \\pi_1 em [\\theta_1, \\theta_2]^c tal que \\varphi(x) = \\frac{\\int f(x|\\theta) \\pi_0(\\theta) \\, d\\theta}{\\int f(x|\\theta) \\pi_0(\\theta) \\, d\\theta + \\int f(x|\\theta) \\pi_1(\\theta) \\, d\\theta}, para x \\in [t_1, t_2] . Alternativamente, se \\varphi \u00e9 admiss\u00edvel, existem [t_1, t_2] , \\pi_0 e \\pi_1 satisfazendo a rela\u00e7\u00e3o acima. Para o teste \\varphi logo acima, quando a distribui\u00e7\u00e3o amostral \u00e9 cont\u00ednua com respeito a Lebesgue, o p-valor \u00e9 inadmiss\u00edvel. Regi\u00f5es de confian\u00e7a Intervalos de credibilidade Para uma priori \\pi , um conjunto C_x \u00e9 dito conjunto \\alpha-cred\u00edvel se \\Pr^{\\pi}(\\theta \\in C_x|x) \\ge 1-\\alpha. Essa regi\u00e3o \u00e9 chamada de regi\u00e3o HPD (\"highest posterior density\") quando pode ser escrita sob a forma \\{\\theta; \\pi(\\theta | x) > k_{\\alpha}\\} \\subset C_x^{\\pi} \\subset \\{\\theta; \\pi(\\theta|x) \\ge k_{\\alpha}\\}, em que k_{\\alpha} \u00e9 o maior valor tal que \\Pr^{\\pi}(\\theta \\in C_x^{\\pi}|x) \\ge 1-\\alpha . Essas regi\u00f5es minimizam o volume entre as regi\u00f5es \\alpha -cred\u00edveis . Uma alternativa \u00e9 definir um intervalo [C_1(x), C_2(x)] tal que \\Pr^{\\pi}(\\theta < C_1(x)|x) = \\Pr(\\theta > C_2(x)|x) = \\alpha/2. Intervalos de confian\u00e7a cl\u00e1ssicos Sob a teoria de Neyman-Pearson, regi\u00f5es de confian\u00e7a podem ser estabelecidas a partir de testes UMPU. Seja C_{\\theta} = \\{x : \\varphi_{\\theta} = 1\\} a regi\u00e3o de n\u00e3o rejei\u00e7\u00e3o de H_0: \\theta = \\theta_0 . Note que C_x = \\{\\theta : x \\in C_{\\theta}\\} = \\{\\theta : \\varphi_{\\theta}(x) = 1\\} e \\Pr(\\theta \\in \\C_x) = 1 - \\alpha . De forma mais geral, se vale que \\Pr(\\theta \\in \\C_x) \\ge 1 - \\alpha para todo \\theta \\in \\Theta , ent\u00e3o C_x \u00e9 dita regi\u00e3o de confian\u00e7a .","title":"Testes e regi\u00f5es de confian\u00e7a"},{"location":"bayesian/tests-regions-confidence/#testes-e-regioes-de-confianca","text":"Com a abordagem bayesiana, podemos fazer afirma\u00e7\u00f5es do tipo \\pi(\\theta \\in \\Theta_0 | x) , o que \u00e9 difere da proposta frequentista, j\u00e1 que l\u00e1 o par\u00e2metro \u00e9 fixo, apesar de desconhecido. Alguns bayesianos acreditam que o teste, em especial o teste com hip\u00f3tese nula pontual, n\u00e3o deveria ser realizado por diversos motivos: representa\u00e7\u00e3o redutiva do modelo; priori modificada de forma n\u00e3o natural para representar a quest\u00e3o pontual; falta de estrutura baseada em teoria da decis\u00e3o; impossibilidade do uso de prioris impr\u00f3prias.","title":"Testes e regi\u00f5es de confian\u00e7a"},{"location":"bayesian/tests-regions-confidence/#uma-primeira-abordagem","text":"Considere um modelo f(x|\\theta) com \\theta \\in \\Theta . Queremos verificar H_0 : \\theta \\in \\Theta_0 com \\Theta_0 \\subseteq \\Theta sendo um subconjunto de interesse. Chamamos H_0 de hip\u00f3tese nula . Em contrapartida, temos a hip\u00f3tese alternativa H_1 : \\theta \\in \\Theta_1 com \\Theta_0 \\cap \\Theta_1 = \\emptyset e \\theta pertencendo a um dos dois conjuntos. A perda proposta por Neyman e Pearson \u00e9 L(\\theta, \\varphi) = \\begin{cases} 1 &\\text{se } \\varphi \\neq 1_{\\Theta_0}(\\theta), \\\\ 0 &\\text{c.c.} \\end{cases} Para essa perda, o estimador de Bayes \u00e9 \\varphi^{\\pi}(x) = \\begin{cases} 1 &\\text{se } \\Pr(\\theta \\in \\Theta_0 | x) > \\Pr(\\theta \\in \\Theta_1 | x), \\\\ 0 &\\text{c.c.} \\end{cases} Podemos generalizar essa perda para L(\\theta, \\varphi) = \\begin{cases} 0 &\\text{se } \\varphi = 1_{\\Theta_0}(\\theta), \\\\ a_0 &\\text{se } \\theta \\in \\Theta_0 \\text{ e } \\varphi = 0, \\\\ a_1 &\\text{se } \\theta \\in \\Theta_1 \\text{ e } \\varphi = 1, \\end{cases} cujo estimador de Bayes \u00e9 \\varphi^{\\pi}(x) = 1 se \\Pr(\\theta \\in \\Theta_0 | x) > a_1/(a_0 + a_1) , sendo essa fra\u00e7\u00e3o o n\u00edvel de aceita\u00e7\u00e3o .","title":"Uma primeira abordagem"},{"location":"bayesian/tests-regions-confidence/#fator-de-bayes","text":"Definimos o fator de Bayes como a raz\u00e3o das raz\u00f5es da posteriori e da priori: B_{01}^{\\pi}(x) = \\frac{\\Pr(\\theta \\in \\Theta_0 | x)}{\\Pr(\\theta \\in \\Theta_1 | x)}\\Big /\\frac{\\Pr(\\theta \\in \\Theta_0)}{\\Pr(\\theta \\in \\Theta_1)} = \\frac{\\int_{\\Theta_0} f(x|\\theta)\\pi_0(\\theta) \\, d\\theta}{\\int_{\\Theta_1} f(x|\\theta)\\pi_1(\\theta) \\, d\\theta}, que avalia a modifica\u00e7\u00e3o das chances de \\Theta_0 contra \\Theta_1 . Observe que \\pi_i \u00e9 a distribui\u00e7\u00e3o a priori sob H_i . Jeffreys desenvolveu uma escala (fora da uma configura\u00e7\u00e3o baseada em teoria da decis\u00e3o) para avaliar a evid\u00eancia trazida por x contra H_0 , considerando o valor \\log_{10}(B_{10}^{\\pi}) : 0 < \\text{ pobre } < 0.5 < \\text{ substancial } < 1 < \\text{ forte } < 2 < \\text{ decisiva } < +\\infty. \ud83d\udcdd Exemplo (Basquete) Um jogador de basquete tem \"m\u00e3o quente\" quando tem bons e maus dias, ao inv\u00e9s de uma probabilidade fixa de vencer em um lan\u00e7amento. O modelo base \u00e9 H_0: y_i \\sim B(n_i, p) para cada jogo i=1,\\dots, G ( p \u00e9 fixo). A Alternativa \u00e9 que p varia com o dia. Considere a priori p_i \\sim Beta(\\xi/\\omega, (1-\\xi)/\\omega) , de forma que \\xi = \\mathbb{E}[p_i|\\xi] \\sim Unif(0,1) . Isto \u00e9, estamos fixando as m\u00e9dias das jogadas dos dias e afirmando que alguns dias teremos p_i > \\xi e outros p_i < \\xi , \u00e9 claro com alguma probabilidade. Al\u00e9m do mais, consideramos p \\sim Unif(0,1) . O fator de Bayes \u00e9 B_{10} = \\frac{\\int_0^1 \\left\\{\\prod_{i=1}^G \\int_0^1 p_i^{y_i}(1-p_i)^{n_i-y_i}p_i^{\\alpha-1}(1-p_i)^{\\beta-1} \\, dp_i \\right\\}\\left(\\Gamma(1/\\omega)/\\Gamma(\\xi/\\omega)\\Gamma((1-\\xi)/\\omega)\\right)^G \\, d\\xi}{\\int_0^1 p^{\\sum y_i}(1-p)^{\\sum (n_i-y_i)} \\, dp} No paper Kass, Raftery (1995) , verificou-se que para \\omega = 0.005 , temos B_{10} = 0.16 , que d\u00e1 baixa evid\u00eancia a favor de H_1 , isto \u00e9, que existem bons e maus dias. Escolhemos \\omega olhando para Var(p_i | \\xi) = \\xi(1-\\xi)/(1+\\omega^{-1}) \\implies \\omega^{-1} = \\xi(1-\\xi)/Var(p_i) - 1. Note que se H_0 for pontual, n\u00e3o conseguimos usar o Fator de Bayes. Para esses casos, precisamos definir \\pi(\\theta) = \\Pr(\\theta \\in \\Theta_0) \\pi_{0}(\\theta) + \\Pr(\\theta \\in \\Theta_1) \\pi_{1}(\\theta) = \\varrho_0 \\pi_0(\\theta) + \\varrho_1 \\pi_1(\\theta). Apesar de hip\u00f3teses pontuais serem problem\u00e1ticas, elas t\u00eam muita utilidade na pr\u00e1tica. Assim, teremos a priori \\pi(\\theta) = \\varrho_0 1_{\\theta = \\theta_0} + (1-\\varrho_0)g_1(\\theta) com posteriori \\pi(\\Theta_0 | x) = \\frac{f(x|\\theta_0) \\varrho_0}{f(x|\\theta_0)\\varrho_0 + (1-\\varrho_0)m_1(x))}, em que m_1 \u00e9 a marginal dos dados sob H_1 . Em particular, o Fator de Bayes \u00e9 B_{01}^{\\pi}(x) = \\frac{f(x|\\theta_0)}{m_1(x)}, levando \u00e0 rela\u00e7\u00e3o \\pi(\\Theta_0 | x) = \\left[1 + \\frac{1-\\varrho_0}{\\varrho_0} \\frac{1}{B_{01}(x)}\\right]^{-1}. \ud83d\udcdd Exemplo (Distribui\u00e7\u00e3o normal) Considere x \\sim \\operatorname{Normal}(\\theta, 1) e H_0: \\theta = 0 . Se usarmos a priori impr\u00f3pria \\pi(\\theta) = \\frac{1}{2}1_{\\theta = 0} + \\frac{1}{2} , teremos que a posteriori de H_0 ser\u00e1 \\pi(\\theta = 0 | x) = \\frac{e^{-x^2/2}}{e^{-x^2/2} + \\int e^{-(x-\\theta)^2/2} \\, d\\theta} = \\frac{1}{1 + \\sqrt{2\\pi}e^{x^2/2}} \\le 0.285, isto \u00e9, a posteriori \u00e9 limitada superiormente por um valor baixo. Fen\u00f4meno parecido acontece quando \\Theta_0 \u00e9 um conjunto compacto. Uma quest\u00e3o interessante \u00e9 que para os n\u00edveis tradicionais, que ocorrem quando x = 1.65, 1.96 e 2.58 (respectivamente n\u00edveis 0.1, 0.05, 0.01 ), a posteriori de H_0 \u00e9 pr\u00f3xima a esses valores (quando a priori \u00e9 a medida de Lebesgue). Em particular, quando H_0 : \\theta \\le 0 para \\pi(\\theta) = 1 , temos que \\pi(\\theta \\le 0 | x) = \\Phi(-x), que \u00e9 o p-valor desse mesmo teste.","title":"Fator de Bayes"},{"location":"bayesian/tests-regions-confidence/#comparacao-com-a-abordagem-classica","text":"A abordagem cl\u00e1ssica da teoria de testagem \u00e9 de Neyman-Pearson. Com isso, consideramos a seguinte defini\u00e7\u00e3o: Poder: O poder de um procedimento de teste \\varphi \u00e9 a probabilidade de rejeitar H_0 sob a hip\u00f3tese alternativa, isto \u00e9, \\beta(\\theta) = 1 - \\mathbb{E}_{\\theta}[\\varphi(x)] quando \\theta \\in \\Theta_1 . A quantidade 1- \\beta(\\theta) \u00e9 o erro do tipo II , enquanto o erro do tipo I \u00e9 1-\\mathbb{E}_{\\theta}[\\varphi(x)] quando \\theta \\in \\Theta_0 . Em resumo, \\theta \\in \\Theta_0 \\implies 1-\\mathbb{E}_{\\theta}[\\varphi(x)] \u00e9 erro do tipo I. \\theta \\in \\Theta_1 \\implies \\mathbb{E}_{\\theta}[\\varphi(x)] \u00e9 erro do tipo II.","title":"Compara\u00e7\u00e3o com a abordagem cl\u00e1ssica"},{"location":"bayesian/tests-regions-confidence/#testes-ump","text":"Testes frequentistas buscam minimizar o risco frequentista \\mathbb{E}_{\\theta}[L(\\theta, \\varphi(x))] sob H_1 . Em particular, minimiza-se na classe de procesdimentos C_{\\alpha} , em que \\alpha \\in (0,1) e C_{\\alpha} = \\{\\varphi : \\sup_{\\theta \\in \\Theta_0} \\mathbb{E}_{\\theta}[L(\\theta, \\varphi(x))] = \\sup_{\\theta \\in \\Theta_0} \\Pr_{\\theta}(\\varphi(x) = 0) \\le \\alpha \\}. Um teste \\varphi \u00e9 dito Uniformemente Mais Poderoso (UMP) a n\u00edvel \\alpha se \\varphi \\in C_{\\alpha} e se ele minimiza o risco frequentista uniformemente em \\Theta_1 em C_{\\alpha} . Esse m\u00e9todo \u00e9 desbalanceado com respeito \u00e0s hip\u00f3teses, porque o erro do tipo II pode ser muito grande, mesmo quando \\Theta_0 \u00e9 uma transforma\u00e7\u00e3o cont\u00ednua de \\Theta_1 . Proposi\u00e7\u00e3o: Seja f(x|\\theta) uma distribui\u00e7\u00e3o que possua raz\u00e3o de verossimilhan\u00e7a crescente em T(x) , isto \u00e9, para \\theta_1 > \\theta_2 , a fun\u00e7\u00e3o f(x|\\theta_1) / f(x|\\theta_2) cresce com T(x) . Se queremos testar H_0 : \\theta \\le \\theta_0 , existe um teste UMP da forma \\varphi(x) = \\begin{cases} 1 &\\text{se } T(x) < c, \\\\ \\gamma &\\text{se } T(x) = c, \\\\ 0 &\\text{c.c.}, \\end{cases} de forma que \\mathbb{E}_{\\theta_0}[\\varphi(x)] = \\alpha . \ud83d\udcdd Exemplo (Fam\u00edlia exponencial) Seja f(x|\\theta) da fam\u00edlia exponencial, isto \u00e9, \\log \\frac{f(x|\\theta_1)}{f(x|\\theta_2)} = \\theta_1\\cdot x - \\psi(\\theta_1) - \\theta_2 \\cdot x + \\psi(\\theta_2) = (\\theta_1 - \\theta_2)\\cdot x - (\\psi(\\theta_1) - \\psi(\\theta_2)), que \u00e9 crescente em T(x) = x . Podemos construir uma proposi\u00e7\u00e3o no mesmo sentido da anterior, s\u00f3 que quando H_0 \u00e9 do tipo \\theta \\not\\in (\\theta_1, \\theta_2) e a densidade do dado pertence \u00e0 fam\u00edlia exponencial. Nesse caso o teste UMP \u00e9 da forma \\varphi(x) = 1 se T(x) \\not \\in [c_1, c_2] e \\varphi(x) = \\gamma_i se T(x) = c_i , em que c_i e \\gamma_i s\u00e3o escolhidos a partir da condi\u00e7\u00e3o de que \\mathbb{E}_{\\theta_i}[\\varphi(x)] = \\alpha . No caso em que o papel de H_0 \u00e9 trocado com o de H_1 , n\u00e3o existe teste UMP. Nessas situa\u00e7\u00f5es, podemos restringir a classe de testes para os n\u00e3o enviesados, em que \\sup_{\\Theta_0} \\Pr_{\\theta}(\\varphi(x) = 0) \\le \\inf_{\\Theta_1} \\Pr_{\\theta}(\\varphi(x) = 0). Isso leva a no\u00e7\u00e3o de teste Uniformemente Mais Poderoso n\u00e3o enviesado (UMPU) . Outra forma de construir testes \u00e9 baseada na distribui\u00e7\u00e3o (em geral assint\u00f3tica) de \\frac{\\sup_{\\Theta_0} f(x|\\theta)}{\\sup_{\\Theta_1} f(x|\\theta)}.","title":"Testes UMP"},{"location":"bayesian/tests-regions-confidence/#p-valores","text":"O p-valor associado a um teste \u00e9 o menor n\u00edvel de signific\u00e2ncia \\alpha para o qual a hip\u00f3tese nula \u00e9 rejeitada. Para a hip\u00f3tese nula pontual, uma defini\u00e7\u00e3o mais geral considera como p-valor qualquer estat\u00edstica com distribui\u00e7\u00e3o uniforme sob a hip\u00f3tese nula. Algumas cr\u00edticas: O p-valor contradiz o Princ\u00edpio da Verossimilhan\u00e7a, pois envolve a distribui\u00e7\u00e3o inteira da observa\u00e7\u00e3o; N\u00e3o s\u00e3o avaliados sob nenhum fun\u00e7\u00e3o de perda, loho n\u00e3o tem otimalidade intr\u00ednseca; O p-valor \u00e9 frequentemente visto como uma aproxima\u00e7\u00e3o para \\Pr(\\theta \\in \\Theta_0|x) , apesar de isso ser insignificante em uma configura\u00e7\u00e3o frequentista. p-valores n\u00e3o sumarizam toda a informa\u00e7\u00e3o do teste, afinal o erro do tipo II \u00e9 omitido. Isso \u00e9 perigoso, afinal na pr\u00e1tica o p-valor \u00e9 visto como o procedimento de teste.","title":"p-valores"},{"location":"bayesian/tests-regions-confidence/#uma-segunda-abordagem","text":"A ideia \u00e9 construir uma alternativa a Newyman-Pearson que justifique o uso de probabilidades a posteriori como medidas para testes. A primeira modifica\u00e7\u00e3o \u00e9 alterar o espa\u00e7o de decis\u00f5es \\mathcal{D} = \\{0,1\\} para o intervalo [0,1] . Queremos que eles indiquem o grau de evid\u00eancia a favor de H_0 . Proposi\u00e7\u00e3o: Sob a perda L_2(\\theta, \\delta) = (\\delta - I_{\\Theta_0}(\\theta))^2 , o estimador de Bayes associado a \\pi \u00e9 a probabilidade a posteriori \\varphi^{\\pi}(x) = \\Pr^{\\pi}(\\theta \\in \\Theta_0|x) . Essa proposi\u00e7\u00e3o \u00e9 consequ\u00eancia direta de o estimador de Bayes ser a m\u00e9dia a posteriori de I_{\\Theta_0}(\\theta) . Consideremos a fam\u00edlia exponencial natural f(x|\\theta) = e^{\\theta x - \\psi(\\theta)}. Para a hip\u00f3tese nula unilateral H_0 : \\theta \\le \\theta_0 , um intervalo [t_1, t_2] \u00e9 dito conjunto trunca\u00e7\u00e3o para \\varphi quando \\varphi(t) = 1 se t < t_1 e \\varphi(t) = 0 para t > t_2 . No caso de uma hip\u00f3tese bilateral, vale quando \\varphi(t) = 0 para t \\not \\in [t_1, t_2] . Para a hip\u00f3tese bilateral com H_0 : \\theta \\in [\\theta_1, \\theta_2] , um estimador \\varphi com conjunto trunca\u00e7\u00e3o [t_1, t_2] \u00e9 admiss\u00edvel se existe uma medida de probabilidade \\pi_0 em [\\theta_1, \\theta_2] e uma medida \\sigma -finita \\pi_1 em [\\theta_1, \\theta_2]^c tal que \\varphi(x) = \\frac{\\int f(x|\\theta) \\pi_0(\\theta) \\, d\\theta}{\\int f(x|\\theta) \\pi_0(\\theta) \\, d\\theta + \\int f(x|\\theta) \\pi_1(\\theta) \\, d\\theta}, para x \\in [t_1, t_2] . Alternativamente, se \\varphi \u00e9 admiss\u00edvel, existem [t_1, t_2] , \\pi_0 e \\pi_1 satisfazendo a rela\u00e7\u00e3o acima. Para o teste \\varphi logo acima, quando a distribui\u00e7\u00e3o amostral \u00e9 cont\u00ednua com respeito a Lebesgue, o p-valor \u00e9 inadmiss\u00edvel.","title":"Uma segunda abordagem"},{"location":"bayesian/tests-regions-confidence/#regioes-de-confianca","text":"","title":"Regi\u00f5es de confian\u00e7a"},{"location":"bayesian/tests-regions-confidence/#intervalos-de-credibilidade","text":"Para uma priori \\pi , um conjunto C_x \u00e9 dito conjunto \\alpha-cred\u00edvel se \\Pr^{\\pi}(\\theta \\in C_x|x) \\ge 1-\\alpha. Essa regi\u00e3o \u00e9 chamada de regi\u00e3o HPD (\"highest posterior density\") quando pode ser escrita sob a forma \\{\\theta; \\pi(\\theta | x) > k_{\\alpha}\\} \\subset C_x^{\\pi} \\subset \\{\\theta; \\pi(\\theta|x) \\ge k_{\\alpha}\\}, em que k_{\\alpha} \u00e9 o maior valor tal que \\Pr^{\\pi}(\\theta \\in C_x^{\\pi}|x) \\ge 1-\\alpha . Essas regi\u00f5es minimizam o volume entre as regi\u00f5es \\alpha -cred\u00edveis . Uma alternativa \u00e9 definir um intervalo [C_1(x), C_2(x)] tal que \\Pr^{\\pi}(\\theta < C_1(x)|x) = \\Pr(\\theta > C_2(x)|x) = \\alpha/2.","title":"Intervalos de credibilidade"},{"location":"bayesian/tests-regions-confidence/#intervalos-de-confianca-classicos","text":"Sob a teoria de Neyman-Pearson, regi\u00f5es de confian\u00e7a podem ser estabelecidas a partir de testes UMPU. Seja C_{\\theta} = \\{x : \\varphi_{\\theta} = 1\\} a regi\u00e3o de n\u00e3o rejei\u00e7\u00e3o de H_0: \\theta = \\theta_0 . Note que C_x = \\{\\theta : x \\in C_{\\theta}\\} = \\{\\theta : \\varphi_{\\theta}(x) = 1\\} e \\Pr(\\theta \\in \\C_x) = 1 - \\alpha . De forma mais geral, se vale que \\Pr(\\theta \\in \\C_x) \\ge 1 - \\alpha para todo \\theta \\in \\Theta , ent\u00e3o C_x \u00e9 dita regi\u00e3o de confian\u00e7a .","title":"Intervalos de confian\u00e7a cl\u00e1ssicos"},{"location":"curvas/assignments-a1/","text":"Temas para o trabalho de A1 Data de apresenta\u00e7\u00e3o dos trabalhos: 12/04/2021 e 16/04/2021 Temas te\u00f3ricos Teoria de Contato (Keti Tenenblat) Propriedades Globais (temas individuais) Desigualdade Isoperim\u00e9trica (Andrew Pressley, John Oprea) Teorema dos 4 v\u00e9rtices (Andrew Pressley) Curvas de Bezier (Duncan Marsh) Teorema Fundamental das Curvas Espaciais : Cristhian Grundmann e Igor Patr\u00edcio Michels Trabalhos h\u00edbridos (te\u00f3rico e computacional) Evolutas e Involutas: Aplica\u00e7\u00f5es, hist\u00f3ria da curva (Keti Tenenblat, John Oprea) Implica\u00e7\u00f5es de curvatura e tor\u00e7\u00e3o (John Oprea) C\u00f4nicas (Duncan Marsh) Trabalhos computacionais Desenho de curva a partir da curvatura (Chapman, John Oprea) Curvas de B\u00e9zier (Duncan Marsh)","title":"Temas para o trabalho de A1"},{"location":"curvas/assignments-a1/#temas-para-o-trabalho-de-a1","text":"Data de apresenta\u00e7\u00e3o dos trabalhos: 12/04/2021 e 16/04/2021","title":"Temas para o trabalho de A1"},{"location":"curvas/assignments-a1/#temas-teoricos","text":"Teoria de Contato (Keti Tenenblat) Propriedades Globais (temas individuais) Desigualdade Isoperim\u00e9trica (Andrew Pressley, John Oprea) Teorema dos 4 v\u00e9rtices (Andrew Pressley) Curvas de Bezier (Duncan Marsh) Teorema Fundamental das Curvas Espaciais : Cristhian Grundmann e Igor Patr\u00edcio Michels","title":"Temas te\u00f3ricos"},{"location":"curvas/assignments-a1/#trabalhos-hibridos-teorico-e-computacional","text":"Evolutas e Involutas: Aplica\u00e7\u00f5es, hist\u00f3ria da curva (Keti Tenenblat, John Oprea) Implica\u00e7\u00f5es de curvatura e tor\u00e7\u00e3o (John Oprea) C\u00f4nicas (Duncan Marsh)","title":"Trabalhos h\u00edbridos (te\u00f3rico e computacional)"},{"location":"curvas/assignments-a1/#trabalhos-computacionais","text":"Desenho de curva a partir da curvatura (Chapman, John Oprea) Curvas de B\u00e9zier (Duncan Marsh)","title":"Trabalhos computacionais"},{"location":"curvas/classic-curves/","text":"Curvas Cl\u00e1ssicas Como uma forma de curiosidade, nessa p\u00e1gina ser\u00e3o descritas algumas curvas que consideradas cl\u00e1ssicas na literatura, dada a hist\u00f3ria que fizeram na matem\u00e1tica, porque s\u00e3o famosas ou s\u00f3 porque s\u00e3o bonitas. Aqui ser\u00e1 fornecido apenas um pequeno resumo sobre cada uma delas e ponteiros para refer\u00eancias mais descritivas. Uma boa refer\u00eancia \u00e9 o livro de J. Dennis Lawrence . Contribui\u00e7\u00e3o volunt\u00e1ria Quem quiser contribuir com esse texto, basta seguir o seguinte processo: Fork o reposit\u00f3rio ta-sessions . Acesse o arquivo docs/curvas/classic-curves.md Fa\u00e7a as modifica\u00e7\u00f5es seguindo o padr\u00e3o das outras curvas, inserindo seu nome, quem descobriu, sua equa\u00e7\u00e3o ou parametriza\u00e7\u00e3o e uma figura, caso poss\u00edvel. Tamb\u00e9m \u00e9 necess\u00e1rio pelo menos uma refer\u00eancia exterior ao site. No caso de colocar imagem, coloque na pasta classic-curves_files e siga o padr\u00e3o daquelas que j\u00e1 est\u00e3o aqui. Procure fazer as curvas no geogebra. Para equa\u00e7\u00f5es n\u00e3o use as express\u00f5es entre $ , use \\(...\\) . Astroide Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico A astroide foi discutida primeiramente pelo matem\u00e1tico Roemer em 1674 como busca para a melhor forma do dente da engrenagem. Ela \u00e9 chamada algumas vezes de tetrac\u00faspide devido \u00e0s quatro c\u00faspides (ponta). Ela ganhou esse nome apenas em 1838 em um livro de Vienna. A equa\u00e7\u00e3o propriamente foi descrita em cartas de Leibniz. Ela \u00e9 o lugar geom\u00e9trico de um ponto em uma circunfer\u00eancia que rola em uma circunfer\u00eancia maior de raio \\(a\\). Refer\u00eancia . \\(x^{2/3} + y^{2/3} = a^{2/3}\\) Cissoide de Diocles Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico \u00c9 uma curva c\u00fabica planar que permite construir duas m\u00e9dias proporcionais a uma dada raz\u00e3o. Seu nome vem do grego \"forma de Hera\" e foi estuda por Diocles 2 s\u00e9culos antes da Era Comum. Ela \u00e9 o lugar geom\u00e9trico da interse\u00e7\u00e3o da reta tangente \u00e0 par\u00e1bola com a reta perpendicular a essa passando pela origem. Refer\u00eancia 1 , Refer\u00eancia 2 \\(2ay^3 - (x^2 + y^2)x = 0\\) Folium de Descartes Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico Seu nome deriva do Latim que significa folha . Ela foi primeiro proposta por Ren\u00e9 Descartes em 1638. Ele desafiou o matem\u00e1tico Pierre de Fermat a encontrar a linha tangente a essa curva em um ponto qualquer. Podemos encontr\u00e1-la facilmente atrav\u00e9s da diferencia\u00e7\u00e3o impl\u00edcita. Refer\u00eancia 1 Refer\u00eancia 2 \\(x^3 + y^3 = 3axy\\) Espiral de Euler Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico \u00c9 uma curva cuja curvatura varia linearmente conforme varia o comprimento de arco (veja o exemplo na p\u00e1gina sobre curvatura). Ela tem outros nomes como clotoide ou espirais de Cornu. Acredita-se que tenha sido primeiramente estudada por James Bernoulli em 1694. Sua equa\u00e7\u00e3o \u00e9 atrav\u00e9s da integral de Fresnel. Ela foi proposta como a solu\u00e7\u00e3o para o problema da elasticidade, mas hoje tem diversas aplica\u00e7\u00f5es como proje\u00e7\u00e3o do de uma esfera. \\(\\alpha(s) = (\\int_0^s \\cos(t^2)dt, \\int_0^s \\sin(t^2)dt\\)","title":"Curvas Cl\u00e1ssicas"},{"location":"curvas/classic-curves/#curvas-classicas","text":"Como uma forma de curiosidade, nessa p\u00e1gina ser\u00e3o descritas algumas curvas que consideradas cl\u00e1ssicas na literatura, dada a hist\u00f3ria que fizeram na matem\u00e1tica, porque s\u00e3o famosas ou s\u00f3 porque s\u00e3o bonitas. Aqui ser\u00e1 fornecido apenas um pequeno resumo sobre cada uma delas e ponteiros para refer\u00eancias mais descritivas. Uma boa refer\u00eancia \u00e9 o livro de J. Dennis Lawrence .","title":"Curvas Cl\u00e1ssicas"},{"location":"curvas/classic-curves/#contribuicao-voluntaria","text":"Quem quiser contribuir com esse texto, basta seguir o seguinte processo: Fork o reposit\u00f3rio ta-sessions . Acesse o arquivo docs/curvas/classic-curves.md Fa\u00e7a as modifica\u00e7\u00f5es seguindo o padr\u00e3o das outras curvas, inserindo seu nome, quem descobriu, sua equa\u00e7\u00e3o ou parametriza\u00e7\u00e3o e uma figura, caso poss\u00edvel. Tamb\u00e9m \u00e9 necess\u00e1rio pelo menos uma refer\u00eancia exterior ao site. No caso de colocar imagem, coloque na pasta classic-curves_files e siga o padr\u00e3o daquelas que j\u00e1 est\u00e3o aqui. Procure fazer as curvas no geogebra. Para equa\u00e7\u00f5es n\u00e3o use as express\u00f5es entre $ , use \\(...\\) . Astroide Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico A astroide foi discutida primeiramente pelo matem\u00e1tico Roemer em 1674 como busca para a melhor forma do dente da engrenagem. Ela \u00e9 chamada algumas vezes de tetrac\u00faspide devido \u00e0s quatro c\u00faspides (ponta). Ela ganhou esse nome apenas em 1838 em um livro de Vienna. A equa\u00e7\u00e3o propriamente foi descrita em cartas de Leibniz. Ela \u00e9 o lugar geom\u00e9trico de um ponto em uma circunfer\u00eancia que rola em uma circunfer\u00eancia maior de raio \\(a\\). Refer\u00eancia . \\(x^{2/3} + y^{2/3} = a^{2/3}\\) Cissoide de Diocles Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico \u00c9 uma curva c\u00fabica planar que permite construir duas m\u00e9dias proporcionais a uma dada raz\u00e3o. Seu nome vem do grego \"forma de Hera\" e foi estuda por Diocles 2 s\u00e9culos antes da Era Comum. Ela \u00e9 o lugar geom\u00e9trico da interse\u00e7\u00e3o da reta tangente \u00e0 par\u00e1bola com a reta perpendicular a essa passando pela origem. Refer\u00eancia 1 , Refer\u00eancia 2 \\(2ay^3 - (x^2 + y^2)x = 0\\) Folium de Descartes Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico Seu nome deriva do Latim que significa folha . Ela foi primeiro proposta por Ren\u00e9 Descartes em 1638. Ele desafiou o matem\u00e1tico Pierre de Fermat a encontrar a linha tangente a essa curva em um ponto qualquer. Podemos encontr\u00e1-la facilmente atrav\u00e9s da diferencia\u00e7\u00e3o impl\u00edcita. Refer\u00eancia 1 Refer\u00eancia 2 \\(x^3 + y^3 = 3axy\\) Espiral de Euler Descri\u00e7\u00e3o Equa\u00e7\u00e3o Gr\u00e1fico \u00c9 uma curva cuja curvatura varia linearmente conforme varia o comprimento de arco (veja o exemplo na p\u00e1gina sobre curvatura). Ela tem outros nomes como clotoide ou espirais de Cornu. Acredita-se que tenha sido primeiramente estudada por James Bernoulli em 1694. Sua equa\u00e7\u00e3o \u00e9 atrav\u00e9s da integral de Fresnel. Ela foi proposta como a solu\u00e7\u00e3o para o problema da elasticidade, mas hoje tem diversas aplica\u00e7\u00f5es como proje\u00e7\u00e3o do de uma esfera. \\(\\alpha(s) = (\\int_0^s \\cos(t^2)dt, \\int_0^s \\sin(t^2)dt\\)","title":"Contribui\u00e7\u00e3o volunt\u00e1ria"},{"location":"curvas/first-definitions/","text":"Defini\u00e7\u00f5es preliminares Quando pensamos em curvas, em geral, expressamos como uma equa\u00e7\u00e3o, como, por exemplo, x^2 + y^2 = 1 que \u00e9 uma circunfer\u00eancia ou a reta y = ax + b . Chamamos essas curvas de curvas de n\u00edvel , aquelas que s\u00e3o do tipo f(x,y) = c para alguma fun\u00e7\u00e3o f:\\mathbb{R}^2 \\to \\mathbb{R} continua. Isso se deve ao fato de que a curva \u00e9 o conjunto de pontos no plano cuja quantidade f(x,y) atinge o n\u00edvel c . Todavia uma defini\u00e7\u00e3o um tanto melhor \u00e9 pensar em uma curva como um caminho tra\u00e7ado por um ponto se movimentando, conceito que \u00e9 formalizado a seguir. Curva parametrizada: Seja I um intervalo. Uma curva parametrizada \u00e9 uma aplica\u00e7\u00e3o cont\u00ednua \\alpha: I \\subset \\mathbb{R} \\to \\mathbb{R}^n , muitas vezes notada como \\alpha(t) = (\\alpha_1(t), ..., \\alpha_n(t)) e t \u00e9 chamado de par\u00e2metro. Algumas defini\u00e7\u00f5es pedem intervalo aberto. Dizemos que ela \u00e9 diferenci\u00e1vel quando a aplica\u00e7\u00e3o \u00e9 diferenci\u00e1vel. Por fim dizemos que a curva \u00e9 regular quando \\alpha '(t) \\neq 0, \\forall t \\in I . Observa\u00e7\u00e3o: Defini\u00e7\u00f5es de curva podem variar em cada livro. Alguns livros pedem que a aplica\u00e7\u00e3o seja de classe C^{\\infty} ou suave, enquanto outras pedem apenas classe C^2 e assim por diante. De forma geral exigir apenas a continuidade \u00e9 mais fraco e podemos pedir diferenciabilidade ou suavidade posteriormente. Tra\u00e7o da curva: Seja uma curma \\alpha:I \\to \\mathbb{R}^n . Dizemos que o tra\u00e7o de \\alpha \u00e9 a imagem da aplica\u00e7\u00e3o \\alpha , denotada \\alpha(I) . Algumas defini\u00e7\u00f5es de curva s\u00e3o precisamente o que definimos de tra\u00e7o da curva. import numpy as np import matplotlib.pyplot as plt fig = plt.figure() ax = fig.gca(projection='3d') # Prepare arrays x, y, z theta = np.linspace(-4 * np.pi, 4 * np.pi, 100) z = np.linspace(-2, 2, 100) r = z**2 + 1 x = r * np.sin(theta) y = r * np.cos(theta) ax.plot(x, y, z, label='arbitrary parametric curve') ax.legend() plt.show() Encontrando parametriza\u00e7\u00f5es Exemplo 1 Vamos encontrar uma parametriza\u00e7\u00e3o para a par\u00e1bola y = x^2 na reta. Seja \\gamma(t) = (\\gamma_1(t), \\gamma_2(t)) . Pela rela\u00e7\u00e3o, temos que \\gamma_2(t) = \\gamma_1(t)^2, \\forall t \\in \\mathbb{R} . Uma solu\u00e7\u00e3o trivial \u00e9 colocar \\gamma_1(t) = t . Nesse caso, \\gamma(t) = (t,t^2) \u00e9 uma curva cujo tra\u00e7o \u00e9 uma par\u00e1bola. Observe que essa n\u00e3o \u00e9 a \u00fanica parametriza\u00e7\u00e3o . Por exemplo (\\frac{t}{2}, \\frac{t^2}{4}) tamb\u00e9m \u00e9 uma parametriza\u00e7\u00e3o na reta. Isso levanta uma quest\u00e3o: temos duas parametriza\u00e7\u00f5es diferentes para a mesma curva. Como dizer que elas s\u00e3o iguais, em um certo sentido, j\u00e1 que suas imagens s\u00e3o iguais? Exemplo 2 Considere a curva astroide dada pela pela equa\u00e7\u00e3o x^{2/3} + y^{2/3} = 1 . Uma maneira \u00e9 propor a parametriza\u00e7\u00e3o dada por x(t) = t e y(t) = (1 - t^{2/3})^{3/2}. Primeiro temos que observar que t \\in [-1,1] devido a raiz quadrada que tomamos na express\u00e3o - o valor dentro do par\u00eanteses n\u00e3o pode ser negativo. Em particular y n\u00e3o pode ser negativo nessa parametriza\u00e7\u00e3o. Isso n\u00e3o corresponde a imagem total da curva, pois y^{2/3} + x^{2/3} = 1 \u00e9 sim\u00e9trico em rela\u00e7\u00e3o ao dois eixos. Poder\u00edamos tentar adaptar essa parametriza\u00e7\u00e3o, mas o mais conveniente \u00e9 lembrar da identidade trigonom\u00e9trica cos^2(t) + sen^2(t) = 1 . Assim podemos escrever que (cos(t)^3)^{2/3} + (sen(t)^3)^{2/3} = 1 . Como consequ\u00eancia (cos^3(t), sen^3(t)) \u00e9 uma parametriza\u00e7\u00e3o da astroide. Note que essa curva \u00e9 cont\u00ednua e definida em toda reta. # Astroid fig = plt.figure(figsize = (5,5)) ax = plt.subplot() ax.grid(alpha=.5) t = np.linspace(-np.pi, np.pi,100) x = np.cos(t)**3 y = np.sin(t)**3 ax.plot(x, y, label='Astroid') ax.axvline(x = 0, color = 'grey', alpha = .7) ax.axhline(y = 0, color = 'grey', alpha = .7) ax.legend() plt.show() Vetor tangente Em geral, quando estudamos curvas e superf\u00edcies, \u00e9 comum encontrar o termo suave associado. A defini\u00e7\u00e3o de fun\u00e7\u00e3o suave varia em cada contexto e pode ir desde uma fun\u00e7\u00e3o diferenci\u00e1vel com derivada cont\u00ednua at\u00e9 fun\u00e7\u00e3o que tem derivada de qualquer ordem (sempre considerando o intervalo I de defini\u00e7\u00e3o). Lembre que se \\gamma(t) = (\\gamma_1(t), ..., \\gamma_n(t) , a derivada de \\gamma \u00e9 \\dot{\\gamma(t)} = (\\dot{\\gamma_1}(t), ..., \\dot{\\gamma_n}(t)). Vetor tangente: Seja \\alpha uma curva parametrizada. Sua primeira derivada \\dot{\\alpha}(t) \u00e9 chamada de vetor tangente a cada tempo t . Proposi\u00e7\u00e3o Se o vetor tangente de uma curva parametrizada \u00e9 constante, ent\u00e3o o tra\u00e7o da curva \u00e9 parte de uma reta. De fato se \\dot{\\alpha}(t) = c , onde c \u00e9 um vetor constante, pelo teorema fundamental do c\u00e1lculo, \\alpha(t) = \\int_{t_0}^t \\dot{\\alpha}(s)ds = (t - t_0)c = ct + d, d = - t_0 c, t_0 \\in I Se c \\neq 0 , esta \u00e9 a equa\u00e7\u00e3o param\u00e9trica de um segmento de reta (potencialmente infinito). Se c = 0 , a imagem da curva \u00e9 um \u00fanico ponto. Comprimento de arco Definimos o comprimento de arco de uma curva \\gamma come\u00e7ando no ponto \\gamma(t_0) como a fun\u00e7\u00e3o s(t) = \\int_{t_0}^t ||\\dot{\\gamma}(s)||ds Se escolhermos um ponto \\tilde{t}_0 diferente, o resultado ser\u00e1 diferente. Dizemos que a curva tem velocidade unit\u00e1ria (ou \u00e9 parametrizada pelo comprimento de arco ) se ||\\dot{\\gamma}(t)|| = 1 Reparametriza\u00e7\u00e3o Sejam I e J intervalos. Uma mudan\u00e7a de par\u00e2metro \u00e9 uma fun\u00e7\u00e3o h: J \\to I bijetiva cont\u00ednua com inversa cont\u00ednua. Em particular, uma fun\u00e7\u00e3o com essa propriedade \u00e9 chamada de homeomorfismo . Sejam \\tilde{\\gamma}:J \\to \\mathbb{R}^n e \\gamma: I \\to \\mathbb{R}^n dua curvas. Dizemos que \\tilde{\\gamma} \u00e9 reparametriza\u00e7\u00e3o da curva \\gamma se existe uma mudan\u00e7a de par\u00e2metro h tal que \\tilde{\\gamma} = \\gamma \\circ h Essa nota\u00e7\u00e3o significa que \\forall t \\in J, \\tilde{\\gamma}(t) = \\gamma(h(t)) . Observe que se \\tilde{\\gamma} \u00e9 reparametriza\u00e7\u00e3o de \\gamma , essa \u00e9 reparametriza\u00e7\u00e3o da primeira. Observa\u00e7\u00e3o: Dependendo em como definimos curva, existem varia\u00e7\u00f5es nessa defini\u00e7\u00e3o. De forma geral, podemos dizer que duas curvas de classe C^k s\u00e3o equivalentes, isto \u00e9, uma \u00e9 reparametriza\u00e7\u00e3o da outra, quando existe uma mapa bijetivo de classe C^k com inversa tamb\u00e9m de classe C^k tal que a igualdade acima \u00e9 v\u00e1lida em todo ponto. Para mais detalhes, consulte o Wikipedia . Lembre que uma curva pode ter muitas parametriza\u00e7\u00f5es, mas nem todas s\u00e3o reparametriza\u00e7\u00f5es uma da outra, como no exemplo a baixo: Exemplo: Considere as seguintes parametriza\u00e7\u00f5es da circunfer\u00eancia: \\alpha(t) = (cos(t), sen(t)), t \\in [0,2\\pi] \\beta(t) = (cos(2t), sen(2t)), t \\in [0,2\\pi] A segunda parametriza\u00e7\u00e3o \"d\u00e1 uma volta a mais na circunfer\u00eancia\". Devemos nos perguntar se existe uma mudan\u00e7a de par\u00e2metro h entre esses intervalos que garanta cos(2t) = cos(h(t)) sen(2t) = sen(h(t)) N\u00e3o conseguimos fazer isso e manter a bijetividade de h entre os intervalos. Uma solu\u00e7\u00e3o para esse problema seria considerar o dom\u00ednio de \\beta o intervalor [0,\\pi] . Nesse caso h(t) = 2t \u00e9 uma mudan\u00e7a de par\u00e2metro entre as parametriza\u00e7\u00f5es. Proposi\u00e7\u00f5es importantes Tente demonstrar essas proposi\u00e7\u00f5es: Toda reparametriza\u00e7\u00e3o de uma curva regular \u00e9 regular. O comprimento de uma curva diferenci\u00e1vel regular n\u00e3o muda depois de uma reparametriza\u00e7\u00e3o. Teorema da reparametriza\u00e7\u00e3o Uma curva parametrizada tem uma reparametriza\u00e7\u00e3o com velocidade unit\u00e1ria se, e somente se, \u00e9 regular. Demonstra\u00e7\u00e3o Um rascunho da demonstra\u00e7\u00e3o supondo a regularidade da curva. Seja \\alpha uma curve (diferenci\u00e1vel). Queremos encontrar \\beta : J \\to \\mathbb{R}^n tal que \\beta = \\alpha \\circ h para algum h difeomorfismo (bijeito diferenci\u00e1vel com inversa diferenci\u00e1vel). Se existesse, ele deveria ter o seguinte comportamento, ||\\beta '(t)|| = ||\\alpha'(h(t))h'(t)|| = 1, por hip\u00f3tese. Dado t_0 \\in I, t \\in I, s(t) := \\int_{t_0}^t ||\\alpha '(\\tau)||d\\tau \u00e9 uma fun\u00e7\u00e3o crescente e deriv\u00e1vel, pois \\alpha \u00e9 regular. Ent\u00e3o ela possui uma fun\u00e7\u00e3o inversa t: s(I) \\to I tamb\u00e9m crescente e deriv\u00e1vel, de forma que t'(s) = \\frac{1}{\\frac{ds}{dt}(t(s))} = \\frac{1}{s'(t(s))} = \\frac{1}{||\\alpha'(t(s))||} Ent\u00e3o defina \\beta: s(I) \\to \\mathbb{R}^n de forma que \\beta(s) = \\alpha(t(s)) . Ent\u00e3o, ||\\beta'(s)|| = ||\\alpha'(t(s))t'(s)|| = 1 Ent\u00e3o a mudan\u00e7a de par\u00e2metro que est\u00e1vamos procurando era a inversa da fun\u00e7\u00e3o de comprimento de curva. Curvas fechadas Curva T-peri\u00f3dica Seja T \\in \\mathbb{R} . Dizemos que uma curva suave \\alpha : \\mathbb{R} \\to \\mathbb{R}^n \u00e9 T-peri\u00f3dica se \\alpha(t + T) = \\alpha(t), t \\in \\mathbb{R} Se \\alpha \u00e9 n\u00e3o constante, mas T-peri\u00f3dica, com T \\neq 0 , ent\u00e3o ela \u00e9 dita fechada . Dizemos que o per\u00edodo da curva fechada \u00e9 o menor n\u00famero positivo T tal que \\alpha seja T-peri\u00f3dica. Exemplo: A elipse \u00e9 um exemplo onde o per\u00eddo \u00e9 2\\pi . Auto-intersec\u00e7\u00e3o Uma curva \\alpha tem uma auto-intersec\u00e7\u00e3o no ponto p se existem a \\neq b tal que \\alpha(a) = \\alpha(b) = p e se \\alpha \u00e9 fechada com per\u00edodo T , ent\u00e3o a - b n\u00e3o \u00e9 um inteiro m\u00faltiplo de T .","title":"Defini\u00e7\u00f5es preliminares"},{"location":"curvas/first-definitions/#definicoes-preliminares","text":"Quando pensamos em curvas, em geral, expressamos como uma equa\u00e7\u00e3o, como, por exemplo, x^2 + y^2 = 1 que \u00e9 uma circunfer\u00eancia ou a reta y = ax + b . Chamamos essas curvas de curvas de n\u00edvel , aquelas que s\u00e3o do tipo f(x,y) = c para alguma fun\u00e7\u00e3o f:\\mathbb{R}^2 \\to \\mathbb{R} continua. Isso se deve ao fato de que a curva \u00e9 o conjunto de pontos no plano cuja quantidade f(x,y) atinge o n\u00edvel c . Todavia uma defini\u00e7\u00e3o um tanto melhor \u00e9 pensar em uma curva como um caminho tra\u00e7ado por um ponto se movimentando, conceito que \u00e9 formalizado a seguir. Curva parametrizada: Seja I um intervalo. Uma curva parametrizada \u00e9 uma aplica\u00e7\u00e3o cont\u00ednua \\alpha: I \\subset \\mathbb{R} \\to \\mathbb{R}^n , muitas vezes notada como \\alpha(t) = (\\alpha_1(t), ..., \\alpha_n(t)) e t \u00e9 chamado de par\u00e2metro. Algumas defini\u00e7\u00f5es pedem intervalo aberto. Dizemos que ela \u00e9 diferenci\u00e1vel quando a aplica\u00e7\u00e3o \u00e9 diferenci\u00e1vel. Por fim dizemos que a curva \u00e9 regular quando \\alpha '(t) \\neq 0, \\forall t \\in I . Observa\u00e7\u00e3o: Defini\u00e7\u00f5es de curva podem variar em cada livro. Alguns livros pedem que a aplica\u00e7\u00e3o seja de classe C^{\\infty} ou suave, enquanto outras pedem apenas classe C^2 e assim por diante. De forma geral exigir apenas a continuidade \u00e9 mais fraco e podemos pedir diferenciabilidade ou suavidade posteriormente. Tra\u00e7o da curva: Seja uma curma \\alpha:I \\to \\mathbb{R}^n . Dizemos que o tra\u00e7o de \\alpha \u00e9 a imagem da aplica\u00e7\u00e3o \\alpha , denotada \\alpha(I) . Algumas defini\u00e7\u00f5es de curva s\u00e3o precisamente o que definimos de tra\u00e7o da curva. import numpy as np import matplotlib.pyplot as plt fig = plt.figure() ax = fig.gca(projection='3d') # Prepare arrays x, y, z theta = np.linspace(-4 * np.pi, 4 * np.pi, 100) z = np.linspace(-2, 2, 100) r = z**2 + 1 x = r * np.sin(theta) y = r * np.cos(theta) ax.plot(x, y, z, label='arbitrary parametric curve') ax.legend() plt.show()","title":"Defini\u00e7\u00f5es preliminares"},{"location":"curvas/first-definitions/#encontrando-parametrizacoes","text":"","title":"Encontrando parametriza\u00e7\u00f5es"},{"location":"curvas/first-definitions/#exemplo-1","text":"Vamos encontrar uma parametriza\u00e7\u00e3o para a par\u00e1bola y = x^2 na reta. Seja \\gamma(t) = (\\gamma_1(t), \\gamma_2(t)) . Pela rela\u00e7\u00e3o, temos que \\gamma_2(t) = \\gamma_1(t)^2, \\forall t \\in \\mathbb{R} . Uma solu\u00e7\u00e3o trivial \u00e9 colocar \\gamma_1(t) = t . Nesse caso, \\gamma(t) = (t,t^2) \u00e9 uma curva cujo tra\u00e7o \u00e9 uma par\u00e1bola. Observe que essa n\u00e3o \u00e9 a \u00fanica parametriza\u00e7\u00e3o . Por exemplo (\\frac{t}{2}, \\frac{t^2}{4}) tamb\u00e9m \u00e9 uma parametriza\u00e7\u00e3o na reta. Isso levanta uma quest\u00e3o: temos duas parametriza\u00e7\u00f5es diferentes para a mesma curva. Como dizer que elas s\u00e3o iguais, em um certo sentido, j\u00e1 que suas imagens s\u00e3o iguais?","title":"Exemplo 1"},{"location":"curvas/first-definitions/#exemplo-2","text":"Considere a curva astroide dada pela pela equa\u00e7\u00e3o x^{2/3} + y^{2/3} = 1 . Uma maneira \u00e9 propor a parametriza\u00e7\u00e3o dada por x(t) = t e y(t) = (1 - t^{2/3})^{3/2}. Primeiro temos que observar que t \\in [-1,1] devido a raiz quadrada que tomamos na express\u00e3o - o valor dentro do par\u00eanteses n\u00e3o pode ser negativo. Em particular y n\u00e3o pode ser negativo nessa parametriza\u00e7\u00e3o. Isso n\u00e3o corresponde a imagem total da curva, pois y^{2/3} + x^{2/3} = 1 \u00e9 sim\u00e9trico em rela\u00e7\u00e3o ao dois eixos. Poder\u00edamos tentar adaptar essa parametriza\u00e7\u00e3o, mas o mais conveniente \u00e9 lembrar da identidade trigonom\u00e9trica cos^2(t) + sen^2(t) = 1 . Assim podemos escrever que (cos(t)^3)^{2/3} + (sen(t)^3)^{2/3} = 1 . Como consequ\u00eancia (cos^3(t), sen^3(t)) \u00e9 uma parametriza\u00e7\u00e3o da astroide. Note que essa curva \u00e9 cont\u00ednua e definida em toda reta. # Astroid fig = plt.figure(figsize = (5,5)) ax = plt.subplot() ax.grid(alpha=.5) t = np.linspace(-np.pi, np.pi,100) x = np.cos(t)**3 y = np.sin(t)**3 ax.plot(x, y, label='Astroid') ax.axvline(x = 0, color = 'grey', alpha = .7) ax.axhline(y = 0, color = 'grey', alpha = .7) ax.legend() plt.show()","title":"Exemplo 2"},{"location":"curvas/first-definitions/#vetor-tangente","text":"Em geral, quando estudamos curvas e superf\u00edcies, \u00e9 comum encontrar o termo suave associado. A defini\u00e7\u00e3o de fun\u00e7\u00e3o suave varia em cada contexto e pode ir desde uma fun\u00e7\u00e3o diferenci\u00e1vel com derivada cont\u00ednua at\u00e9 fun\u00e7\u00e3o que tem derivada de qualquer ordem (sempre considerando o intervalo I de defini\u00e7\u00e3o). Lembre que se \\gamma(t) = (\\gamma_1(t), ..., \\gamma_n(t) , a derivada de \\gamma \u00e9 \\dot{\\gamma(t)} = (\\dot{\\gamma_1}(t), ..., \\dot{\\gamma_n}(t)). Vetor tangente: Seja \\alpha uma curva parametrizada. Sua primeira derivada \\dot{\\alpha}(t) \u00e9 chamada de vetor tangente a cada tempo t .","title":"Vetor tangente"},{"location":"curvas/first-definitions/#proposicao","text":"Se o vetor tangente de uma curva parametrizada \u00e9 constante, ent\u00e3o o tra\u00e7o da curva \u00e9 parte de uma reta. De fato se \\dot{\\alpha}(t) = c , onde c \u00e9 um vetor constante, pelo teorema fundamental do c\u00e1lculo, \\alpha(t) = \\int_{t_0}^t \\dot{\\alpha}(s)ds = (t - t_0)c = ct + d, d = - t_0 c, t_0 \\in I Se c \\neq 0 , esta \u00e9 a equa\u00e7\u00e3o param\u00e9trica de um segmento de reta (potencialmente infinito). Se c = 0 , a imagem da curva \u00e9 um \u00fanico ponto.","title":"Proposi\u00e7\u00e3o"},{"location":"curvas/first-definitions/#comprimento-de-arco","text":"Definimos o comprimento de arco de uma curva \\gamma come\u00e7ando no ponto \\gamma(t_0) como a fun\u00e7\u00e3o s(t) = \\int_{t_0}^t ||\\dot{\\gamma}(s)||ds Se escolhermos um ponto \\tilde{t}_0 diferente, o resultado ser\u00e1 diferente. Dizemos que a curva tem velocidade unit\u00e1ria (ou \u00e9 parametrizada pelo comprimento de arco ) se ||\\dot{\\gamma}(t)|| = 1","title":"Comprimento de arco"},{"location":"curvas/first-definitions/#reparametrizacao","text":"Sejam I e J intervalos. Uma mudan\u00e7a de par\u00e2metro \u00e9 uma fun\u00e7\u00e3o h: J \\to I bijetiva cont\u00ednua com inversa cont\u00ednua. Em particular, uma fun\u00e7\u00e3o com essa propriedade \u00e9 chamada de homeomorfismo . Sejam \\tilde{\\gamma}:J \\to \\mathbb{R}^n e \\gamma: I \\to \\mathbb{R}^n dua curvas. Dizemos que \\tilde{\\gamma} \u00e9 reparametriza\u00e7\u00e3o da curva \\gamma se existe uma mudan\u00e7a de par\u00e2metro h tal que \\tilde{\\gamma} = \\gamma \\circ h Essa nota\u00e7\u00e3o significa que \\forall t \\in J, \\tilde{\\gamma}(t) = \\gamma(h(t)) . Observe que se \\tilde{\\gamma} \u00e9 reparametriza\u00e7\u00e3o de \\gamma , essa \u00e9 reparametriza\u00e7\u00e3o da primeira. Observa\u00e7\u00e3o: Dependendo em como definimos curva, existem varia\u00e7\u00f5es nessa defini\u00e7\u00e3o. De forma geral, podemos dizer que duas curvas de classe C^k s\u00e3o equivalentes, isto \u00e9, uma \u00e9 reparametriza\u00e7\u00e3o da outra, quando existe uma mapa bijetivo de classe C^k com inversa tamb\u00e9m de classe C^k tal que a igualdade acima \u00e9 v\u00e1lida em todo ponto. Para mais detalhes, consulte o Wikipedia . Lembre que uma curva pode ter muitas parametriza\u00e7\u00f5es, mas nem todas s\u00e3o reparametriza\u00e7\u00f5es uma da outra, como no exemplo a baixo: Exemplo: Considere as seguintes parametriza\u00e7\u00f5es da circunfer\u00eancia: \\alpha(t) = (cos(t), sen(t)), t \\in [0,2\\pi] \\beta(t) = (cos(2t), sen(2t)), t \\in [0,2\\pi] A segunda parametriza\u00e7\u00e3o \"d\u00e1 uma volta a mais na circunfer\u00eancia\". Devemos nos perguntar se existe uma mudan\u00e7a de par\u00e2metro h entre esses intervalos que garanta cos(2t) = cos(h(t)) sen(2t) = sen(h(t)) N\u00e3o conseguimos fazer isso e manter a bijetividade de h entre os intervalos. Uma solu\u00e7\u00e3o para esse problema seria considerar o dom\u00ednio de \\beta o intervalor [0,\\pi] . Nesse caso h(t) = 2t \u00e9 uma mudan\u00e7a de par\u00e2metro entre as parametriza\u00e7\u00f5es.","title":"Reparametriza\u00e7\u00e3o"},{"location":"curvas/first-definitions/#proposicoes-importantes","text":"Tente demonstrar essas proposi\u00e7\u00f5es: Toda reparametriza\u00e7\u00e3o de uma curva regular \u00e9 regular. O comprimento de uma curva diferenci\u00e1vel regular n\u00e3o muda depois de uma reparametriza\u00e7\u00e3o.","title":"Proposi\u00e7\u00f5es importantes"},{"location":"curvas/first-definitions/#teorema-da-reparametrizacao","text":"Uma curva parametrizada tem uma reparametriza\u00e7\u00e3o com velocidade unit\u00e1ria se, e somente se, \u00e9 regular.","title":"Teorema da reparametriza\u00e7\u00e3o"},{"location":"curvas/first-definitions/#demonstracao","text":"Um rascunho da demonstra\u00e7\u00e3o supondo a regularidade da curva. Seja \\alpha uma curve (diferenci\u00e1vel). Queremos encontrar \\beta : J \\to \\mathbb{R}^n tal que \\beta = \\alpha \\circ h para algum h difeomorfismo (bijeito diferenci\u00e1vel com inversa diferenci\u00e1vel). Se existesse, ele deveria ter o seguinte comportamento, ||\\beta '(t)|| = ||\\alpha'(h(t))h'(t)|| = 1, por hip\u00f3tese. Dado t_0 \\in I, t \\in I, s(t) := \\int_{t_0}^t ||\\alpha '(\\tau)||d\\tau \u00e9 uma fun\u00e7\u00e3o crescente e deriv\u00e1vel, pois \\alpha \u00e9 regular. Ent\u00e3o ela possui uma fun\u00e7\u00e3o inversa t: s(I) \\to I tamb\u00e9m crescente e deriv\u00e1vel, de forma que t'(s) = \\frac{1}{\\frac{ds}{dt}(t(s))} = \\frac{1}{s'(t(s))} = \\frac{1}{||\\alpha'(t(s))||} Ent\u00e3o defina \\beta: s(I) \\to \\mathbb{R}^n de forma que \\beta(s) = \\alpha(t(s)) . Ent\u00e3o, ||\\beta'(s)|| = ||\\alpha'(t(s))t'(s)|| = 1 Ent\u00e3o a mudan\u00e7a de par\u00e2metro que est\u00e1vamos procurando era a inversa da fun\u00e7\u00e3o de comprimento de curva.","title":"Demonstra\u00e7\u00e3o"},{"location":"curvas/first-definitions/#curvas-fechadas","text":"","title":"Curvas fechadas"},{"location":"curvas/first-definitions/#curva-t-periodica","text":"Seja T \\in \\mathbb{R} . Dizemos que uma curva suave \\alpha : \\mathbb{R} \\to \\mathbb{R}^n \u00e9 T-peri\u00f3dica se \\alpha(t + T) = \\alpha(t), t \\in \\mathbb{R} Se \\alpha \u00e9 n\u00e3o constante, mas T-peri\u00f3dica, com T \\neq 0 , ent\u00e3o ela \u00e9 dita fechada . Dizemos que o per\u00edodo da curva fechada \u00e9 o menor n\u00famero positivo T tal que \\alpha seja T-peri\u00f3dica. Exemplo: A elipse \u00e9 um exemplo onde o per\u00eddo \u00e9 2\\pi .","title":"Curva T-peri\u00f3dica"},{"location":"curvas/first-definitions/#auto-interseccao","text":"Uma curva \\alpha tem uma auto-intersec\u00e7\u00e3o no ponto p se existem a \\neq b tal que \\alpha(a) = \\alpha(b) = p e se \\alpha \u00e9 fechada com per\u00edodo T , ent\u00e3o a - b n\u00e3o \u00e9 um inteiro m\u00faltiplo de T .","title":"Auto-intersec\u00e7\u00e3o"},{"location":"curvas/fundamental-forms/","text":"Formas fundamentais Primeira forma fundamental A primeira preocupa\u00e7\u00e3o que podemos ter em uma superf\u00edcie \u00e9 medir a dist\u00e2ncia entre dois pontos (o ser humano sempre fez isso de diversas formas no planeta terra). Para isso, introduzimos o conceito da primeira forma fundamental . Defini\u00e7\u00e3o: Seja p um ponto na superf\u00edcie de S . A primeira forma fundamental de S em p associa v, w \\in T_pS ao escalar \\langle v, w \\rangle_{p,S} = v \\cdot w Seja v \\in T_pS . Ent\u00e3o, podemos representar w como uma combina\u00e7\u00e3o linear dos vetores \\sigma_u e \\sigma_v : w = \\lambda \\sigma_u + \\mu \\sigma_v . Assim \\langle w, w \\rangle = \\lambda^2\\langle \\sigma_u, \\sigma_u \\rangle + 2\\lambda\\mu\\langle \\sigma_u, \\sigma_v \\rangle + \\mu^2\\langle \\sigma_v, \\sigma_v \\rangle Definimos E = ||\\sigma_u||^2, F = \\sigma_u \\cdot \\sigma_v, G = ||\\sigma_v||^2 e assim, exprimimos a forma fundamental como Edu^2 + 2Fdudv + Gdv^2 em que du = \\lambda e dv = \\mu . Se \\gamma(t) = \\sigma(u(t), v(t)) , podemos calcular o comprimento de arco utilizando que \\dot{\\gamma} = \\dot{u}\\sigma_u + \\dot{v}\\sigma_v e, ent\u00e3o, ||\\dot{\\gamma}||^2 = E\\dot{u}^2 + 2F\\dot{u}\\dot{v} + G\\dot{v}^2 Portanto L(\\gamma) = \\int (E\\dot{u}^2 + 2F\\dot{u}\\dot{v} + G\\dot{v}^2)^{1/2} dt Isometrias em superf\u00edcies Se \\mathcal{S}_1 e \\mathcal{S}_2 s\u00e3o superf\u00edcies, dizemos que eles s\u00e3o localmente isom\u00e9tricos se qualquer curva de \\mathcal{S}_1 pode ser levada por um mapa suave para uma curva em \\mathcal{S}_2 de mesmo comprimento, isto \u00e9, toda curva pode ser levada de uma superf\u00edcie para outra, mantendo comprimento. O mapa que realiza essa fun\u00e7\u00e3o \u00e9 uma isometria local . Seja o mapa D_pf : T_p\\mathcal{S}_1 \\to T_{f(p)}\\mathcal{S}_2 a derivada da fun\u00e7\u00e3o suave f entre as superf\u00edcies. Podemos provar que f ser\u00e1 uma isometria local se, e somente se, D_p\\mathcal{S}_1 \u00e9 uma isometria (isto \u00e9, preserva dist\u00e2ncias) para todo ponto p \\in \\mathcal{S}_1 . Lembrando que por isometria, queremos dizer que \\langle v, v \\rangle_p = \\langle D_p f(v), D_p f(v) \\rangle_{f(p)}. Se f for uma isometria local, ele ser\u00e1 um difeomorfismo local dada a invertibilidade de sua derivada D_pf . Um corol\u00e1rio interessante \u00e9 que para todo mapa \\sigma_1 de \\mathcal{S}_1 , os patches \\sigma_1 de \\mathcal{S}_1 , e f \\circ \\sigma_1 de \\mathcal{S}_2 tem a mesma forma fundamental. Segunda forma fundamental Seja p=\\sigma(u,v) um ponto em uma superf\u00edcie. A medida que nos afastamos de (u,v) , a superf\u00edcie se distancia do plano tangente segundo a dist\u00e2ncia (aproximada) (\\sigma(u + \\Delta u, v + \\Delta v) - \\sigma(u,v))\\cdot N. Pelo teorema de Taylor, essa dist\u00e2ncia vale (\\sigma_u\\Delta u + \\sigma_v\\Delta v + \\frac{1}{2}(\\sigma_{uu}(\\Delta u)^2 + 2\\sigma_{uv}\\Delta u\\Delta v + \\sigma_{vv}(\\Delta v)^2) + R(\\Delta u, \\Delta v))\\cdot N onde esse esse resto sobre (\\Delta u)^2 + (\\Delta v)^2 tende a 0. Como N \u00e9 perpendicular a \\sigma_u, \\sigma_v , o resto da express\u00e3o \u00e9 escrita como \\frac{1}{2}(e(\\Delta u)^2 + 2f\\Delta u\\Delta v + g(\\Delta v)^2) + R(\\Delta u, \\Delta v), em que e = \\sigma_{uu}\\cdot N, f = \\sigma_{uv}\\cdot N, f = \\sigma_{vv}\\cdot N. A express\u00e3o acima \u00e9 a segunda forma fundamental e est\u00e1 associada ao curvamento da superf\u00edcie em rela\u00e7\u00e3o ao plano tangente. Mapa de Gauss e de Weingarten Mapa de Gauss: O mapa \\mathcal{G} : \\mathcal{S} \\to \\mathcal{S}^2 que associa cada ponto da superf\u00edcie p \\in \\mathcal{S} , o ponto N_p \\in \\mathcal{S}^2 que \u00e9 o vetor normal unit\u00e1rio de \\mathcal{S} em p . Medimos a varia\u00e7\u00e3o de N ao longo de \\mathcal{S} pela derivada D_p \\mathcal{G} : T_p\\mathcal{S} \\to T_{\\mathcal{G}(p)}\\mathcal{S}^2. Seja q = \\mathcal{G}(p) . O plano tangente a esse ponto \u00e9 perpendicular a q e passa pela origem. Observe, no entanto, que esse plano, \u00e9 perpendicular a N_p , que \u00e9 exatamente T_p\\mathcal{S} . Portanto esse mapa pode ser escrito como D_p \\mathcal{G} : T_p\\mathcal{S} \\to T_p\\mathcal{S}. Mapa de Weingarten: O mapa de Weingarten da superf\u00edcie \\mathcal{S} no ponto p \u00e9 definida como \\mathcal{W}_{p,S} = - D_p\\mathcal{S} . A segunda forma fundamental pode ser equivalentemente escrita como \\langle \\langle v, w \\rangle \\rangle_{p,S} := \\langle \\mathcal{W}_{p,S}(v), w \\rangle_{p,S}, v, w \\in T_p\\mathcal{S}. Podemos provar essa rela\u00e7\u00e3o, al\u00e9m de provar que o mapa de Weingarten \u00e9 autoadjunto . Curvatura normal e geod\u00e9sica: Seja uma curva \\gamma na superf\u00edcie \\mathcal{S} . A segunda derivada de \\gamma (relacionada com sua curvatura) pode ser escrita como combina\u00e7\u00e3o linear \\ddot{\\gamma} = \\kappa_n N + \\kappa_g(N\\times\\dot{\\gamma}). Chamamos \\kappa_n de curvatura normal e \\kappa_g de geod\u00e9sica. Em geral s\u00f3 a magnitude desses valores \u00e9 bem definida. Al\u00e9m disso \\kappa_n = L\\dot{u}^2 + 2M\\dot{u}\\dot{v} + N\\dot{v}^2","title":"Formas fundamentais"},{"location":"curvas/fundamental-forms/#formas-fundamentais","text":"","title":"Formas fundamentais"},{"location":"curvas/fundamental-forms/#primeira-forma-fundamental","text":"A primeira preocupa\u00e7\u00e3o que podemos ter em uma superf\u00edcie \u00e9 medir a dist\u00e2ncia entre dois pontos (o ser humano sempre fez isso de diversas formas no planeta terra). Para isso, introduzimos o conceito da primeira forma fundamental . Defini\u00e7\u00e3o: Seja p um ponto na superf\u00edcie de S . A primeira forma fundamental de S em p associa v, w \\in T_pS ao escalar \\langle v, w \\rangle_{p,S} = v \\cdot w Seja v \\in T_pS . Ent\u00e3o, podemos representar w como uma combina\u00e7\u00e3o linear dos vetores \\sigma_u e \\sigma_v : w = \\lambda \\sigma_u + \\mu \\sigma_v . Assim \\langle w, w \\rangle = \\lambda^2\\langle \\sigma_u, \\sigma_u \\rangle + 2\\lambda\\mu\\langle \\sigma_u, \\sigma_v \\rangle + \\mu^2\\langle \\sigma_v, \\sigma_v \\rangle Definimos E = ||\\sigma_u||^2, F = \\sigma_u \\cdot \\sigma_v, G = ||\\sigma_v||^2 e assim, exprimimos a forma fundamental como Edu^2 + 2Fdudv + Gdv^2 em que du = \\lambda e dv = \\mu . Se \\gamma(t) = \\sigma(u(t), v(t)) , podemos calcular o comprimento de arco utilizando que \\dot{\\gamma} = \\dot{u}\\sigma_u + \\dot{v}\\sigma_v e, ent\u00e3o, ||\\dot{\\gamma}||^2 = E\\dot{u}^2 + 2F\\dot{u}\\dot{v} + G\\dot{v}^2 Portanto L(\\gamma) = \\int (E\\dot{u}^2 + 2F\\dot{u}\\dot{v} + G\\dot{v}^2)^{1/2} dt","title":"Primeira forma fundamental"},{"location":"curvas/fundamental-forms/#isometrias-em-superficies","text":"Se \\mathcal{S}_1 e \\mathcal{S}_2 s\u00e3o superf\u00edcies, dizemos que eles s\u00e3o localmente isom\u00e9tricos se qualquer curva de \\mathcal{S}_1 pode ser levada por um mapa suave para uma curva em \\mathcal{S}_2 de mesmo comprimento, isto \u00e9, toda curva pode ser levada de uma superf\u00edcie para outra, mantendo comprimento. O mapa que realiza essa fun\u00e7\u00e3o \u00e9 uma isometria local . Seja o mapa D_pf : T_p\\mathcal{S}_1 \\to T_{f(p)}\\mathcal{S}_2 a derivada da fun\u00e7\u00e3o suave f entre as superf\u00edcies. Podemos provar que f ser\u00e1 uma isometria local se, e somente se, D_p\\mathcal{S}_1 \u00e9 uma isometria (isto \u00e9, preserva dist\u00e2ncias) para todo ponto p \\in \\mathcal{S}_1 . Lembrando que por isometria, queremos dizer que \\langle v, v \\rangle_p = \\langle D_p f(v), D_p f(v) \\rangle_{f(p)}. Se f for uma isometria local, ele ser\u00e1 um difeomorfismo local dada a invertibilidade de sua derivada D_pf . Um corol\u00e1rio interessante \u00e9 que para todo mapa \\sigma_1 de \\mathcal{S}_1 , os patches \\sigma_1 de \\mathcal{S}_1 , e f \\circ \\sigma_1 de \\mathcal{S}_2 tem a mesma forma fundamental.","title":"Isometrias em superf\u00edcies"},{"location":"curvas/fundamental-forms/#segunda-forma-fundamental","text":"Seja p=\\sigma(u,v) um ponto em uma superf\u00edcie. A medida que nos afastamos de (u,v) , a superf\u00edcie se distancia do plano tangente segundo a dist\u00e2ncia (aproximada) (\\sigma(u + \\Delta u, v + \\Delta v) - \\sigma(u,v))\\cdot N. Pelo teorema de Taylor, essa dist\u00e2ncia vale (\\sigma_u\\Delta u + \\sigma_v\\Delta v + \\frac{1}{2}(\\sigma_{uu}(\\Delta u)^2 + 2\\sigma_{uv}\\Delta u\\Delta v + \\sigma_{vv}(\\Delta v)^2) + R(\\Delta u, \\Delta v))\\cdot N onde esse esse resto sobre (\\Delta u)^2 + (\\Delta v)^2 tende a 0. Como N \u00e9 perpendicular a \\sigma_u, \\sigma_v , o resto da express\u00e3o \u00e9 escrita como \\frac{1}{2}(e(\\Delta u)^2 + 2f\\Delta u\\Delta v + g(\\Delta v)^2) + R(\\Delta u, \\Delta v), em que e = \\sigma_{uu}\\cdot N, f = \\sigma_{uv}\\cdot N, f = \\sigma_{vv}\\cdot N. A express\u00e3o acima \u00e9 a segunda forma fundamental e est\u00e1 associada ao curvamento da superf\u00edcie em rela\u00e7\u00e3o ao plano tangente.","title":"Segunda forma fundamental"},{"location":"curvas/fundamental-forms/#mapa-de-gauss-e-de-weingarten","text":"Mapa de Gauss: O mapa \\mathcal{G} : \\mathcal{S} \\to \\mathcal{S}^2 que associa cada ponto da superf\u00edcie p \\in \\mathcal{S} , o ponto N_p \\in \\mathcal{S}^2 que \u00e9 o vetor normal unit\u00e1rio de \\mathcal{S} em p . Medimos a varia\u00e7\u00e3o de N ao longo de \\mathcal{S} pela derivada D_p \\mathcal{G} : T_p\\mathcal{S} \\to T_{\\mathcal{G}(p)}\\mathcal{S}^2. Seja q = \\mathcal{G}(p) . O plano tangente a esse ponto \u00e9 perpendicular a q e passa pela origem. Observe, no entanto, que esse plano, \u00e9 perpendicular a N_p , que \u00e9 exatamente T_p\\mathcal{S} . Portanto esse mapa pode ser escrito como D_p \\mathcal{G} : T_p\\mathcal{S} \\to T_p\\mathcal{S}. Mapa de Weingarten: O mapa de Weingarten da superf\u00edcie \\mathcal{S} no ponto p \u00e9 definida como \\mathcal{W}_{p,S} = - D_p\\mathcal{S} . A segunda forma fundamental pode ser equivalentemente escrita como \\langle \\langle v, w \\rangle \\rangle_{p,S} := \\langle \\mathcal{W}_{p,S}(v), w \\rangle_{p,S}, v, w \\in T_p\\mathcal{S}. Podemos provar essa rela\u00e7\u00e3o, al\u00e9m de provar que o mapa de Weingarten \u00e9 autoadjunto . Curvatura normal e geod\u00e9sica: Seja uma curva \\gamma na superf\u00edcie \\mathcal{S} . A segunda derivada de \\gamma (relacionada com sua curvatura) pode ser escrita como combina\u00e7\u00e3o linear \\ddot{\\gamma} = \\kappa_n N + \\kappa_g(N\\times\\dot{\\gamma}). Chamamos \\kappa_n de curvatura normal e \\kappa_g de geod\u00e9sica. Em geral s\u00f3 a magnitude desses valores \u00e9 bem definida. Al\u00e9m disso \\kappa_n = L\\dot{u}^2 + 2M\\dot{u}\\dot{v} + N\\dot{v}^2","title":"Mapa de Gauss e de Weingarten"},{"location":"curvas/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de Curvas e Superf\u00edcies. Ementa da disciplina Notebooks documentados T\u00f3picos Nesse item podemos encontrar diversas curvas cl\u00e1ssicas . Voc\u00ea pode contribuir para aumentar o acervo! Conceitos introdut\u00f3rios de curvas Defini\u00e7\u00f5es preliminares Curvatura Curvas no espa\u00e7o Notas sobre equa\u00e7\u00f5es de Frenet Conceitos introdut\u00f3rios de superf\u00edcies Resumo de topologia Superf\u00edcies Formas fundamentais Curvatura Gaussiana, m\u00e9dia, e principais Sagemath Notebook introdut\u00f3rio Per\u00edodo 2022.1 Dia: Quinta-feira 16h Monitorias gravadas Listas N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Conceito de curva param\u00e9trica, vetor tangente e exemplos 1 2 Reparametriza\u00e7\u00e3o e comprimento de arco 2 3 Comprimento de arco, curvatura e diferenciabilidade 3 4 Curvatura e tor\u00e7\u00e3o, triedro de Frenet 4 5 Superf\u00edcies regulares, mudan\u00e7a de par\u00e2metro 5 6 Superf\u00edcies e desenhos 6 7 Topologia 7 8 Primeira e segunda formas fundamentais 8 Notas Monitoria Itens discutidos Arquivo V\u00eddeo 15/02/2022 Lista 1 e Geogebra Visualizar N\u00e3o 17/02/2022 Lista 1 e exerc\u00edcios adicionais Visualizar Sim 08/03/2022 Reparametriza\u00e7\u00e3o pelo comprimento de arco Visualizar Sim 10/03/2022 Lista 3 Visualizar N\u00e3o 10/03/2022 Lista 3 SageMath Visualizar N\u00e3o 24/03/2022 Transforma\u00e7\u00e3o linear e diferenciabilidade Monitoria presencial N\u00e3o 31/03/2022 D\u00favidas sobre o trabalho da disciplina Monitoria presencial N\u00e3o 07/04/2022 Diferenciabilidade e lista 4 Monitoria presencial N\u00e3o 29/04/2022 Defini\u00e7\u00e3o de superf\u00edcie Monitoria presencial N\u00e3o 02/05/2022 Lista 5 Monitoria presencial N\u00e3o 09/05/2022 Lista 5 Monitoria presencial N\u00e3o 26/05/2022 Lista 6 Monitoria presencial N\u00e3o 29/05/2022 Lista 7 Visualizar N\u00e3o 07/06/2022 C\u00e1lculo em superf\u00edcies Visualizar N\u00e3o 07/06/2022 Formas fundamentais Visualizar N\u00e3o Per\u00edodo 2021.1 Dia: Quinta-feira 17h30min Trabalhos Trabalhos sobre curvas Listas N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Conceito de curva param\u00e9trica, vetor tangente e exemplos 1 2 Reparametriza\u00e7\u00e3o e comprimento de arco 2 3 Comprimento de arco, curvatura e diferenciabilidade 3 4 Curvatura e tor\u00e7\u00e3o, triedro de Frenet 4 4.1 F\u00f3rmulas de Frenet para n\u00e3o unit-speed curvas - 5 Superf\u00edcies regulares, mudan\u00e7a de par\u00e2metro 5 6 Introdu\u00e7\u00e3o \u00e0 Topologia 6 7 Primeira forma fundamental 7 8 Formas fundamentais 8 Notas Monitoria Itens discutidos Arquivo V\u00eddeo 26/02/2021 Lista 1 e GeoGebra - Sim 05/03/2021 Lista 2 e conceito de reparametriza\u00e7\u00e3o Visualizar N\u00e3o 12/03/2021 Aplica\u00e7\u00f5es diferenci\u00e1veis Visualizar Sim 19/03/2021 Lista 3 e aplica\u00e7\u00f5es diferenci\u00e1veis Visualizar Sim 25/03/2021 Lista 4 e 4.1 e p\u00e1gina \"Curvas no Espa\u00e7o\" Frenet N\u00e3o 30/04/2021 Introdu\u00e7\u00e3o a superf\u00edcies, esfera e duplo cone Visualizar Sim 13/05/2021 Defini\u00e7\u00f5es de superf\u00edcies Visualizar N\u00e3o 20/05/2021 Lista 6 e conceitos de topologia Visualizar Sim Sugest\u00f5es Adicionais Teoria Curso de Geometria Diferencial IMPA Professor NJ WildBerger : Curvas cl\u00e1ssicas e Hist\u00f3ria da Geometria Diferencial Livro Introdu\u00e7\u00e3o \u00e0 Geometria Diferencial do professor Ronaldo F. de Lima Recursos Computacionais Recursos Geogebra de curvas Recursos Maxima de curvas Lista de sistemas alg\u00e9bricos : Eu tenho bastante familiaridade com o SymPy (Python) e Matlab. Comandos CAS Geogebra Artigo sobre open source CAS","title":"Curvas"},{"location":"curvas/info/#informacoes-gerais","text":"Monitoria de Curvas e Superf\u00edcies. Ementa da disciplina Notebooks documentados","title":"Informa\u00e7\u00f5es Gerais"},{"location":"curvas/info/#topicos","text":"Nesse item podemos encontrar diversas curvas cl\u00e1ssicas . Voc\u00ea pode contribuir para aumentar o acervo! Conceitos introdut\u00f3rios de curvas Defini\u00e7\u00f5es preliminares Curvatura Curvas no espa\u00e7o Notas sobre equa\u00e7\u00f5es de Frenet Conceitos introdut\u00f3rios de superf\u00edcies Resumo de topologia Superf\u00edcies Formas fundamentais Curvatura Gaussiana, m\u00e9dia, e principais Sagemath Notebook introdut\u00f3rio","title":"T\u00f3picos"},{"location":"curvas/info/#periodo-20221","text":"Dia: Quinta-feira 16h Monitorias gravadas","title":"Per\u00edodo 2022.1"},{"location":"curvas/info/#listas","text":"N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Conceito de curva param\u00e9trica, vetor tangente e exemplos 1 2 Reparametriza\u00e7\u00e3o e comprimento de arco 2 3 Comprimento de arco, curvatura e diferenciabilidade 3 4 Curvatura e tor\u00e7\u00e3o, triedro de Frenet 4 5 Superf\u00edcies regulares, mudan\u00e7a de par\u00e2metro 5 6 Superf\u00edcies e desenhos 6 7 Topologia 7 8 Primeira e segunda formas fundamentais 8","title":"Listas"},{"location":"curvas/info/#notas","text":"Monitoria Itens discutidos Arquivo V\u00eddeo 15/02/2022 Lista 1 e Geogebra Visualizar N\u00e3o 17/02/2022 Lista 1 e exerc\u00edcios adicionais Visualizar Sim 08/03/2022 Reparametriza\u00e7\u00e3o pelo comprimento de arco Visualizar Sim 10/03/2022 Lista 3 Visualizar N\u00e3o 10/03/2022 Lista 3 SageMath Visualizar N\u00e3o 24/03/2022 Transforma\u00e7\u00e3o linear e diferenciabilidade Monitoria presencial N\u00e3o 31/03/2022 D\u00favidas sobre o trabalho da disciplina Monitoria presencial N\u00e3o 07/04/2022 Diferenciabilidade e lista 4 Monitoria presencial N\u00e3o 29/04/2022 Defini\u00e7\u00e3o de superf\u00edcie Monitoria presencial N\u00e3o 02/05/2022 Lista 5 Monitoria presencial N\u00e3o 09/05/2022 Lista 5 Monitoria presencial N\u00e3o 26/05/2022 Lista 6 Monitoria presencial N\u00e3o 29/05/2022 Lista 7 Visualizar N\u00e3o 07/06/2022 C\u00e1lculo em superf\u00edcies Visualizar N\u00e3o 07/06/2022 Formas fundamentais Visualizar N\u00e3o","title":"Notas"},{"location":"curvas/info/#periodo-20211","text":"Dia: Quinta-feira 17h30min Trabalhos Trabalhos sobre curvas","title":"Per\u00edodo 2021.1"},{"location":"curvas/info/#listas_1","text":"N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Conceito de curva param\u00e9trica, vetor tangente e exemplos 1 2 Reparametriza\u00e7\u00e3o e comprimento de arco 2 3 Comprimento de arco, curvatura e diferenciabilidade 3 4 Curvatura e tor\u00e7\u00e3o, triedro de Frenet 4 4.1 F\u00f3rmulas de Frenet para n\u00e3o unit-speed curvas - 5 Superf\u00edcies regulares, mudan\u00e7a de par\u00e2metro 5 6 Introdu\u00e7\u00e3o \u00e0 Topologia 6 7 Primeira forma fundamental 7 8 Formas fundamentais 8","title":"Listas"},{"location":"curvas/info/#notas_1","text":"Monitoria Itens discutidos Arquivo V\u00eddeo 26/02/2021 Lista 1 e GeoGebra - Sim 05/03/2021 Lista 2 e conceito de reparametriza\u00e7\u00e3o Visualizar N\u00e3o 12/03/2021 Aplica\u00e7\u00f5es diferenci\u00e1veis Visualizar Sim 19/03/2021 Lista 3 e aplica\u00e7\u00f5es diferenci\u00e1veis Visualizar Sim 25/03/2021 Lista 4 e 4.1 e p\u00e1gina \"Curvas no Espa\u00e7o\" Frenet N\u00e3o 30/04/2021 Introdu\u00e7\u00e3o a superf\u00edcies, esfera e duplo cone Visualizar Sim 13/05/2021 Defini\u00e7\u00f5es de superf\u00edcies Visualizar N\u00e3o 20/05/2021 Lista 6 e conceitos de topologia Visualizar Sim","title":"Notas"},{"location":"curvas/info/#sugestoes-adicionais","text":"","title":"Sugest\u00f5es Adicionais"},{"location":"curvas/info/#teoria","text":"Curso de Geometria Diferencial IMPA Professor NJ WildBerger : Curvas cl\u00e1ssicas e Hist\u00f3ria da Geometria Diferencial Livro Introdu\u00e7\u00e3o \u00e0 Geometria Diferencial do professor Ronaldo F. de Lima","title":"Teoria"},{"location":"curvas/info/#recursos-computacionais","text":"Recursos Geogebra de curvas Recursos Maxima de curvas Lista de sistemas alg\u00e9bricos : Eu tenho bastante familiaridade com o SymPy (Python) e Matlab. Comandos CAS Geogebra Artigo sobre open source CAS","title":"Recursos Computacionais"},{"location":"curvas/intro-topology/","text":"Introdu\u00e7\u00e3o \u00e0 topologia Topologia \u00e9 uma \u00e1rea da matem\u00e1tica que estuda objetos geom\u00e9tricos com no\u00e7\u00f5es de continuidade e converg\u00eancia. Material na internet sobre esse t\u00f3pico n\u00e3o falta, mas eu gostaria de destacar o curso de Introdu\u00e7\u00e3o \u00e0 Topologia Geral da Universidade de Bras\u00edlia pelo professor Andr\u00e9 Caldas. Estudar topologia pode contribuir para a compreens\u00e3o de superf\u00edcies em um n\u00edvel mais profundo, pois uma superf\u00edcie em \\mathbb{R}^3 \u00e9 um objeto que se parece com um plano na vizinhan\u00e7a de todo ponto. Mas esses conceitos s\u00f3 ficam precisos com o estudo de topologia. Aqui faremos apenas um resumo de alguns conceitos sob a \u00f3tica do que chamamos de espa\u00e7os m\u00e9tricos . Defini\u00e7\u00f5es b\u00e1sicas Bola aberta: A bola aberta de centro x \\in \\mathbb{R}^n e raio r > 0 \u00e9 o conjunto \\mathcal{B}_r(x) = \\{y \\in \\mathbb{R}^n : ||x-y|| < r\\} . Se n=1 , esses conjuntos s\u00e3o chamados de intervalos abertos e se n=2 de discos abertos. Bola fechada: \\bar{\\mathcal{B}}_r(x) = \\{y \\in \\mathbb{R}^n : ||x-y|| \\le r\\} . Conjunto aberto: Dizemos que o conjunto U \\subseteq \\mathbb{R}^n \u00e9 aberto se \\forall x \\in U, \\exists \\epsilon > 0 tal que \\mathcal{B}_{\\epsilon}(x) \\subseteq U . Conjunto fechado: Dizemos que F \\subseteq \\mathbb{R}^n \u00e9 fechado se F^c \u00e9 aberto. Lema: Toda bola aberta \u00e9 um conjunto aberto. Proposi\u00e7\u00e3o: A partir da defini\u00e7\u00e3o de conjunto aberto, podemos demonstrar que: (i) O conjunto vazio \\emptyset \u00e9 aberto. (ii) A uni\u00e3o de conjuntos abertos \u00e9 um conjunto aberto. (iii) A intersec\u00e7\u00e3o finita de de conjuntos abertos \u00e9 aberta. Podemos definir uma topologia usando (i)-(iii). Ponto interior: se x \\in A \\subseteq \\mathbb{R}^n \u00e9 ponto interior de A se existe \\epsilon > 0 tal que \\mathcal{B}_{\\epsilon}(x) \\subseteq A . Vizinhan\u00e7a: Dizemos que U \\subseteq \\mathbb{R}^n \u00e9 uma vizinhan\u00e7a do ponto x \\in \\mathbb{R}^n se existe \\epsilon > 0 tal que \\mathcal{B}_{\\epsilon}(x) \\subseteq U . Ponto de fronteira: Se x \\in \\mathbb{R} \u00e9 ponto de fronteira de A se \\forall \\epsilon > 0 , a bola \\mathcal{B}_{\\epsilon}(x) cont\u00e9m pontos de A e pontos de A^c . O conjunto desses pontos \u00e9 notado como \\partial A . Caracteriza\u00e7\u00e3o de conjuntos fechados: Seja F \\subseteq \\mathbb{R}^n . Ele \u00e9 fechado se, e somente se, toda sequ\u00eancia de elementos de F converge a um elemento de F . Converg\u00eancia e continuidade Converg\u00eancia: Seja \\{x_n\\}_{n \\in \\mathbb{N}} \\subseteq \\mathbb{R}^n uma sequ\u00eancia de pontos. Dizemos que x_n converge para um ponto x^* \\in \\mathbb{R}^n , quando \\forall \\epsilon > 0 , existir N \\in \\mathbb{N} tal que n \\ge N, d(x_n, x^*) < \\epsilon e denotamos x_n \\to x^* . Ponto de ader\u00eancia: Seja A \\subseteq \\mathbb{R}^n . Dizemos que a \\in \\mathbb{R}^n \u00e9 ponto aderente de A se existe \\{x_n\\} \\subseteq A tal que x_n converge para a . Continuidade: Uma fun\u00e7\u00e3o f : \\mathbb{R}^n \\to \\mathbb{R}^m \u00e9 cont\u00ednua se para todo conjunto aberto V \\subseteq \\mathbb{R}^m , a imagem inversa f^{-1}(V) = \\{x \\in \\mathbb{R}^n | f(x) \\in V\\} \u00e9 conjunto aberto. De forma equivalente, dizemos que f \u00e9 cont\u00ednua em a \\in \\mathbb{R}^n quando para todo sequ\u00eancia \\{x_n\\} \\subseteq \\mathbb{R}^n convergente para a , ent\u00e3o f(x_n) converge para f(a) e f \u00e9 cont\u00ednua quando \u00e9 cont\u00ednua para todo ponto a \\in \\mathbb{R}^n . Agora, se f: X \\subseteq \\mathbb{R}^n \\to Y \\subseteq \\mathbb{R}^m , dizemos que ela \u00e9 cont\u00ednua quando para todo V \\subseteq \\mathbb{R}^m , existe um conjunto aberto U \\subseteq \\mathbb{R}^n tal que U \\cap X = \\{x \\in U: f(x) \\in V\\} . Homeomorfismo: Se f : X \\to Y \u00e9 cont\u00ednua e bijetiva e se o mapa inverso f^{-1} : Y \\to X tamb\u00e9m \u00e9 cont\u00ednuo, dizemos que f \u00e9 um homeomorfismo e X e Y s\u00e3o homeomorfos . Difeomorfismo: Sejam X \\subseteq \\mathbb{R}^n e Y \\subseteq \\mathbb{R}^m . Dizemos que f : X \\to Y \u00e9 diferenci\u00e1vel quando para cada a \\in X , existe uma extens\u00e3o um aberto U \\subseteq \\mathbb{R}^n contendo a tal que F: U \\to \\mathbb{R}^m \u00e9 diferenci\u00e1vel e F|_{U \\cap X} = f|_{U \\cap X} . Se f \u00e9 um homeomorfismo diferenci\u00e1vel e f^{-1} \u00e9 diferenci\u00e1vel, ent\u00e3o f \u00e9 um difeomorfismo. Ent\u00e3o X e Y s\u00e3o difeomorfos.","title":"Introdu\u00e7\u00e3o \u00e0 topologia"},{"location":"curvas/intro-topology/#introducao-a-topologia","text":"Topologia \u00e9 uma \u00e1rea da matem\u00e1tica que estuda objetos geom\u00e9tricos com no\u00e7\u00f5es de continuidade e converg\u00eancia. Material na internet sobre esse t\u00f3pico n\u00e3o falta, mas eu gostaria de destacar o curso de Introdu\u00e7\u00e3o \u00e0 Topologia Geral da Universidade de Bras\u00edlia pelo professor Andr\u00e9 Caldas. Estudar topologia pode contribuir para a compreens\u00e3o de superf\u00edcies em um n\u00edvel mais profundo, pois uma superf\u00edcie em \\mathbb{R}^3 \u00e9 um objeto que se parece com um plano na vizinhan\u00e7a de todo ponto. Mas esses conceitos s\u00f3 ficam precisos com o estudo de topologia. Aqui faremos apenas um resumo de alguns conceitos sob a \u00f3tica do que chamamos de espa\u00e7os m\u00e9tricos .","title":"Introdu\u00e7\u00e3o \u00e0 topologia"},{"location":"curvas/intro-topology/#definicoes-basicas","text":"Bola aberta: A bola aberta de centro x \\in \\mathbb{R}^n e raio r > 0 \u00e9 o conjunto \\mathcal{B}_r(x) = \\{y \\in \\mathbb{R}^n : ||x-y|| < r\\} . Se n=1 , esses conjuntos s\u00e3o chamados de intervalos abertos e se n=2 de discos abertos. Bola fechada: \\bar{\\mathcal{B}}_r(x) = \\{y \\in \\mathbb{R}^n : ||x-y|| \\le r\\} . Conjunto aberto: Dizemos que o conjunto U \\subseteq \\mathbb{R}^n \u00e9 aberto se \\forall x \\in U, \\exists \\epsilon > 0 tal que \\mathcal{B}_{\\epsilon}(x) \\subseteq U . Conjunto fechado: Dizemos que F \\subseteq \\mathbb{R}^n \u00e9 fechado se F^c \u00e9 aberto. Lema: Toda bola aberta \u00e9 um conjunto aberto. Proposi\u00e7\u00e3o: A partir da defini\u00e7\u00e3o de conjunto aberto, podemos demonstrar que: (i) O conjunto vazio \\emptyset \u00e9 aberto. (ii) A uni\u00e3o de conjuntos abertos \u00e9 um conjunto aberto. (iii) A intersec\u00e7\u00e3o finita de de conjuntos abertos \u00e9 aberta. Podemos definir uma topologia usando (i)-(iii). Ponto interior: se x \\in A \\subseteq \\mathbb{R}^n \u00e9 ponto interior de A se existe \\epsilon > 0 tal que \\mathcal{B}_{\\epsilon}(x) \\subseteq A . Vizinhan\u00e7a: Dizemos que U \\subseteq \\mathbb{R}^n \u00e9 uma vizinhan\u00e7a do ponto x \\in \\mathbb{R}^n se existe \\epsilon > 0 tal que \\mathcal{B}_{\\epsilon}(x) \\subseteq U . Ponto de fronteira: Se x \\in \\mathbb{R} \u00e9 ponto de fronteira de A se \\forall \\epsilon > 0 , a bola \\mathcal{B}_{\\epsilon}(x) cont\u00e9m pontos de A e pontos de A^c . O conjunto desses pontos \u00e9 notado como \\partial A . Caracteriza\u00e7\u00e3o de conjuntos fechados: Seja F \\subseteq \\mathbb{R}^n . Ele \u00e9 fechado se, e somente se, toda sequ\u00eancia de elementos de F converge a um elemento de F .","title":"Defini\u00e7\u00f5es b\u00e1sicas"},{"location":"curvas/intro-topology/#convergencia-e-continuidade","text":"Converg\u00eancia: Seja \\{x_n\\}_{n \\in \\mathbb{N}} \\subseteq \\mathbb{R}^n uma sequ\u00eancia de pontos. Dizemos que x_n converge para um ponto x^* \\in \\mathbb{R}^n , quando \\forall \\epsilon > 0 , existir N \\in \\mathbb{N} tal que n \\ge N, d(x_n, x^*) < \\epsilon e denotamos x_n \\to x^* . Ponto de ader\u00eancia: Seja A \\subseteq \\mathbb{R}^n . Dizemos que a \\in \\mathbb{R}^n \u00e9 ponto aderente de A se existe \\{x_n\\} \\subseteq A tal que x_n converge para a . Continuidade: Uma fun\u00e7\u00e3o f : \\mathbb{R}^n \\to \\mathbb{R}^m \u00e9 cont\u00ednua se para todo conjunto aberto V \\subseteq \\mathbb{R}^m , a imagem inversa f^{-1}(V) = \\{x \\in \\mathbb{R}^n | f(x) \\in V\\} \u00e9 conjunto aberto. De forma equivalente, dizemos que f \u00e9 cont\u00ednua em a \\in \\mathbb{R}^n quando para todo sequ\u00eancia \\{x_n\\} \\subseteq \\mathbb{R}^n convergente para a , ent\u00e3o f(x_n) converge para f(a) e f \u00e9 cont\u00ednua quando \u00e9 cont\u00ednua para todo ponto a \\in \\mathbb{R}^n . Agora, se f: X \\subseteq \\mathbb{R}^n \\to Y \\subseteq \\mathbb{R}^m , dizemos que ela \u00e9 cont\u00ednua quando para todo V \\subseteq \\mathbb{R}^m , existe um conjunto aberto U \\subseteq \\mathbb{R}^n tal que U \\cap X = \\{x \\in U: f(x) \\in V\\} . Homeomorfismo: Se f : X \\to Y \u00e9 cont\u00ednua e bijetiva e se o mapa inverso f^{-1} : Y \\to X tamb\u00e9m \u00e9 cont\u00ednuo, dizemos que f \u00e9 um homeomorfismo e X e Y s\u00e3o homeomorfos . Difeomorfismo: Sejam X \\subseteq \\mathbb{R}^n e Y \\subseteq \\mathbb{R}^m . Dizemos que f : X \\to Y \u00e9 diferenci\u00e1vel quando para cada a \\in X , existe uma extens\u00e3o um aberto U \\subseteq \\mathbb{R}^n contendo a tal que F: U \\to \\mathbb{R}^m \u00e9 diferenci\u00e1vel e F|_{U \\cap X} = f|_{U \\cap X} . Se f \u00e9 um homeomorfismo diferenci\u00e1vel e f^{-1} \u00e9 diferenci\u00e1vel, ent\u00e3o f \u00e9 um difeomorfismo. Ent\u00e3o X e Y s\u00e3o difeomorfos.","title":"Converg\u00eancia e continuidade"},{"location":"curvas/notes-frenet/","text":"F\u00f3rmulas de Frenet de Curvas Regulares Seja \\alpha : I \\to \\mathbb{R}^3 uma curva regular, isto \u00e9, tal que ||\\alpha'(t)|| nunca se anula. Por esse motivo, existe uma reparametriza\u00e7\u00e3o de \\alpha pelo comprimento de arco. Defina h : I \\to J \\\\ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ t \\mapsto h(t) = \\int_{t_0}^t ||\\alpha'(r)||dr e \\phi(s) := h^{-1}(s) . Assim, seja \\beta a reparametriza\u00e7\u00e3o pelo comprimento de arco, \\beta = \\alpha \\circ \\phi : J \\to \\mathbb{R}^3 Observe que \\dfrac{d}{dt}h(c) = ||\\alpha'(c)|| e que \\dfrac{d}{ds}\\phi(d) = \\dfrac{1}{\\frac{dh}{dt}(\\phi(d))} pelo Teorema da Fun\u00e7\u00e3o Inversa na reta. Diremos que \\phi(s) = t e que h(t) = s . Sejam T_{\\beta}, N_{\\beta}, B_{\\beta} o triedro de Frenet de \\beta (que \u00e9 parametrizada pelo comprimento de arco) e \\kappa_{\\beta} e \\tau_{\\beta} a curvatura e a tor\u00e7\u00e3o, respectivamente. Ent\u00e3o, definimos o triedro de Frenet para \\alpha da seguinte forma: T_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto T_{\\beta}(h(t)) N_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto N_{\\beta}(h(t)) B_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto B_{\\beta}(h(t)) \\kappa_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto \\kappa_{\\beta}(h(t)) \\tau_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto \\tau_{\\beta}(h(t)) Como h(t) \\in J , tudo est\u00e1 bem definido acima. Nesse caso, vamos provar que T_{\\alpha}(t) = \\dfrac{\\dot\\alpha(t)}{||\\dot\\alpha(t)||} . Veja que T_{\\alpha}(t) = T_{\\beta}(h(t)) = \\dot\\beta(h(t)) por defini\u00e7\u00e3o. Al\u00e9m disso, \\dot\\beta(h(t)) = \\dfrac{d}{ds}\\alpha(\\phi(h(t))) (lembrando que s = h(t) ).Pela regra da cadeia, \\dot\\beta(h(t)) = \\dot{\\alpha}(\\phi(h(t)))\\dot\\phi(h(t)) = \\dot\\alpha(t)\\dot{\\phi}(h(t)), pois \\phi(h(t)) = \\phi(s) = t . Por fim, usando o Teorema da Fun\u00e7\u00e3o Inversa, \\dot\\alpha(t)\\dot{\\phi}(h(t)) = \\frac{\\dot\\alpha(t)}{\\dot{h}(\\phi(h(t)))} = \\frac{\\dot\\alpha(t)}{||\\dot{\\alpha}(t)||} o que prova nosso resultado inicial. Esse texto foi feito para deixar mais claro como fazer o exerc\u00edcio 6 da lista 4 e como calcular os vetores normal e binormal para uma curva regular qualquer.","title":"F\u00f3rmulas de Frenet de Curvas Regulares"},{"location":"curvas/notes-frenet/#formulas-de-frenet-de-curvas-regulares","text":"Seja \\alpha : I \\to \\mathbb{R}^3 uma curva regular, isto \u00e9, tal que ||\\alpha'(t)|| nunca se anula. Por esse motivo, existe uma reparametriza\u00e7\u00e3o de \\alpha pelo comprimento de arco. Defina h : I \\to J \\\\ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ t \\mapsto h(t) = \\int_{t_0}^t ||\\alpha'(r)||dr e \\phi(s) := h^{-1}(s) . Assim, seja \\beta a reparametriza\u00e7\u00e3o pelo comprimento de arco, \\beta = \\alpha \\circ \\phi : J \\to \\mathbb{R}^3 Observe que \\dfrac{d}{dt}h(c) = ||\\alpha'(c)|| e que \\dfrac{d}{ds}\\phi(d) = \\dfrac{1}{\\frac{dh}{dt}(\\phi(d))} pelo Teorema da Fun\u00e7\u00e3o Inversa na reta. Diremos que \\phi(s) = t e que h(t) = s . Sejam T_{\\beta}, N_{\\beta}, B_{\\beta} o triedro de Frenet de \\beta (que \u00e9 parametrizada pelo comprimento de arco) e \\kappa_{\\beta} e \\tau_{\\beta} a curvatura e a tor\u00e7\u00e3o, respectivamente. Ent\u00e3o, definimos o triedro de Frenet para \\alpha da seguinte forma: T_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto T_{\\beta}(h(t)) N_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto N_{\\beta}(h(t)) B_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto B_{\\beta}(h(t)) \\kappa_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto \\kappa_{\\beta}(h(t)) \\tau_{\\alpha} : I \\to \\mathbb{R}^3 \\\\ ~~~~~~~~~~~~~~~~~~ t \\mapsto \\tau_{\\beta}(h(t)) Como h(t) \\in J , tudo est\u00e1 bem definido acima. Nesse caso, vamos provar que T_{\\alpha}(t) = \\dfrac{\\dot\\alpha(t)}{||\\dot\\alpha(t)||} . Veja que T_{\\alpha}(t) = T_{\\beta}(h(t)) = \\dot\\beta(h(t)) por defini\u00e7\u00e3o. Al\u00e9m disso, \\dot\\beta(h(t)) = \\dfrac{d}{ds}\\alpha(\\phi(h(t))) (lembrando que s = h(t) ).Pela regra da cadeia, \\dot\\beta(h(t)) = \\dot{\\alpha}(\\phi(h(t)))\\dot\\phi(h(t)) = \\dot\\alpha(t)\\dot{\\phi}(h(t)), pois \\phi(h(t)) = \\phi(s) = t . Por fim, usando o Teorema da Fun\u00e7\u00e3o Inversa, \\dot\\alpha(t)\\dot{\\phi}(h(t)) = \\frac{\\dot\\alpha(t)}{\\dot{h}(\\phi(h(t)))} = \\frac{\\dot\\alpha(t)}{||\\dot{\\alpha}(t)||} o que prova nosso resultado inicial. Esse texto foi feito para deixar mais claro como fazer o exerc\u00edcio 6 da lista 4 e como calcular os vetores normal e binormal para uma curva regular qualquer.","title":"F\u00f3rmulas de Frenet de Curvas Regulares"},{"location":"curvas/regular-surfaces/","text":"Superf\u00edcies Poder\u00edamos definir uma superf\u00edcie da mesma forma que fizemos com curvas, s\u00f3 que com o dom\u00ednio sendo um subconjunto convexo de \\mathbb{R}^2 . S\u00f3 que isso n\u00e3o abrigaria superf\u00edcies como a esfera, por exemplo. De forma geral, vamos querer que uma superf\u00edcie seja um subconjunto em \\mathbb{R}^3 que localmente pare\u00e7a um plano. Defini\u00e7\u00e3o: Dizemos que S \\subseteq \\mathbb{R}^n \u00e9 uma superf\u00edcie se para todo ponto p \\in S , existe um conjunto aberto U \\subset \\mathbb{R}^2 e um conjunto aberto W \\subseteq \\mathbb{R}^3 contendo p tal que S \\cap W \u00e9 homeomorfo a U . O homeomorfismo \\sigma : U \\to S \\cap W \u00e9 um patch ou parametriza\u00e7\u00e3o local . A cole\u00e7\u00e3o desses homeomorfismos que cobrem S \u00e9 chamada de atlas . Dizemos que S \\cap W \u00e9 aberto em S sempre que W for aberto. Observe que para cada ponto da superf\u00edcie, olhamos para uma vizinhan\u00e7a dele (conjunto aberto que o cont\u00e9m). Essa vizinhan\u00e7a restrita \u00e0 superf\u00edcie deve ser \"parecida\" com um subconjunto do plano. Quando caminhamos no planeta, temos a sensa\u00e7\u00e3o de caminharmos num plano justamente por esse conceito. Tamb\u00e9m observe que nosso atlas pode ser formado por uma quantidade infinita de patches, mas, em geral, existe um n\u00famero m\u00ednimo. Superf\u00edcies suaves Defini\u00e7\u00e3o: Um patch \\sigma : U \\to \\mathbb{R}^3 \u00e9 regular se \u00e9 uma fun\u00e7\u00e3o suave (diferenci\u00e1vel nas tr\u00eas componentes com derivadas parciais cont\u00ednuas de todas as ordens) e os vetores \\sigma_u = \\frac{\\partial}{\\partial u} \\sigma e \\sigma_v = \\frac{\\partial}{\\partial v} \\sigma s\u00e3o linearmente independentes para todo (u,v) \\in U . Isto \u00e9, basta exigir que \\sigma_u \\times \\sigma_v \\neq 0 . Uma superf\u00edcie regular \u00e9 uma superf\u00edcie tal que p \\in S , existe um patch regular cuja imagem contenha p . Proposi\u00e7\u00e3o: Sejam U e V abertos de \\mathbb{R}^2 e \\sigma : U \\to \\mathbb{R}^3 um patch regular. Seja \\phi : V \\to U um difeomorfismo. Ent\u00e3o \\tilde{\\sigma} = \\sigma \\circ \\phi : V \\to \\mathbb{R}^3 \u00e9 um patch regular. Dizemos que \\sigma e \\tilde{\\sigma} s\u00e3o reparametriza\u00e7\u00f5es um do outro e \\phi um mapa reparametriza\u00e7\u00e3o. Esse princ\u00edpio permite que definhamos uma propriedade para superf\u00edcies regulares desde que definhamos para qualquer patch regular de forma que ser\u00e1 inalterada para outra parametriza\u00e7\u00e3o. Mapas suaves Queremos entender a no\u00e7\u00e3o de mapa suave entre duas superf\u00edcies suaves. At\u00e9 agora, definimos a suavidade entre conjuntos de \\mathbb{R}^n . Suponha que as superf\u00edcies S_1 e S_2 s\u00e3o cobertas pelos patches \\sigma_1 : U_1 \\to \\mathbb{R}^3 e \\sigma_2 : U_2 \\to \\mathbb{R}^3 , respectivamente. Dizemos que f \u00e9 suave se \\sigma_2^{-1} \\circ f \\circ \\sigma_1 : U_1 \\to U_2 \u00e9 suave. Al\u00e9m disso, se f: S_1 \\to S_2 \u00e9 um difeomorfismo e \\sigma_1 \u00e9 um patch regular S_1 , ent\u00e3o f \\circ \\sigma_1 \u00e9 um patch regular em S_2 . Tangentes Defini\u00e7\u00e3o: Um vetor tangente a uma superf\u00edcie S em um ponto p \\in S \u00e9 o vetor tangente em p de uma curva em S que passa por p . O espa\u00e7o tangente \u00e9 o conjunto desses vetores e \u00e9 denotado por T_pS . Seja uma curva \\alpha em S . que passa por p . Em uma vizinhan\u00e7a de p , podemos dizer que \\alpha(t) = \\sigma(u(t), v(t)) tal que \\alpha(t_0) = p . Podemos provar que u e v s\u00e3o suaves. Essa curva tem uma tangente em p denotada por \\alpha '(t_0) . Mas note que, essa curva n\u00e3o \u00e9 necessariamente a \u00fanica que passa por p . Essa defini\u00e7\u00e3o \u00e9 pouco trat\u00e1vel para enxergar o espa\u00e7o tangente. Para isso, o teorema a seguir prop\u00f5e uma caracteriza\u00e7\u00e3o mais palat\u00e1vel: Proposi\u00e7\u00e3o: Seja \\sigma : U \\to \\mathbb{R}^3 um patch da superf\u00edcie S que passa pelo ponto p em (u_0, v_0) . O espa\u00e7o tangente a S em p \u00e9 o plano gerado pelos vetores \\sigma_u(u_0, v_0) e \\sigma_v(u_0, v_0) que passa pela origem. Observe que o plano tangente \u00e9 independente da escolha do patch. Derivada de mapa suave: Com a defini\u00e7\u00e3o de mapa tangente, podemos definir a derivada de um para f: S_1 \\to S_2 . A derivada de f em p \\in S_1 mede a varia\u00e7\u00e3o de f(p) \\in S_2 quando p se move em sua vizinhan\u00e7a. Assim, dizemos que a derivada D_pf de f em p \\in S \u00e9 um mapa D_pf : T_pS_1 \\to T_{f(p)}S_2 tal que, se w \\in T_pS_1 , existe uma curva \\alpha em S_1 tal que w = \\alpha '(t_0) . Ent\u00e3o \\gamma = f \\circ \\alpha \u00e9 uma curva em S_2 passando por f(p) em t_0 e, ent\u00e3o D_{f(p)}S_2 = \\gamma '(t_0) . Normais e orientabilidade Defini\u00e7\u00e3o: O vetor normal unit\u00e1rio a S no ponto p \u00e9 dado por N_{\\sigma} = \\frac{\\sigma_u \\times \\sigma_v}{||\\sigma_u \\times \\sigma_v||} que \u00e9 exatamente o vetor normal ao plano tangente. Note que esse vetor n\u00e3o \u00e9 independente da escolha do patch \\sigma : U \\to \\mathbb{R}^3 . Se \\tilde{\\sigma} : \\tilde{U} \\to \\mathbb{R}^3 \u00e9 outro patch regular para S em p , podemos demostrar que: Proposi\u00e7\u00e3o: Seja \\phi : \\tilde{U} \\to U um difeomorfismo. Ent\u00e3o \\tilde{\\sigma} = \\sigma \\circ \\phi : \\tilde{U} \\to \\mathbb{R}^3 \u00e9 um mapa regular. Al\u00e9m disso, \\tilde{\\sigma}_{\\tilde{u}} \\times \\tilde{\\sigma}_{\\tilde{v}} = \\operatorname{det}(J(\\phi)) \\, \\tilde{\\sigma}_u \\times \\tilde{\\sigma}_v em que \\operatorname{det}(J(\\phi)) \u00e9 o determinante do Jacobiano da transforma\u00e7\u00e3o \\phi . Portanto N_{\\tilde{\\sigma}} = \\operatorname{sign}(\\operatorname{det}(J(\\phi))) N_{\\sigma} . Orientabilidade: Dizemos que S \u00e9 superf\u00edcie orient\u00e1vel se existe um atlas A para S , de forma que, para quaisquer dois patches \\sigma_1 e \\sigma_2 em A , se \\phi \u00e9 o mapa de transi\u00e7\u00e3o entre eles, ent\u00e3o \\operatorname{det}(J(\\phi)) > 0 . Abordagem alternativa A abordagem utilizada por Ronaldo Freire \u00e9 um pouco diferente e ser\u00e1 apresentada nessa subse\u00e7\u00e3o. Defini\u00e7\u00e3o (campos): Seja uma superf\u00edcie \\mathcal{S} . Uma aplica\u00e7\u00e3o f: \\mathcal{S} \\to \\mathbb{R}^3 \u00e9 chamada de campo. Ela ser\u00e1 unit\u00e1ria quando ||f(p)|| = 1 para todo ponto da superf\u00edcie; tangente se sua imagem est\u00e1 contida no espa\u00e7o (plano) tangente; e normal se pertence ao complemento ortogonal do espa\u00e7o tangente, isto \u00e9, sua imagem \u00e9 ortogonal ao plano tangente. Quando o campo \u00e9 normal, unit\u00e1rio, e diferencial, denotamos por N : \\mathcal{S} \\to \\mathbb{R}^3 . Note que sempre podemos definir essa fun\u00e7\u00e3o associando cada ponto da superf\u00edcie ao vetor normal \u00e0 superf\u00edcie que passe no ponto. Superf\u00edcie orient\u00e1vel: Uma superf\u00edcie regular \u00e9 orient\u00e1vel quando podemos definir um campo N nessa superf\u00edcie. Esse campo define a orienta\u00e7\u00e3o da superf\u00edcie, quando existir. Uma superf\u00edcie regular n\u00e3o orient\u00e1vel \u00e9 a fita de Moebius , uma faixa de papel com uma das extremidades torcidas. Atlas coerente: Um atlas (fam\u00edlia de parametriza\u00e7\u00f5es de uma superf\u00edcie) \u00e9 coerente quando dadas duas parametriza\u00e7\u00f5es \\sigma_1 e \\sigma_2 do atlas, o mapa de transi\u00e7\u00e3o entre as parametriza\u00e7\u00f5es tem determinante positivo. Note que essa \u00e9 a defini\u00e7\u00e3o que Pressley utiliza. Podemos provar (inclusive Freire prova) que ambas s\u00e3o defini\u00e7\u00f5es equivalentes.","title":"Superf\u00edcies"},{"location":"curvas/regular-surfaces/#superficies","text":"Poder\u00edamos definir uma superf\u00edcie da mesma forma que fizemos com curvas, s\u00f3 que com o dom\u00ednio sendo um subconjunto convexo de \\mathbb{R}^2 . S\u00f3 que isso n\u00e3o abrigaria superf\u00edcies como a esfera, por exemplo. De forma geral, vamos querer que uma superf\u00edcie seja um subconjunto em \\mathbb{R}^3 que localmente pare\u00e7a um plano. Defini\u00e7\u00e3o: Dizemos que S \\subseteq \\mathbb{R}^n \u00e9 uma superf\u00edcie se para todo ponto p \\in S , existe um conjunto aberto U \\subset \\mathbb{R}^2 e um conjunto aberto W \\subseteq \\mathbb{R}^3 contendo p tal que S \\cap W \u00e9 homeomorfo a U . O homeomorfismo \\sigma : U \\to S \\cap W \u00e9 um patch ou parametriza\u00e7\u00e3o local . A cole\u00e7\u00e3o desses homeomorfismos que cobrem S \u00e9 chamada de atlas . Dizemos que S \\cap W \u00e9 aberto em S sempre que W for aberto. Observe que para cada ponto da superf\u00edcie, olhamos para uma vizinhan\u00e7a dele (conjunto aberto que o cont\u00e9m). Essa vizinhan\u00e7a restrita \u00e0 superf\u00edcie deve ser \"parecida\" com um subconjunto do plano. Quando caminhamos no planeta, temos a sensa\u00e7\u00e3o de caminharmos num plano justamente por esse conceito. Tamb\u00e9m observe que nosso atlas pode ser formado por uma quantidade infinita de patches, mas, em geral, existe um n\u00famero m\u00ednimo.","title":"Superf\u00edcies"},{"location":"curvas/regular-surfaces/#superficies-suaves","text":"Defini\u00e7\u00e3o: Um patch \\sigma : U \\to \\mathbb{R}^3 \u00e9 regular se \u00e9 uma fun\u00e7\u00e3o suave (diferenci\u00e1vel nas tr\u00eas componentes com derivadas parciais cont\u00ednuas de todas as ordens) e os vetores \\sigma_u = \\frac{\\partial}{\\partial u} \\sigma e \\sigma_v = \\frac{\\partial}{\\partial v} \\sigma s\u00e3o linearmente independentes para todo (u,v) \\in U . Isto \u00e9, basta exigir que \\sigma_u \\times \\sigma_v \\neq 0 . Uma superf\u00edcie regular \u00e9 uma superf\u00edcie tal que p \\in S , existe um patch regular cuja imagem contenha p . Proposi\u00e7\u00e3o: Sejam U e V abertos de \\mathbb{R}^2 e \\sigma : U \\to \\mathbb{R}^3 um patch regular. Seja \\phi : V \\to U um difeomorfismo. Ent\u00e3o \\tilde{\\sigma} = \\sigma \\circ \\phi : V \\to \\mathbb{R}^3 \u00e9 um patch regular. Dizemos que \\sigma e \\tilde{\\sigma} s\u00e3o reparametriza\u00e7\u00f5es um do outro e \\phi um mapa reparametriza\u00e7\u00e3o. Esse princ\u00edpio permite que definhamos uma propriedade para superf\u00edcies regulares desde que definhamos para qualquer patch regular de forma que ser\u00e1 inalterada para outra parametriza\u00e7\u00e3o.","title":"Superf\u00edcies suaves"},{"location":"curvas/regular-surfaces/#mapas-suaves","text":"Queremos entender a no\u00e7\u00e3o de mapa suave entre duas superf\u00edcies suaves. At\u00e9 agora, definimos a suavidade entre conjuntos de \\mathbb{R}^n . Suponha que as superf\u00edcies S_1 e S_2 s\u00e3o cobertas pelos patches \\sigma_1 : U_1 \\to \\mathbb{R}^3 e \\sigma_2 : U_2 \\to \\mathbb{R}^3 , respectivamente. Dizemos que f \u00e9 suave se \\sigma_2^{-1} \\circ f \\circ \\sigma_1 : U_1 \\to U_2 \u00e9 suave. Al\u00e9m disso, se f: S_1 \\to S_2 \u00e9 um difeomorfismo e \\sigma_1 \u00e9 um patch regular S_1 , ent\u00e3o f \\circ \\sigma_1 \u00e9 um patch regular em S_2 .","title":"Mapas suaves"},{"location":"curvas/regular-surfaces/#tangentes","text":"Defini\u00e7\u00e3o: Um vetor tangente a uma superf\u00edcie S em um ponto p \\in S \u00e9 o vetor tangente em p de uma curva em S que passa por p . O espa\u00e7o tangente \u00e9 o conjunto desses vetores e \u00e9 denotado por T_pS . Seja uma curva \\alpha em S . que passa por p . Em uma vizinhan\u00e7a de p , podemos dizer que \\alpha(t) = \\sigma(u(t), v(t)) tal que \\alpha(t_0) = p . Podemos provar que u e v s\u00e3o suaves. Essa curva tem uma tangente em p denotada por \\alpha '(t_0) . Mas note que, essa curva n\u00e3o \u00e9 necessariamente a \u00fanica que passa por p . Essa defini\u00e7\u00e3o \u00e9 pouco trat\u00e1vel para enxergar o espa\u00e7o tangente. Para isso, o teorema a seguir prop\u00f5e uma caracteriza\u00e7\u00e3o mais palat\u00e1vel: Proposi\u00e7\u00e3o: Seja \\sigma : U \\to \\mathbb{R}^3 um patch da superf\u00edcie S que passa pelo ponto p em (u_0, v_0) . O espa\u00e7o tangente a S em p \u00e9 o plano gerado pelos vetores \\sigma_u(u_0, v_0) e \\sigma_v(u_0, v_0) que passa pela origem. Observe que o plano tangente \u00e9 independente da escolha do patch. Derivada de mapa suave: Com a defini\u00e7\u00e3o de mapa tangente, podemos definir a derivada de um para f: S_1 \\to S_2 . A derivada de f em p \\in S_1 mede a varia\u00e7\u00e3o de f(p) \\in S_2 quando p se move em sua vizinhan\u00e7a. Assim, dizemos que a derivada D_pf de f em p \\in S \u00e9 um mapa D_pf : T_pS_1 \\to T_{f(p)}S_2 tal que, se w \\in T_pS_1 , existe uma curva \\alpha em S_1 tal que w = \\alpha '(t_0) . Ent\u00e3o \\gamma = f \\circ \\alpha \u00e9 uma curva em S_2 passando por f(p) em t_0 e, ent\u00e3o D_{f(p)}S_2 = \\gamma '(t_0) .","title":"Tangentes"},{"location":"curvas/regular-surfaces/#normais-e-orientabilidade","text":"Defini\u00e7\u00e3o: O vetor normal unit\u00e1rio a S no ponto p \u00e9 dado por N_{\\sigma} = \\frac{\\sigma_u \\times \\sigma_v}{||\\sigma_u \\times \\sigma_v||} que \u00e9 exatamente o vetor normal ao plano tangente. Note que esse vetor n\u00e3o \u00e9 independente da escolha do patch \\sigma : U \\to \\mathbb{R}^3 . Se \\tilde{\\sigma} : \\tilde{U} \\to \\mathbb{R}^3 \u00e9 outro patch regular para S em p , podemos demostrar que: Proposi\u00e7\u00e3o: Seja \\phi : \\tilde{U} \\to U um difeomorfismo. Ent\u00e3o \\tilde{\\sigma} = \\sigma \\circ \\phi : \\tilde{U} \\to \\mathbb{R}^3 \u00e9 um mapa regular. Al\u00e9m disso, \\tilde{\\sigma}_{\\tilde{u}} \\times \\tilde{\\sigma}_{\\tilde{v}} = \\operatorname{det}(J(\\phi)) \\, \\tilde{\\sigma}_u \\times \\tilde{\\sigma}_v em que \\operatorname{det}(J(\\phi)) \u00e9 o determinante do Jacobiano da transforma\u00e7\u00e3o \\phi . Portanto N_{\\tilde{\\sigma}} = \\operatorname{sign}(\\operatorname{det}(J(\\phi))) N_{\\sigma} . Orientabilidade: Dizemos que S \u00e9 superf\u00edcie orient\u00e1vel se existe um atlas A para S , de forma que, para quaisquer dois patches \\sigma_1 e \\sigma_2 em A , se \\phi \u00e9 o mapa de transi\u00e7\u00e3o entre eles, ent\u00e3o \\operatorname{det}(J(\\phi)) > 0 .","title":"Normais e orientabilidade"},{"location":"curvas/regular-surfaces/#abordagem-alternativa","text":"A abordagem utilizada por Ronaldo Freire \u00e9 um pouco diferente e ser\u00e1 apresentada nessa subse\u00e7\u00e3o. Defini\u00e7\u00e3o (campos): Seja uma superf\u00edcie \\mathcal{S} . Uma aplica\u00e7\u00e3o f: \\mathcal{S} \\to \\mathbb{R}^3 \u00e9 chamada de campo. Ela ser\u00e1 unit\u00e1ria quando ||f(p)|| = 1 para todo ponto da superf\u00edcie; tangente se sua imagem est\u00e1 contida no espa\u00e7o (plano) tangente; e normal se pertence ao complemento ortogonal do espa\u00e7o tangente, isto \u00e9, sua imagem \u00e9 ortogonal ao plano tangente. Quando o campo \u00e9 normal, unit\u00e1rio, e diferencial, denotamos por N : \\mathcal{S} \\to \\mathbb{R}^3 . Note que sempre podemos definir essa fun\u00e7\u00e3o associando cada ponto da superf\u00edcie ao vetor normal \u00e0 superf\u00edcie que passe no ponto. Superf\u00edcie orient\u00e1vel: Uma superf\u00edcie regular \u00e9 orient\u00e1vel quando podemos definir um campo N nessa superf\u00edcie. Esse campo define a orienta\u00e7\u00e3o da superf\u00edcie, quando existir. Uma superf\u00edcie regular n\u00e3o orient\u00e1vel \u00e9 a fita de Moebius , uma faixa de papel com uma das extremidades torcidas. Atlas coerente: Um atlas (fam\u00edlia de parametriza\u00e7\u00f5es de uma superf\u00edcie) \u00e9 coerente quando dadas duas parametriza\u00e7\u00f5es \\sigma_1 e \\sigma_2 do atlas, o mapa de transi\u00e7\u00e3o entre as parametriza\u00e7\u00f5es tem determinante positivo. Note que essa \u00e9 a defini\u00e7\u00e3o que Pressley utiliza. Podemos provar (inclusive Freire prova) que ambas s\u00e3o defini\u00e7\u00f5es equivalentes.","title":"Abordagem alternativa"},{"location":"curvas/space-curves/","text":"Curvas no espa\u00e7o Vimos que as curvas no plano s\u00e3o essencialmente definidas por sua curvatura. Por\u00e9m, isso n\u00e3o \u00e9 verdade no espa\u00e7o. Considere, por exemplo, a h\u00e9lice circular \\alpha(t) = \\left(\\frac{1}{2}\\cos(t), \\frac{1}{2}\\sin(t), \\frac{1}{2}t\\right) Ela est\u00e1 parametrizada pelo comprimento de arco. Ent\u00e3o, se formos calcular a curvatura (usando a mesma no\u00e7\u00e3o de curvas planas), teremos que \\kappa_{\\alpha}(t) = ||\\alpha''(t)|| = 1 Entretanto, a h\u00e9lice n\u00e3o \u00e9 uma circunfer\u00eancia de raio 1! Para isso, vamos precisar introduzir o conceito de tor\u00e7\u00e3o . Veja tamb\u00e9m que n\u00e3o \u00e9 poss\u00edvel definir uma fun\u00e7\u00e3o \u00e2ngulo! Ent\u00e3o, curvatura com sinal \u00e9 um conceito vago no espa\u00e7o. Assim, para lembrar, definimos curvatura para uma curva parametrizada pelo comprimento de arco no espa\u00e7o como \\kappa_{\\alpha}(t) = ||\\alpha''(t)|| Se a curva \\alpha \u00e9 regular qualquer, seja \\beta = \\alpha \\circ h uma reparametriza\u00e7\u00e3o pelo comprimento de arco. Definimos a curvatura de \\alpha como \\kappa_{\\alpha}(t) = \\kappa_{\\beta}(h^{-1}(t)) Em particular, teremos que \\kappa_{\\alpha}(t) = \\frac{||\\alpha'(t) \\times \\alpha''(t)||}{||\\alpha'(t)||^3} Dizemos que uma curva \\alpha \u00e9 2-regular (ou regular de sgunda ordem) quando os vetores \\alpha'(t) e \\alpha''(t) s\u00e3o linearmente independentes para todo t . Se a curva for parametrizada pelo comprimento de arco, isso equivale a dizer que \\alpha'' \\neq 0 (verifique!). Se a curva for regular, isso \u00e9 equivalente a provar que \\kappa_{\\alpha}(t) > 0 (verifique!). Triedo de Frenet J\u00e1 vimos que n\u00e3o faz sentido rotacionar o vetor tangente. Por isso, definimos o vetor normal principal de uma curva \\alpha parametrizada pelo comprimento de arco como N(s) = \\frac{1}{\\kappa_{\\alpha}(s)}(\\dot{T}(s)) onde T(s) = \\alpha'(s) . Veja que ambos os vetores s\u00e3o unit\u00e1rios e ortogonais (quando a curva \u00e9 parametrizada pelo comprimento de arco, a primeira e segunda derivadas s\u00e3o ortogonais). Definimos, ent\u00e3o o vetor binormal como B(s) = T(s) \\times N(s) De fato B \u00e9 ortogonal a T e a N e, al\u00e9m disso, ||B(s)|| = ||T(s)||||N(s)|| - 2\\langle T(s), N(s) \\rangle = 1 Definimos, portanto, uma base ortonormal \\{T(s), N(s), B(s)\\} para \\mathbb{R}^3 . Tor\u00e7\u00e3o Observe que \\dot{B}(s) = \\dot{T}(s) \\times N(s) + T(s) \\times \\dot{N}(s) Como \\dot{T}(s) \\parallel N(s) m, ent\u00e3o \\dot{B}(s) = T(s) \\times \\dot{N}(s) Como B(s) \u00e9 um vetor unit\u00e1rio, B(s) \\perp \\dot{B}(s) (veja que isso acontece com qualquer fun\u00e7\u00e3o vetorial unit\u00e1ria). Al\u00e9m disso \\dot{B}(s) \\perp T(s) pela equa\u00e7\u00e3o acima. Pelo Triedro de Frenet, \\dot{B}(s) \\perp N(s) \\implies \\dot{B}(s) = \\tau(s)N(s) chamamos \\tau de tor\u00e7\u00e3o. Alguns livros, \\tau tem o sinal oposto (mas isso \u00e9 s\u00f3 quest\u00e3o de conven\u00e7\u00e3o e n\u00e3o tr\u00e1s problemas te\u00f3ricos. Mais uma vez, se a curva \\alpha \u00e9 uma curva regular, tomamos uma reparametriza\u00e7\u00e3o pelo comprimento de arco \\beta = \\alpha \\circ h e \\tau_{\\alpha}(s) = \\tau_{\\beta}(h^{-1}(s)) Em especial \\tau = \\frac{\\langle \\dot{\\alpha} \\times \\ddot{\\alpha}, \\dddot{\\alpha} \\rangle}{||\\dot{\\alpha}\\times\\ddot{\\alpha}||^2} Equa\u00e7\u00f5es de Frenet J\u00e1 sabemos que \\dot{T}(s) = \\kappa_{\\alpha}(s)N(s) e \\dot{B}(s) = \\tau_{\\alpha}(s)N(s) . Agora, vamos calcular \\dot{N}(s) . Sabemos que B = T \\times N \\implies N = B \\times T \\implies \\dot{N}(s) = \\dot{B}(s) \\times T(s) + B(s) \\times \\dot{T}(s) Portanto, \\dot{N}(s) = \\tau_{\\alpha} N(s) \\times T(s) + \\kappa_{\\alpha} B(s) \\times N(s) = -\\tau_{\\alpha} B(s) - \\kappa_{\\alpha} T(s) Chegamos ent\u00e3o que, se \\alpha \u00e9 uma curva parametrizada pelo comprimento de arco regular de ordem 2, ent\u00e3o \\dot{T} = \\kappa N \\dot{N} = - \\kappa T - \\tau B \\dot{B} = \\tau N Observe que se x(s) = (T(s), N(s), B(s)) \\in \\mathbb{R}^9, \\dot{x}(s) = Ax(s) , onde A = \\begin{bmatrix} 0_{3\\times 3} & \\kappa I_{3\\times 3} & 0_{3\\times 3} \\\\ -\\kappa I_{3\\times 3} & 0_{3\\times 3} & -\\tau I_{3\\times 3} \\\\ 0_{3\\times 3} & \\tau I_{3\\times 3} & 0_{3\\times 3} \\end{bmatrix} Para curvas regulares quaisquer, a defini\u00e7\u00e3o se d\u00e1 usando a inversa do comprimento de arco, como fizemos com a curvatura e a tor\u00e7\u00e3o. Planos Plano Osculador: Determinado pelos vetores tangente e normal. O vetor binormal \u00e9 normal ao plano osculador e, portanto, podemos escrever sua equa\u00e7\u00e3o como (x - \\alpha(s))\\cdot B(s) = 0 . Plano Normal: Plano determinado pelos vetores normal e binormal. Plano Retificante: Plano determinando pelos vetores tangente e normal. Alguns simples desenhos podem ser vistos nesse site . Consequ\u00eancias Curva plana e tor\u00e7\u00e3o nula Seja \\alpha : I \\to \\mathbb{R}^3 uma curva 2-regular, parametrizada por comprimento de arco. Ent\u00e3o, \\alpha \u00e9 plana se, e somente se, sua tor\u00e7\u00e3o \\tau_{\\alpha} \u00e9 identicamente nula. A demonstra\u00e7\u00e3o desse fato se divide na ida e volta. Supondo que a curva seja plana, devemos inserir a curva em um plano. Assim, para cada s \\in I, \\langle \\alpha(s) - p, v \\rangle = 0 para algum p nesse plano. Em particular, obtermos que v = \\pm B_{\\alpha}(s) , pois obteremos, derivando, que a tangente e a normal s\u00e3o ortogonais a v . Nesse caso B'_{\\alpha}(s) = 0 , pois esse vetor ser\u00e1 constante. A rec\u00edproca usa o fato que B_{\\alpha}(s) ser\u00e1 constante e quer se provar que f(s) = \\langle \\alpha(s) - \\alpha(s_0), B_{\\alpha}(s) \\rangle \\equiv 0 . Circunfer\u00eancia e curvatura constante Seja \\alpha uma curva parametrizada pelo comprimento de arco em \\mathbb{R}^3 com consntate curvatura e tor\u00e7\u00e3o nula. Ent\u00e3o \\alpha \u00e9 parametriza\u00e7\u00e3o de (parte de) um c\u00edrculo. Sabemos pelo item anterior que estaremos em um plano. A ideia \u00e9 provar que \\alpha - \\frac{1}{\\kappa_{\\alpha}}n \u00e9 um vetor constante para provarmos que a curva est\u00e1 contida em uma esfera. Assim, basta provar que a curva est\u00e1 contida na intersec\u00e7\u00e3o de uma esfera e um plano. Teorema Fundamental da Teoria Local das Curvas Espaciais Sejam \\alpha(s) e \\gamma(s) duas curvas parametrizadas pelo comprimento de arco em \\mathbb{R}^3 com a mesma curvatura \\kappa(s) > 0 e a mesma tor\u00e7\u00e3o \\tau(s), \\forall s . Ent\u00e3o, existe um movimento r\u00edgido direto M tal que \\alpha(s) = M(\\gamma(s)), \\forall s . Al\u00e9m disso, se \\kappa e \\tau s\u00e3o fun\u00e7\u00f5es suaves, tal que \\kappa > 0 em toda parte, existe uma curva parametrizada pelo comprimento de arco em \\mathbb{R}^3 cuja curvatura \u00e9 \\kappa e cuja tor\u00e7\u00e3o \u00e9 \\tau . A demonstra\u00e7\u00e3o desse teorema super importante pode ser encontrada na p\u00e1gina 52 do livro do Pressley de Introdu\u00e7\u00e3o \u00e0 Geometria Diferencial. \u00c9 uma aplica\u00e7\u00e3o do Teorema da Exist\u00eancia e Unicidade de Equa\u00e7\u00f5es Diferenciais nas Equa\u00e7\u00f5es de Frenet-Sarret.","title":"Curvas no espa\u00e7o"},{"location":"curvas/space-curves/#curvas-no-espaco","text":"Vimos que as curvas no plano s\u00e3o essencialmente definidas por sua curvatura. Por\u00e9m, isso n\u00e3o \u00e9 verdade no espa\u00e7o. Considere, por exemplo, a h\u00e9lice circular \\alpha(t) = \\left(\\frac{1}{2}\\cos(t), \\frac{1}{2}\\sin(t), \\frac{1}{2}t\\right) Ela est\u00e1 parametrizada pelo comprimento de arco. Ent\u00e3o, se formos calcular a curvatura (usando a mesma no\u00e7\u00e3o de curvas planas), teremos que \\kappa_{\\alpha}(t) = ||\\alpha''(t)|| = 1 Entretanto, a h\u00e9lice n\u00e3o \u00e9 uma circunfer\u00eancia de raio 1! Para isso, vamos precisar introduzir o conceito de tor\u00e7\u00e3o . Veja tamb\u00e9m que n\u00e3o \u00e9 poss\u00edvel definir uma fun\u00e7\u00e3o \u00e2ngulo! Ent\u00e3o, curvatura com sinal \u00e9 um conceito vago no espa\u00e7o. Assim, para lembrar, definimos curvatura para uma curva parametrizada pelo comprimento de arco no espa\u00e7o como \\kappa_{\\alpha}(t) = ||\\alpha''(t)|| Se a curva \\alpha \u00e9 regular qualquer, seja \\beta = \\alpha \\circ h uma reparametriza\u00e7\u00e3o pelo comprimento de arco. Definimos a curvatura de \\alpha como \\kappa_{\\alpha}(t) = \\kappa_{\\beta}(h^{-1}(t)) Em particular, teremos que \\kappa_{\\alpha}(t) = \\frac{||\\alpha'(t) \\times \\alpha''(t)||}{||\\alpha'(t)||^3} Dizemos que uma curva \\alpha \u00e9 2-regular (ou regular de sgunda ordem) quando os vetores \\alpha'(t) e \\alpha''(t) s\u00e3o linearmente independentes para todo t . Se a curva for parametrizada pelo comprimento de arco, isso equivale a dizer que \\alpha'' \\neq 0 (verifique!). Se a curva for regular, isso \u00e9 equivalente a provar que \\kappa_{\\alpha}(t) > 0 (verifique!).","title":"Curvas no espa\u00e7o"},{"location":"curvas/space-curves/#triedo-de-frenet","text":"J\u00e1 vimos que n\u00e3o faz sentido rotacionar o vetor tangente. Por isso, definimos o vetor normal principal de uma curva \\alpha parametrizada pelo comprimento de arco como N(s) = \\frac{1}{\\kappa_{\\alpha}(s)}(\\dot{T}(s)) onde T(s) = \\alpha'(s) . Veja que ambos os vetores s\u00e3o unit\u00e1rios e ortogonais (quando a curva \u00e9 parametrizada pelo comprimento de arco, a primeira e segunda derivadas s\u00e3o ortogonais). Definimos, ent\u00e3o o vetor binormal como B(s) = T(s) \\times N(s) De fato B \u00e9 ortogonal a T e a N e, al\u00e9m disso, ||B(s)|| = ||T(s)||||N(s)|| - 2\\langle T(s), N(s) \\rangle = 1 Definimos, portanto, uma base ortonormal \\{T(s), N(s), B(s)\\} para \\mathbb{R}^3 .","title":"Triedo de Frenet"},{"location":"curvas/space-curves/#torcao","text":"Observe que \\dot{B}(s) = \\dot{T}(s) \\times N(s) + T(s) \\times \\dot{N}(s) Como \\dot{T}(s) \\parallel N(s) m, ent\u00e3o \\dot{B}(s) = T(s) \\times \\dot{N}(s) Como B(s) \u00e9 um vetor unit\u00e1rio, B(s) \\perp \\dot{B}(s) (veja que isso acontece com qualquer fun\u00e7\u00e3o vetorial unit\u00e1ria). Al\u00e9m disso \\dot{B}(s) \\perp T(s) pela equa\u00e7\u00e3o acima. Pelo Triedro de Frenet, \\dot{B}(s) \\perp N(s) \\implies \\dot{B}(s) = \\tau(s)N(s) chamamos \\tau de tor\u00e7\u00e3o. Alguns livros, \\tau tem o sinal oposto (mas isso \u00e9 s\u00f3 quest\u00e3o de conven\u00e7\u00e3o e n\u00e3o tr\u00e1s problemas te\u00f3ricos. Mais uma vez, se a curva \\alpha \u00e9 uma curva regular, tomamos uma reparametriza\u00e7\u00e3o pelo comprimento de arco \\beta = \\alpha \\circ h e \\tau_{\\alpha}(s) = \\tau_{\\beta}(h^{-1}(s)) Em especial \\tau = \\frac{\\langle \\dot{\\alpha} \\times \\ddot{\\alpha}, \\dddot{\\alpha} \\rangle}{||\\dot{\\alpha}\\times\\ddot{\\alpha}||^2}","title":"Tor\u00e7\u00e3o"},{"location":"curvas/space-curves/#equacoes-de-frenet","text":"J\u00e1 sabemos que \\dot{T}(s) = \\kappa_{\\alpha}(s)N(s) e \\dot{B}(s) = \\tau_{\\alpha}(s)N(s) . Agora, vamos calcular \\dot{N}(s) . Sabemos que B = T \\times N \\implies N = B \\times T \\implies \\dot{N}(s) = \\dot{B}(s) \\times T(s) + B(s) \\times \\dot{T}(s) Portanto, \\dot{N}(s) = \\tau_{\\alpha} N(s) \\times T(s) + \\kappa_{\\alpha} B(s) \\times N(s) = -\\tau_{\\alpha} B(s) - \\kappa_{\\alpha} T(s) Chegamos ent\u00e3o que, se \\alpha \u00e9 uma curva parametrizada pelo comprimento de arco regular de ordem 2, ent\u00e3o \\dot{T} = \\kappa N \\dot{N} = - \\kappa T - \\tau B \\dot{B} = \\tau N Observe que se x(s) = (T(s), N(s), B(s)) \\in \\mathbb{R}^9, \\dot{x}(s) = Ax(s) , onde A = \\begin{bmatrix} 0_{3\\times 3} & \\kappa I_{3\\times 3} & 0_{3\\times 3} \\\\ -\\kappa I_{3\\times 3} & 0_{3\\times 3} & -\\tau I_{3\\times 3} \\\\ 0_{3\\times 3} & \\tau I_{3\\times 3} & 0_{3\\times 3} \\end{bmatrix} Para curvas regulares quaisquer, a defini\u00e7\u00e3o se d\u00e1 usando a inversa do comprimento de arco, como fizemos com a curvatura e a tor\u00e7\u00e3o.","title":"Equa\u00e7\u00f5es de Frenet"},{"location":"curvas/space-curves/#planos","text":"Plano Osculador: Determinado pelos vetores tangente e normal. O vetor binormal \u00e9 normal ao plano osculador e, portanto, podemos escrever sua equa\u00e7\u00e3o como (x - \\alpha(s))\\cdot B(s) = 0 . Plano Normal: Plano determinado pelos vetores normal e binormal. Plano Retificante: Plano determinando pelos vetores tangente e normal. Alguns simples desenhos podem ser vistos nesse site .","title":"Planos"},{"location":"curvas/space-curves/#consequencias","text":"","title":"Consequ\u00eancias"},{"location":"curvas/space-curves/#curva-plana-e-torcao-nula","text":"Seja \\alpha : I \\to \\mathbb{R}^3 uma curva 2-regular, parametrizada por comprimento de arco. Ent\u00e3o, \\alpha \u00e9 plana se, e somente se, sua tor\u00e7\u00e3o \\tau_{\\alpha} \u00e9 identicamente nula. A demonstra\u00e7\u00e3o desse fato se divide na ida e volta. Supondo que a curva seja plana, devemos inserir a curva em um plano. Assim, para cada s \\in I, \\langle \\alpha(s) - p, v \\rangle = 0 para algum p nesse plano. Em particular, obtermos que v = \\pm B_{\\alpha}(s) , pois obteremos, derivando, que a tangente e a normal s\u00e3o ortogonais a v . Nesse caso B'_{\\alpha}(s) = 0 , pois esse vetor ser\u00e1 constante. A rec\u00edproca usa o fato que B_{\\alpha}(s) ser\u00e1 constante e quer se provar que f(s) = \\langle \\alpha(s) - \\alpha(s_0), B_{\\alpha}(s) \\rangle \\equiv 0 .","title":"Curva plana e tor\u00e7\u00e3o nula"},{"location":"curvas/space-curves/#circunferencia-e-curvatura-constante","text":"Seja \\alpha uma curva parametrizada pelo comprimento de arco em \\mathbb{R}^3 com consntate curvatura e tor\u00e7\u00e3o nula. Ent\u00e3o \\alpha \u00e9 parametriza\u00e7\u00e3o de (parte de) um c\u00edrculo. Sabemos pelo item anterior que estaremos em um plano. A ideia \u00e9 provar que \\alpha - \\frac{1}{\\kappa_{\\alpha}}n \u00e9 um vetor constante para provarmos que a curva est\u00e1 contida em uma esfera. Assim, basta provar que a curva est\u00e1 contida na intersec\u00e7\u00e3o de uma esfera e um plano.","title":"Circunfer\u00eancia e curvatura constante"},{"location":"curvas/space-curves/#teorema-fundamental-da-teoria-local-das-curvas-espaciais","text":"Sejam \\alpha(s) e \\gamma(s) duas curvas parametrizadas pelo comprimento de arco em \\mathbb{R}^3 com a mesma curvatura \\kappa(s) > 0 e a mesma tor\u00e7\u00e3o \\tau(s), \\forall s . Ent\u00e3o, existe um movimento r\u00edgido direto M tal que \\alpha(s) = M(\\gamma(s)), \\forall s . Al\u00e9m disso, se \\kappa e \\tau s\u00e3o fun\u00e7\u00f5es suaves, tal que \\kappa > 0 em toda parte, existe uma curva parametrizada pelo comprimento de arco em \\mathbb{R}^3 cuja curvatura \u00e9 \\kappa e cuja tor\u00e7\u00e3o \u00e9 \\tau . A demonstra\u00e7\u00e3o desse teorema super importante pode ser encontrada na p\u00e1gina 52 do livro do Pressley de Introdu\u00e7\u00e3o \u00e0 Geometria Diferencial. \u00c9 uma aplica\u00e7\u00e3o do Teorema da Exist\u00eancia e Unicidade de Equa\u00e7\u00f5es Diferenciais nas Equa\u00e7\u00f5es de Frenet-Sarret.","title":"Teorema Fundamental da Teoria Local das Curvas Espaciais"},{"location":"curvas/surface-curvature/","text":"Curvaturas de Superf\u00edcies Curvaturas Gaussiana e m\u00e9dia Defini\u00e7\u00e3o: Seja \\mathcal{W} o mapa de Weingarten de uma superf\u00edcie orientada \\mathcal{S} e p \\in \\mathcal{S} . A curvatura Gaussiana K e a curvatura m\u00e9dia H de \\mathcal{S} em p s\u00e3o K = \\det(\\mathcal{W}), \\; \\; H = \\frac{1}{2}\\operatorname{tra\u00e7o}(\\mathcal{W}). Defina, considerando a primeira e segunda formas fundamentais, \\mathcal{F}_I = \\begin{pmatrix} E & F \\\\ F & G \\end{pmatrix}, \\; \\; \\mathcal{F}_{II} = \\begin{pmatrix} e & f \\\\ f & g \\end{pmatrix}. Proposi\u00e7\u00e3o: Seja \\sigma uma parametriza\u00e7\u00e3o da superf\u00edcie orientada \\mathcal{S} . Ent\u00e3o a matriz do mapa de Weingarten com respeito a base \\{\\sigma_u, \\sigma_v\\} de T_pS \u00e9 \\mathcal{F}_I^{-1}\\mathcal{F}_{II} . Corol\u00e1rio: H = \\frac{eG - 2fF + gE}{2(EF - F^2)}, \\; \\; K = \\frac{eg - f^2}{EG - F^2} Curvaturas principais Seja p \\in \\mathcal{S} . Existem \\kappa_1, \\kappa_2 e uma base \\{u, v\\} do plano tangente T_p\\mathcal{S} tal que \\mathcal{W}(u) = \\kappa_1u, \\; \\; \\mathcal{W}(v) = \\kappa_2v, em outras palavras, o mapa de Weingarten possui autovalores e autovetores. As curvaturas principais de \\mathcal{S} s\u00e3o os autovalores do mapa, e u, v s\u00e3o os vetores principais correspondentes. Pontos umb\u00edlicos (ou umbilicais): Pontos em que \\kappa_1 = \\kappa_2 . Em particular, p \u00e9 umb\u00edlico se, e somente se, o mapa de Weinngarten \u00e9 um mapa identidade multiplicado por um escalar. N Proposi\u00e7\u00e3o: As curvaturas principais em um ponto da superf\u00edcie s\u00e3o os valores m\u00e1ximo e m\u00ednimo da curvatura normal de todas as curvas da superf\u00edcie que passam pelo ponto. A demonstra\u00e7\u00e3o dessa proposi\u00e7\u00e3o utiliza o Teorema de Euler que afirma que \\kappa_n = \\kappa_1 \\cos^2(\\theta) + \\kappa_2 \\sin^2(\\theta), onde \\theta \u00e9 o \u00e2ngulo orientado \\hat{u\\dot{\\gamma}} . Proposi\u00e7\u00e3o: Seja \\mathcal{S} uma superf\u00edcie conectada em que todo ponto \u00e9 umb\u00edlico. Ent\u00e3o \\mathcal{S} \u00e9 um conjunto aberto da esfera ou do plano.","title":"Curvaturas de Superf\u00edcies"},{"location":"curvas/surface-curvature/#curvaturas-de-superficies","text":"","title":"Curvaturas de Superf\u00edcies"},{"location":"curvas/surface-curvature/#curvaturas-gaussiana-e-media","text":"Defini\u00e7\u00e3o: Seja \\mathcal{W} o mapa de Weingarten de uma superf\u00edcie orientada \\mathcal{S} e p \\in \\mathcal{S} . A curvatura Gaussiana K e a curvatura m\u00e9dia H de \\mathcal{S} em p s\u00e3o K = \\det(\\mathcal{W}), \\; \\; H = \\frac{1}{2}\\operatorname{tra\u00e7o}(\\mathcal{W}). Defina, considerando a primeira e segunda formas fundamentais, \\mathcal{F}_I = \\begin{pmatrix} E & F \\\\ F & G \\end{pmatrix}, \\; \\; \\mathcal{F}_{II} = \\begin{pmatrix} e & f \\\\ f & g \\end{pmatrix}. Proposi\u00e7\u00e3o: Seja \\sigma uma parametriza\u00e7\u00e3o da superf\u00edcie orientada \\mathcal{S} . Ent\u00e3o a matriz do mapa de Weingarten com respeito a base \\{\\sigma_u, \\sigma_v\\} de T_pS \u00e9 \\mathcal{F}_I^{-1}\\mathcal{F}_{II} . Corol\u00e1rio: H = \\frac{eG - 2fF + gE}{2(EF - F^2)}, \\; \\; K = \\frac{eg - f^2}{EG - F^2}","title":"Curvaturas Gaussiana e m\u00e9dia"},{"location":"curvas/surface-curvature/#curvaturas-principais","text":"Seja p \\in \\mathcal{S} . Existem \\kappa_1, \\kappa_2 e uma base \\{u, v\\} do plano tangente T_p\\mathcal{S} tal que \\mathcal{W}(u) = \\kappa_1u, \\; \\; \\mathcal{W}(v) = \\kappa_2v, em outras palavras, o mapa de Weingarten possui autovalores e autovetores. As curvaturas principais de \\mathcal{S} s\u00e3o os autovalores do mapa, e u, v s\u00e3o os vetores principais correspondentes. Pontos umb\u00edlicos (ou umbilicais): Pontos em que \\kappa_1 = \\kappa_2 . Em particular, p \u00e9 umb\u00edlico se, e somente se, o mapa de Weinngarten \u00e9 um mapa identidade multiplicado por um escalar. N Proposi\u00e7\u00e3o: As curvaturas principais em um ponto da superf\u00edcie s\u00e3o os valores m\u00e1ximo e m\u00ednimo da curvatura normal de todas as curvas da superf\u00edcie que passam pelo ponto. A demonstra\u00e7\u00e3o dessa proposi\u00e7\u00e3o utiliza o Teorema de Euler que afirma que \\kappa_n = \\kappa_1 \\cos^2(\\theta) + \\kappa_2 \\sin^2(\\theta), onde \\theta \u00e9 o \u00e2ngulo orientado \\hat{u\\dot{\\gamma}} . Proposi\u00e7\u00e3o: Seja \\mathcal{S} uma superf\u00edcie conectada em que todo ponto \u00e9 umb\u00edlico. Ent\u00e3o \\mathcal{S} \u00e9 um conjunto aberto da esfera ou do plano.","title":"Curvaturas principais"},{"location":"curvas/curvature/curvature/","text":"Curvatura de uma curva import sympy as sp import numpy as np from scipy.integrate import solve_ivp import matplotlib.pyplot as plt Curvatura Seja \\gamma uma curva parametrizada pelo comprimeto de arco. Definimos curvatura como a fun\u00e7\u00e3o \\kappa(t) = ||\\ddot{\\gamma}(t)|| . Essa defini\u00e7\u00e3o \u00e9 consistente com o que esper\u00e1vamos de uma reta (curvatura nula) e de um c\u00edrculo (curvatura constante e inversamente proporcional ao raio). Al\u00e9m disso, se \\gamma \u00e9 uma curva regular qualquer, ela tem uma reparametriza\u00e7\u00e3o pelo comprimento de arco. Portanto, podemos definir a sua curvatura como sendo a curvatura de sua reparametriza\u00e7\u00e3o pelo comprimento de arco. Isto \u00e9, seja \\hat{\\gamma} uma reparametriza\u00e7\u00e3o pelo comprimento de arco de \\gamma com curvatura \\kappa . Ent\u00e3o a curvatura de \\gamma ser\u00e1 \\kappa . Uma quest\u00e3o que se levanta \u00e9: e se houver outra reparametriza\u00e7\u00e3o pelo comprimento de arco para \\gamma ? Para isso, precisamos mostrar que a curvatura \u00e9 invariante (n\u00e3o muda) segundo a reparametriza\u00e7\u00e3o. Isso n\u00e3o \u00e9 dific\u00edl de ver, pois as tangentes das reparametriza\u00e7\u00f5es t\u00eam mesmo tamanho e, possivelmente, diferentes sinal. Curvatura de uma curva regular Seja \\gamma(t) uma curva em \\mathbb{R}^3 , ent\u00e3o sua curvatura \u00e9 dada pela express\u00e3o \\kappa = \\frac{||\\ddot{\\gamma} \\times \\dot{\\gamma}||}{||\\dot{\\gamma}||^3} Observe que para curvas no plano essa express\u00e3o pode tamb\u00e9m ser utilizada, \\kappa = \\frac{|\\ddot{\\gamma}_1\\dot{\\gamma}_2 - \\ddot{\\gamma}_2\\dot{\\gamma}_1|}{||\\dot{\\gamma}||^3} Curvatura com sinal Definimos a normal unit\u00e1ria com sinal n(s) o vetor unit\u00e1rio que rotaciona o vetor tangente no sentido anti-hor\u00e1rio em \\pi/2 . Em particular, \\ddot{\\gamma}(s) e \\dot{\\gamma}(s) s\u00e3o perpendiculares (pois a curva \u00e9 parametrizada pelo comprimento de arco) e, portanto, \u00e9 paralelo a n(s) , e assim \\ddot{\\gamma}(s) = \\kappa_s(s) n(s) Chamamos \\kappa_s de curvatura com sinal . Em particular, \\kappa = |\\kappa_s| . Fun\u00e7\u00e3o \u00c2ngulo Dada uma curva diferenci\u00e1vel \\gamma: I \\to \\mathcal{S}^1 , onde \\mathcal{S}^1 \u00e9 o c\u00edrculo centrado na origem, dizemos que \\theta : I \\to \\mathbb{R} \u00e9 fun\u00e7\u00e3o \u00e2ngulo de \\gamma quando \\gamma(s) = (\\cos(\\theta(s)), \\sin(\\theta(s)), \\forall s \\in I Observe que nessa defini\u00e7\u00e3o, a imagem de \\gamma \u00e9 um subconjunto de \\mathcal{S}^1 , como se fosse um arco. Por exemplo, \\gamma(s) = (\\cos(2s), \\sin(2s)) \\implies \\theta(s) = 2s . Considere o operador que rotaciona no sentido anti-hor\u00e1rio em 90\u00b0 um vetor. Podemos descrev\u00ea-lo em forma matricial como J = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} Defina o determinante entre dois vetores como det(v,w) = \\langle Jv , w \\rangle que \u00e9 o produto interno do vetor v rotacionado e w . Diferenciabilidade Seja \\gamma : I \\to \\mathcal{S}^1 uma curva diferenci\u00e1vel. Ent\u00e3o \\gamma admite uma fun\u00e7\u00e3o \u00e2ngulo \\theta diferenci\u00e1vel. Al\u00e9m disso, se \\hat{\\theta} \u00e9 fun\u00e7\u00e3o \u00e2ngulo diferenci\u00e1vel de \\gamma , ela difere de \\theta por uma constante. Note que supondo a exist\u00eancia dessa fun\u00e7\u00e3o diferenci\u00e1vel, temos que, por aplica\u00e7\u00e3o da Regra da Cadeia, \\gamma '(s) = \\theta '(s)(-\\sin(\\theta(s)), \\cos(\\theta(s))) = \\theta '(s) J\\gamma(s) Portanto, aplicando o produto interno em ambos os lados, observamos que \\theta '(s) = det(\\gamma(s), \\gamma '(s)) Assim, a demonstra\u00e7\u00e3o se d\u00e1 defininido \\theta com essa derivada (Teorema Fundamental do C\u00e1lculo). Agora seja \\alpha uma curva regular, sem perda de generalidade, parametrizada pelo comprimento de arco. Seja t(s) = \\alpha '(s) . Como ||t(s)|| = 1 , pela proposi\u00e7\u00e3o anterior, existe uma fun\u00e7\u00e3o \u00e2ngulo diferenci\u00e1vel \\theta de forma que definimos a curvatura de \\alpha como k(s) = \\theta '(s) = det(t(s), t'(s)) = det(\\alpha'(s), \\alpha''(s)) Observa\u00e7\u00e3o 1: Se \\alpha \u00e9 regular, sua curvatura \u00e9 \\kappa(s) = \\frac{det(\\alpha'(s), \\alpha''(s))}{||\\alpha '(s)||^3} Observa\u00e7\u00e3o 2: Estamos rotacionando o vetor tangente, obtendo o que chamamos de vetor normal unit\u00e1rio e fazendo o produto interno com a acelera\u00e7\u00e3o da curva, o que coincide com a defini\u00e7\u00e3o pr\u00e9via! Exemplo: Considere a parametriza\u00e7\u00e3o do c\u00edrculo \\alpha(s) = p + r(\\cos(s/r), \\sin(s/r)), s \\in \\mathbb{R} . Assim \\alpha'(s) = (-\\sin(s/r), \\cos(s/r)) \\alpha''(s) = -\\frac{1}{r}(\\cos(s/r), \\sin(s/r)) = \\frac{1}{r}J\\alpha '(s) \\kappa(s) = \\langle J\\alpha '(s), \\alpha ''(s) \\rangle = \\frac{1}{r} Exemplo 2: Vamos usar python para calcular a curvatura da espiral equiangular z(t) = e^{(a + i)t} onde a \u00e9 uma constante e i^2 = -1 . # Definimos as vari\u00e1veis t = sp.symbols('t', real = True) a = sp.symbols('a', real = True, constant = True) # Definimos a fun\u00e7\u00e3o z z = sp.exp((a + sp.I)*t) # Derivando zt = sp.diff(z, t) ztt = sp.diff(zt, t) # Rotaciona zt Jzt = zt*sp.exp(sp.I*sp.pi/2) Observe que nossa curva est\u00e1 definida no plano complexo (isomorfo ao plano real). Para calcular a curvatura de uma curva, rotacionamos o vetor e fazemos o produto dot entre esses n\u00fameros como produto escalar de dois vetores. Nesse caso, teremos que \\langle z_1, z_2 \\rangle = Re(z_1\\cdot\\bar{z}_2) . k = sp.re(Jzt*sp.conjugate(ztt))/sp.Abs(zt)**3 k \\displaystyle \\frac{\\left(2 a^{2} e^{2 a t} - \\left(a^{2} - 1\\right) e^{2 a t}\\right) e^{- 3 a t}}{\\left(a^{2} + 1\\right)^{\\frac{3}{2}}} Precisamos indicar para o programa que queremos uma resposta fatorizada k = sp.factor(k) k \\displaystyle \\frac{e^{- a t}}{\\sqrt{a^{2} + 1}} Assim, essa \u00e9 a curvatura da espiral equiangular. Movimento R\u00edgido Isometria: Uma aplica\u00e7\u00e3o F: \\mathbb{R}^n \\to \\mathbb{R}^n que preserva dist\u00e2ncia, isto \u00e9, ||x - y|| = ||F(x) - F(y)||, \\forall x, y \\in \\mathbb{R}^n . Diremos uma movimento r\u00edgido \u00e9 uma isometria (a rigidez em mudar dist\u00e2ncias). Transla\u00e7\u00e3o: Uma aplica\u00e7\u00e3o F: \\mathbb{R}^n \\to \\mathbb{R}^n do tipo v \\mapsto F(v) := v + a , para algum a \\in \\mathbb{R}^n fixo. Transforma\u00e7\u00e3o Ortogonal: Uma transforma\u00e7\u00e3o linear que presetva o produto interno, isto \u00e9, \\langle u, v \\rangle = \\langle T(u), T(v) \\rangle . A matriz associada a essa transforma\u00e7\u00e3o \u00e9 ortogonal. Dizemos que o movimento \u00e9 direto se det(P) = 1 e oposto ou inverso quando det(P) = -1 . Teorema Seja P_{n\\times n} uma matriz ortogonal e a \\in \\mathbb{R}^n . Ent\u00e3o F: \\mathbb{R}^n \\to \\mathbb{R}^n definido como F(v) = Pv + a \u00e9 uma isometria. Reciprocamente, toda isometria pode ser escrito nessa forma. Esse teorema permite uma caracteriza\u00e7\u00e3o simples de um movimento r\u00edgido. Invari\u00e2ncia da curvatura Sejam \\Phi = A + p_0 um movimento r\u00edgido direto de R^2 e \\alpha : I \\to \\mathbb{R}^2 uma curva regular parametrizada por comprimento de arco. Ent\u00e3o, \\beta = \\Phi \\circ \\alpha : I \\to \\mathbb{R}^2 \u00e9 uma curva regular de \\mathbb{R}^2 , parametrizada por comprimento de arco, cuja fun\u00e7\u00e3o curvatura coincide com a de \\alpha , isto \u00e9, \\kappa_{\\alpha}(s) = \\kappa_{\\beta}(s) \\forall s \\in I . Observe que derivar \\beta ' = \\Phi'(\\alpha) = A\\alpha ' o que garante que \\beta \u00e9 parametrizada pelo comprimento de arco e que \\beta '' = A\\alpha '' . Portanto, como det(A) = 1 , vale que as curvaturas s\u00e3o as mesmas. Equa\u00e7\u00f5es de Frenet Seja \\alpha uma curva parametrizada pelo compimento de arco com vetor normal n(s) e vetor tangente t(s) . Observe que para cada s , esses vetores formam um base ortonormal para \\mathbb{R}^2 . Chamamos essa base de diedro de Frenet . J\u00e1 definimos curvatura com sinal \\kappa quando t'(s) = \\kappa(s) n(s) . Al\u00e9m disso, ||n(s)|| = 1 \\implies n(s) \\perp n'(s) . Portanto n'(s) \u00e9 paralelo a t(s) . Logo n'(s) = \\langle n'(s), t(s) \\rangle t(s) e: \\langle n'(s), t(s) \\rangle = \\langle Jt'(s), t(s) \\rangle = -\\langle t'(s), Jt(s) \\rangle = -det(t(s), t'(s)) = -\\kappa(s) Assim obtemos as equa\u00e7\u00f5es de Frenet : t' = \\kappa \\cdot n n' = -\\kappa \\cdot t As equa\u00e7\u00f5es de Frenet s\u00e3o, portanto, um sistema de equa\u00e7\u00f5es diferenciais envolvendo a base ortonormal para cada s . Teorema Fundamental das Curvas no Plano Sejam I um intervalo aberto e \\kappa : I \\to \\mathbb{R} uma fun\u00e7\u00e3o diferenci\u00e1vel. Ent\u00e3o, existe uma curva diferenci\u00e1vel, \\alpha : I \\to \\mathbb{R}^2 , parametrizada por comprimento de arco, cuja fun\u00e7\u00e3o curvatura \\kappa_{\\alpha} coincide com \\kappa . Al\u00e9m disso, para toda curva \\beta : I \\to \\mathbb{R}^2 , parametrizada por comprimento de arco, que cumpre \\kappa_{\\beta} = \\kappa , existe um movimento r\u00edgido \\Phi : \\mathbb{R}^2 \\to \\mathbb{R}^2 , tal que \\alpha = \\Phi \\circ \\beta . Exemplo: (Reconstru\u00e7\u00e3o de uma curva plana) Suponha que nos \u00e9 dado uma curvatura \\kappa . Como \\kappa = \\theta ' e \\alpha' = (\\cos(\\theta), \\sin(\\theta)) . Vamos supor que o intervalo \u00e9 do tipo [0,l] , onde l \u00e9 o comprimento da curva. def get_curve_from_curvature(k, tf, x0, y0): # \u00e2ngulo theta_ = solve_ivp(fun = k, t_span = (0,tf), y0 = [0], t_eval = np.arange(0,tf+1e-4,0.01) ).y[0] def theta(t): li = int(t*(len(theta_)-1)/tf) gi = min(int(t*(len(theta_)-1)/tf) + 1, len(theta_)-1) convex = t*len(theta_)/tf - li return (1-convex)*theta_[li] + convex*theta_[gi] # Componentes def f(t,x): return [np.cos(theta(t)), np.sin(theta(t))] # Curva alpha = solve_ivp(fun = f, t_span = (0,tf), y0 = [x0,y0], t_eval = np.arange(0,tf+1e-4,0.01) ).y return alpha C\u00edrculo # Curvatura k = lambda t,x: 1 # Um ponto em que passa a curva x0 = 0 y0 = 0 # Comprimento da curva tf = 2*np.pi+0.1 alpha = get_curve_from_curvature(k,tf,x0,y0) plt.figure(figsize = (6,6)) plt.plot(alpha[0,:], alpha[1,:]) plt.show() Reta # Curvatura k = lambda t,x: 0 # Um ponto em que passa a curva x0 = 0 y0 = 0 # Comprimento da curva tf = 10*np.pi+0.1 alpha = get_curve_from_curvature(k,tf,x0,y0) plt.figure(figsize = (6,6)) plt.plot(alpha[0,:], alpha[1,:]) plt.show() Clotoide # Curvatura k = lambda t,x: t # Um ponto em que passa a curva x0 = 0 y0 = 0 # Comprimento da curva tf = 10*np.pi+0.1 alpha = get_curve_from_curvature(k,tf,x0,y0) plt.figure(figsize = (6,6)) plt.plot(alpha[0,:], alpha[1,:]) plt.show()","title":"Curvatura de uma curva"},{"location":"curvas/curvature/curvature/#curvatura-de-uma-curva","text":"import sympy as sp import numpy as np from scipy.integrate import solve_ivp import matplotlib.pyplot as plt","title":"Curvatura de uma curva"},{"location":"curvas/curvature/curvature/#curvatura","text":"Seja \\gamma uma curva parametrizada pelo comprimeto de arco. Definimos curvatura como a fun\u00e7\u00e3o \\kappa(t) = ||\\ddot{\\gamma}(t)|| . Essa defini\u00e7\u00e3o \u00e9 consistente com o que esper\u00e1vamos de uma reta (curvatura nula) e de um c\u00edrculo (curvatura constante e inversamente proporcional ao raio). Al\u00e9m disso, se \\gamma \u00e9 uma curva regular qualquer, ela tem uma reparametriza\u00e7\u00e3o pelo comprimento de arco. Portanto, podemos definir a sua curvatura como sendo a curvatura de sua reparametriza\u00e7\u00e3o pelo comprimento de arco. Isto \u00e9, seja \\hat{\\gamma} uma reparametriza\u00e7\u00e3o pelo comprimento de arco de \\gamma com curvatura \\kappa . Ent\u00e3o a curvatura de \\gamma ser\u00e1 \\kappa . Uma quest\u00e3o que se levanta \u00e9: e se houver outra reparametriza\u00e7\u00e3o pelo comprimento de arco para \\gamma ? Para isso, precisamos mostrar que a curvatura \u00e9 invariante (n\u00e3o muda) segundo a reparametriza\u00e7\u00e3o. Isso n\u00e3o \u00e9 dific\u00edl de ver, pois as tangentes das reparametriza\u00e7\u00f5es t\u00eam mesmo tamanho e, possivelmente, diferentes sinal.","title":"Curvatura"},{"location":"curvas/curvature/curvature/#curvatura-de-uma-curva-regular","text":"Seja \\gamma(t) uma curva em \\mathbb{R}^3 , ent\u00e3o sua curvatura \u00e9 dada pela express\u00e3o \\kappa = \\frac{||\\ddot{\\gamma} \\times \\dot{\\gamma}||}{||\\dot{\\gamma}||^3} Observe que para curvas no plano essa express\u00e3o pode tamb\u00e9m ser utilizada, \\kappa = \\frac{|\\ddot{\\gamma}_1\\dot{\\gamma}_2 - \\ddot{\\gamma}_2\\dot{\\gamma}_1|}{||\\dot{\\gamma}||^3}","title":"Curvatura de uma curva regular"},{"location":"curvas/curvature/curvature/#curvatura-com-sinal","text":"Definimos a normal unit\u00e1ria com sinal n(s) o vetor unit\u00e1rio que rotaciona o vetor tangente no sentido anti-hor\u00e1rio em \\pi/2 . Em particular, \\ddot{\\gamma}(s) e \\dot{\\gamma}(s) s\u00e3o perpendiculares (pois a curva \u00e9 parametrizada pelo comprimento de arco) e, portanto, \u00e9 paralelo a n(s) , e assim \\ddot{\\gamma}(s) = \\kappa_s(s) n(s) Chamamos \\kappa_s de curvatura com sinal . Em particular, \\kappa = |\\kappa_s| .","title":"Curvatura com sinal"},{"location":"curvas/curvature/curvature/#funcao-angulo","text":"Dada uma curva diferenci\u00e1vel \\gamma: I \\to \\mathcal{S}^1 , onde \\mathcal{S}^1 \u00e9 o c\u00edrculo centrado na origem, dizemos que \\theta : I \\to \\mathbb{R} \u00e9 fun\u00e7\u00e3o \u00e2ngulo de \\gamma quando \\gamma(s) = (\\cos(\\theta(s)), \\sin(\\theta(s)), \\forall s \\in I Observe que nessa defini\u00e7\u00e3o, a imagem de \\gamma \u00e9 um subconjunto de \\mathcal{S}^1 , como se fosse um arco. Por exemplo, \\gamma(s) = (\\cos(2s), \\sin(2s)) \\implies \\theta(s) = 2s . Considere o operador que rotaciona no sentido anti-hor\u00e1rio em 90\u00b0 um vetor. Podemos descrev\u00ea-lo em forma matricial como J = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} Defina o determinante entre dois vetores como det(v,w) = \\langle Jv , w \\rangle que \u00e9 o produto interno do vetor v rotacionado e w .","title":"Fun\u00e7\u00e3o \u00c2ngulo"},{"location":"curvas/curvature/curvature/#diferenciabilidade","text":"Seja \\gamma : I \\to \\mathcal{S}^1 uma curva diferenci\u00e1vel. Ent\u00e3o \\gamma admite uma fun\u00e7\u00e3o \u00e2ngulo \\theta diferenci\u00e1vel. Al\u00e9m disso, se \\hat{\\theta} \u00e9 fun\u00e7\u00e3o \u00e2ngulo diferenci\u00e1vel de \\gamma , ela difere de \\theta por uma constante. Note que supondo a exist\u00eancia dessa fun\u00e7\u00e3o diferenci\u00e1vel, temos que, por aplica\u00e7\u00e3o da Regra da Cadeia, \\gamma '(s) = \\theta '(s)(-\\sin(\\theta(s)), \\cos(\\theta(s))) = \\theta '(s) J\\gamma(s) Portanto, aplicando o produto interno em ambos os lados, observamos que \\theta '(s) = det(\\gamma(s), \\gamma '(s)) Assim, a demonstra\u00e7\u00e3o se d\u00e1 defininido \\theta com essa derivada (Teorema Fundamental do C\u00e1lculo). Agora seja \\alpha uma curva regular, sem perda de generalidade, parametrizada pelo comprimento de arco. Seja t(s) = \\alpha '(s) . Como ||t(s)|| = 1 , pela proposi\u00e7\u00e3o anterior, existe uma fun\u00e7\u00e3o \u00e2ngulo diferenci\u00e1vel \\theta de forma que definimos a curvatura de \\alpha como k(s) = \\theta '(s) = det(t(s), t'(s)) = det(\\alpha'(s), \\alpha''(s)) Observa\u00e7\u00e3o 1: Se \\alpha \u00e9 regular, sua curvatura \u00e9 \\kappa(s) = \\frac{det(\\alpha'(s), \\alpha''(s))}{||\\alpha '(s)||^3} Observa\u00e7\u00e3o 2: Estamos rotacionando o vetor tangente, obtendo o que chamamos de vetor normal unit\u00e1rio e fazendo o produto interno com a acelera\u00e7\u00e3o da curva, o que coincide com a defini\u00e7\u00e3o pr\u00e9via! Exemplo: Considere a parametriza\u00e7\u00e3o do c\u00edrculo \\alpha(s) = p + r(\\cos(s/r), \\sin(s/r)), s \\in \\mathbb{R} . Assim \\alpha'(s) = (-\\sin(s/r), \\cos(s/r)) \\alpha''(s) = -\\frac{1}{r}(\\cos(s/r), \\sin(s/r)) = \\frac{1}{r}J\\alpha '(s) \\kappa(s) = \\langle J\\alpha '(s), \\alpha ''(s) \\rangle = \\frac{1}{r} Exemplo 2: Vamos usar python para calcular a curvatura da espiral equiangular z(t) = e^{(a + i)t} onde a \u00e9 uma constante e i^2 = -1 . # Definimos as vari\u00e1veis t = sp.symbols('t', real = True) a = sp.symbols('a', real = True, constant = True) # Definimos a fun\u00e7\u00e3o z z = sp.exp((a + sp.I)*t) # Derivando zt = sp.diff(z, t) ztt = sp.diff(zt, t) # Rotaciona zt Jzt = zt*sp.exp(sp.I*sp.pi/2) Observe que nossa curva est\u00e1 definida no plano complexo (isomorfo ao plano real). Para calcular a curvatura de uma curva, rotacionamos o vetor e fazemos o produto dot entre esses n\u00fameros como produto escalar de dois vetores. Nesse caso, teremos que \\langle z_1, z_2 \\rangle = Re(z_1\\cdot\\bar{z}_2) . k = sp.re(Jzt*sp.conjugate(ztt))/sp.Abs(zt)**3 k \\displaystyle \\frac{\\left(2 a^{2} e^{2 a t} - \\left(a^{2} - 1\\right) e^{2 a t}\\right) e^{- 3 a t}}{\\left(a^{2} + 1\\right)^{\\frac{3}{2}}} Precisamos indicar para o programa que queremos uma resposta fatorizada k = sp.factor(k) k \\displaystyle \\frac{e^{- a t}}{\\sqrt{a^{2} + 1}} Assim, essa \u00e9 a curvatura da espiral equiangular.","title":"Diferenciabilidade"},{"location":"curvas/curvature/curvature/#movimento-rigido","text":"Isometria: Uma aplica\u00e7\u00e3o F: \\mathbb{R}^n \\to \\mathbb{R}^n que preserva dist\u00e2ncia, isto \u00e9, ||x - y|| = ||F(x) - F(y)||, \\forall x, y \\in \\mathbb{R}^n . Diremos uma movimento r\u00edgido \u00e9 uma isometria (a rigidez em mudar dist\u00e2ncias). Transla\u00e7\u00e3o: Uma aplica\u00e7\u00e3o F: \\mathbb{R}^n \\to \\mathbb{R}^n do tipo v \\mapsto F(v) := v + a , para algum a \\in \\mathbb{R}^n fixo. Transforma\u00e7\u00e3o Ortogonal: Uma transforma\u00e7\u00e3o linear que presetva o produto interno, isto \u00e9, \\langle u, v \\rangle = \\langle T(u), T(v) \\rangle . A matriz associada a essa transforma\u00e7\u00e3o \u00e9 ortogonal. Dizemos que o movimento \u00e9 direto se det(P) = 1 e oposto ou inverso quando det(P) = -1 .","title":"Movimento R\u00edgido"},{"location":"curvas/curvature/curvature/#teorema","text":"Seja P_{n\\times n} uma matriz ortogonal e a \\in \\mathbb{R}^n . Ent\u00e3o F: \\mathbb{R}^n \\to \\mathbb{R}^n definido como F(v) = Pv + a \u00e9 uma isometria. Reciprocamente, toda isometria pode ser escrito nessa forma. Esse teorema permite uma caracteriza\u00e7\u00e3o simples de um movimento r\u00edgido.","title":"Teorema"},{"location":"curvas/curvature/curvature/#invariancia-da-curvatura","text":"Sejam \\Phi = A + p_0 um movimento r\u00edgido direto de R^2 e \\alpha : I \\to \\mathbb{R}^2 uma curva regular parametrizada por comprimento de arco. Ent\u00e3o, \\beta = \\Phi \\circ \\alpha : I \\to \\mathbb{R}^2 \u00e9 uma curva regular de \\mathbb{R}^2 , parametrizada por comprimento de arco, cuja fun\u00e7\u00e3o curvatura coincide com a de \\alpha , isto \u00e9, \\kappa_{\\alpha}(s) = \\kappa_{\\beta}(s) \\forall s \\in I . Observe que derivar \\beta ' = \\Phi'(\\alpha) = A\\alpha ' o que garante que \\beta \u00e9 parametrizada pelo comprimento de arco e que \\beta '' = A\\alpha '' . Portanto, como det(A) = 1 , vale que as curvaturas s\u00e3o as mesmas.","title":"Invari\u00e2ncia da curvatura"},{"location":"curvas/curvature/curvature/#equacoes-de-frenet","text":"Seja \\alpha uma curva parametrizada pelo compimento de arco com vetor normal n(s) e vetor tangente t(s) . Observe que para cada s , esses vetores formam um base ortonormal para \\mathbb{R}^2 . Chamamos essa base de diedro de Frenet . J\u00e1 definimos curvatura com sinal \\kappa quando t'(s) = \\kappa(s) n(s) . Al\u00e9m disso, ||n(s)|| = 1 \\implies n(s) \\perp n'(s) . Portanto n'(s) \u00e9 paralelo a t(s) . Logo n'(s) = \\langle n'(s), t(s) \\rangle t(s) e: \\langle n'(s), t(s) \\rangle = \\langle Jt'(s), t(s) \\rangle = -\\langle t'(s), Jt(s) \\rangle = -det(t(s), t'(s)) = -\\kappa(s) Assim obtemos as equa\u00e7\u00f5es de Frenet : t' = \\kappa \\cdot n n' = -\\kappa \\cdot t As equa\u00e7\u00f5es de Frenet s\u00e3o, portanto, um sistema de equa\u00e7\u00f5es diferenciais envolvendo a base ortonormal para cada s .","title":"Equa\u00e7\u00f5es de Frenet"},{"location":"curvas/curvature/curvature/#teorema-fundamental-das-curvas-no-plano","text":"Sejam I um intervalo aberto e \\kappa : I \\to \\mathbb{R} uma fun\u00e7\u00e3o diferenci\u00e1vel. Ent\u00e3o, existe uma curva diferenci\u00e1vel, \\alpha : I \\to \\mathbb{R}^2 , parametrizada por comprimento de arco, cuja fun\u00e7\u00e3o curvatura \\kappa_{\\alpha} coincide com \\kappa . Al\u00e9m disso, para toda curva \\beta : I \\to \\mathbb{R}^2 , parametrizada por comprimento de arco, que cumpre \\kappa_{\\beta} = \\kappa , existe um movimento r\u00edgido \\Phi : \\mathbb{R}^2 \\to \\mathbb{R}^2 , tal que \\alpha = \\Phi \\circ \\beta . Exemplo: (Reconstru\u00e7\u00e3o de uma curva plana) Suponha que nos \u00e9 dado uma curvatura \\kappa . Como \\kappa = \\theta ' e \\alpha' = (\\cos(\\theta), \\sin(\\theta)) . Vamos supor que o intervalo \u00e9 do tipo [0,l] , onde l \u00e9 o comprimento da curva. def get_curve_from_curvature(k, tf, x0, y0): # \u00e2ngulo theta_ = solve_ivp(fun = k, t_span = (0,tf), y0 = [0], t_eval = np.arange(0,tf+1e-4,0.01) ).y[0] def theta(t): li = int(t*(len(theta_)-1)/tf) gi = min(int(t*(len(theta_)-1)/tf) + 1, len(theta_)-1) convex = t*len(theta_)/tf - li return (1-convex)*theta_[li] + convex*theta_[gi] # Componentes def f(t,x): return [np.cos(theta(t)), np.sin(theta(t))] # Curva alpha = solve_ivp(fun = f, t_span = (0,tf), y0 = [x0,y0], t_eval = np.arange(0,tf+1e-4,0.01) ).y return alpha","title":"Teorema Fundamental das Curvas no Plano"},{"location":"curvas/curvature/curvature/#circulo","text":"# Curvatura k = lambda t,x: 1 # Um ponto em que passa a curva x0 = 0 y0 = 0 # Comprimento da curva tf = 2*np.pi+0.1 alpha = get_curve_from_curvature(k,tf,x0,y0) plt.figure(figsize = (6,6)) plt.plot(alpha[0,:], alpha[1,:]) plt.show()","title":"C\u00edrculo"},{"location":"curvas/curvature/curvature/#reta","text":"# Curvatura k = lambda t,x: 0 # Um ponto em que passa a curva x0 = 0 y0 = 0 # Comprimento da curva tf = 10*np.pi+0.1 alpha = get_curve_from_curvature(k,tf,x0,y0) plt.figure(figsize = (6,6)) plt.plot(alpha[0,:], alpha[1,:]) plt.show()","title":"Reta"},{"location":"curvas/curvature/curvature/#clotoide","text":"# Curvatura k = lambda t,x: t # Um ponto em que passa a curva x0 = 0 y0 = 0 # Comprimento da curva tf = 10*np.pi+0.1 alpha = get_curve_from_curvature(k,tf,x0,y0) plt.figure(figsize = (6,6)) plt.plot(alpha[0,:], alpha[1,:]) plt.show()","title":"Clotoide"},{"location":"edo/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de Equa\u00e7\u00f5es Diferenciais Ordin\u00e1rias correspondente ao per\u00edodo de 2020.1. Os temas abordados s\u00e3o: Equa\u00e7\u00f5es diferenciais lineares de 1\u00aa ordem. Equa\u00e7\u00f5es com vari\u00e1veis separ\u00e1veis. Equa\u00e7\u00f5es exatas. Equa\u00e7\u00f5es de 2\u00aa ordem. Modelos de din\u00e2mica populacional e ci\u00eancias naturais. Sistemas lineares homog\u00eaneos e n\u00e3o homog\u00eaneos. Sistemas n\u00e3o lineares: an\u00e1lise qualitativa. M\u00e9todos num\u00e9ricos. Grava\u00e7\u00f5es Grava\u00e7\u00f5es indispon\u00edveis. Resumos Os conte\u00fados referentes \u00e0 primeira parte do curso se encontram resumidos e escritos em Latex . Sistemas Lineares Exponencial de uma matriz : Desci\u00e7\u00e3o da exponencial de uma matriz para resolver sistemas lineares e caracteriza\u00e7\u00e3o no \\mathbb{R}^2 . Autovalores complexos e repetidos : An\u00e1lise no \\mathbb{R}^2 de sistemas lineares com autovalores complexos ou repetidos. Sistemas lineares, uma revis\u00e3o : Revis\u00e3o geral sobre o t\u00f3pico com exerc\u00edcios espec\u00edficos para cada caso de matriz em \\mathbb{R}^2 . Teorema de Exist\u00eancia e Unicidade Proposta de demonstra\u00e7\u00e3o : Rascunho de uma das demonstra\u00e7\u00f5es mais interessantes sobre o teorema. Sistemas n\u00e3o lineares Aproxima\u00e7\u00e3o pr\u00f3ximo \u00e0s singularidades : Uso de um variante do teorema de Hartman-Grobman. Plano Tra\u00e7o-Determinante : Revis\u00e3o do gr\u00e1fico que resume os sistemas lineares e nos ajudam a identificar o sistema pr\u00f3ximo do equil\u00edbrio. M\u00e9todos num\u00e9ricos : Simples revis\u00e3o dos m\u00e9todos de Euler e Runge-Kutta. Modelos com EDOs Modelos : Modelo predador-presa e modelos SEIR, com c\u00e1lculo do R_0 . Conte\u00fado Extra Resumos de assuntos do livro Differential equations, dynamical systems, and linear algebra , de Morris W. Hirsch and Stephen Smale. Sistema planar simples : Descri\u00e7\u00e3o dos sistemas lineares planares simples, como caso especial. Sistemas de alta dimens\u00e3o : Generaliza\u00e7\u00e3o do t\u00f3pico anterior. Teorema fundamental de exist\u00eancia e unicidade : Cap\u00edtulo 8 do livro que cobre o principal teorema, sendo uma varia\u00e7\u00e3o do apresentado na monitoria. Estabilidade : Estudos de estabilidade de sistemas n\u00e3o lineares. Cap\u00edtulo 9 do livro. Teorema de Pointcar\u00e9-Bendixson : Teorema sobre \u00f3rbitas que n\u00e3o foi descorrido no curso, mas tema interessante. Avalia\u00e7\u00f5es Per\u00edodo T\u00f3picos Cobertos PDF A1 Equa\u00e7\u00f5es exatas, de 1\u00aa e 2\u00aa ordem e modelagem de problemas [questoes] [respostas] A2 Sistemas lineares homog\u00eaneos e n\u00e3o homog\u00eaneos [respostas]","title":"Equa\u00e7\u00f5es Diferenciais Ordin\u00e1rias"},{"location":"edo/info/#informacoes-gerais","text":"Monitoria de Equa\u00e7\u00f5es Diferenciais Ordin\u00e1rias correspondente ao per\u00edodo de 2020.1. Os temas abordados s\u00e3o: Equa\u00e7\u00f5es diferenciais lineares de 1\u00aa ordem. Equa\u00e7\u00f5es com vari\u00e1veis separ\u00e1veis. Equa\u00e7\u00f5es exatas. Equa\u00e7\u00f5es de 2\u00aa ordem. Modelos de din\u00e2mica populacional e ci\u00eancias naturais. Sistemas lineares homog\u00eaneos e n\u00e3o homog\u00eaneos. Sistemas n\u00e3o lineares: an\u00e1lise qualitativa. M\u00e9todos num\u00e9ricos.","title":"Informa\u00e7\u00f5es Gerais"},{"location":"edo/info/#gravacoes","text":"Grava\u00e7\u00f5es indispon\u00edveis.","title":"Grava\u00e7\u00f5es"},{"location":"edo/info/#resumos","text":"Os conte\u00fados referentes \u00e0 primeira parte do curso se encontram resumidos e escritos em Latex .","title":"Resumos"},{"location":"edo/info/#sistemas-lineares","text":"Exponencial de uma matriz : Desci\u00e7\u00e3o da exponencial de uma matriz para resolver sistemas lineares e caracteriza\u00e7\u00e3o no \\mathbb{R}^2 . Autovalores complexos e repetidos : An\u00e1lise no \\mathbb{R}^2 de sistemas lineares com autovalores complexos ou repetidos. Sistemas lineares, uma revis\u00e3o : Revis\u00e3o geral sobre o t\u00f3pico com exerc\u00edcios espec\u00edficos para cada caso de matriz em \\mathbb{R}^2 .","title":"Sistemas Lineares"},{"location":"edo/info/#teorema-de-existencia-e-unicidade","text":"Proposta de demonstra\u00e7\u00e3o : Rascunho de uma das demonstra\u00e7\u00f5es mais interessantes sobre o teorema.","title":"Teorema de Exist\u00eancia e Unicidade"},{"location":"edo/info/#sistemas-nao-lineares","text":"Aproxima\u00e7\u00e3o pr\u00f3ximo \u00e0s singularidades : Uso de um variante do teorema de Hartman-Grobman. Plano Tra\u00e7o-Determinante : Revis\u00e3o do gr\u00e1fico que resume os sistemas lineares e nos ajudam a identificar o sistema pr\u00f3ximo do equil\u00edbrio. M\u00e9todos num\u00e9ricos : Simples revis\u00e3o dos m\u00e9todos de Euler e Runge-Kutta.","title":"Sistemas n\u00e3o lineares"},{"location":"edo/info/#modelos-com-edos","text":"Modelos : Modelo predador-presa e modelos SEIR, com c\u00e1lculo do R_0 .","title":"Modelos com EDOs"},{"location":"edo/info/#conteudo-extra","text":"Resumos de assuntos do livro Differential equations, dynamical systems, and linear algebra , de Morris W. Hirsch and Stephen Smale. Sistema planar simples : Descri\u00e7\u00e3o dos sistemas lineares planares simples, como caso especial. Sistemas de alta dimens\u00e3o : Generaliza\u00e7\u00e3o do t\u00f3pico anterior. Teorema fundamental de exist\u00eancia e unicidade : Cap\u00edtulo 8 do livro que cobre o principal teorema, sendo uma varia\u00e7\u00e3o do apresentado na monitoria. Estabilidade : Estudos de estabilidade de sistemas n\u00e3o lineares. Cap\u00edtulo 9 do livro. Teorema de Pointcar\u00e9-Bendixson : Teorema sobre \u00f3rbitas que n\u00e3o foi descorrido no curso, mas tema interessante.","title":"Conte\u00fado Extra"},{"location":"edo/info/#avaliacoes","text":"Per\u00edodo T\u00f3picos Cobertos PDF A1 Equa\u00e7\u00f5es exatas, de 1\u00aa e 2\u00aa ordem e modelagem de problemas [questoes] [respostas] A2 Sistemas lineares homog\u00eaneos e n\u00e3o homog\u00eaneos [respostas]","title":"Avalia\u00e7\u00f5es"},{"location":"edp/calculus_of_variations/","text":"C\u00e1lculo de Varia\u00e7\u00f5es Para entender um pouco da intui\u00e7\u00e3o por tr\u00e1s e da sua import\u00e2ncia, esse \u00e9 um bom texto . Equa\u00e7\u00f5es caracter\u00edsticas Hamilton-Jacobi A equa\u00e7\u00e3o de Hamilton Jacobi \u00e9 dada pela express\u00e3o u_t + H(Du, x) = 0, em que H : \\mathbb{R}^{2n} \\to \\mathbb{R} e Du = (u_{x_1,}, \\dots, u_{x_n}) . Reescreva p = Du , p_{n+1} = u_t e q = (p,p_{n+1}) (como se o tempo fosse a dimens\u00e3o n+1 ) e assim teremos y = (x,t) nossa vari\u00e1vel. Podemos reescrever como o sistema G(p, p_{n+1}, z, y) = p_{n+1} + H(p, x) = 0. Note que, em particular, D_qG = (D_pG(p,x), 1) e D_yG = (D_xH(p,x), 0) e D_zG = 0 . Remark: Lembre que a nota\u00e7\u00e3o D_vG \u00e9 o mesmo que dizer que voc\u00ea vai derivar G com respeito a cada componente de v . Se v \\in \\mathbb{R} , temos que D_vG = G'(v) . Com isso, n\u00f3s podemos montar as equa\u00e7\u00f5es caracter\u00edsticas (dadas pela curvas caracter\u00edsticas \\gamma(s) ), \\begin{cases} \\dot{x}_1(s) = D_{p_1}G = H_{p_1}(p(s), \\gamma(s)) \\\\ \\dot{x}_2(s) = D_{p_2}G = H_{p_2}(p(s), \\gamma(s)) \\\\ \\dots \\\\ \\dot{x}_n(s) = D_{p_n}G = H_{p_n}(p(s), \\gamma(s)) \\\\ \\dot{t}(s) = D_{p_{n+1}}G = 1 \\implies t = s, \\\\ \\end{cases} o que permite intercambiar os par\u00e2metros t e s . \\begin{cases} \\dot{p}_1(t) = -D_{x_1}G - D_zG\\cdot p_1 = -H_{x_1}(p(s), \\gamma(s)) \\\\ \\dot{p}_2(t) = -D_{x_2}G - D_zG\\cdot p_1 = -H_{x_2}(p(s), \\gamma(s)) \\\\ \\dots \\\\ \\dot{p}_n(t) = -D_{x_n}G - D_zG\\cdot p_1 = -H_{x_n}(p(s), \\gamma(s)) \\\\ \\dot{p}_{n+1}(t) = -D_{t}G - D_zG\\cdot p_1 = 0 \\\\ \\end{cases} e, por fim \\begin{split} \\dot{z} &= D_qG\\cdot q = D_pH(p(t), \\gamma(t))\\cdot p(t) + p^{n+1}(t) \\\\ &= D_pH(p(t), \\gamma(t))\\cdot p(t) - H(p(t), \\gamma(s)) \\end{split} Note portanto que podemos sumarizar essas equa\u00e7\u00f5es em \\begin{cases} \\dot{\\gamma}(t) = D_pH(p(t),\\gamma(t)) \\\\ \\dot{p}(t) = -D_{x}H(p(t), \\gamma(t)) \\\\ \\dot{z}(t) = D_pH(p(t), \\gamma(t)) - H(p(t), \\gamma(t)) \\end{cases} em que as primeiras duas equa\u00e7\u00f5es s\u00e3o as equa\u00e7\u00f5es de Hamilton . A fun\u00e7\u00e3o H tamb\u00e9m \u00e9 chamada de Hamiltoniano. Um exemplo muito conhecido na estat\u00edstica \u00e9 o Hamiltonian Monte Carlo (HMC), que usa essa estrutura para procurar no espa\u00e7o dos par\u00e2metros. O c\u00e1lculo das varia\u00e7\u00f5es Introduzimos o Lagrangiano \\begin{align*} L : \\mathbb{R}^n \\times \\mathbb{R}^n &\\to \\mathbb{R} \\\\ (v_1,\\dots,v_n,x_1,\\dots,x_n) &\\mapsto L(v,x). \\end{align*} Al\u00e9m disso, definimos o funcional a\u00e7\u00e3o como I[w(\\cdot)] := \\int_0^t L(\\dot{w}(s), w(s))\\, ds, em que as fun\u00e7\u00f5es w s\u00e3o duas vezes continuamente diferenci\u00e1veis com w(0) = y e w(t) = x fixados anteriormente. Um problema geral do c\u00e1lculo das varia\u00e7\u00f5es \u00e9 encontrar uma fun\u00e7\u00e3o x(\\cdot) que minimiza I[w(\\cdot)] dentre todas as w . Exemplo Qual a fun\u00e7\u00e3o y = f(x) cuja curva (diferenci\u00e1vel) entre os pontos (x_1, y_1) e (x_2, y_2) tem o comprimento. Dada uma fun\u00e7\u00e3o, sabemos que seu comprimento de arco \u00e9 Comp[y] = \\int_{x_1}^{x_2} \\sqrt{1 + y'(x)^2} \\, dx. Nesse caso, L(f'(x), f(x)) = \\sqrt{1 + f'(x)^2} e sabemos que sua solu\u00e7\u00e3o \u00e9 o segmento de reta entre esses pontos. Equa\u00e7\u00f5es de Euler-Lagrange A solu\u00e7\u00e3o \u00f3tima para o problema de c\u00e1lculo das varia\u00e7\u00f5es resolve o sistema de equa\u00e7\u00f5es Euler-Lagrange: -\\frac{d}{ds}(D_v L(\\dot{x}(s), x(s))) + D_xL(\\dot{x}(s), x(s)) = 0, 0 \\le s \\le t. Note que resolver as equa\u00e7\u00f5es n\u00e3o nos d\u00e3o as fun\u00e7\u00f5es \u00f3timas, da mesma forma que derivada igual a zero n\u00e3o implica m\u00ednimo local de forma geral. Por isso, dizemos que as solu\u00e7\u00f5es das equa\u00e7\u00f5es de Euler-Lagrange s\u00e3o pontos cr\u00edticos de I[\\cdot] . Exemplo: Seja L(v,x) = \\frac{1}{2}m||v||^2 - \\phi(x) , em que m > 0 . Ent\u00e3o a equa\u00e7\u00e3o de Lagrange \u00e9 -m\\dot{v}(s) + D_x\\phi(x) = 0 \\implies m\\ddot{x}(s) = D_x \\phi(x), que a lei de Newton para uma part\u00edcula de massa m e campo de for\u00e7as D_x \\phi . O interessante \u00e9 que, dado uma Lagrangiano L , podemos introduzir o Hamiltoniano H(p,x) := p\\cdot v(p,x) - L(v(p,x), x), em que v \u00e9 definido como a fun\u00e7\u00e3o tal que p = D_vL(v,x) . Podemos demonstrar que a partir desse H , temos as equa\u00e7\u00f5es de Hamilton, o que permite uma conex\u00e3o entre as duas teorias. Transformada de Legendre Desconsidere a depend\u00eancia de H em x e defina L^*(p) = \\sup_{v \\in \\mathbb{R}^n} \\{pv - L(v)\\} . Chamamos L^* de transformada de Lagrange . Com algumas hip\u00f3teses em L (convexidade e limita\u00e7\u00e3o superior de pv - L(v) ), sabemos que existe v^* tal que L^*(v^*) = pv^* - L(v^*). Se L for diferenci\u00e1vel, teremos que D_vL(v^*) = p (derive a equa\u00e7\u00e3o a cima com respeito a v e avalie em v^* ). Ent\u00e3o a equa\u00e7\u00e3o p = D_v L(v) tem solu\u00e7\u00e3o em v , isto \u00e9, L^*(p) = p v(p) - L(v(p)) = H(p). Al\u00e9m disso, com algumas hip\u00f3teses sobre H , tamb\u00e9m podemos provar que H^* = L . Esse resultado \u00e9 conhecido como Dualidade convexa do Hamiltoniano e Lagrangiano . A ideia de Dualidade vai ser melhor compreendida em otimiza\u00e7\u00e3o, mas aqui, vale lembrar que L^* = H, H^* = L. Agora, sem a depend\u00eancia do x , as equa\u00e7\u00f5es caracter\u00edsticas s\u00e3o \\begin{cases} \\dot{\\gamma}(t) = D_pH(p(t)) \\\\ \\dot{p}(t) = 0 \\\\ \\dot{z}(t) = D_pH(p(t)) - H(p(t)) \\end{cases} Exerc\u00edcio: Prove que L^* = H, H^* = L implica que \\begin{cases} u\\cdot v = L(v) + H(u) \\\\ u = D_vL(v) \\\\ v = D_uH(u) \\end{cases} Agora note que \\dot{z} = D_v H(p)\\cdot p - H(p) = L(D_pH(p)) = L(\\dot{\\gamma}) . Assim, para valores de tempo pequenos u(x,t) = z(t) = \\int_0^t L(\\dot{\\gamma}(s)) \\, ds + g(x(0)), em que g \u00e9 condi\u00e7\u00e3o de fronteira quando t = 0 . Para generalizar esse valor para valores de t grandes, u(x,t) = \\inf_{w} \\left\\{\\int_0^t L(\\dot{w}(s)) \\, ds + g(x(0) \\mid w(t) = x\\right\\}. Com mais algumas condi\u00e7\u00f5es, conseguimos provar que essa express\u00e3o tem de fato uma solu\u00e7\u00e3o. A formula de Hopf-Lax diz que a solu\u00e7\u00e3o ser\u00e1 u(x,t) = \\min_{y \\in \\mathbb{R}^n} \\left\\{tL\\left(\\frac{x-y}{t}\\right) + g(y)\\right\\}.","title":"C\u00e1lculo de Varia\u00e7\u00f5es"},{"location":"edp/calculus_of_variations/#calculo-de-variacoes","text":"Para entender um pouco da intui\u00e7\u00e3o por tr\u00e1s e da sua import\u00e2ncia, esse \u00e9 um bom texto .","title":"C\u00e1lculo de Varia\u00e7\u00f5es"},{"location":"edp/calculus_of_variations/#equacoes-caracteristicas-hamilton-jacobi","text":"A equa\u00e7\u00e3o de Hamilton Jacobi \u00e9 dada pela express\u00e3o u_t + H(Du, x) = 0, em que H : \\mathbb{R}^{2n} \\to \\mathbb{R} e Du = (u_{x_1,}, \\dots, u_{x_n}) . Reescreva p = Du , p_{n+1} = u_t e q = (p,p_{n+1}) (como se o tempo fosse a dimens\u00e3o n+1 ) e assim teremos y = (x,t) nossa vari\u00e1vel. Podemos reescrever como o sistema G(p, p_{n+1}, z, y) = p_{n+1} + H(p, x) = 0. Note que, em particular, D_qG = (D_pG(p,x), 1) e D_yG = (D_xH(p,x), 0) e D_zG = 0 . Remark: Lembre que a nota\u00e7\u00e3o D_vG \u00e9 o mesmo que dizer que voc\u00ea vai derivar G com respeito a cada componente de v . Se v \\in \\mathbb{R} , temos que D_vG = G'(v) . Com isso, n\u00f3s podemos montar as equa\u00e7\u00f5es caracter\u00edsticas (dadas pela curvas caracter\u00edsticas \\gamma(s) ), \\begin{cases} \\dot{x}_1(s) = D_{p_1}G = H_{p_1}(p(s), \\gamma(s)) \\\\ \\dot{x}_2(s) = D_{p_2}G = H_{p_2}(p(s), \\gamma(s)) \\\\ \\dots \\\\ \\dot{x}_n(s) = D_{p_n}G = H_{p_n}(p(s), \\gamma(s)) \\\\ \\dot{t}(s) = D_{p_{n+1}}G = 1 \\implies t = s, \\\\ \\end{cases} o que permite intercambiar os par\u00e2metros t e s . \\begin{cases} \\dot{p}_1(t) = -D_{x_1}G - D_zG\\cdot p_1 = -H_{x_1}(p(s), \\gamma(s)) \\\\ \\dot{p}_2(t) = -D_{x_2}G - D_zG\\cdot p_1 = -H_{x_2}(p(s), \\gamma(s)) \\\\ \\dots \\\\ \\dot{p}_n(t) = -D_{x_n}G - D_zG\\cdot p_1 = -H_{x_n}(p(s), \\gamma(s)) \\\\ \\dot{p}_{n+1}(t) = -D_{t}G - D_zG\\cdot p_1 = 0 \\\\ \\end{cases} e, por fim \\begin{split} \\dot{z} &= D_qG\\cdot q = D_pH(p(t), \\gamma(t))\\cdot p(t) + p^{n+1}(t) \\\\ &= D_pH(p(t), \\gamma(t))\\cdot p(t) - H(p(t), \\gamma(s)) \\end{split} Note portanto que podemos sumarizar essas equa\u00e7\u00f5es em \\begin{cases} \\dot{\\gamma}(t) = D_pH(p(t),\\gamma(t)) \\\\ \\dot{p}(t) = -D_{x}H(p(t), \\gamma(t)) \\\\ \\dot{z}(t) = D_pH(p(t), \\gamma(t)) - H(p(t), \\gamma(t)) \\end{cases} em que as primeiras duas equa\u00e7\u00f5es s\u00e3o as equa\u00e7\u00f5es de Hamilton . A fun\u00e7\u00e3o H tamb\u00e9m \u00e9 chamada de Hamiltoniano. Um exemplo muito conhecido na estat\u00edstica \u00e9 o Hamiltonian Monte Carlo (HMC), que usa essa estrutura para procurar no espa\u00e7o dos par\u00e2metros.","title":"Equa\u00e7\u00f5es caracter\u00edsticas Hamilton-Jacobi"},{"location":"edp/calculus_of_variations/#o-calculo-das-variacoes","text":"Introduzimos o Lagrangiano \\begin{align*} L : \\mathbb{R}^n \\times \\mathbb{R}^n &\\to \\mathbb{R} \\\\ (v_1,\\dots,v_n,x_1,\\dots,x_n) &\\mapsto L(v,x). \\end{align*} Al\u00e9m disso, definimos o funcional a\u00e7\u00e3o como I[w(\\cdot)] := \\int_0^t L(\\dot{w}(s), w(s))\\, ds, em que as fun\u00e7\u00f5es w s\u00e3o duas vezes continuamente diferenci\u00e1veis com w(0) = y e w(t) = x fixados anteriormente. Um problema geral do c\u00e1lculo das varia\u00e7\u00f5es \u00e9 encontrar uma fun\u00e7\u00e3o x(\\cdot) que minimiza I[w(\\cdot)] dentre todas as w .","title":"O c\u00e1lculo das varia\u00e7\u00f5es"},{"location":"edp/calculus_of_variations/#exemplo","text":"Qual a fun\u00e7\u00e3o y = f(x) cuja curva (diferenci\u00e1vel) entre os pontos (x_1, y_1) e (x_2, y_2) tem o comprimento. Dada uma fun\u00e7\u00e3o, sabemos que seu comprimento de arco \u00e9 Comp[y] = \\int_{x_1}^{x_2} \\sqrt{1 + y'(x)^2} \\, dx. Nesse caso, L(f'(x), f(x)) = \\sqrt{1 + f'(x)^2} e sabemos que sua solu\u00e7\u00e3o \u00e9 o segmento de reta entre esses pontos.","title":"Exemplo"},{"location":"edp/calculus_of_variations/#equacoes-de-euler-lagrange","text":"A solu\u00e7\u00e3o \u00f3tima para o problema de c\u00e1lculo das varia\u00e7\u00f5es resolve o sistema de equa\u00e7\u00f5es Euler-Lagrange: -\\frac{d}{ds}(D_v L(\\dot{x}(s), x(s))) + D_xL(\\dot{x}(s), x(s)) = 0, 0 \\le s \\le t. Note que resolver as equa\u00e7\u00f5es n\u00e3o nos d\u00e3o as fun\u00e7\u00f5es \u00f3timas, da mesma forma que derivada igual a zero n\u00e3o implica m\u00ednimo local de forma geral. Por isso, dizemos que as solu\u00e7\u00f5es das equa\u00e7\u00f5es de Euler-Lagrange s\u00e3o pontos cr\u00edticos de I[\\cdot] . Exemplo: Seja L(v,x) = \\frac{1}{2}m||v||^2 - \\phi(x) , em que m > 0 . Ent\u00e3o a equa\u00e7\u00e3o de Lagrange \u00e9 -m\\dot{v}(s) + D_x\\phi(x) = 0 \\implies m\\ddot{x}(s) = D_x \\phi(x), que a lei de Newton para uma part\u00edcula de massa m e campo de for\u00e7as D_x \\phi . O interessante \u00e9 que, dado uma Lagrangiano L , podemos introduzir o Hamiltoniano H(p,x) := p\\cdot v(p,x) - L(v(p,x), x), em que v \u00e9 definido como a fun\u00e7\u00e3o tal que p = D_vL(v,x) . Podemos demonstrar que a partir desse H , temos as equa\u00e7\u00f5es de Hamilton, o que permite uma conex\u00e3o entre as duas teorias.","title":"Equa\u00e7\u00f5es de Euler-Lagrange"},{"location":"edp/calculus_of_variations/#transformada-de-legendre","text":"Desconsidere a depend\u00eancia de H em x e defina L^*(p) = \\sup_{v \\in \\mathbb{R}^n} \\{pv - L(v)\\} . Chamamos L^* de transformada de Lagrange . Com algumas hip\u00f3teses em L (convexidade e limita\u00e7\u00e3o superior de pv - L(v) ), sabemos que existe v^* tal que L^*(v^*) = pv^* - L(v^*). Se L for diferenci\u00e1vel, teremos que D_vL(v^*) = p (derive a equa\u00e7\u00e3o a cima com respeito a v e avalie em v^* ). Ent\u00e3o a equa\u00e7\u00e3o p = D_v L(v) tem solu\u00e7\u00e3o em v , isto \u00e9, L^*(p) = p v(p) - L(v(p)) = H(p). Al\u00e9m disso, com algumas hip\u00f3teses sobre H , tamb\u00e9m podemos provar que H^* = L . Esse resultado \u00e9 conhecido como Dualidade convexa do Hamiltoniano e Lagrangiano . A ideia de Dualidade vai ser melhor compreendida em otimiza\u00e7\u00e3o, mas aqui, vale lembrar que L^* = H, H^* = L. Agora, sem a depend\u00eancia do x , as equa\u00e7\u00f5es caracter\u00edsticas s\u00e3o \\begin{cases} \\dot{\\gamma}(t) = D_pH(p(t)) \\\\ \\dot{p}(t) = 0 \\\\ \\dot{z}(t) = D_pH(p(t)) - H(p(t)) \\end{cases} Exerc\u00edcio: Prove que L^* = H, H^* = L implica que \\begin{cases} u\\cdot v = L(v) + H(u) \\\\ u = D_vL(v) \\\\ v = D_uH(u) \\end{cases} Agora note que \\dot{z} = D_v H(p)\\cdot p - H(p) = L(D_pH(p)) = L(\\dot{\\gamma}) . Assim, para valores de tempo pequenos u(x,t) = z(t) = \\int_0^t L(\\dot{\\gamma}(s)) \\, ds + g(x(0)), em que g \u00e9 condi\u00e7\u00e3o de fronteira quando t = 0 . Para generalizar esse valor para valores de t grandes, u(x,t) = \\inf_{w} \\left\\{\\int_0^t L(\\dot{w}(s)) \\, ds + g(x(0) \\mid w(t) = x\\right\\}. Com mais algumas condi\u00e7\u00f5es, conseguimos provar que essa express\u00e3o tem de fato uma solu\u00e7\u00e3o. A formula de Hopf-Lax diz que a solu\u00e7\u00e3o ser\u00e1 u(x,t) = \\min_{y \\in \\mathbb{R}^n} \\left\\{tL\\left(\\frac{x-y}{t}\\right) + g(y)\\right\\}.","title":"Transformada de Legendre"},{"location":"edp/heat_equation/","text":"Equa\u00e7\u00e3o do calor Adicionais Solving the heat equation 3b1b Difus\u00e3o em redes com a equa\u00e7\u00e3o discreta do calor","title":"Equa\u00e7\u00e3o do calor"},{"location":"edp/heat_equation/#equacao-do-calor","text":"","title":"Equa\u00e7\u00e3o do calor"},{"location":"edp/heat_equation/#adicionais","text":"Solving the heat equation 3b1b Difus\u00e3o em redes com a equa\u00e7\u00e3o discreta do calor","title":"Adicionais"},{"location":"edp/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de Equa\u00e7\u00f5es Diferenciais Parciais. T\u00f3picos Conceitos introdut\u00f3rios Teorema de Picard-Lindel\u00f6f aplicado Defini\u00e7\u00f5es preliminares M\u00e9todos de solu\u00e7\u00e3o M\u00e9todo das caracter\u00edsticas C\u00e1lculo das varia\u00e7\u00f5es e Hamiltoniano Equa\u00e7\u00f5es Semiliniares de Segunda Ordem Teoria de Fourier Algumas equa\u00e7\u00f5es Equa\u00e7\u00e3o da onda Equa\u00e7\u00e3o do calor Per\u00edodo 2022.2 Dia: Sexta-feira, 14h Listas N\u00famero Itens discutidos Solu\u00e7\u00e3o Notas Monitoria Itens discutidos Arquivo V\u00eddeo 12/08/2022 M\u00e9todo das caracter\u00edsticas ver N\u00e3o Provas Ano Bimestre Quest\u00f5es Solu\u00e7\u00f5es Observa\u00e7\u00f5es Per\u00edodo 2021.2 Dia: Quinta-feira 18h Listas N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Equa\u00e7\u00f5es diferenciais ordin\u00e1rias 1 2 M\u00e9todo das caracter\u00edsticas 2 3 M\u00e9todo das caracter\u00edsticas e energia 3 4 Equa\u00e7\u00e3o da onda 4 5 Transformada de Fourier e Equa\u00e7\u00e3o do Calor 5 Notas Monitoria Itens discutidos Arquivo V\u00eddeo 19/08/2021 Teorema da Retifica\u00e7\u00e3o Visualizar N\u00e3o 09/09/2021 Exemplos equa\u00e7\u00f5es caracter\u00edsticas Visualizar N\u00e3o 30/09/2021 Hamilton-Jacobi e c\u00e1lculo das varia\u00e7\u00f5es Visualizar N\u00e3o 28/10/2021 Convolu\u00e7\u00e3o, Fourier e Lista 5 Viualizar Sim Provas Ano Bimestre Quest\u00f5es Solu\u00e7\u00f5es Observa\u00e7\u00f5es 2021 A1 ver arquivo ver arquivo ver c\u00f3digo Sugest\u00f5es Adicionais O que \u00e9 fluxo? Differential equations 3b1b Livro mais avan\u00e7ado por Rafael Jos\u00e9 Iorio Jr e Val\u00e9ria de Magalh\u00e3es Iorio Livro Partial Differential Equations de Lawrence C.Evans","title":"Equa\u00e7\u00f5es Diferenciais Parciais"},{"location":"edp/info/#informacoes-gerais","text":"Monitoria de Equa\u00e7\u00f5es Diferenciais Parciais.","title":"Informa\u00e7\u00f5es Gerais"},{"location":"edp/info/#topicos","text":"Conceitos introdut\u00f3rios Teorema de Picard-Lindel\u00f6f aplicado Defini\u00e7\u00f5es preliminares M\u00e9todos de solu\u00e7\u00e3o M\u00e9todo das caracter\u00edsticas C\u00e1lculo das varia\u00e7\u00f5es e Hamiltoniano Equa\u00e7\u00f5es Semiliniares de Segunda Ordem Teoria de Fourier Algumas equa\u00e7\u00f5es Equa\u00e7\u00e3o da onda Equa\u00e7\u00e3o do calor","title":"T\u00f3picos"},{"location":"edp/info/#periodo-20222","text":"Dia: Sexta-feira, 14h","title":"Per\u00edodo 2022.2"},{"location":"edp/info/#listas","text":"N\u00famero Itens discutidos Solu\u00e7\u00e3o","title":"Listas"},{"location":"edp/info/#notas","text":"Monitoria Itens discutidos Arquivo V\u00eddeo 12/08/2022 M\u00e9todo das caracter\u00edsticas ver N\u00e3o","title":"Notas"},{"location":"edp/info/#provas","text":"Ano Bimestre Quest\u00f5es Solu\u00e7\u00f5es Observa\u00e7\u00f5es","title":"Provas"},{"location":"edp/info/#periodo-20212","text":"Dia: Quinta-feira 18h","title":"Per\u00edodo 2021.2"},{"location":"edp/info/#listas_1","text":"N\u00famero Itens discutidos Solu\u00e7\u00e3o 1 Equa\u00e7\u00f5es diferenciais ordin\u00e1rias 1 2 M\u00e9todo das caracter\u00edsticas 2 3 M\u00e9todo das caracter\u00edsticas e energia 3 4 Equa\u00e7\u00e3o da onda 4 5 Transformada de Fourier e Equa\u00e7\u00e3o do Calor 5","title":"Listas"},{"location":"edp/info/#notas_1","text":"Monitoria Itens discutidos Arquivo V\u00eddeo 19/08/2021 Teorema da Retifica\u00e7\u00e3o Visualizar N\u00e3o 09/09/2021 Exemplos equa\u00e7\u00f5es caracter\u00edsticas Visualizar N\u00e3o 30/09/2021 Hamilton-Jacobi e c\u00e1lculo das varia\u00e7\u00f5es Visualizar N\u00e3o 28/10/2021 Convolu\u00e7\u00e3o, Fourier e Lista 5 Viualizar Sim","title":"Notas"},{"location":"edp/info/#provas_1","text":"Ano Bimestre Quest\u00f5es Solu\u00e7\u00f5es Observa\u00e7\u00f5es 2021 A1 ver arquivo ver arquivo ver c\u00f3digo","title":"Provas"},{"location":"edp/info/#sugestoes-adicionais","text":"O que \u00e9 fluxo? Differential equations 3b1b Livro mais avan\u00e7ado por Rafael Jos\u00e9 Iorio Jr e Val\u00e9ria de Magalh\u00e3es Iorio Livro Partial Differential Equations de Lawrence C.Evans","title":"Sugest\u00f5es Adicionais"},{"location":"edp/introduction/","text":"Defini\u00e7\u00f5es iniciais Uma equa\u00e7\u00e3o diferencial parcial (EDP) ou partial differential equation (PDE) \u00e9 uma equa\u00e7\u00e3o que envolve duas ou mais vari\u00e1veis independentes e derivadas parciais de uma fun\u00e7\u00e3o que depende dessas vari\u00e1veis. De forma bem geral, \u00e9 uma equa\u00e7\u00e3o com forma F\\left(x_1, \\dots, x_n, u, \\frac{\\partial u}{\\partial x_1}, \\dots, \\frac{\\partial u}{\\partial x_n}, \\dots, \\frac{\\partial^k u}{\\partial x_{i_1} \\dots \\partial x_{i_k}}, \\dots, \\frac{\\partial^k u}{\\partial x_n^k}\\right) = 0, em que u = u(x_1, ..., x_n) e na express\u00e3o aparece a k -\u00e9sima derivada de u com respeito a k vari\u00e1veis x_{i_1}, \\dots, x_{i_k} . A fun\u00e7\u00e3o u \u00e9 em geral desconhecida e assumidamente de classe C^k , isto \u00e9, k vezes continuamente diferenci\u00e1vel. Denotamos tamb\u00e9m, \\nabla^2 u := \\frac{\\partial u}{\\partial x_1^2} + \\dots + \\frac{\\partial u}{\\partial x_n^2} = \\Delta u, o operador de Laplace . Exemplos Equa\u00e7\u00e3o da Onda : \\Delta u = \\frac{1}{c^2}\\frac{\\partial^2 u}{\\partial t^2}, em que c mede a velocidade da propaga\u00e7\u00e3o da onda. Nesse exemplo temos n vari\u00e1veis espaciais x_1, \\dots, x_n e uma vari\u00e1vel temporal t . Podemos definir F\\left(x_1, \\dots, x_n, t, u, \\frac{\\partial^2 u}{\\partial x_1^2}, \\dots, \\frac{\\partial^2 u}{\\partial x_n^2}, \\frac{\\partial^2 u}{\\partial t^2}\\right) = c^2\\Delta u - u_{tt} Equa\u00e7\u00e3o do calor : \\Delta u = \\frac{1}{k}\\frac{\\partial u}{\\partial t}, Equa\u00e7\u00e3o de Laplace \\Delta u = 0 As solu\u00e7\u00f5es dessa equa\u00e7\u00e3o s\u00e3o as fun\u00e7\u00f5es harm\u00f4nicas . Equa\u00e7\u00e3o de Schr\u00f6dinger iu_t + \\Delta u = 0 \u00e9 a fun\u00e7\u00e3o que governa a fun\u00e7\u00e3o de onda no sistema mec\u00e2nico qu\u00e2ntico. Classifica\u00e7\u00e3o de uma EDP Ordem: Dada pela ordem parcial de maior ordem que ocorre na equa\u00e7\u00e3o. Nos exemplos anteriores, as equa\u00e7\u00f5es tem ordem 2. Linear: Ela \u00e9 dita linear se \u00e9 de primeiro grau em u e em todas suas derivadas parciais. Caso contr\u00e1rio ela \u00e9 dita n\u00e3o linear . Por exemplo, uma EDP linear de ordem 1 \u00e9 dada pela express\u00e3o (veja as diferentes express\u00f5es para derivada parcial ) \\sum_{j=1}^n a_j(x) D_{x_j} u + b(x) u + c(x) = 0 Homog\u00eanea: Os termos das vari\u00e1veis independentes s\u00e3o nulos, isto \u00e9, as vari\u00e1veis independentes n\u00e3o aparecem na express\u00e3o da fun\u00e7\u00e3o F . Parte principal: A parte da equa\u00e7\u00e3o que cont\u00e9m as derivadas de maior ordem. Se uma equa\u00e7\u00e3o n\u00e3o linear possui parte principal linear, ela \u00e9 dita semilinear . Exemplos Equa\u00e7\u00e3o telegrafo u_{tt} + 2du_t - u_{xx} = 0, \u00e9 uma equa\u00e7\u00e3o linear de ordem 2. Equa\u00e7\u00e3o de beam u_{tt} + u_{xxxx} = 0 \u00e9 uma equa\u00e7\u00e3o linear de ordem 4 Equa\u00e7\u00e3o da onda (n\u00e3o linear) u_{tt} - \\Delta u + f(u) = 0, em que f \u00e9 uma fun\u00e7\u00e3o n\u00e3o linear de u . Observe que a parte principal \u00e9 u_{tt} - \\Delta u que \u00e9 linear e, portanto, a equa\u00e7\u00e3o \u00e9 semilinear. Exemplo: Suponha que u e v s\u00e3o duas fun\u00e7\u00f5es solu\u00e7\u00e3o da equa\u00e7\u00e3o (linear de ordem 1) \\frac{\\partial u}{\\partial x} + xu = 0 Seja z := \\alpha u + \\beta v , em que \\alpha, \\beta \\in \\mathbb{R} . Assim, \\frac{\\partial z}{\\partial x} + xz = \\alpha\\frac{\\partial u}{\\partial x} + \\beta\\frac{\\partial v}{\\partial x} + \\alpha x u + \\beta x v = \\alpha\\left(\\frac{\\partial u}{\\partial x} + xu\\right) + \\beta\\left(\\frac{\\partial v}{\\partial x} + xv\\right) = 0, o que implica que z tamb\u00e9m \u00e9 solu\u00e7\u00e3o. Essa propriedade n\u00e3o \u00e9 exclusividade do exemplo acima. Na verdade, vale para toda equa\u00e7\u00e3o linear, isto \u00e9, combina\u00e7\u00e3o linear de solu\u00e7\u00f5es \u00e9 solu\u00e7\u00e3o em problemas lineares. Princ\u00edpio da Superposi\u00e7\u00e3o Considere uma EDP linear de ordem 2: \\sum_{i,j}^n a_{ij}(x)D_{x_i}D_{x_j} u(x) + \\sum_{j=1}^n b_{j}(x)D_{x_j}u(x) + c(x)u(x) + d(x) = 0. Podemos reescrever essa equa\u00e7\u00e3o na forma (Lu)(x) = \\sum_{i,j}^n a_{ij}(x)D_{x_i}D_{x_j} u(x) + \\sum_{j=1}^n b_{j}(x)D_{x_j}u(x) + c(x)u(x) e Lu = -d . Dizemos que L \u00e9 um operador (no espa\u00e7o de fun\u00e7\u00f5es de classe C^k para o espa\u00e7o de fun\u00e7\u00f5es cont\u00ednuas). no caso ele \u00e9 linear de ordem 2. Podemos generalizar para um de ordem k qualquer. Seja L um operador diferencial linear de ordem k cujos coeficientes est\u00e3o definidos em \\mathbb{R}^n . Suponha que \\{u_m\\}_{m=1}^{+\\infty} \u00e9 um conjunto de fun\u00e7\u00f5es k vezes continuamente diferenci\u00e1veis satisfazendo Lu = 0, e que \\{\\alpha_m\\}_{m=1}^{+\\infty} \u00e9 uma sequ\u00eancia tal que u(x) = \\sum_{m=1}^{+\\infty} \\alpha_m u_m(x) \u00e9 convergente e k vezes diferenci\u00e1vel termo a t . Ent\u00e3o u tamb\u00e9m \u00e9 solu\u00e7\u00e3o. Condi\u00e7\u00f5es de Contorno e Iniciais Observe que para EDPs, estamos interessados em definir uma fun\u00e7\u00e3o em uma regi\u00e3o em um espa\u00e7o de dimens\u00e3o pelo menos 2. Por isso, n\u00e3o \u00e9 mais suficiente atribuir uma condi\u00e7\u00e3o inicial ao problema, como se fazia aos problemas de EDO, para determinar uma solu\u00e7\u00e3o. Nesse caso, substitu\u00edmos os extremos do intervalo pelo fronteira do conjunto que queremos estudar. Nesse caso, o problema \u00e9 dito problema de contorno . \u00c9 comum fixar, por exemplo u(x,0) = f(x) e u(x,1) = g(x) , em que a segunda vari\u00e1vel \u00e9 o tempo.","title":"Defini\u00e7\u00f5es iniciais"},{"location":"edp/introduction/#definicoes-iniciais","text":"Uma equa\u00e7\u00e3o diferencial parcial (EDP) ou partial differential equation (PDE) \u00e9 uma equa\u00e7\u00e3o que envolve duas ou mais vari\u00e1veis independentes e derivadas parciais de uma fun\u00e7\u00e3o que depende dessas vari\u00e1veis. De forma bem geral, \u00e9 uma equa\u00e7\u00e3o com forma F\\left(x_1, \\dots, x_n, u, \\frac{\\partial u}{\\partial x_1}, \\dots, \\frac{\\partial u}{\\partial x_n}, \\dots, \\frac{\\partial^k u}{\\partial x_{i_1} \\dots \\partial x_{i_k}}, \\dots, \\frac{\\partial^k u}{\\partial x_n^k}\\right) = 0, em que u = u(x_1, ..., x_n) e na express\u00e3o aparece a k -\u00e9sima derivada de u com respeito a k vari\u00e1veis x_{i_1}, \\dots, x_{i_k} . A fun\u00e7\u00e3o u \u00e9 em geral desconhecida e assumidamente de classe C^k , isto \u00e9, k vezes continuamente diferenci\u00e1vel. Denotamos tamb\u00e9m, \\nabla^2 u := \\frac{\\partial u}{\\partial x_1^2} + \\dots + \\frac{\\partial u}{\\partial x_n^2} = \\Delta u, o operador de Laplace .","title":"Defini\u00e7\u00f5es iniciais"},{"location":"edp/introduction/#exemplos","text":"Equa\u00e7\u00e3o da Onda : \\Delta u = \\frac{1}{c^2}\\frac{\\partial^2 u}{\\partial t^2}, em que c mede a velocidade da propaga\u00e7\u00e3o da onda. Nesse exemplo temos n vari\u00e1veis espaciais x_1, \\dots, x_n e uma vari\u00e1vel temporal t . Podemos definir F\\left(x_1, \\dots, x_n, t, u, \\frac{\\partial^2 u}{\\partial x_1^2}, \\dots, \\frac{\\partial^2 u}{\\partial x_n^2}, \\frac{\\partial^2 u}{\\partial t^2}\\right) = c^2\\Delta u - u_{tt} Equa\u00e7\u00e3o do calor : \\Delta u = \\frac{1}{k}\\frac{\\partial u}{\\partial t}, Equa\u00e7\u00e3o de Laplace \\Delta u = 0 As solu\u00e7\u00f5es dessa equa\u00e7\u00e3o s\u00e3o as fun\u00e7\u00f5es harm\u00f4nicas . Equa\u00e7\u00e3o de Schr\u00f6dinger iu_t + \\Delta u = 0 \u00e9 a fun\u00e7\u00e3o que governa a fun\u00e7\u00e3o de onda no sistema mec\u00e2nico qu\u00e2ntico.","title":"Exemplos"},{"location":"edp/introduction/#classificacao-de-uma-edp","text":"Ordem: Dada pela ordem parcial de maior ordem que ocorre na equa\u00e7\u00e3o. Nos exemplos anteriores, as equa\u00e7\u00f5es tem ordem 2. Linear: Ela \u00e9 dita linear se \u00e9 de primeiro grau em u e em todas suas derivadas parciais. Caso contr\u00e1rio ela \u00e9 dita n\u00e3o linear . Por exemplo, uma EDP linear de ordem 1 \u00e9 dada pela express\u00e3o (veja as diferentes express\u00f5es para derivada parcial ) \\sum_{j=1}^n a_j(x) D_{x_j} u + b(x) u + c(x) = 0 Homog\u00eanea: Os termos das vari\u00e1veis independentes s\u00e3o nulos, isto \u00e9, as vari\u00e1veis independentes n\u00e3o aparecem na express\u00e3o da fun\u00e7\u00e3o F . Parte principal: A parte da equa\u00e7\u00e3o que cont\u00e9m as derivadas de maior ordem. Se uma equa\u00e7\u00e3o n\u00e3o linear possui parte principal linear, ela \u00e9 dita semilinear .","title":"Classifica\u00e7\u00e3o de uma EDP"},{"location":"edp/introduction/#exemplos_1","text":"Equa\u00e7\u00e3o telegrafo u_{tt} + 2du_t - u_{xx} = 0, \u00e9 uma equa\u00e7\u00e3o linear de ordem 2. Equa\u00e7\u00e3o de beam u_{tt} + u_{xxxx} = 0 \u00e9 uma equa\u00e7\u00e3o linear de ordem 4 Equa\u00e7\u00e3o da onda (n\u00e3o linear) u_{tt} - \\Delta u + f(u) = 0, em que f \u00e9 uma fun\u00e7\u00e3o n\u00e3o linear de u . Observe que a parte principal \u00e9 u_{tt} - \\Delta u que \u00e9 linear e, portanto, a equa\u00e7\u00e3o \u00e9 semilinear. Exemplo: Suponha que u e v s\u00e3o duas fun\u00e7\u00f5es solu\u00e7\u00e3o da equa\u00e7\u00e3o (linear de ordem 1) \\frac{\\partial u}{\\partial x} + xu = 0 Seja z := \\alpha u + \\beta v , em que \\alpha, \\beta \\in \\mathbb{R} . Assim, \\frac{\\partial z}{\\partial x} + xz = \\alpha\\frac{\\partial u}{\\partial x} + \\beta\\frac{\\partial v}{\\partial x} + \\alpha x u + \\beta x v = \\alpha\\left(\\frac{\\partial u}{\\partial x} + xu\\right) + \\beta\\left(\\frac{\\partial v}{\\partial x} + xv\\right) = 0, o que implica que z tamb\u00e9m \u00e9 solu\u00e7\u00e3o. Essa propriedade n\u00e3o \u00e9 exclusividade do exemplo acima. Na verdade, vale para toda equa\u00e7\u00e3o linear, isto \u00e9, combina\u00e7\u00e3o linear de solu\u00e7\u00f5es \u00e9 solu\u00e7\u00e3o em problemas lineares.","title":"Exemplos"},{"location":"edp/introduction/#principio-da-superposicao","text":"Considere uma EDP linear de ordem 2: \\sum_{i,j}^n a_{ij}(x)D_{x_i}D_{x_j} u(x) + \\sum_{j=1}^n b_{j}(x)D_{x_j}u(x) + c(x)u(x) + d(x) = 0. Podemos reescrever essa equa\u00e7\u00e3o na forma (Lu)(x) = \\sum_{i,j}^n a_{ij}(x)D_{x_i}D_{x_j} u(x) + \\sum_{j=1}^n b_{j}(x)D_{x_j}u(x) + c(x)u(x) e Lu = -d . Dizemos que L \u00e9 um operador (no espa\u00e7o de fun\u00e7\u00f5es de classe C^k para o espa\u00e7o de fun\u00e7\u00f5es cont\u00ednuas). no caso ele \u00e9 linear de ordem 2. Podemos generalizar para um de ordem k qualquer. Seja L um operador diferencial linear de ordem k cujos coeficientes est\u00e3o definidos em \\mathbb{R}^n . Suponha que \\{u_m\\}_{m=1}^{+\\infty} \u00e9 um conjunto de fun\u00e7\u00f5es k vezes continuamente diferenci\u00e1veis satisfazendo Lu = 0, e que \\{\\alpha_m\\}_{m=1}^{+\\infty} \u00e9 uma sequ\u00eancia tal que u(x) = \\sum_{m=1}^{+\\infty} \\alpha_m u_m(x) \u00e9 convergente e k vezes diferenci\u00e1vel termo a t . Ent\u00e3o u tamb\u00e9m \u00e9 solu\u00e7\u00e3o.","title":"Princ\u00edpio da Superposi\u00e7\u00e3o"},{"location":"edp/introduction/#condicoes-de-contorno-e-iniciais","text":"Observe que para EDPs, estamos interessados em definir uma fun\u00e7\u00e3o em uma regi\u00e3o em um espa\u00e7o de dimens\u00e3o pelo menos 2. Por isso, n\u00e3o \u00e9 mais suficiente atribuir uma condi\u00e7\u00e3o inicial ao problema, como se fazia aos problemas de EDO, para determinar uma solu\u00e7\u00e3o. Nesse caso, substitu\u00edmos os extremos do intervalo pelo fronteira do conjunto que queremos estudar. Nesse caso, o problema \u00e9 dito problema de contorno . \u00c9 comum fixar, por exemplo u(x,0) = f(x) e u(x,1) = g(x) , em que a segunda vari\u00e1vel \u00e9 o tempo.","title":"Condi\u00e7\u00f5es de Contorno e Iniciais"},{"location":"edp/second_order_semilinear/","text":"Equa\u00e7\u00f5es Semiliares de Segunda Ordem Esse \u00e9 uma breve introdu\u00e7\u00e3o ao t\u00f3pico para indicar o estudo. Para mais detalhes, o livro da professora Val\u00e9ria I\u00f3rio \u00e9 sugerido. Temos que uma EDP semilinear de segunda ordem com duas vari\u00e1veis \u00e9 da forma a(x,y) u_{xx} + 2b(x,y)u_{xy} + c(x,y)u_{yy} = f(x,y,u,u_x,u_y), cuja parte principal \u00e9 o lado esquerdo da equa\u00e7\u00e3o. Suponha que as fun\u00e7\u00f5es a,b,c sejam cont\u00ednuas e defina a fun\u00e7\u00e3o discriminate : \\delta(x,y) = b^2(x,y) - a(x,y)c(x,y). O ponto (x,y) \\in \\Omega \u00e9 dito Parab\u00f3lico se \\delta(x,y) = 0 Hiperb\u00f3lica se \\delta(x,y) > 0 El\u00edptica se \\delta(x,y) < 0 Se a condi\u00e7\u00e3o (1), (2) ou (3) vale para todo o ponto em \\Omega , ent\u00e3o a EDP \u00e9 dita parab\u00f3lica, hiperb\u00f3lica ou el\u00edptica, respectivamente. Propriedade importante: O tipo da EDP \u00e9 invariante sob mudan\u00e7as de vari\u00e1veis se o Jacobiano da transforma\u00e7\u00e3o for n\u00e3o nulo em uma vizinhan\u00e7a de cada ponto. Curvas caracter\u00edsticas Para EDPs de segunda ordem, as curvas caracter\u00edsticas s\u00e3o curvas planas ao longo das quais a EDP pode ser escrita em uma forma que contenha as derivadas de u_x e u_y . Suponha que a n\u00e3o se anula na regi\u00e3o de interesse. Caso se anule, considere c ou $b, pois uma delas n\u00e3o se anula na regi\u00e3o de interesse, se n\u00e3o a EDP seria de primeira ordem. Reescreva o problema como \\begin{cases} p = u_x \\\\ q = u_y \\\\ a(x,y)p_x + 2b(x,y)p_y + c(x,y)q_y = f(x,y,u,p,q). \\end{cases} Quando u \u00e9 de classe C^2 como \u00e9 nosso caso, p_y = q_x . Assim, para qualquer fun\u00e7\u00e3o \\lambda(x,y) \\neq 0 temos que ap_x + 2bp_y + \\lambda p_y - \\lambda q_x + cq_y = 0. Defina P(x) = p(x,y(x)) e Q(x) = q(x,y(x)) e teremos (sim, regra da cadeia de novo) que a\\frac{dP}{dx} - \\lambda\\frac{dQ}{dx} = 0, em que \\frac{dy}{dx} = \\frac{2b + \\lambda}{a} = -\\frac{c}{\\lambda}. Assim, a fun\u00e7\u00e3o \\lambda deve satisfazer \\lambda^2 + 2b\\lambda + ac = 0 Agora se -c/\\lambda = \\mu , ent\u00e3o \\left(\\frac{c}{\\mu}\\right)^2 - 2\\frac{bc}{\\mu} + ac =0 \\overset{\\times \\mu^2/c}{\\implies} a\\mu^2 - 2b\\mu + c = 0, isto \u00e9, se dy/dx = \\mu(x,y) , ent\u00e3o a\\mu^2 - 2b\\mu + c = 0 . Portanto, o sinal do discriminante \\delta = b^2 - ac introduzido acima determina se existem uma, duas, ou nenhuma solu\u00e7\u00e3o \\mu . No caso parab\u00f3lico, existe uma fam\u00edlia de fun\u00e7\u00f5es que satisfaz essa equa\u00e7\u00e3o para \\mu . No caso hiperb\u00f3lico, duas fam\u00edlias satisfazem. No caso el\u00edptico, n\u00e3o existem solu\u00e7\u00f5es para \\mu . As curvas definadas por \\frac{dy}{dx} = \\mu(x,y) s\u00e3o as curvas caracter\u00edsticas da EDP. Exemplo: Vamos encontrar as curvas caracter\u00edsticas da equa\u00e7\u00e3o da onda (que \u00e9 hiperb\u00f3lica) u_{tt} = c^2u_{xx}. Nessse caso a, b, c s\u00e3o constantes com a = 1, b = 0 e c = -c^2 . Assim, \\mu^2 -c^2 = 0 \\implies \\mu \\pm c. Obtemos que \\frac{dx}{dt} = \\pm c \\implies x = \\pm ct + x_0. Logo as curvas s\u00e3o as fam\u00edlias de retas x + ct = k_1 e x - ct = k_2 , para k_1, k_2 constantes. A ideia para resolver esses problemas \u00e9, portanto, fazer a mudan\u00e7a de vari\u00e1veis \\begin{cases} \\xi = x + ct \\\\ \\eta = x - ct. \\end{cases} e introduzir a fun\u00e7\u00e3o v(\\xi,\\eta) = u(x(\\xi,\\eta),t(\\xi,\\eta)) . Outra forma de resolver a equa\u00e7\u00e3o da onda \u00e9 introduzida aqui .","title":"Equa\u00e7\u00f5es Semiliares de Segunda Ordem"},{"location":"edp/second_order_semilinear/#equacoes-semiliares-de-segunda-ordem","text":"Esse \u00e9 uma breve introdu\u00e7\u00e3o ao t\u00f3pico para indicar o estudo. Para mais detalhes, o livro da professora Val\u00e9ria I\u00f3rio \u00e9 sugerido. Temos que uma EDP semilinear de segunda ordem com duas vari\u00e1veis \u00e9 da forma a(x,y) u_{xx} + 2b(x,y)u_{xy} + c(x,y)u_{yy} = f(x,y,u,u_x,u_y), cuja parte principal \u00e9 o lado esquerdo da equa\u00e7\u00e3o. Suponha que as fun\u00e7\u00f5es a,b,c sejam cont\u00ednuas e defina a fun\u00e7\u00e3o discriminate : \\delta(x,y) = b^2(x,y) - a(x,y)c(x,y). O ponto (x,y) \\in \\Omega \u00e9 dito Parab\u00f3lico se \\delta(x,y) = 0 Hiperb\u00f3lica se \\delta(x,y) > 0 El\u00edptica se \\delta(x,y) < 0 Se a condi\u00e7\u00e3o (1), (2) ou (3) vale para todo o ponto em \\Omega , ent\u00e3o a EDP \u00e9 dita parab\u00f3lica, hiperb\u00f3lica ou el\u00edptica, respectivamente. Propriedade importante: O tipo da EDP \u00e9 invariante sob mudan\u00e7as de vari\u00e1veis se o Jacobiano da transforma\u00e7\u00e3o for n\u00e3o nulo em uma vizinhan\u00e7a de cada ponto.","title":"Equa\u00e7\u00f5es Semiliares de Segunda Ordem"},{"location":"edp/second_order_semilinear/#curvas-caracteristicas","text":"Para EDPs de segunda ordem, as curvas caracter\u00edsticas s\u00e3o curvas planas ao longo das quais a EDP pode ser escrita em uma forma que contenha as derivadas de u_x e u_y . Suponha que a n\u00e3o se anula na regi\u00e3o de interesse. Caso se anule, considere c ou $b, pois uma delas n\u00e3o se anula na regi\u00e3o de interesse, se n\u00e3o a EDP seria de primeira ordem. Reescreva o problema como \\begin{cases} p = u_x \\\\ q = u_y \\\\ a(x,y)p_x + 2b(x,y)p_y + c(x,y)q_y = f(x,y,u,p,q). \\end{cases} Quando u \u00e9 de classe C^2 como \u00e9 nosso caso, p_y = q_x . Assim, para qualquer fun\u00e7\u00e3o \\lambda(x,y) \\neq 0 temos que ap_x + 2bp_y + \\lambda p_y - \\lambda q_x + cq_y = 0. Defina P(x) = p(x,y(x)) e Q(x) = q(x,y(x)) e teremos (sim, regra da cadeia de novo) que a\\frac{dP}{dx} - \\lambda\\frac{dQ}{dx} = 0, em que \\frac{dy}{dx} = \\frac{2b + \\lambda}{a} = -\\frac{c}{\\lambda}. Assim, a fun\u00e7\u00e3o \\lambda deve satisfazer \\lambda^2 + 2b\\lambda + ac = 0 Agora se -c/\\lambda = \\mu , ent\u00e3o \\left(\\frac{c}{\\mu}\\right)^2 - 2\\frac{bc}{\\mu} + ac =0 \\overset{\\times \\mu^2/c}{\\implies} a\\mu^2 - 2b\\mu + c = 0, isto \u00e9, se dy/dx = \\mu(x,y) , ent\u00e3o a\\mu^2 - 2b\\mu + c = 0 . Portanto, o sinal do discriminante \\delta = b^2 - ac introduzido acima determina se existem uma, duas, ou nenhuma solu\u00e7\u00e3o \\mu . No caso parab\u00f3lico, existe uma fam\u00edlia de fun\u00e7\u00f5es que satisfaz essa equa\u00e7\u00e3o para \\mu . No caso hiperb\u00f3lico, duas fam\u00edlias satisfazem. No caso el\u00edptico, n\u00e3o existem solu\u00e7\u00f5es para \\mu . As curvas definadas por \\frac{dy}{dx} = \\mu(x,y) s\u00e3o as curvas caracter\u00edsticas da EDP. Exemplo: Vamos encontrar as curvas caracter\u00edsticas da equa\u00e7\u00e3o da onda (que \u00e9 hiperb\u00f3lica) u_{tt} = c^2u_{xx}. Nessse caso a, b, c s\u00e3o constantes com a = 1, b = 0 e c = -c^2 . Assim, \\mu^2 -c^2 = 0 \\implies \\mu \\pm c. Obtemos que \\frac{dx}{dt} = \\pm c \\implies x = \\pm ct + x_0. Logo as curvas s\u00e3o as fam\u00edlias de retas x + ct = k_1 e x - ct = k_2 , para k_1, k_2 constantes. A ideia para resolver esses problemas \u00e9, portanto, fazer a mudan\u00e7a de vari\u00e1veis \\begin{cases} \\xi = x + ct \\\\ \\eta = x - ct. \\end{cases} e introduzir a fun\u00e7\u00e3o v(\\xi,\\eta) = u(x(\\xi,\\eta),t(\\xi,\\eta)) . Outra forma de resolver a equa\u00e7\u00e3o da onda \u00e9 introduzida aqui .","title":"Curvas caracter\u00edsticas"},{"location":"edp/wave_equation/","text":"Equa\u00e7\u00e3o da Onda Estamos interessados em entender o comportamento da EDP u_{tt} - c^2u_{xx} = 0, sujeita a condi\u00e7\u00f5es iniciais e de fronteira. Interpreta\u00e7\u00e3o f\u00edsica Em uma dimens\u00e3o (mais outra temporal), a equa\u00e7\u00e3o da onda \u00e9 um modelo simpleficado de uma corda vibrando e u(x,t) representa o deslocamento de um ponto x no tempo t \\ge 0 . Suponha que a massa da corda seja \\rho = \\rho(x) e a elasticidade seja k = k(x) . Considere o peda\u00e7o da corda [x, x + \\Delta x] . Ent\u00e3o, a massa desse peda\u00e7o ser\u00e1 \\rho(x) \\Delta x , sua velocidade \u00e9 dada por u_t(x,t) (a derivada do deslocamento com respeito ao tempo) e a sua energia cin\u00e9tica \u00e9 \\Delta K = \\rho (u_t)^2 \\Delta x / 2 (lembre que energia cin\u00e9tica \u00e9 proporcional \u00e0 massa e ao quadrado da velocidade). Portanto, a enegia cin\u00e9tica total ser\u00e1 K(t) = \\frac{1}{2} \\int_0^L \\rho(x) (u_t(x,t))^2 \\, dx. Pela Lei de Hooke , a energia potencial da corda \u00e9 (k/2)y^2 em que y \u00e9 o aumento da corda dado pelo comprimento de u(x,t) na vari\u00e1vel x . Esse comprimento pode ser medido usando \\sqrt{1 + u_x^2} e, portanto, a energia potencial \u00e9 P(t) = \\frac{1}{2}\\int_0^L k(x)(1 + (u_x(x,t))^2) \\, dx. A a\u00e7\u00e3o da fun\u00e7\u00e3o u \u00e9, portanto, I(u) = \\int_0^T K(t) - P(t) \\, dt = \\frac{1}{2} \\int_0^T\\int_0^L \\rho(u_t)^2 - k(1 + u_x^2) \\, dx \\, dt. Se u minimiza essa a\u00e7\u00e3o, pelo Teorema de Euler-Lagrange, derivando com respeito a u_t e u_x , -2\\rho u_{tt} - 2(k_xu_x + ku_{xx}) = 0, que implica \\rho u_{tt} = -ku_{xx} - k_xu_x , isto \u00e9, se a elasticidade for constante na corda, temos a equa\u00e7\u00e3o da onda. Note que a nossa constru\u00e7\u00e3o sup\u00f5e a velocidade inicial conhecida. Portanto, al\u00e9m da condi\u00e7\u00e3o de fronteira usal, precisamos de uma condi\u00e7\u00e3o adicional (o que era de se esperar, pois nas EDOs, esse comportamento tamb\u00e9m acontecia). F\u00f3rmula d'Alembert Considere o problema \\begin{cases} u_{tt} - u_{xx} = 0, \\\\ u(x,0) = g(x), u_t(x,0) = h(x), \\end{cases} em que g e h s\u00e3o dadas. Note que u_{tt} - u_{xx} = u_{tt} + u_{tx} - u_{xt} - u_{xx} = \\left(\\frac{\\partial}{\\partial t} + \\frac{\\partial}{\\partial x}\\right)(u_t - u_x) = \\left(\\frac{\\partial}{\\partial t} + \\frac{\\partial}{\\partial x}\\right)\\left(\\frac{\\partial}{\\partial t} - \\frac{\\partial}{\\partial x}\\right)u. Apesar de ser uma nota\u00e7\u00e3o de funcional que pode ser complicada, ela \u00e9 muito \u00fatil para definir v(x,t) = \\left(\\frac{\\partial}{\\partial t} - \\frac{\\partial}{\\partial x}\\right)u, e, ent\u00e3o, v_t + v_x = 0, (x \\in \\mathbb{R}, t > 0), que \u00e9 a equa\u00e7\u00e3o do transporte. Sabemos que a solu\u00e7\u00e3o para esse problema \u00e9 v(x,t) = a(x - t), em que a(x) := v(x,0) , e, portanto u_t(x,t) - u_x(x,t) = a(x,t), que tamb\u00e9m o problema do transporte n\u00e3o homog\u00eaneo. Esse problema tem solu\u00e7\u00e3o conhecida : u(x,t) = \\int_0^t a(x + (t-s) - s) \\, ds + b(x+t) = \\frac{1}{2}\\int_{x-t}^{x+t} a(y) \\, dy + b(x,t). Al\u00e9m disso, em t=0 , temos que b(x,0) = g(x) e a(x) = h(x) - g'(x) . Portanto, u(x,t) = \\frac{1}{2} \\int_{x-t}^{x+t} h(y) - g'(y) \\,dy + g(x+t), que implica u(x,t) = \\frac{1}{2}[g(x+t) + g(x-t)] + \\frac{1}{2}\\int_{x-t}^{x+t} h(y) \\, dy. Esta \u00e9 a f\u00f3rmula de d'Alembert. Note que na solu\u00e7\u00e3o, assumimos que u \u00e9 suficientemente suave (classe C^2 ). Essa condi\u00e7\u00e3o \u00e9 verificada se g \\in C^2(\\mathbb{R}) e h \\in C^1(\\mathbb{R}) . Dizemos, ent\u00e3o, que a solu\u00e7\u00e3o u tem a forma u(x,t) = F(x+t) + G(x-t) Observa\u00e7\u00e3o: Esse desenvolvimento assume que a corda \u00e9 infinita. Extens\u00f5es Esse \u00e9 um t\u00f3pico muito rico que pode ter muitas varia\u00e7\u00f5es. Aqui apresentarei duas extens\u00f5es que podem ser mais estudadas nas refer\u00eancais do curso. M\u00e9dias esf\u00e9ricas e f\u00f3rmulas de Poisson e Kirchhoff Para dimens\u00f5es espaciais n \\ge 2 , a solu\u00e7\u00e3o \u00e9 um pouco mais complexa. Para isso, \u00e9 preciso resolver a equa\u00e7\u00e3o de Euler-Poisson-Darboux e reduzir o problema ao formato que se possa aplicar a f\u00f3rmula de d'Alembert. Para o detalhamento desse processo, o livro do professor Lawrence Evans \u00e9 bastante indicado. Corda finita Nesse caso, temos restri\u00e7\u00f5es de contorno quando x=0 e x=l (comprimento da corda): \\begin{cases} u_{tt} = c^2u_{xx} \\\\ u(0,t) = u(l,t) = 0, \\forall t \\ge 0, \\\\ u(x,0) = f(x), x \\in [0,l] \\\\ u_t(x,0) = g(x), x \\in [0,l]. \\end{cases} Podemos provar o seguinte teorema: Teorema: Seja f \\in C^2[0,l] e g \\in C^1[0,l] tais que f, f'' e g se anulam em x=0 e x=l , ent\u00e3o u(x,t) = \\frac{F(x+ct) + F(x-ct)}{2} + \\frac{1}{2c} \\int_{x-ct}^{x+ct} G(s) \\, ds, em que F e G s\u00e3o extens\u00f5es peri\u00f3dicas (per\u00edodo 2l ) e \u00edmpares das fun\u00e7\u00f5es f e g , respecticamente, \u00e9 a \u00fanica solu\u00e7\u00e3o do problema de corda infinita. O que \u00e9 extens\u00e3o peri\u00f3dica e \u00edmpar? Dizemos que uma fun\u00e7\u00e3o f: A \\to B tem per\u00edodo T se para todo x \\in A , temos que x+ T \\in A e f(x) = f(x+ T) . A fun\u00e7\u00e3o ser\u00e1 \u00edmpar se f(x) = -f(-x) para x \\le -l Assim, definimos a extens\u00e3o peri\u00f3dica \u00edmpar de f como F(x) = \\begin{cases} -f(-x + 2kl ) &x \\in [(2k-1)l, 2kl] \\\\ f(x - 2kl) &x \\in [2kl, (2k+1)l],\\\\ \\end{cases} que \u00e9 definida em intervalor fechados. M\u00e9todos de energia Esses m\u00e9todos nos ajudam a estudar a unicidade da solu\u00e7\u00e3o u do problema. Considere novamente o problema \\begin{cases} u_{tt} - u_{xx} = 0, \\\\ u(x,0) = g(x), u_t(x,0) = h(x), \\end{cases} Suponha que u e v sejam solu\u00e7\u00f5es para esse problema. Ent\u00e3o w = u - v \u00e9 uma solu\u00e7\u00e3o para \\begin{cases} u_{tt} - u_{xx} = 0, \\\\ u(x,0) =0, u_t(x,0) = 0. \\end{cases} Defina a energia E(t) := \\frac{1}{2}\\int_U w_t^2(x,t) + w_x(x,t)^2 \\, dx, 0 \\le t \\le T. Derivando com respeito a t e supondo U limitado com fronteira suave, temos que (F\u00f3rmula de Leibniz) \\dot{E}(t) = \\int_U w_tw_{tt} + w_xw_{tx} \\, dx = \\int_U w_t(w_{tt} - w_{xx}) = 0, em que a segunda igualdade vem de uma integra\u00e7\u00e3o por partes combinada com o fato de na fronteira w = 0 . Portanto E(t) = E(0) para todo T \\ge t \\ge 0 , que implica E(t) = 0 . Em paricular, w_t(x,t) = w_x(x,t) = 0, \\forall x, t e, portanto w \\equiv 0 . Sugest\u00f5es CFL (1928) Discretiza\u00e7\u00e3o da equa\u00e7\u00e3o da onda","title":"Equa\u00e7\u00e3o da Onda"},{"location":"edp/wave_equation/#equacao-da-onda","text":"Estamos interessados em entender o comportamento da EDP u_{tt} - c^2u_{xx} = 0, sujeita a condi\u00e7\u00f5es iniciais e de fronteira.","title":"Equa\u00e7\u00e3o da Onda"},{"location":"edp/wave_equation/#interpretacao-fisica","text":"Em uma dimens\u00e3o (mais outra temporal), a equa\u00e7\u00e3o da onda \u00e9 um modelo simpleficado de uma corda vibrando e u(x,t) representa o deslocamento de um ponto x no tempo t \\ge 0 . Suponha que a massa da corda seja \\rho = \\rho(x) e a elasticidade seja k = k(x) . Considere o peda\u00e7o da corda [x, x + \\Delta x] . Ent\u00e3o, a massa desse peda\u00e7o ser\u00e1 \\rho(x) \\Delta x , sua velocidade \u00e9 dada por u_t(x,t) (a derivada do deslocamento com respeito ao tempo) e a sua energia cin\u00e9tica \u00e9 \\Delta K = \\rho (u_t)^2 \\Delta x / 2 (lembre que energia cin\u00e9tica \u00e9 proporcional \u00e0 massa e ao quadrado da velocidade). Portanto, a enegia cin\u00e9tica total ser\u00e1 K(t) = \\frac{1}{2} \\int_0^L \\rho(x) (u_t(x,t))^2 \\, dx. Pela Lei de Hooke , a energia potencial da corda \u00e9 (k/2)y^2 em que y \u00e9 o aumento da corda dado pelo comprimento de u(x,t) na vari\u00e1vel x . Esse comprimento pode ser medido usando \\sqrt{1 + u_x^2} e, portanto, a energia potencial \u00e9 P(t) = \\frac{1}{2}\\int_0^L k(x)(1 + (u_x(x,t))^2) \\, dx. A a\u00e7\u00e3o da fun\u00e7\u00e3o u \u00e9, portanto, I(u) = \\int_0^T K(t) - P(t) \\, dt = \\frac{1}{2} \\int_0^T\\int_0^L \\rho(u_t)^2 - k(1 + u_x^2) \\, dx \\, dt. Se u minimiza essa a\u00e7\u00e3o, pelo Teorema de Euler-Lagrange, derivando com respeito a u_t e u_x , -2\\rho u_{tt} - 2(k_xu_x + ku_{xx}) = 0, que implica \\rho u_{tt} = -ku_{xx} - k_xu_x , isto \u00e9, se a elasticidade for constante na corda, temos a equa\u00e7\u00e3o da onda. Note que a nossa constru\u00e7\u00e3o sup\u00f5e a velocidade inicial conhecida. Portanto, al\u00e9m da condi\u00e7\u00e3o de fronteira usal, precisamos de uma condi\u00e7\u00e3o adicional (o que era de se esperar, pois nas EDOs, esse comportamento tamb\u00e9m acontecia).","title":"Interpreta\u00e7\u00e3o f\u00edsica"},{"location":"edp/wave_equation/#formula-dalembert","text":"Considere o problema \\begin{cases} u_{tt} - u_{xx} = 0, \\\\ u(x,0) = g(x), u_t(x,0) = h(x), \\end{cases} em que g e h s\u00e3o dadas. Note que u_{tt} - u_{xx} = u_{tt} + u_{tx} - u_{xt} - u_{xx} = \\left(\\frac{\\partial}{\\partial t} + \\frac{\\partial}{\\partial x}\\right)(u_t - u_x) = \\left(\\frac{\\partial}{\\partial t} + \\frac{\\partial}{\\partial x}\\right)\\left(\\frac{\\partial}{\\partial t} - \\frac{\\partial}{\\partial x}\\right)u. Apesar de ser uma nota\u00e7\u00e3o de funcional que pode ser complicada, ela \u00e9 muito \u00fatil para definir v(x,t) = \\left(\\frac{\\partial}{\\partial t} - \\frac{\\partial}{\\partial x}\\right)u, e, ent\u00e3o, v_t + v_x = 0, (x \\in \\mathbb{R}, t > 0), que \u00e9 a equa\u00e7\u00e3o do transporte. Sabemos que a solu\u00e7\u00e3o para esse problema \u00e9 v(x,t) = a(x - t), em que a(x) := v(x,0) , e, portanto u_t(x,t) - u_x(x,t) = a(x,t), que tamb\u00e9m o problema do transporte n\u00e3o homog\u00eaneo. Esse problema tem solu\u00e7\u00e3o conhecida : u(x,t) = \\int_0^t a(x + (t-s) - s) \\, ds + b(x+t) = \\frac{1}{2}\\int_{x-t}^{x+t} a(y) \\, dy + b(x,t). Al\u00e9m disso, em t=0 , temos que b(x,0) = g(x) e a(x) = h(x) - g'(x) . Portanto, u(x,t) = \\frac{1}{2} \\int_{x-t}^{x+t} h(y) - g'(y) \\,dy + g(x+t), que implica u(x,t) = \\frac{1}{2}[g(x+t) + g(x-t)] + \\frac{1}{2}\\int_{x-t}^{x+t} h(y) \\, dy. Esta \u00e9 a f\u00f3rmula de d'Alembert. Note que na solu\u00e7\u00e3o, assumimos que u \u00e9 suficientemente suave (classe C^2 ). Essa condi\u00e7\u00e3o \u00e9 verificada se g \\in C^2(\\mathbb{R}) e h \\in C^1(\\mathbb{R}) . Dizemos, ent\u00e3o, que a solu\u00e7\u00e3o u tem a forma u(x,t) = F(x+t) + G(x-t) Observa\u00e7\u00e3o: Esse desenvolvimento assume que a corda \u00e9 infinita.","title":"F\u00f3rmula d'Alembert"},{"location":"edp/wave_equation/#extensoes","text":"Esse \u00e9 um t\u00f3pico muito rico que pode ter muitas varia\u00e7\u00f5es. Aqui apresentarei duas extens\u00f5es que podem ser mais estudadas nas refer\u00eancais do curso.","title":"Extens\u00f5es"},{"location":"edp/wave_equation/#medias-esfericas-e-formulas-de-poisson-e-kirchhoff","text":"Para dimens\u00f5es espaciais n \\ge 2 , a solu\u00e7\u00e3o \u00e9 um pouco mais complexa. Para isso, \u00e9 preciso resolver a equa\u00e7\u00e3o de Euler-Poisson-Darboux e reduzir o problema ao formato que se possa aplicar a f\u00f3rmula de d'Alembert. Para o detalhamento desse processo, o livro do professor Lawrence Evans \u00e9 bastante indicado.","title":"M\u00e9dias esf\u00e9ricas e f\u00f3rmulas de Poisson e Kirchhoff"},{"location":"edp/wave_equation/#corda-finita","text":"Nesse caso, temos restri\u00e7\u00f5es de contorno quando x=0 e x=l (comprimento da corda): \\begin{cases} u_{tt} = c^2u_{xx} \\\\ u(0,t) = u(l,t) = 0, \\forall t \\ge 0, \\\\ u(x,0) = f(x), x \\in [0,l] \\\\ u_t(x,0) = g(x), x \\in [0,l]. \\end{cases} Podemos provar o seguinte teorema: Teorema: Seja f \\in C^2[0,l] e g \\in C^1[0,l] tais que f, f'' e g se anulam em x=0 e x=l , ent\u00e3o u(x,t) = \\frac{F(x+ct) + F(x-ct)}{2} + \\frac{1}{2c} \\int_{x-ct}^{x+ct} G(s) \\, ds, em que F e G s\u00e3o extens\u00f5es peri\u00f3dicas (per\u00edodo 2l ) e \u00edmpares das fun\u00e7\u00f5es f e g , respecticamente, \u00e9 a \u00fanica solu\u00e7\u00e3o do problema de corda infinita.","title":"Corda finita"},{"location":"edp/wave_equation/#o-que-e-extensao-periodica-e-impar","text":"Dizemos que uma fun\u00e7\u00e3o f: A \\to B tem per\u00edodo T se para todo x \\in A , temos que x+ T \\in A e f(x) = f(x+ T) . A fun\u00e7\u00e3o ser\u00e1 \u00edmpar se f(x) = -f(-x) para x \\le -l Assim, definimos a extens\u00e3o peri\u00f3dica \u00edmpar de f como F(x) = \\begin{cases} -f(-x + 2kl ) &x \\in [(2k-1)l, 2kl] \\\\ f(x - 2kl) &x \\in [2kl, (2k+1)l],\\\\ \\end{cases} que \u00e9 definida em intervalor fechados.","title":"O que \u00e9 extens\u00e3o peri\u00f3dica e \u00edmpar?"},{"location":"edp/wave_equation/#metodos-de-energia","text":"Esses m\u00e9todos nos ajudam a estudar a unicidade da solu\u00e7\u00e3o u do problema. Considere novamente o problema \\begin{cases} u_{tt} - u_{xx} = 0, \\\\ u(x,0) = g(x), u_t(x,0) = h(x), \\end{cases} Suponha que u e v sejam solu\u00e7\u00f5es para esse problema. Ent\u00e3o w = u - v \u00e9 uma solu\u00e7\u00e3o para \\begin{cases} u_{tt} - u_{xx} = 0, \\\\ u(x,0) =0, u_t(x,0) = 0. \\end{cases} Defina a energia E(t) := \\frac{1}{2}\\int_U w_t^2(x,t) + w_x(x,t)^2 \\, dx, 0 \\le t \\le T. Derivando com respeito a t e supondo U limitado com fronteira suave, temos que (F\u00f3rmula de Leibniz) \\dot{E}(t) = \\int_U w_tw_{tt} + w_xw_{tx} \\, dx = \\int_U w_t(w_{tt} - w_{xx}) = 0, em que a segunda igualdade vem de uma integra\u00e7\u00e3o por partes combinada com o fato de na fronteira w = 0 . Portanto E(t) = E(0) para todo T \\ge t \\ge 0 , que implica E(t) = 0 . Em paricular, w_t(x,t) = w_x(x,t) = 0, \\forall x, t e, portanto w \\equiv 0 .","title":"M\u00e9todos de energia"},{"location":"edp/wave_equation/#sugestoes","text":"CFL (1928) Discretiza\u00e7\u00e3o da equa\u00e7\u00e3o da onda","title":"Sugest\u00f5es"},{"location":"edp/____solutions5/solutions5/","text":"Lista 5 - Equa\u00e7\u00f5es Diferenciais Parciais Essas s\u00e3o poss\u00edveis solu\u00e7\u00f5es. Exerc\u00edcios Te\u00f3ricos Nesse parte, exponho os exerc\u00edcios propostos para serem realizados de forma te\u00f3rica. Exerc\u00edcios Computacionais Afinador Nessa se\u00e7\u00e3o, apresentados um simples afinador constru\u00eddo com Matlab. O princ\u00edpio \u00e9 bem b\u00e1sico: Fa\u00e7o a transforma\u00e7\u00e3o do espa\u00e7o temporal para o espa\u00e7o das frequ\u00eancias utilizando a transformada de Fourier FFT. Depois, observamos que teremos a frequ\u00eancia dominante e seus harm\u00f4nicos, dado o comportamento da onda. Por esse motivo, precisamos de uma t\u00e9cnica para, dentre os harm\u00f4nicos, ver qual de fato \u00e9 o dominante. A t\u00e9cnica utilizada \u00e9 o HPS (Harmonic Product Spectrum). Ele se utiliza do fato de um harm\u00f4nico ter frequ\u00eancia m\u00faltipla daa frequ\u00eancia dominante. A t\u00e9cnica funciona rapidamente, o que \u00e9 interessante. N\u00e3o h\u00e1 nenhuma outra filtragem, portanto, em um local muito ruidoso, talvez n\u00e3o funcione o afinador. A entrada \u00e9 uma string com o nome do arquivo .wav e a sa\u00edda \u00e9 a nota com frequ\u00eancia mais aproximada entre C, C#, D, D#, E, F, F#, G, G#, A, A#, B . note = afinador('CordaViolao.wav'); disp('A nota tocada foi: ') disp(note) function [note] = hps(yhat, n, N) % Esta fun\u00e7\u00e3o tem o objetivo de calcular o espectro produto harm\u00f4nico % A partir de uma yhat na frequ\u00eancia e de um n\u00famero N de harm\u00f4nicos % calcula m\u00e9dia geom\u00e9trica. % Queremos reconhecer os harm\u00f4nicos do tom principal. note = ones(n,1); bins = 1:n; for i = 1:N bins_temp = bins*i; xhat = ones(n,1); mask = bins_temp <= n; xhat(mask) = yhat(bins_temp(mask)); note = note.*abs(xhat); end note = (note).^(1/N); end function [strnote] = get_note(frequency) notes = char({'C','C#','D','D#','E','F','F#','G','G#','A','A#','B'}); freqs = [523, 554,587, 622,659,698, 740,784, 831,880, 932, 988]; % Se a frequ\u00eancia n\u00e3o est\u00e1 na oitava, o coloco multiplicando e dividindo % por 2. Sabemos que a nota ser\u00e1 a mesma, em uma diferente frequ\u00eancia. while frequency <= 508 frequency = frequency*2; end while frequency > 988 + 19 frequency = frequency/2; end [~, argmin] = min(abs(freqs - frequency)); strnote = notes(argmin,:); strnote = strnote(~isspace(strnote)); end function [note] = afinador(name) [y,Fs] = audioread(name); n = length(y); % transform the time scale to frequency scale yhat = fft(y); yhat = abs(yhat); freq = (1:n)*Fs/n; % use the function harmonic product spectrum yhat = hps(yhat, n, 5); % get the dominant tone [~, argmax] = max(yhat); note = freq(argmax); note = get_note(note); end O Polvo Nesse exerc\u00edcio, primeiro temos o polvo, representado pelos vetores x e y Na letra b somos indagados a produzir sequ\u00eancias (x_1(t), x_2(t), ... , x_n(t)) e (y_1(t), y_2(t), ... , y_n(t)) que sejam vers\u00f5es suavizadas das sequ\u00eancias. Para isso, \u00e9 sugerido utilizar a solu\u00e7\u00e3o da equa\u00e7\u00e3o do calor. Sabemos que L \u00e9 uma matrix circulante e se descrevermos P como uma matriz de permuta\u00e7\u00e3o que coloca a primeira coordenada de um vetor na n -\u00e9sima posi\u00e7\u00e3o, teremos que: L = l_0I + l_1P + ... + l_{n-1}P^{n-1} = 2I - P - P^{n-1} E se v_j \u00e9 autovalor de P , ser\u00e1 tamb\u00e9m de L , pois: Lv_j = 2v_j - \\lambda_jv_j - \\lambda_j^{n-1}v_j = (2 - \\lambda_j - \\lambda_j^{n-1})v_j Al\u00e9m disso, sabemos que autovetores de uma matriz circulante formam a base de Fourier, e, al\u00e9m disso, os autovalores dessa matriz de permuta\u00e7\u00e3o s\u00e3o aqueles que resolvem a equa\u00e7\u00e3o \\lambda^n = 1 . E portanto, \\lambda_j = e^{2\\pi ij/n} . Isto \u00e9, podemos obter os autovalores da matriz L fazendo a transforma\u00e7\u00e3o de Fourier na primeira linha de L ! (Sei que estava no lembrete j\u00e1, mas quis conferir as contas). Aqui segue o c\u00f3digo que gera uma sequ\u00eancia de 10 itera\u00e7\u00f5es do Polvo. Eu fa\u00e7o um pr\u00e9-c\u00e1lculo da matriz diagonal do sistema como mencionado acima e, se F \u00e9 a matriz da transformada de Fourier: x(t) = e^{-cLt}x_0 = Fe^{-Dt}F^{-1}x_0 = \\text{fft}(e^{-Dt}\\cdot\\text{ifft}(x_0)) y(t) = e^{-cLt}y_0 = Fe^{-Dt}F^{-1}y_0 = \\text{fft}(e^{-Dt}\\cdot\\text{ifft}(y_0)) Aqui vemos o polvo com c = 0.5 e t = 501, 5001, 10001, 14501 . Como conclus\u00e3o, conseguimos sequ\u00eancias \\vec{x}(t) e \\vec{y}(t) que sejam vers\u00f5es suavizadas de \\vec{x} e \\vec{y} . polvo = csvread('Polvo.csv'); c = 0.5; [D,x0,y0] = pre_calculation(polvo(:,1), polvo(:,2), c); figure plot(polvo(:,1), polvo(:,2),'r.') xlim([min(polvo(:,1)), max(polvo(:,1))]); ylim([min(polvo(:,2)), max(polvo(:,2))]); title('O Polvo de EDP') xlabel('x') ylabel('y') pause(2) for t = 1:250:15000 x = heat_filter(x0, t, D); y = heat_filter(y0, t, D); plot(x,y) disp(t) pause(0.1) if ismember(t, [501, 10001, 14501]) saveas(gcf, join([\"Polvo-\", t, \".png\"], '')); end end clear D; clear x; clear y; clear x0; clear y0; function [D, x0, y0] = pre_calculation(x, y, c) % Calculate the diagonal matrix of convolucional matrix of size n n = length(x); D = fft(c*[-2,1,zeros(1,n-3),1]'); x0 = ifft(x); y0 = ifft(y); end function [filter] = heat_filter(u0, t, D) % This function get a vector, make a convolucional filter (gaussian), % that is equivalent to the solution of heat equation and return, for % each t, the x(t) smoothed. filter = exp(D*t).*u0; filter = real(fft(filter)); end","title":"Lista 5 - Equa\u00e7\u00f5es Diferenciais Parciais"},{"location":"edp/____solutions5/solutions5/#lista-5-equacoes-diferenciais-parciais","text":"Essas s\u00e3o poss\u00edveis solu\u00e7\u00f5es.","title":"Lista 5 - Equa\u00e7\u00f5es Diferenciais Parciais"},{"location":"edp/____solutions5/solutions5/#exercicios-teoricos","text":"Nesse parte, exponho os exerc\u00edcios propostos para serem realizados de forma te\u00f3rica.","title":"Exerc\u00edcios Te\u00f3ricos"},{"location":"edp/____solutions5/solutions5/#exercicios-computacionais","text":"","title":"Exerc\u00edcios Computacionais"},{"location":"edp/____solutions5/solutions5/#afinador","text":"Nessa se\u00e7\u00e3o, apresentados um simples afinador constru\u00eddo com Matlab. O princ\u00edpio \u00e9 bem b\u00e1sico: Fa\u00e7o a transforma\u00e7\u00e3o do espa\u00e7o temporal para o espa\u00e7o das frequ\u00eancias utilizando a transformada de Fourier FFT. Depois, observamos que teremos a frequ\u00eancia dominante e seus harm\u00f4nicos, dado o comportamento da onda. Por esse motivo, precisamos de uma t\u00e9cnica para, dentre os harm\u00f4nicos, ver qual de fato \u00e9 o dominante. A t\u00e9cnica utilizada \u00e9 o HPS (Harmonic Product Spectrum). Ele se utiliza do fato de um harm\u00f4nico ter frequ\u00eancia m\u00faltipla daa frequ\u00eancia dominante. A t\u00e9cnica funciona rapidamente, o que \u00e9 interessante. N\u00e3o h\u00e1 nenhuma outra filtragem, portanto, em um local muito ruidoso, talvez n\u00e3o funcione o afinador. A entrada \u00e9 uma string com o nome do arquivo .wav e a sa\u00edda \u00e9 a nota com frequ\u00eancia mais aproximada entre C, C#, D, D#, E, F, F#, G, G#, A, A#, B . note = afinador('CordaViolao.wav'); disp('A nota tocada foi: ') disp(note) function [note] = hps(yhat, n, N) % Esta fun\u00e7\u00e3o tem o objetivo de calcular o espectro produto harm\u00f4nico % A partir de uma yhat na frequ\u00eancia e de um n\u00famero N de harm\u00f4nicos % calcula m\u00e9dia geom\u00e9trica. % Queremos reconhecer os harm\u00f4nicos do tom principal. note = ones(n,1); bins = 1:n; for i = 1:N bins_temp = bins*i; xhat = ones(n,1); mask = bins_temp <= n; xhat(mask) = yhat(bins_temp(mask)); note = note.*abs(xhat); end note = (note).^(1/N); end function [strnote] = get_note(frequency) notes = char({'C','C#','D','D#','E','F','F#','G','G#','A','A#','B'}); freqs = [523, 554,587, 622,659,698, 740,784, 831,880, 932, 988]; % Se a frequ\u00eancia n\u00e3o est\u00e1 na oitava, o coloco multiplicando e dividindo % por 2. Sabemos que a nota ser\u00e1 a mesma, em uma diferente frequ\u00eancia. while frequency <= 508 frequency = frequency*2; end while frequency > 988 + 19 frequency = frequency/2; end [~, argmin] = min(abs(freqs - frequency)); strnote = notes(argmin,:); strnote = strnote(~isspace(strnote)); end function [note] = afinador(name) [y,Fs] = audioread(name); n = length(y); % transform the time scale to frequency scale yhat = fft(y); yhat = abs(yhat); freq = (1:n)*Fs/n; % use the function harmonic product spectrum yhat = hps(yhat, n, 5); % get the dominant tone [~, argmax] = max(yhat); note = freq(argmax); note = get_note(note); end","title":"Afinador"},{"location":"edp/____solutions5/solutions5/#o-polvo","text":"Nesse exerc\u00edcio, primeiro temos o polvo, representado pelos vetores x e y Na letra b somos indagados a produzir sequ\u00eancias (x_1(t), x_2(t), ... , x_n(t)) e (y_1(t), y_2(t), ... , y_n(t)) que sejam vers\u00f5es suavizadas das sequ\u00eancias. Para isso, \u00e9 sugerido utilizar a solu\u00e7\u00e3o da equa\u00e7\u00e3o do calor. Sabemos que L \u00e9 uma matrix circulante e se descrevermos P como uma matriz de permuta\u00e7\u00e3o que coloca a primeira coordenada de um vetor na n -\u00e9sima posi\u00e7\u00e3o, teremos que: L = l_0I + l_1P + ... + l_{n-1}P^{n-1} = 2I - P - P^{n-1} E se v_j \u00e9 autovalor de P , ser\u00e1 tamb\u00e9m de L , pois: Lv_j = 2v_j - \\lambda_jv_j - \\lambda_j^{n-1}v_j = (2 - \\lambda_j - \\lambda_j^{n-1})v_j Al\u00e9m disso, sabemos que autovetores de uma matriz circulante formam a base de Fourier, e, al\u00e9m disso, os autovalores dessa matriz de permuta\u00e7\u00e3o s\u00e3o aqueles que resolvem a equa\u00e7\u00e3o \\lambda^n = 1 . E portanto, \\lambda_j = e^{2\\pi ij/n} . Isto \u00e9, podemos obter os autovalores da matriz L fazendo a transforma\u00e7\u00e3o de Fourier na primeira linha de L ! (Sei que estava no lembrete j\u00e1, mas quis conferir as contas). Aqui segue o c\u00f3digo que gera uma sequ\u00eancia de 10 itera\u00e7\u00f5es do Polvo. Eu fa\u00e7o um pr\u00e9-c\u00e1lculo da matriz diagonal do sistema como mencionado acima e, se F \u00e9 a matriz da transformada de Fourier: x(t) = e^{-cLt}x_0 = Fe^{-Dt}F^{-1}x_0 = \\text{fft}(e^{-Dt}\\cdot\\text{ifft}(x_0)) y(t) = e^{-cLt}y_0 = Fe^{-Dt}F^{-1}y_0 = \\text{fft}(e^{-Dt}\\cdot\\text{ifft}(y_0)) Aqui vemos o polvo com c = 0.5 e t = 501, 5001, 10001, 14501 . Como conclus\u00e3o, conseguimos sequ\u00eancias \\vec{x}(t) e \\vec{y}(t) que sejam vers\u00f5es suavizadas de \\vec{x} e \\vec{y} . polvo = csvread('Polvo.csv'); c = 0.5; [D,x0,y0] = pre_calculation(polvo(:,1), polvo(:,2), c); figure plot(polvo(:,1), polvo(:,2),'r.') xlim([min(polvo(:,1)), max(polvo(:,1))]); ylim([min(polvo(:,2)), max(polvo(:,2))]); title('O Polvo de EDP') xlabel('x') ylabel('y') pause(2) for t = 1:250:15000 x = heat_filter(x0, t, D); y = heat_filter(y0, t, D); plot(x,y) disp(t) pause(0.1) if ismember(t, [501, 10001, 14501]) saveas(gcf, join([\"Polvo-\", t, \".png\"], '')); end end clear D; clear x; clear y; clear x0; clear y0; function [D, x0, y0] = pre_calculation(x, y, c) % Calculate the diagonal matrix of convolucional matrix of size n n = length(x); D = fft(c*[-2,1,zeros(1,n-3),1]'); x0 = ifft(x); y0 = ifft(y); end function [filter] = heat_filter(u0, t, D) % This function get a vector, make a convolucional filter (gaussian), % that is equivalent to the solution of heat equation and return, for % each t, the x(t) smoothed. filter = exp(D*t).*u0; filter = real(fft(filter)); end","title":"O Polvo"},{"location":"edp/characteristics/characteristics/","text":"O M\u00e9todo das Caracter\u00edsticas Considere a Equa\u00e7\u00e3o Diferencial Parcial (EDP) F\\left(\\frac{\\partial u}{\\partial x_1}, \\dots, u, \\frac{\\partial u}{\\partial x_n}, x_1, \\dots, x_n\\right) = F(x, u, \\nabla u) = F(x, u, Du) = 0, definida em um conjunto U . Al\u00e9m disso, suponha que u = g na fronteira de U , em que g \u00e9 dada fun\u00e7\u00e3o suave. Descri\u00e7\u00e3o do m\u00e9todo A ideia geral desse m\u00e9todo \u00e9 transformar a EDP em um sistema de EDOs, em que temos uma teoria de resolu\u00e7\u00e3o bem estabelecida. Nisso, vamos construir curvas da superf\u00edcie formada por u e integrar nessas curvas. Seja \\gamma(s) = (a_1(s), \\dots, a_n(s)) essa curva (definida em U ). Assumindo que u \u00e9 duas vezes continuamente diferenci\u00e1vel, defina z(s) := u(\\gamma(s)). Tamb\u00e9m defina p(s) = \\nabla u(\\gamma(s)) = (u_{x_1}(\\gamma(s)), \\dots, u_{x_n}(\\gamma(s))) . Temos que (Regra da Cadeia) \\frac{d}{ds}p_i(s) = \\sum_{j=1}^n u_{x_ix_j}(\\gamma(s))\\frac{d}{ds}a_j(s). Voltando \u00e0 EDP F(x, u, Du) = 0 , derivando com respeito a x_i , \\sum_{j=1}^n F_{u_j} (Du, u, x)u_{x_j x_i} + F_z(Du, u, x)u_{x_i} + F_{x_i}(Du, u, x) = 0. Vamos usar essa express\u00e3o para remover as segundas derivadas de u (que s\u00e3o em geral complicadas de se encontrar) Para isso, definimos \\frac{d}{ds}a_i(s) = F_{p_i}(p(s), z(s), \\gamma(s)), i = 1, \\dots n, isto \u00e9, estamos definindo uma curva a partir de sua fun\u00e7\u00e3o tangente. Assumindo isso e avaliando a express\u00e3o em \\gamma(s) , obtemos que u_{x_i}(\\gamma(s)) = p_i(s) , logo \\sum_{j=1}^n F_{p_j} (p(s), z(s), \\gamma(s))u_{x_j x_i}(\\gamma(s)) + F_z(p(s), z(s), \\gamma(s))p_i(s) + F_{x_i}(p(s), z(s), \\gamma(s)) = 0. Usando a express\u00e3o de \\frac{d}{ds}p_i(s) dada mais acima, teremos que \\frac{d}{ds}p_i(s) = - F_{x_i}(p(s), z(s), \\gamma(s)) - F_z(p(s), z(s), \\gamma(s))p_i(s), o que nos d\u00e1 uma EDO para a fun\u00e7\u00e3o p(s) = \\nabla u(\\gamma(s)) . Al\u00e9m disso, diferenciando z obtemos \\frac{d}{ds}z(s) = \\sum_{j=1}^n u_{x_j}(\\gamma(s))\\frac{d}{ds}a_j(s) = \\sum_{j=1}^n p_j(s)F_{p_j}(p(s), z(s), \\gamma(s)). Isso nos reduz a um sistema de EDOs: \\begin{cases} \\dot{p}(s) = - D_x F(p(s), z(s), \\gamma(s)) - D_z F(p(s), z(s), \\gamma(s))p(s) \\\\ \\dot{z}(s) = D_p F(p(s), z(s), \\gamma(s))\\cdot p(s) \\\\ \\dot{\\gamma}(s) = D_pF(p(s), z(s), \\gamma(s)), \\end{cases} em que D \u00e9 a derivada (no caso vetorial, mas voc\u00ea pode pensar indiv\u00edduo a ind\u00edviduo usando as express\u00f5es derivadas acima). Al\u00e9m disso, ao longo da curva \\gamma(s) , F(p(s), z(s), \\gamma(s)) = 0, pela pr\u00f3pria defini\u00e7\u00e3o da F . F \u00e9 linear Considere F(Du, u, x) = b(x)\\cdot Du(x) + c(x)u(x) = 0 , isto \u00e9, o caso linear. Ao longo das curvas caracter\u00edsticas, F(p,z,x) = b(x)\\cdot p + c(x)z . Assim, \\begin{cases} \\dot{p}(s) = - D_x F(p(s), z(s), \\gamma(s)) - D_z F(p(s), z(s), \\gamma(s))p(s) \\\\ \\dot{z}(s) = D_p F(p(s), z(s), \\gamma(s))\\cdot p(s) = b(\\gamma(s))\\cdot p(s) = -c(\\gamma(s))z(s) \\\\ \\dot{\\gamma}(s) = D_p(p(s), z(s), \\gamma(s)) = b(\\gamma(s)), \\end{cases} Com isso, mesmo sem saber p , ainda conseguimos derivar z , o que simplifica bastante o problema. Condi\u00e7\u00f5es de fronteira Anteriormente, definimos um sistema de equa\u00e7\u00f5es diferenciais para resolver u(x) . Todavia, esse sistema admite infinitas solu\u00e7\u00f5es quando n\u00e3o especificado uma condi\u00e7\u00e3o inicial. Para isso, tome x_0 na fronteira de U , onde sabemos que u = g . Em geral, assumimos que essa fronteira fica no plano {x_n = 0} pr\u00f3ximo de x_0 . Como assim? Suponha que estamos com U \\subseteq \\mathbb{R}^2 e que a solu\u00e7\u00e3o seja dada pela fun\u00e7\u00e3o u(x,t) . Estamos dizendo que se \\gamma(0) = (x_0, 0) , sendo \\gamma a curva caracter\u00edstica. Suponha que U \\subseteq \\mathbb{R}^4 e que a solu\u00e7\u00e3o seja dada pela fun\u00e7\u00e3o u(x_1, \\dots, x_4) . Estamos dizendo que se \\gamma(0) = (x^{1}_0, x^{2}_0, x^{3}_0, 0) . Quando temos uma vari\u00e1vel temporal, em geral denotada po t , dizemos que ela sempre come\u00e7a em 0, uma forma de \"padronizar\". Sugiro o livro do Lawrence, se\u00e7\u00e3o 3.2.3 para uma demonstra\u00e7\u00e3o de que essa suposi\u00e7\u00e3o faz sentido. Dado um x_0 , falta agora definir p(0) = p_0, z(0) = z_0, \\gamma(0) = (x_0,0). Est\u00e1 claro que z_0 = u(\\gamma(0)) = u(x_0, 0) = g(x_0) . Al\u00e9m disso, u(x_1, \\dots, x_{n-1}, 0) = g(x_1, \\dots, x_{n-1}) na vizinhan\u00e7a de x_0 e, portanto, podemos diferenciar para obter u_{x_i}(x_0,0) = g_{x_i}(x_0), \\text{ para } i = 1,\\dots, n-1. Dessa fora, (p_0)^i = g_{x_i}(x_0) para cada i . Para determinar (p_0)^n , usamos a rela\u00e7\u00e3o dada por F , isto \u00e9, F(p_0, z_0, x_0) = 0, por defini\u00e7\u00e3o. As rela\u00e7\u00f5es de z(0) e p(0) s\u00e3o chamadas de condi\u00e7\u00f5es de compatibilidade . Note que pode n\u00e3o existir ou pode n\u00e3o ser \u00fanico a solu\u00e7\u00e3o de p_0 atrav\u00e9s da equa\u00e7\u00e3o F = 0 . Exist\u00eancia local de solu\u00e7\u00f5es Dado y = (y_1, \\dots, y_{n-1}, 0) , queremos resolver \\begin{cases} \\dot{p}(s) = - D_x F(p(s), z(s), \\gamma(s)) - D_z F(p(s), z(s), \\gamma(s))p(s) \\\\ \\dot{z}(s) = D_p F(p(s), z(s), \\gamma(s))\\cdot p(s) = b(\\gamma(s))\\cdot p(s) = -c(\\gamma(s))z(s) \\\\ \\dot{\\gamma}(s) = D_p(p(s), z(s), \\gamma(s)) = b(\\gamma(s)), \\end{cases} com \\gamma(0) = y, p(0) = p_0, z(0) = z_0 , com as express\u00f5es de compatibilidade derivadas acima. Lema (Uma aplica\u00e7\u00e3o do Teorema da Fun\u00e7\u00e3o Inversa): Assuma que F_{p_n}(p_0, z_0, x_0) \\neq 0 . Ent\u00e3o existe I \\ni 0 \\subseteq \\mathbb{R} , uma vizinhan\u00e7a W de x_0 na fronteira de U e uma vizinhan\u00e7a V de x_0 em \\mathbb{R}^n tal que para cada x \\in V , existe um \u00fanico s \\in I, y \\in W tal que x = \\gamma(y,s) = \\gamma(y_1, \\dots, y_{n-1},s). Essa lema \u00e9 uma consequ\u00eancia do Teorema da Fun\u00e7\u00e3o Inversa. N\u00e3o se preocupe tanto com a demonstra\u00e7\u00e3o. Mas a ideia \u00e9 que se provarmos que \\det D\\gamma(x_0, 0) \\neq 0 , valer\u00e1 a invertibilidade que estamos propondo, isto \u00e9, um mapa x \\mapsto (y,s) . Assim, seja y = y(x) e s = s(x) de forma que x = \\gamma(y,s) (que exite pelo que o lema prova). Assim, obtemos o seguinte Teorema: Teorema da Exist\u00eancia Local A fun\u00e7\u00e3o u(x) := z(y(x), s(x)) \u00e9 solu\u00e7\u00e3o para a EDP F(Du(x), u(x), x) = 0, em que x \\in V e u(x) = g(x) para x \\in \\partial U \\cap V , lembrando que \\partial U \u00e9 a fronteira de U . Leis da Conserva\u00e7\u00e3o Considere o problema da lei de conserva\u00e7\u00e3o para a dimens\u00e3o 1 u_t + [f(u)]_x = 0, x \\in \\mathbb{R}, t > 0 \\\\ u(x,0) = \\phi(x). Como nem sempre temos solu\u00e7\u00f5es diferenci\u00e1veis para u , temos que relaxar um pouco nossa defini\u00e7\u00e3o de solu\u00e7\u00e3o, e para isso introduzimos as solu\u00e7\u00f5es fracas . Solu\u00e7\u00e3o fraca (ou integral) Lembre que um conjunto em \\mathbb{R}^n \u00e9 compacto quando \u00e9 fechado e limitado. Uma fun\u00e7\u00e3o tem suporte compacto quando existe um compacto \\Lambda tal que para todo x \\in \\mathbb{R}^n / \\Lambda a fun\u00e7\u00e3o se anula. Definimos uma solu\u00e7\u00e3o fraca u quando \\int_0^{+\\infty}\\int_{-\\infty}^{+\\infty} [uv_t + f(u)v_x] \\, dx \\, dt + \\int_{-\\infty}^{+\\infty} \\phi(x) v(x,0) \\, dx = 0, para todas as fun\u00e7\u00f5es infinitamente diferenci\u00e1veis definidas em um conjunto compacto v (que chamamos de fun\u00e7\u00e3o teste ). Assim, a suavidade \u00e9 transferida para a fun\u00e7\u00e3o v . De forma equivalente, \\int_0^{+\\infty}\\int_{-\\infty}^{+\\infty} v[u + f(u)_x] \\, dx \\, dt = 0, Teorema: Se u \u00e9 uma solu\u00e7\u00e3o forte (no sentido de ser k vezes continuamente diferenci\u00e1vel), ent\u00e3o u ser\u00e1 uma solu\u00e7\u00e3o fraca. Como v \u00e9 um solu\u00e7\u00e3o que \u00e9 nula para um valor suficientemente grande e o integrando \u00e9 zero na solu\u00e7\u00e3o, ent\u00e3o a integral converge e, em particular, ser\u00e1 zero. Claro que precisamos primeiro mostrar a equival\u00eancia acima usando Integral por partes. Agora suponha que u \u00e9 uma fun\u00e7\u00e3o n\u00e3o cont\u00ednua em uma curva x = \\xi(t) , mas u \u00e9 suave em ambos os lados da curva (pensando em \\mathbb{R}^2 ). Denotamos u^{+}(x,t) para o limite de u quando se aproxima de (x,t) pela direita e u^{-}(x,t) pela esquerda. Vamos mostrar que existe uma rela\u00e7\u00e3o entre \\xi(t) , u^{-} , e u^+ . Teorema: Se u \u00e9 uma solu\u00e7\u00e3o fraca com a descontinuidade mencionada acima, ent\u00e3o, \\frac{f(u^{-}) - f(u^{+})}{u^{-} - u^{+}} = \\xi'(t) na curva de descontinuidade. Chamamos \\xi '(t) de velocidade da curva de descontinuidade . O denominador e o numerador s\u00e3o chamados de saltos . Essa condi\u00e7\u00e3o \u00e9 chamada de Condi\u00e7\u00e3o de Salto Rankine-Hugoniot . Nas imagens voc\u00ea confere um exemplo de quando u n\u00e3o \u00e9 cont\u00ednua da Equa\u00e7\u00e3o de Berger.","title":"O M\u00e9todo das Caracter\u00edsticas"},{"location":"edp/characteristics/characteristics/#o-metodo-das-caracteristicas","text":"Considere a Equa\u00e7\u00e3o Diferencial Parcial (EDP) F\\left(\\frac{\\partial u}{\\partial x_1}, \\dots, u, \\frac{\\partial u}{\\partial x_n}, x_1, \\dots, x_n\\right) = F(x, u, \\nabla u) = F(x, u, Du) = 0, definida em um conjunto U . Al\u00e9m disso, suponha que u = g na fronteira de U , em que g \u00e9 dada fun\u00e7\u00e3o suave.","title":"O M\u00e9todo das Caracter\u00edsticas"},{"location":"edp/characteristics/characteristics/#descricao-do-metodo","text":"A ideia geral desse m\u00e9todo \u00e9 transformar a EDP em um sistema de EDOs, em que temos uma teoria de resolu\u00e7\u00e3o bem estabelecida. Nisso, vamos construir curvas da superf\u00edcie formada por u e integrar nessas curvas. Seja \\gamma(s) = (a_1(s), \\dots, a_n(s)) essa curva (definida em U ). Assumindo que u \u00e9 duas vezes continuamente diferenci\u00e1vel, defina z(s) := u(\\gamma(s)). Tamb\u00e9m defina p(s) = \\nabla u(\\gamma(s)) = (u_{x_1}(\\gamma(s)), \\dots, u_{x_n}(\\gamma(s))) . Temos que (Regra da Cadeia) \\frac{d}{ds}p_i(s) = \\sum_{j=1}^n u_{x_ix_j}(\\gamma(s))\\frac{d}{ds}a_j(s). Voltando \u00e0 EDP F(x, u, Du) = 0 , derivando com respeito a x_i , \\sum_{j=1}^n F_{u_j} (Du, u, x)u_{x_j x_i} + F_z(Du, u, x)u_{x_i} + F_{x_i}(Du, u, x) = 0. Vamos usar essa express\u00e3o para remover as segundas derivadas de u (que s\u00e3o em geral complicadas de se encontrar) Para isso, definimos \\frac{d}{ds}a_i(s) = F_{p_i}(p(s), z(s), \\gamma(s)), i = 1, \\dots n, isto \u00e9, estamos definindo uma curva a partir de sua fun\u00e7\u00e3o tangente. Assumindo isso e avaliando a express\u00e3o em \\gamma(s) , obtemos que u_{x_i}(\\gamma(s)) = p_i(s) , logo \\sum_{j=1}^n F_{p_j} (p(s), z(s), \\gamma(s))u_{x_j x_i}(\\gamma(s)) + F_z(p(s), z(s), \\gamma(s))p_i(s) + F_{x_i}(p(s), z(s), \\gamma(s)) = 0. Usando a express\u00e3o de \\frac{d}{ds}p_i(s) dada mais acima, teremos que \\frac{d}{ds}p_i(s) = - F_{x_i}(p(s), z(s), \\gamma(s)) - F_z(p(s), z(s), \\gamma(s))p_i(s), o que nos d\u00e1 uma EDO para a fun\u00e7\u00e3o p(s) = \\nabla u(\\gamma(s)) . Al\u00e9m disso, diferenciando z obtemos \\frac{d}{ds}z(s) = \\sum_{j=1}^n u_{x_j}(\\gamma(s))\\frac{d}{ds}a_j(s) = \\sum_{j=1}^n p_j(s)F_{p_j}(p(s), z(s), \\gamma(s)). Isso nos reduz a um sistema de EDOs: \\begin{cases} \\dot{p}(s) = - D_x F(p(s), z(s), \\gamma(s)) - D_z F(p(s), z(s), \\gamma(s))p(s) \\\\ \\dot{z}(s) = D_p F(p(s), z(s), \\gamma(s))\\cdot p(s) \\\\ \\dot{\\gamma}(s) = D_pF(p(s), z(s), \\gamma(s)), \\end{cases} em que D \u00e9 a derivada (no caso vetorial, mas voc\u00ea pode pensar indiv\u00edduo a ind\u00edviduo usando as express\u00f5es derivadas acima). Al\u00e9m disso, ao longo da curva \\gamma(s) , F(p(s), z(s), \\gamma(s)) = 0, pela pr\u00f3pria defini\u00e7\u00e3o da F .","title":"Descri\u00e7\u00e3o do m\u00e9todo"},{"location":"edp/characteristics/characteristics/#f-e-linear","text":"Considere F(Du, u, x) = b(x)\\cdot Du(x) + c(x)u(x) = 0 , isto \u00e9, o caso linear. Ao longo das curvas caracter\u00edsticas, F(p,z,x) = b(x)\\cdot p + c(x)z . Assim, \\begin{cases} \\dot{p}(s) = - D_x F(p(s), z(s), \\gamma(s)) - D_z F(p(s), z(s), \\gamma(s))p(s) \\\\ \\dot{z}(s) = D_p F(p(s), z(s), \\gamma(s))\\cdot p(s) = b(\\gamma(s))\\cdot p(s) = -c(\\gamma(s))z(s) \\\\ \\dot{\\gamma}(s) = D_p(p(s), z(s), \\gamma(s)) = b(\\gamma(s)), \\end{cases} Com isso, mesmo sem saber p , ainda conseguimos derivar z , o que simplifica bastante o problema.","title":"F \u00e9 linear"},{"location":"edp/characteristics/characteristics/#condicoes-de-fronteira","text":"Anteriormente, definimos um sistema de equa\u00e7\u00f5es diferenciais para resolver u(x) . Todavia, esse sistema admite infinitas solu\u00e7\u00f5es quando n\u00e3o especificado uma condi\u00e7\u00e3o inicial. Para isso, tome x_0 na fronteira de U , onde sabemos que u = g . Em geral, assumimos que essa fronteira fica no plano {x_n = 0} pr\u00f3ximo de x_0 . Como assim? Suponha que estamos com U \\subseteq \\mathbb{R}^2 e que a solu\u00e7\u00e3o seja dada pela fun\u00e7\u00e3o u(x,t) . Estamos dizendo que se \\gamma(0) = (x_0, 0) , sendo \\gamma a curva caracter\u00edstica. Suponha que U \\subseteq \\mathbb{R}^4 e que a solu\u00e7\u00e3o seja dada pela fun\u00e7\u00e3o u(x_1, \\dots, x_4) . Estamos dizendo que se \\gamma(0) = (x^{1}_0, x^{2}_0, x^{3}_0, 0) . Quando temos uma vari\u00e1vel temporal, em geral denotada po t , dizemos que ela sempre come\u00e7a em 0, uma forma de \"padronizar\". Sugiro o livro do Lawrence, se\u00e7\u00e3o 3.2.3 para uma demonstra\u00e7\u00e3o de que essa suposi\u00e7\u00e3o faz sentido. Dado um x_0 , falta agora definir p(0) = p_0, z(0) = z_0, \\gamma(0) = (x_0,0). Est\u00e1 claro que z_0 = u(\\gamma(0)) = u(x_0, 0) = g(x_0) . Al\u00e9m disso, u(x_1, \\dots, x_{n-1}, 0) = g(x_1, \\dots, x_{n-1}) na vizinhan\u00e7a de x_0 e, portanto, podemos diferenciar para obter u_{x_i}(x_0,0) = g_{x_i}(x_0), \\text{ para } i = 1,\\dots, n-1. Dessa fora, (p_0)^i = g_{x_i}(x_0) para cada i . Para determinar (p_0)^n , usamos a rela\u00e7\u00e3o dada por F , isto \u00e9, F(p_0, z_0, x_0) = 0, por defini\u00e7\u00e3o. As rela\u00e7\u00f5es de z(0) e p(0) s\u00e3o chamadas de condi\u00e7\u00f5es de compatibilidade . Note que pode n\u00e3o existir ou pode n\u00e3o ser \u00fanico a solu\u00e7\u00e3o de p_0 atrav\u00e9s da equa\u00e7\u00e3o F = 0 .","title":"Condi\u00e7\u00f5es de fronteira"},{"location":"edp/characteristics/characteristics/#existencia-local-de-solucoes","text":"Dado y = (y_1, \\dots, y_{n-1}, 0) , queremos resolver \\begin{cases} \\dot{p}(s) = - D_x F(p(s), z(s), \\gamma(s)) - D_z F(p(s), z(s), \\gamma(s))p(s) \\\\ \\dot{z}(s) = D_p F(p(s), z(s), \\gamma(s))\\cdot p(s) = b(\\gamma(s))\\cdot p(s) = -c(\\gamma(s))z(s) \\\\ \\dot{\\gamma}(s) = D_p(p(s), z(s), \\gamma(s)) = b(\\gamma(s)), \\end{cases} com \\gamma(0) = y, p(0) = p_0, z(0) = z_0 , com as express\u00f5es de compatibilidade derivadas acima. Lema (Uma aplica\u00e7\u00e3o do Teorema da Fun\u00e7\u00e3o Inversa): Assuma que F_{p_n}(p_0, z_0, x_0) \\neq 0 . Ent\u00e3o existe I \\ni 0 \\subseteq \\mathbb{R} , uma vizinhan\u00e7a W de x_0 na fronteira de U e uma vizinhan\u00e7a V de x_0 em \\mathbb{R}^n tal que para cada x \\in V , existe um \u00fanico s \\in I, y \\in W tal que x = \\gamma(y,s) = \\gamma(y_1, \\dots, y_{n-1},s). Essa lema \u00e9 uma consequ\u00eancia do Teorema da Fun\u00e7\u00e3o Inversa. N\u00e3o se preocupe tanto com a demonstra\u00e7\u00e3o. Mas a ideia \u00e9 que se provarmos que \\det D\\gamma(x_0, 0) \\neq 0 , valer\u00e1 a invertibilidade que estamos propondo, isto \u00e9, um mapa x \\mapsto (y,s) . Assim, seja y = y(x) e s = s(x) de forma que x = \\gamma(y,s) (que exite pelo que o lema prova). Assim, obtemos o seguinte Teorema:","title":"Exist\u00eancia local de solu\u00e7\u00f5es"},{"location":"edp/characteristics/characteristics/#teorema-da-existencia-local","text":"A fun\u00e7\u00e3o u(x) := z(y(x), s(x)) \u00e9 solu\u00e7\u00e3o para a EDP F(Du(x), u(x), x) = 0, em que x \\in V e u(x) = g(x) para x \\in \\partial U \\cap V , lembrando que \\partial U \u00e9 a fronteira de U .","title":"Teorema da Exist\u00eancia Local"},{"location":"edp/characteristics/characteristics/#leis-da-conservacao","text":"Considere o problema da lei de conserva\u00e7\u00e3o para a dimens\u00e3o 1 u_t + [f(u)]_x = 0, x \\in \\mathbb{R}, t > 0 \\\\ u(x,0) = \\phi(x). Como nem sempre temos solu\u00e7\u00f5es diferenci\u00e1veis para u , temos que relaxar um pouco nossa defini\u00e7\u00e3o de solu\u00e7\u00e3o, e para isso introduzimos as solu\u00e7\u00f5es fracas .","title":"Leis da Conserva\u00e7\u00e3o"},{"location":"edp/characteristics/characteristics/#solucao-fraca-ou-integral","text":"Lembre que um conjunto em \\mathbb{R}^n \u00e9 compacto quando \u00e9 fechado e limitado. Uma fun\u00e7\u00e3o tem suporte compacto quando existe um compacto \\Lambda tal que para todo x \\in \\mathbb{R}^n / \\Lambda a fun\u00e7\u00e3o se anula. Definimos uma solu\u00e7\u00e3o fraca u quando \\int_0^{+\\infty}\\int_{-\\infty}^{+\\infty} [uv_t + f(u)v_x] \\, dx \\, dt + \\int_{-\\infty}^{+\\infty} \\phi(x) v(x,0) \\, dx = 0, para todas as fun\u00e7\u00f5es infinitamente diferenci\u00e1veis definidas em um conjunto compacto v (que chamamos de fun\u00e7\u00e3o teste ). Assim, a suavidade \u00e9 transferida para a fun\u00e7\u00e3o v . De forma equivalente, \\int_0^{+\\infty}\\int_{-\\infty}^{+\\infty} v[u + f(u)_x] \\, dx \\, dt = 0, Teorema: Se u \u00e9 uma solu\u00e7\u00e3o forte (no sentido de ser k vezes continuamente diferenci\u00e1vel), ent\u00e3o u ser\u00e1 uma solu\u00e7\u00e3o fraca. Como v \u00e9 um solu\u00e7\u00e3o que \u00e9 nula para um valor suficientemente grande e o integrando \u00e9 zero na solu\u00e7\u00e3o, ent\u00e3o a integral converge e, em particular, ser\u00e1 zero. Claro que precisamos primeiro mostrar a equival\u00eancia acima usando Integral por partes. Agora suponha que u \u00e9 uma fun\u00e7\u00e3o n\u00e3o cont\u00ednua em uma curva x = \\xi(t) , mas u \u00e9 suave em ambos os lados da curva (pensando em \\mathbb{R}^2 ). Denotamos u^{+}(x,t) para o limite de u quando se aproxima de (x,t) pela direita e u^{-}(x,t) pela esquerda. Vamos mostrar que existe uma rela\u00e7\u00e3o entre \\xi(t) , u^{-} , e u^+ . Teorema: Se u \u00e9 uma solu\u00e7\u00e3o fraca com a descontinuidade mencionada acima, ent\u00e3o, \\frac{f(u^{-}) - f(u^{+})}{u^{-} - u^{+}} = \\xi'(t) na curva de descontinuidade. Chamamos \\xi '(t) de velocidade da curva de descontinuidade . O denominador e o numerador s\u00e3o chamados de saltos . Essa condi\u00e7\u00e3o \u00e9 chamada de Condi\u00e7\u00e3o de Salto Rankine-Hugoniot . Nas imagens voc\u00ea confere um exemplo de quando u n\u00e3o \u00e9 cont\u00ednua da Equa\u00e7\u00e3o de Berger.","title":"Solu\u00e7\u00e3o fraca (ou integral)"},{"location":"edp/existence_theorem/existence_theorem/","text":"Teorema de Picard-Lindelof Este \u00e9 um belo teorema de exst\u00eancia e unicidade de equa\u00e7\u00f5es diferenciais ordin\u00e1rias da forma y'(t) = f(t, y(t)), y(t_0) = y_0. import numpy as np from scipy.integrate import solve_ivp, quad import matplotlib.pyplot as plt import seaborn as sns sns.set() O teorema basicamente consiste em construir um operador da seguinte forma: L(y(t)) = y_0 + \\int_{t_0}^t f(s,y(s)) \\, ds, em que L \u00e9 um operador funcional. Note que se se L(y(t)) = y(t) para todo t , teremos resolvido o problema, dado que y(t) = y_0 + \\int_{t_0}^t f(s,y(s)) \\, ds, \u00e9 uma solu\u00e7\u00e3o para y' = f(t,y) . Nesse caso, dizemos que queremos encontrar um ponto fixo do operador L . Se demonstrarmos que L possui um ponto fixo e ele \u00e9 \u00fanico em algum intevalo suficientemente pequeno, ent\u00e3o teremos demonstrado nosso teorema. Um teorema de exist\u00eancia (e unicidade local tamb\u00e9m) \u00e9 o Teorema de Banach . Para isso, bastaria mostrar que L leva fun\u00e7\u00f5es de um espa\u00e7o no mesmo espa\u00e7o e que \u00e9 uma contra\u00e7\u00e3o, isto \u00e9, intuitivamente, se a cada vez que aplicamos L , as fun\u00e7\u00f5es se aproximam e continuam no mesmo espa\u00e7o, ent\u00e3o vai existir um ponto fixo para ele. Nossa ideia aqui n\u00e3o \u00e9 demonstrar o Teorema, mas sim mostrar a intera\u00e7\u00e3o de Picard que \u00e9 escrita da seguinte forma: y_{k+1}(t) = y_0 + \\int_{0}^t f(s,y_k(s)) \\, ds, \\\\ y_0(t) = y_0. O teorema de ponto fixo de Banach nos mostra que y_{k} converge para a solu\u00e7\u00e3o y para todo ponto t em uma vizinhan\u00e7a de 0. Usaremos a fun\u00e7\u00e3o de quadratura para integrar a fun\u00e7\u00e3o f . a = (1,2) (1,*a) (1, 1, 2) def picard_iteration(fun, t_eval, y0, yk, args): pieces = [quad(func = fun, a = t_eval[i], b = t_eval[i+1], args = ((yk[i] + yk[i+1])/2, *args) )[0] for i in range(len(t_eval)-1)] pieces.insert(0,0) yk = np.cumsum(np.array(pieces)) + y0 return yk Considere o problema y' = ay, \\\\ y(0) = y_0 # defining the function f = lambda t, y, a: a*y a = 2 # time of definition upper = 1 n = 100 t = np.linspace(0,upper,n) Esta \u00e9 a solu\u00e7\u00e3o usando Runge-Kutta. solution = solve_ivp(fun = f, t_span = (0, upper), t_eval = t, y0 = (1,), method = 'RK45', args = (a,)) Aqui n\u00f3s apresentamos as primeiras itera\u00e7\u00f5es de Picard. y0 = np.ones(n) y1 = picard_iteration(fun = f, t_eval = t, y0 = (1,), yk = y0, args = (a,)) y2 = picard_iteration(fun = f, t_eval = t, y0 = (1,), yk = y1, args = (a,)) y3 = picard_iteration(fun = f, t_eval = t, y0 = (1,), yk = y2, args = (a,)) y4 = picard_iteration(fun = f, t_eval = t, y0 = (1,), yk = y3, args = (a,)) plt.figure(figsize = (10,6)) plt.title('Picard iterations to approximate a function') plt.plot(solution.t, solution.y[0], label = 'Solution y(t)') plt.plot(solution.t, y0, label = r'$y_0(t)$') plt.plot(solution.t, y1, label = r'$y_1(t)$') plt.plot(solution.t, y2, label = r'$y_2(t)$') plt.plot(solution.t, y3, label = r'$y_3(t)$') plt.plot(solution.t, y4, label = r'$y_4(t)$') plt.legend() plt.show()","title":"Teorema de Picard-Lindelof"},{"location":"edp/existence_theorem/existence_theorem/#teorema-de-picard-lindelof","text":"Este \u00e9 um belo teorema de exst\u00eancia e unicidade de equa\u00e7\u00f5es diferenciais ordin\u00e1rias da forma y'(t) = f(t, y(t)), y(t_0) = y_0. import numpy as np from scipy.integrate import solve_ivp, quad import matplotlib.pyplot as plt import seaborn as sns sns.set() O teorema basicamente consiste em construir um operador da seguinte forma: L(y(t)) = y_0 + \\int_{t_0}^t f(s,y(s)) \\, ds, em que L \u00e9 um operador funcional. Note que se se L(y(t)) = y(t) para todo t , teremos resolvido o problema, dado que y(t) = y_0 + \\int_{t_0}^t f(s,y(s)) \\, ds, \u00e9 uma solu\u00e7\u00e3o para y' = f(t,y) . Nesse caso, dizemos que queremos encontrar um ponto fixo do operador L . Se demonstrarmos que L possui um ponto fixo e ele \u00e9 \u00fanico em algum intevalo suficientemente pequeno, ent\u00e3o teremos demonstrado nosso teorema. Um teorema de exist\u00eancia (e unicidade local tamb\u00e9m) \u00e9 o Teorema de Banach . Para isso, bastaria mostrar que L leva fun\u00e7\u00f5es de um espa\u00e7o no mesmo espa\u00e7o e que \u00e9 uma contra\u00e7\u00e3o, isto \u00e9, intuitivamente, se a cada vez que aplicamos L , as fun\u00e7\u00f5es se aproximam e continuam no mesmo espa\u00e7o, ent\u00e3o vai existir um ponto fixo para ele. Nossa ideia aqui n\u00e3o \u00e9 demonstrar o Teorema, mas sim mostrar a intera\u00e7\u00e3o de Picard que \u00e9 escrita da seguinte forma: y_{k+1}(t) = y_0 + \\int_{0}^t f(s,y_k(s)) \\, ds, \\\\ y_0(t) = y_0. O teorema de ponto fixo de Banach nos mostra que y_{k} converge para a solu\u00e7\u00e3o y para todo ponto t em uma vizinhan\u00e7a de 0. Usaremos a fun\u00e7\u00e3o de quadratura para integrar a fun\u00e7\u00e3o f . a = (1,2) (1,*a) (1, 1, 2) def picard_iteration(fun, t_eval, y0, yk, args): pieces = [quad(func = fun, a = t_eval[i], b = t_eval[i+1], args = ((yk[i] + yk[i+1])/2, *args) )[0] for i in range(len(t_eval)-1)] pieces.insert(0,0) yk = np.cumsum(np.array(pieces)) + y0 return yk Considere o problema y' = ay, \\\\ y(0) = y_0 # defining the function f = lambda t, y, a: a*y a = 2 # time of definition upper = 1 n = 100 t = np.linspace(0,upper,n) Esta \u00e9 a solu\u00e7\u00e3o usando Runge-Kutta. solution = solve_ivp(fun = f, t_span = (0, upper), t_eval = t, y0 = (1,), method = 'RK45', args = (a,)) Aqui n\u00f3s apresentamos as primeiras itera\u00e7\u00f5es de Picard. y0 = np.ones(n) y1 = picard_iteration(fun = f, t_eval = t, y0 = (1,), yk = y0, args = (a,)) y2 = picard_iteration(fun = f, t_eval = t, y0 = (1,), yk = y1, args = (a,)) y3 = picard_iteration(fun = f, t_eval = t, y0 = (1,), yk = y2, args = (a,)) y4 = picard_iteration(fun = f, t_eval = t, y0 = (1,), yk = y3, args = (a,)) plt.figure(figsize = (10,6)) plt.title('Picard iterations to approximate a function') plt.plot(solution.t, solution.y[0], label = 'Solution y(t)') plt.plot(solution.t, y0, label = r'$y_0(t)$') plt.plot(solution.t, y1, label = r'$y_1(t)$') plt.plot(solution.t, y2, label = r'$y_2(t)$') plt.plot(solution.t, y3, label = r'$y_3(t)$') plt.plot(solution.t, y4, label = r'$y_4(t)$') plt.legend() plt.show()","title":"Teorema de Picard-Lindelof"},{"location":"edp/fft/fft/","text":"Teoria de Fourier Essa se\u00e7\u00e3o \u00e9 apenas um pequeno resumo sobre o assunto de Fourier, que \u00e9 uma mat\u00e9ria bem densa e pode ficar matematicamente bem complicada. Para um detalhamento mais acurado da teoria, o cap\u00edtulo 2 do livro Fourier Analysis and Partial Differential Equations dos autores Rafael Iorio e Val\u00e9ria Iorio \u00e9 uma boa refer\u00eancia. Convolu\u00e7\u00f5es Dizemos que uma transforma\u00e7\u00e3o L \u00e9 linear se L(f + \\lambda g) = L(f) + \\lambda L(g) para todas as fun\u00e7\u00f5es f,g em um espa\u00e7o apropriado, como, por exemplo, das cont\u00ednuas e \\lambda \\in \\mathbb{R} . Ela ser\u00e1 invariante por transla\u00e7\u00f5es quando L(f(x-a)) = L(f(x) - a) para todo x,a em um espa\u00e7o vetorial. Por exemplo, considere as fun\u00e7\u00f5es f: \\mathbb{R} \\to \\mathbb{R} que representam algum sinal, tipo o som. A figura abaixo mostra que uma transforma\u00e7\u00e3o invariante por transla\u00e7\u00e3o deve levar as curvas azul e vermelha no mesmo lugar. Toda transforma\u00e7\u00e3o linear L invariante por transla\u00e7\u00e3o \u00e9 dada por uma convolu\u00e7\u00e3o L(f)(x) = (f * h)(x) := \\int_{\\mathbb{R}} f(u) h(x-u) \\, du em que h = L\\delta e \\delta \u00e9 a delta de Dirac , uma fun\u00e7\u00e3o generalizada estudada na An\u00e1lise Funcional que \u00e9 zero em todo ponto diferente de zero e \\int_{\\mathbb{R}} \\delta(x) \\, dx = 1. Essa propriedade adv\u00e9m do fato de que \\int_{\\mathbb{R}} f(x) \\delta(x-u) \\, du = f(x), isto \u00e9, a convolu\u00e7\u00e3o de f por \\delta \u00e9 exatamente f . Essa opera\u00e7\u00e3o de convolu\u00e7\u00e3o satisfaz as propriedades de comutatividade, associatividade, exist\u00eancia de elemento identidade ( \\delta ), al\u00e9m \u00e9 claro dela ser fechada no espa\u00e7o das fun\u00e7\u00f5es por exemplo. \u00c9 poss\u00edvel mostrar que \\frac{d(f * g)}{dx} = \\frac{df}{dx} * g = f * \\frac{dg}{dx}, o que permite dizer que a convolu\u00e7\u00e3o de fun\u00e7\u00f5es diferenci\u00e1veis \u00e9 diferenci\u00e1vel. Podemos, portanto, definir um grupo em que o espa\u00e7o \u00e9 das fun\u00e7\u00f5es deriv\u00e1veis e a opera\u00e7\u00e3o \u00e9 a convolu\u00e7\u00e3o. Defina L_h(f) = f * h , isto \u00e9, L_h \u00e9 uma transforma\u00e7\u00e3o que faz a convolu\u00e7\u00e3o de f por h . Nesse v\u00eddeo temos uma intui\u00e7\u00e3o com a quantidade de fuma\u00e7a S(t) e a quantidade de f\u00f3sforos f(t) no tempo t . Transformada de Fourier A fun\u00e7\u00e3o e_w(x) = e^{2\\pi w x i} \u00e9 um autovetor do operador L_h no espa\u00e7o das fun\u00e7\u00f5es para qualquer h (sim, qualquer!). Essa proposi\u00e7\u00e3o \u00e9 um bom exerc\u00edcio. Como dica, observe que o seu autovetor correspondente ser\u00e1 \\hat{h}(w) = \\int_{\\mathbb{R}} h(u) e^{-2\\pi w u i} \\, du Chamamos essa fun\u00e7\u00e3o de fun\u00e7\u00e3o de transfer\u00eancia . (Note que muitos dos nomes aqui vem da teoria dos sinais). Temos ent\u00e3o uma base formada pelos autovetores da transforma\u00e7\u00e3o L_h . Considere uma fun\u00e7\u00e3o f e decomponha na base formada por esses autovetores. Assim: f(x) = \\int_{\\mathbb{R}} \\hat{f}(w)e_w(x) \\, dw, em que os coeficientes \\hat{f}(w) precisam ser determinados. A extens\u00e3o do somat\u00f3rio para integral pode ser levado apenas como intui\u00e7\u00e3o at\u00e9 aqui. Assim, teremos que L_h(f) = \\int_{\\mathbb{R}} \\hat{f}(w)L(e_w) \\, dw = \\int_{\\mathbb{R}} \\hat{h}(w)\\hat{f}(w)e_w \\, dw. No espa\u00e7o de fun\u00e7\u00f5es, definimos o produto interno da seguinte forma: \\langle f, g \\rangle = \\int_{\\mathbb{R}} f(x)\\overline{g(x)} \\, dx. Lembre que os coeficientes de uma decomposi\u00e7\u00e3o t\u00eam rela\u00e7\u00e3o com o coeficiente interno quando a base \u00e9 ortogonal. Vamos ver que esse \u00e9 o caso! E ent\u00e3o, teremos que \\hat{f}(w) = \\langle f, e_w \\rangle = \\int_{\\mathbb{R}} f(u)e^{-2\\pi w u i} \\, du. Para ver que a base de fun\u00e7\u00f5es \u00e9 de fato ortogonal, precisamos do resultado \\langle e_{w_1}, e_{w_2} \\rangle = \\delta(w_1 - w_2) que n\u00e3o vai ser demonstrado aqui. Definimos a Transformada de Fourier de uma fun\u00e7\u00e3o f(t) por \\hat{f}(w) = \\int_{\\mathbb{R}} f(u)e^{-2\\pi w u i} \\, du, sempre que essa integral \u00e9 definida. Resumindo, a Transformada de Fourier \u00e9 a fun\u00e7\u00e3o que para cada w determina o coeficiente correspondente ao autovetor e_w da opera\u00e7\u00e3o de convolu\u00e7\u00e3o. A Inversa da Transformada de Fourier \u00e9 dada por f(x) = \\int_{\\mathbb{R}} \\hat{f}(w) e^{2\\pi w x i} \\, dw := \\check{f}(x). Uma propriedade resultante interessante \u00e9 que \\check{g}(w) = \\widehat{g(-w)} . Em Teoria dos Sinais, \\hat{f}(w) mede o quanto uma frequ\u00eancia w est\u00e1 presente no sinal f(x) . Isso acontece, porque o sinal e_w(x) = e^{2\\pi w x i} = \\cos(2\\pi w x) + i\\sin(2\\pi w x) \u00e9 o sinal \"puro\" da frequ\u00eancia w e \\hat{f}(w) \u00e9 o peso correspondente. Por isso, dizemos que f(x) est\u00e1 definida no dom\u00ednio do tempo ou espa\u00e7o enquanto \\hat{f}(w) est\u00e1 no dom\u00ednio da frequ\u00eancia. Propriedades: Valem as seguintes propriedades. \\hat{\\delta} = 1 \\\\ \\widehat{f * h} = \\hat{f}\\cdot \\hat{h} \\\\ g(x) = f(x-a) \\implies \\hat{g}(w) = \\hat{f}(w)e^{-2\\pi w a i} \\\\ g(x) = f(ax) \\implies \\hat{g}(w) =a^{-1}\\hat{f}(w/a) \\\\ g(x) = f'(x) \\implies \\hat{g}(w) = \\widehat{(\\delta')}\\cdot \\hat{f} = 2\\pi w \\hat{f}(w) i \\\\ \\int_{\\mathbb{R}} |f(u)|^2 \\, du = \\int_{\\mathbb{R}} |\\hat{f}(w)|^2 \\, dw Transformada de Fourier em Espa\u00e7o Discreto Agora considere f(x) com x \\in \\mathbb{Z} , por exemplo. Nesse caso, a fun\u00e7\u00e3o de dirac tem a diferen\u00e7a de \\delta(0) = 1 , enquanto 0 no resto dos inteiros. Vale tamb\u00e9m que \\sum_{i=-\\infty}^{+\\infty} f(i)\\delta(i) = f(0) e f(n) = \\sum_{i=-\\infty}^{+\\infty} f(i)\\delta(n-i). A convolu\u00e7\u00e3o discreta tamb\u00e9m tem uma defini\u00e7\u00e3o similar Lf(n) = (f * h)(n) := \\sum_{i=-\\infty}^{+\\infty} f(i)h(n-i). As propriedades de convolu\u00e7\u00f5es cont\u00ednuas, mas a derivada \u00e9 substitu\u00edda por \\Delta(f * g) = \\Delta(f) * g = f * \\Delta(g) para \\Delta(f)(n) = f(n+1) - f(n) . Os autovetores da convolu\u00e7\u00e3o s\u00e3o da forma e_w(n) = e^{2\\pi w n i} com autovalores \\hat{h}(w) = \\sum_{i=-\\infty}^{+\\infty} h(i)e^{-2\\pi w a i}. E, por fim, a transformada de Fourier de Tempo Discreto \u00e9 \\hat{f}(w) = \\sum_{i=-\\infty}^{+\\infty} f(i) e^{-2\\pi w a i}, sempre que o somat\u00f3rio for v\u00e1lido. A inversa ser\u00e1 f(n) = \\int_0^1 \\hat{f}(w) e^{2\\pi w n i}. Note que \\hat{f}(w) \u00e9 peri\u00f3dica com per\u00edodo 1. Transformada de Fourier em Espa\u00e7o Finito Agora os transformadores L s\u00e3o do tipo L:\\mathbb{C}^N \\to \\mathbb{C}^N e portanto possuem representa\u00e7\u00e3o matricial. Em particular, vamos estar interessados quando L(a_0, a_1, \\dots a_{N-1}) = (b_0, \\dots, b_{N-1}) \\implies L(a_{N-1}, a_0, \\dots a_{N-2}) = (b_{N-1}, b_0, \\dots, b_{N-2}), que \u00e9 semelhante \u00e0 invari\u00e2ncia por transla\u00e7\u00e3o. Seja \\{\\delta_0, \\dots, \\delta_{N-1}\\} a base can\u00f4nica de \\mathbb{R}^N , em que (\\delta_i)_i = 1 e (\\delta_i)_j = 0, j \\neq i . Nessa base, teremos que L = \\begin{bmatrix} h_0 & h_{N-1} & \\cdots & h_1 \\\\ h_1 & h_{0} & \\cdots & h_2 \\\\ h_2 & h_{1} & \\cdots & h_3 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ h_{N-1} & h_{N-2} & \\cdots & h_0 \\\\ \\end{bmatrix}, em que L\\delta_0 = (h_0, h_1, h_2, \\dots, h_N) . Toda transforma\u00e7\u00e3o dessa maneira \u00e9 dada por uma convolu\u00e7\u00e3o circular (Lf)_j = (f *_c h ) := \\sum_{k=0}^j f_k h_{j-k} + \\sum_{k=j+1}^{N-1} f_k h_{N+j-k} = \\sum_{k=0}^{N-1} f_k h_{j-k \\mod N} em que h = L\\delta_0 . O operador L_h dado por L_h f = f *_c h . Essa matriz tem os autovalores e autovetores conhecidos: se \\alpha \u00e9 uma raiz N -\u00e9sima da unidade, ent\u00e3o e = (1, \\alpha, \\dots, \\alpha^{N-1}) \u00e9 um autovetor de L_h cujo autovalor \u00e9 dado por \\lambda = \\sum_{k=0}^{N-1} h_k e^{-\\frac{2\\pi k}{N} wi}. Para obter uma base ortonormal, basta fazer e_w = N^{-1/2}e . A Transformada de Fourier Finita de um vetor f = (f_0, \\dots, f_{N-1}) \u00e9 dada por \\hat{f}_w = \\frac{1}{\\sqrt{N}} \\sum_{k=0}^{N-1} f_k e^{-\\frac{2\\pi w}{N} ki}. Um coment\u00e1rio importante \u00e9 que na literatura, essa transforma\u00e7\u00e3o \u00e9 chamada de Transformada de Fourier Discreta (DFT) e em alguns textos o coeficiente 1/\\sqrt{N} \u00e9 substitu\u00eddo por 1/N ou 1 . A Inversa dessa transforma\u00e7\u00e3o \u00e9 f = \\sum_{w=0}^{N-1} \\hat{f}_w e_w. Resolvendo uma EDP com Fourier Vamos lembrar que se g(x) = f'(x) para uma fun\u00e7\u00e3o diferenci\u00e1vel g , ent\u00e3o a transformada de Fourier de g \u00e9 \\hat{g}(w) = 2\\pi w i \\hat{f}(w). Isso \u00e9 \"uma m\u00e3o na roda\", porque quando aplicamos a transformada na derivada de uma fun\u00e7\u00e3o, estamos voltando para a transformada da pr\u00f3pria fun\u00e7\u00e3o multiplicada por uma fun\u00e7\u00e3o linear em w . Isso acontece quando a fun\u00e7\u00e3o \u00e9 definida na reta. Para fun\u00e7\u00f5es definidas apenas em um intervalo, \u00e9 importante observar o seguinte trecho do site do Wikipedia . Nesse caso, \u00e9 muito comum tamb\u00e9m encontrar a f\u00f3rmula \\hat{g}(w) = w i \\hat{f}(w) para fun\u00e7\u00f5es definidas em [-\\pi, \\pi] . S\u00e3o algumas complica\u00e7\u00f5es relacionadas \u00e0 pr\u00f3pria deriva\u00e7\u00e3o do m\u00e9todo. Vamos aplicar esse m\u00e9todo na equa\u00e7\u00e3o de difus\u00e3o \\frac{\\partial^2 u(x,t)}{\\partial x^2} = c\\frac{\\partial u(x,t)}{\\partial t}. O primeiro passo \u00e9 aplicar a transforma\u00e7\u00e3o de Fourier em ambos os lados da equa\u00e7\u00e3o. Na pr\u00e1tica, o que se faz \u00e9 multiplicar por e^{-i w x} (ou e^{-2\\pi i w x} , mas lembre que \u00e9 quest\u00e3o de parametriza\u00e7\u00e3o) e integrar de x = -\\infty a x = +\\infty . Se a fun\u00e7\u00e3o for definida em [-\\pi, \\pi] , voc\u00ea integra nesse intervalo, respectivamente. Nesse caso, observe que \\widehat{u_{xx}}(w,t) = ik \\widehat{u_x} = -w^2\\hat{u}(w, t). Com algumas hip\u00f3teses de regularidade, pela Regra de Leibniz, \\int_{-\\infty}^{+\\infty} u_t(x,t)e^{-i w x} \\, dx = \\frac{d}{dt}\\int_{-\\infty}^{+\\infty} u(x,t)e^{-i w x} \\, dx Dessa forma \\widehat{u_t}(w,t) = \\frac{\\partial \\hat{u}}{\\partial t}(w,t). Isso faz com que tenhamos o sistema -w^2\\hat{u}(w,t) = c\\hat{u}_t(w,t) que \u00e9 uma EDO com solu\u00e7\u00e3o \\hat{u}(w,t) = \\hat{u}(w,0)e^{-w^2 t/c}. Agora, basta aplicar a transformada inversa u(x,t) = \\int_{-\\infty}^{\\infty} \\hat{u}(w,t)e^{iw x} \\, dw. Voc\u00ea deve ter observado que nessa parametriza\u00e7\u00e3o (sem o 2\\pi no expoente), eu deveria dividir cada integral por 2\\pi para corretamente aplicar a Transformada de Fourier. Acontece que elas v\u00e3o se anular por esse motivo. Para obter \\hat{u}(w,0) basta aplicar a transformada de Fourier em u(x,0) , que \u00e9 usualmente dado. Adicionais Mas o que \u00e9 a Transformada de Fourier? Uma introdu\u00e7\u00e3o visual. 3B1B Least deep and most incomplete explanation for Fourier Transform Implementando Fast Fourier Transform em Python","title":"Teoria de Fourier"},{"location":"edp/fft/fft/#teoria-de-fourier","text":"Essa se\u00e7\u00e3o \u00e9 apenas um pequeno resumo sobre o assunto de Fourier, que \u00e9 uma mat\u00e9ria bem densa e pode ficar matematicamente bem complicada. Para um detalhamento mais acurado da teoria, o cap\u00edtulo 2 do livro Fourier Analysis and Partial Differential Equations dos autores Rafael Iorio e Val\u00e9ria Iorio \u00e9 uma boa refer\u00eancia.","title":"Teoria de Fourier"},{"location":"edp/fft/fft/#convolucoes","text":"Dizemos que uma transforma\u00e7\u00e3o L \u00e9 linear se L(f + \\lambda g) = L(f) + \\lambda L(g) para todas as fun\u00e7\u00f5es f,g em um espa\u00e7o apropriado, como, por exemplo, das cont\u00ednuas e \\lambda \\in \\mathbb{R} . Ela ser\u00e1 invariante por transla\u00e7\u00f5es quando L(f(x-a)) = L(f(x) - a) para todo x,a em um espa\u00e7o vetorial. Por exemplo, considere as fun\u00e7\u00f5es f: \\mathbb{R} \\to \\mathbb{R} que representam algum sinal, tipo o som. A figura abaixo mostra que uma transforma\u00e7\u00e3o invariante por transla\u00e7\u00e3o deve levar as curvas azul e vermelha no mesmo lugar. Toda transforma\u00e7\u00e3o linear L invariante por transla\u00e7\u00e3o \u00e9 dada por uma convolu\u00e7\u00e3o L(f)(x) = (f * h)(x) := \\int_{\\mathbb{R}} f(u) h(x-u) \\, du em que h = L\\delta e \\delta \u00e9 a delta de Dirac , uma fun\u00e7\u00e3o generalizada estudada na An\u00e1lise Funcional que \u00e9 zero em todo ponto diferente de zero e \\int_{\\mathbb{R}} \\delta(x) \\, dx = 1. Essa propriedade adv\u00e9m do fato de que \\int_{\\mathbb{R}} f(x) \\delta(x-u) \\, du = f(x), isto \u00e9, a convolu\u00e7\u00e3o de f por \\delta \u00e9 exatamente f . Essa opera\u00e7\u00e3o de convolu\u00e7\u00e3o satisfaz as propriedades de comutatividade, associatividade, exist\u00eancia de elemento identidade ( \\delta ), al\u00e9m \u00e9 claro dela ser fechada no espa\u00e7o das fun\u00e7\u00f5es por exemplo. \u00c9 poss\u00edvel mostrar que \\frac{d(f * g)}{dx} = \\frac{df}{dx} * g = f * \\frac{dg}{dx}, o que permite dizer que a convolu\u00e7\u00e3o de fun\u00e7\u00f5es diferenci\u00e1veis \u00e9 diferenci\u00e1vel. Podemos, portanto, definir um grupo em que o espa\u00e7o \u00e9 das fun\u00e7\u00f5es deriv\u00e1veis e a opera\u00e7\u00e3o \u00e9 a convolu\u00e7\u00e3o. Defina L_h(f) = f * h , isto \u00e9, L_h \u00e9 uma transforma\u00e7\u00e3o que faz a convolu\u00e7\u00e3o de f por h . Nesse v\u00eddeo temos uma intui\u00e7\u00e3o com a quantidade de fuma\u00e7a S(t) e a quantidade de f\u00f3sforos f(t) no tempo t .","title":"Convolu\u00e7\u00f5es"},{"location":"edp/fft/fft/#transformada-de-fourier","text":"A fun\u00e7\u00e3o e_w(x) = e^{2\\pi w x i} \u00e9 um autovetor do operador L_h no espa\u00e7o das fun\u00e7\u00f5es para qualquer h (sim, qualquer!). Essa proposi\u00e7\u00e3o \u00e9 um bom exerc\u00edcio. Como dica, observe que o seu autovetor correspondente ser\u00e1 \\hat{h}(w) = \\int_{\\mathbb{R}} h(u) e^{-2\\pi w u i} \\, du Chamamos essa fun\u00e7\u00e3o de fun\u00e7\u00e3o de transfer\u00eancia . (Note que muitos dos nomes aqui vem da teoria dos sinais). Temos ent\u00e3o uma base formada pelos autovetores da transforma\u00e7\u00e3o L_h . Considere uma fun\u00e7\u00e3o f e decomponha na base formada por esses autovetores. Assim: f(x) = \\int_{\\mathbb{R}} \\hat{f}(w)e_w(x) \\, dw, em que os coeficientes \\hat{f}(w) precisam ser determinados. A extens\u00e3o do somat\u00f3rio para integral pode ser levado apenas como intui\u00e7\u00e3o at\u00e9 aqui. Assim, teremos que L_h(f) = \\int_{\\mathbb{R}} \\hat{f}(w)L(e_w) \\, dw = \\int_{\\mathbb{R}} \\hat{h}(w)\\hat{f}(w)e_w \\, dw. No espa\u00e7o de fun\u00e7\u00f5es, definimos o produto interno da seguinte forma: \\langle f, g \\rangle = \\int_{\\mathbb{R}} f(x)\\overline{g(x)} \\, dx. Lembre que os coeficientes de uma decomposi\u00e7\u00e3o t\u00eam rela\u00e7\u00e3o com o coeficiente interno quando a base \u00e9 ortogonal. Vamos ver que esse \u00e9 o caso! E ent\u00e3o, teremos que \\hat{f}(w) = \\langle f, e_w \\rangle = \\int_{\\mathbb{R}} f(u)e^{-2\\pi w u i} \\, du. Para ver que a base de fun\u00e7\u00f5es \u00e9 de fato ortogonal, precisamos do resultado \\langle e_{w_1}, e_{w_2} \\rangle = \\delta(w_1 - w_2) que n\u00e3o vai ser demonstrado aqui. Definimos a Transformada de Fourier de uma fun\u00e7\u00e3o f(t) por \\hat{f}(w) = \\int_{\\mathbb{R}} f(u)e^{-2\\pi w u i} \\, du, sempre que essa integral \u00e9 definida. Resumindo, a Transformada de Fourier \u00e9 a fun\u00e7\u00e3o que para cada w determina o coeficiente correspondente ao autovetor e_w da opera\u00e7\u00e3o de convolu\u00e7\u00e3o. A Inversa da Transformada de Fourier \u00e9 dada por f(x) = \\int_{\\mathbb{R}} \\hat{f}(w) e^{2\\pi w x i} \\, dw := \\check{f}(x). Uma propriedade resultante interessante \u00e9 que \\check{g}(w) = \\widehat{g(-w)} . Em Teoria dos Sinais, \\hat{f}(w) mede o quanto uma frequ\u00eancia w est\u00e1 presente no sinal f(x) . Isso acontece, porque o sinal e_w(x) = e^{2\\pi w x i} = \\cos(2\\pi w x) + i\\sin(2\\pi w x) \u00e9 o sinal \"puro\" da frequ\u00eancia w e \\hat{f}(w) \u00e9 o peso correspondente. Por isso, dizemos que f(x) est\u00e1 definida no dom\u00ednio do tempo ou espa\u00e7o enquanto \\hat{f}(w) est\u00e1 no dom\u00ednio da frequ\u00eancia. Propriedades: Valem as seguintes propriedades. \\hat{\\delta} = 1 \\\\ \\widehat{f * h} = \\hat{f}\\cdot \\hat{h} \\\\ g(x) = f(x-a) \\implies \\hat{g}(w) = \\hat{f}(w)e^{-2\\pi w a i} \\\\ g(x) = f(ax) \\implies \\hat{g}(w) =a^{-1}\\hat{f}(w/a) \\\\ g(x) = f'(x) \\implies \\hat{g}(w) = \\widehat{(\\delta')}\\cdot \\hat{f} = 2\\pi w \\hat{f}(w) i \\\\ \\int_{\\mathbb{R}} |f(u)|^2 \\, du = \\int_{\\mathbb{R}} |\\hat{f}(w)|^2 \\, dw","title":"Transformada de Fourier"},{"location":"edp/fft/fft/#transformada-de-fourier-em-espaco-discreto","text":"Agora considere f(x) com x \\in \\mathbb{Z} , por exemplo. Nesse caso, a fun\u00e7\u00e3o de dirac tem a diferen\u00e7a de \\delta(0) = 1 , enquanto 0 no resto dos inteiros. Vale tamb\u00e9m que \\sum_{i=-\\infty}^{+\\infty} f(i)\\delta(i) = f(0) e f(n) = \\sum_{i=-\\infty}^{+\\infty} f(i)\\delta(n-i). A convolu\u00e7\u00e3o discreta tamb\u00e9m tem uma defini\u00e7\u00e3o similar Lf(n) = (f * h)(n) := \\sum_{i=-\\infty}^{+\\infty} f(i)h(n-i). As propriedades de convolu\u00e7\u00f5es cont\u00ednuas, mas a derivada \u00e9 substitu\u00edda por \\Delta(f * g) = \\Delta(f) * g = f * \\Delta(g) para \\Delta(f)(n) = f(n+1) - f(n) . Os autovetores da convolu\u00e7\u00e3o s\u00e3o da forma e_w(n) = e^{2\\pi w n i} com autovalores \\hat{h}(w) = \\sum_{i=-\\infty}^{+\\infty} h(i)e^{-2\\pi w a i}. E, por fim, a transformada de Fourier de Tempo Discreto \u00e9 \\hat{f}(w) = \\sum_{i=-\\infty}^{+\\infty} f(i) e^{-2\\pi w a i}, sempre que o somat\u00f3rio for v\u00e1lido. A inversa ser\u00e1 f(n) = \\int_0^1 \\hat{f}(w) e^{2\\pi w n i}. Note que \\hat{f}(w) \u00e9 peri\u00f3dica com per\u00edodo 1.","title":"Transformada de Fourier em Espa\u00e7o Discreto"},{"location":"edp/fft/fft/#transformada-de-fourier-em-espaco-finito","text":"Agora os transformadores L s\u00e3o do tipo L:\\mathbb{C}^N \\to \\mathbb{C}^N e portanto possuem representa\u00e7\u00e3o matricial. Em particular, vamos estar interessados quando L(a_0, a_1, \\dots a_{N-1}) = (b_0, \\dots, b_{N-1}) \\implies L(a_{N-1}, a_0, \\dots a_{N-2}) = (b_{N-1}, b_0, \\dots, b_{N-2}), que \u00e9 semelhante \u00e0 invari\u00e2ncia por transla\u00e7\u00e3o. Seja \\{\\delta_0, \\dots, \\delta_{N-1}\\} a base can\u00f4nica de \\mathbb{R}^N , em que (\\delta_i)_i = 1 e (\\delta_i)_j = 0, j \\neq i . Nessa base, teremos que L = \\begin{bmatrix} h_0 & h_{N-1} & \\cdots & h_1 \\\\ h_1 & h_{0} & \\cdots & h_2 \\\\ h_2 & h_{1} & \\cdots & h_3 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ h_{N-1} & h_{N-2} & \\cdots & h_0 \\\\ \\end{bmatrix}, em que L\\delta_0 = (h_0, h_1, h_2, \\dots, h_N) . Toda transforma\u00e7\u00e3o dessa maneira \u00e9 dada por uma convolu\u00e7\u00e3o circular (Lf)_j = (f *_c h ) := \\sum_{k=0}^j f_k h_{j-k} + \\sum_{k=j+1}^{N-1} f_k h_{N+j-k} = \\sum_{k=0}^{N-1} f_k h_{j-k \\mod N} em que h = L\\delta_0 . O operador L_h dado por L_h f = f *_c h . Essa matriz tem os autovalores e autovetores conhecidos: se \\alpha \u00e9 uma raiz N -\u00e9sima da unidade, ent\u00e3o e = (1, \\alpha, \\dots, \\alpha^{N-1}) \u00e9 um autovetor de L_h cujo autovalor \u00e9 dado por \\lambda = \\sum_{k=0}^{N-1} h_k e^{-\\frac{2\\pi k}{N} wi}. Para obter uma base ortonormal, basta fazer e_w = N^{-1/2}e . A Transformada de Fourier Finita de um vetor f = (f_0, \\dots, f_{N-1}) \u00e9 dada por \\hat{f}_w = \\frac{1}{\\sqrt{N}} \\sum_{k=0}^{N-1} f_k e^{-\\frac{2\\pi w}{N} ki}. Um coment\u00e1rio importante \u00e9 que na literatura, essa transforma\u00e7\u00e3o \u00e9 chamada de Transformada de Fourier Discreta (DFT) e em alguns textos o coeficiente 1/\\sqrt{N} \u00e9 substitu\u00eddo por 1/N ou 1 . A Inversa dessa transforma\u00e7\u00e3o \u00e9 f = \\sum_{w=0}^{N-1} \\hat{f}_w e_w.","title":"Transformada de Fourier em Espa\u00e7o Finito"},{"location":"edp/fft/fft/#resolvendo-uma-edp-com-fourier","text":"Vamos lembrar que se g(x) = f'(x) para uma fun\u00e7\u00e3o diferenci\u00e1vel g , ent\u00e3o a transformada de Fourier de g \u00e9 \\hat{g}(w) = 2\\pi w i \\hat{f}(w). Isso \u00e9 \"uma m\u00e3o na roda\", porque quando aplicamos a transformada na derivada de uma fun\u00e7\u00e3o, estamos voltando para a transformada da pr\u00f3pria fun\u00e7\u00e3o multiplicada por uma fun\u00e7\u00e3o linear em w . Isso acontece quando a fun\u00e7\u00e3o \u00e9 definida na reta. Para fun\u00e7\u00f5es definidas apenas em um intervalo, \u00e9 importante observar o seguinte trecho do site do Wikipedia . Nesse caso, \u00e9 muito comum tamb\u00e9m encontrar a f\u00f3rmula \\hat{g}(w) = w i \\hat{f}(w) para fun\u00e7\u00f5es definidas em [-\\pi, \\pi] . S\u00e3o algumas complica\u00e7\u00f5es relacionadas \u00e0 pr\u00f3pria deriva\u00e7\u00e3o do m\u00e9todo. Vamos aplicar esse m\u00e9todo na equa\u00e7\u00e3o de difus\u00e3o \\frac{\\partial^2 u(x,t)}{\\partial x^2} = c\\frac{\\partial u(x,t)}{\\partial t}. O primeiro passo \u00e9 aplicar a transforma\u00e7\u00e3o de Fourier em ambos os lados da equa\u00e7\u00e3o. Na pr\u00e1tica, o que se faz \u00e9 multiplicar por e^{-i w x} (ou e^{-2\\pi i w x} , mas lembre que \u00e9 quest\u00e3o de parametriza\u00e7\u00e3o) e integrar de x = -\\infty a x = +\\infty . Se a fun\u00e7\u00e3o for definida em [-\\pi, \\pi] , voc\u00ea integra nesse intervalo, respectivamente. Nesse caso, observe que \\widehat{u_{xx}}(w,t) = ik \\widehat{u_x} = -w^2\\hat{u}(w, t). Com algumas hip\u00f3teses de regularidade, pela Regra de Leibniz, \\int_{-\\infty}^{+\\infty} u_t(x,t)e^{-i w x} \\, dx = \\frac{d}{dt}\\int_{-\\infty}^{+\\infty} u(x,t)e^{-i w x} \\, dx Dessa forma \\widehat{u_t}(w,t) = \\frac{\\partial \\hat{u}}{\\partial t}(w,t). Isso faz com que tenhamos o sistema -w^2\\hat{u}(w,t) = c\\hat{u}_t(w,t) que \u00e9 uma EDO com solu\u00e7\u00e3o \\hat{u}(w,t) = \\hat{u}(w,0)e^{-w^2 t/c}. Agora, basta aplicar a transformada inversa u(x,t) = \\int_{-\\infty}^{\\infty} \\hat{u}(w,t)e^{iw x} \\, dw. Voc\u00ea deve ter observado que nessa parametriza\u00e7\u00e3o (sem o 2\\pi no expoente), eu deveria dividir cada integral por 2\\pi para corretamente aplicar a Transformada de Fourier. Acontece que elas v\u00e3o se anular por esse motivo. Para obter \\hat{u}(w,0) basta aplicar a transformada de Fourier em u(x,0) , que \u00e9 usualmente dado.","title":"Resolvendo uma EDP com Fourier"},{"location":"edp/fft/fft/#adicionais","text":"Mas o que \u00e9 a Transformada de Fourier? Uma introdu\u00e7\u00e3o visual. 3B1B Least deep and most incomplete explanation for Fourier Transform Implementando Fast Fourier Transform em Python","title":"Adicionais"},{"location":"functional_analysis/arzela/","text":"Teorema de Arzel\u00e0-Ascoli Considere (C[a,b], \\rho) o conjunto das fun\u00e7\u00f5es cont\u00ednuas definidas no intervalo [a,b] munido com a m\u00e9trica \\rho(f,g) = \\max_{x \\in [a,b]} |f(x)-g(x)|, f,g \\in C[a,b]. N\u00f3s provamos que esta tupla \u00e9 um espa\u00e7o m\u00e9trico completo na lista. Vamos estender esse espa\u00e7o para as fun\u00e7\u00f5es cont\u00ednuas f : X \\to Y , em que (X, d_X) e (Y, d_Y) s\u00e3o espa\u00e7os m\u00e9tricos. O teorema de Arzel\u00e0 apresenta um crit\u00e9rio de compacidade para subconjuntos de C(X,Y) . Seja f:X\\to Y uma fun\u00e7\u00e3o. Ela \u00e9 dita fun\u00e7\u00e3o limitada se im(f) \u00e9 um conjunto limitado em Y , isto \u00e9, \\exists M > 0 tal que \\forall a,b \\in X, d_Y(f(a), f(b)) \\le M . Teorema: O conjunto de fun\u00e7\u00f5es limitadas f:X \\to Y sob a m\u00e9trica d(f,g) = \\sup_{x \\in X} d_Y(f(x), g(x)) \u00e9 um espa\u00e7o m\u00e9trico. Al\u00e9m disso, se Y \u00e9 completo, ent\u00e3o o espa\u00e7o tamb\u00e9m ser\u00e1. Mostrar que d define uma m\u00e9trica n\u00e3o \u00e9 complicado pois ela herda as propriedades de d_Y . Al\u00e9m disso, ela \u00e9 limitada, pois d_Y(f(x), g(x)) \\le \\operatorname{diam}(im(f) \\cup im(g)), \\forall x . A completude vem do fato de que a converg\u00eancia ponto a ponto ocorre pela completude de Y e do fato de que a converg\u00eancia \u00e9 uniforme, dado que se o supremo \u00e9 limitado e, todo ponto \u00e9. Denotamos C_b(X,Y) o conjunto das fun\u00e7\u00f5es cont\u00ednuas limitadas de X a Y . Se Y = \\mathbb{C} , denotamos por C(X) . Se f_n \\to f em C_b(X,Y) , chamamos de converg\u00eancia uniforme . Teorema (Stone-Weierstra \\beta ): Os polin\u00f4mios (em z e \\bar{z} ) s\u00e3o densos em C(K) quando K \\subseteq \\mathbb{C} \u00e9 compacto. A demonstra\u00e7\u00e3o dada no livro do Muscat consiste em cinco etapas: (I) Existe uma sequ\u00eancia de polin\u00f4mios que aproxima |x| em x \\in [-1,1] . Defina q_{n+1}(x) = q_n(x) + (x^2 - q_n(x)^2) e q_0(x) = 0 . Primeiro mostra-se a converg\u00eancia ponto a ponto, depois a uniforme. (II) Seja f \\in C(K, \\mathbb{R}) com c = \\max_{x \\in K} |f(x)| > 0 . Ent\u00e3o |f/c| pode ser aproximado por q_n \\circ f/c . Al\u00e9m do mais, se p_n \\to f , ent\u00e3o cq_n \\circ (p_n/c) \\to |f| . (III) Se f e g podem ser aproximados por polin\u00f4mios, ent\u00e3o f+g e f-g tamb\u00e9m podem. Portanto |f-g| , \\max\\{f,g\\} e \\min\\{f,g\\} tamb\u00e9m podem ser aproximados. (IV) Polin\u00f4mios reais s\u00e3o densos em C(K, \\mathbb{R}) . Para isso, tome uma fun\u00e7\u00e3o f \\in C(K, \\mathbb{R}) e z \\neq w . Seja p_{w,z} a a reta que passa por f(w) e f(z) e defina U_{z,w} como o conjunto dos pontos a \\in K que satisfazem p_{w,z}(a) < f(a) + \\epsilon , claramente aberto e n\u00e3o-vazio, cuja uni\u00e3o sobre w \\neq z cobre K (que \u00e9 compacto e, portanto, admite subcobertura finita). Assim g_{z} := \\min\\{p_{z,w_1}, \\dots, p_{z,w_M}\\} < f + \\epsilon pode ser aproximada por polin\u00f4mios ( w_1, \\dots, w_M \u00e9 a subcobertura finita). Faz-se o mesmo com os conjuntos V_z = \\{a \\in K : g_z(a) > f(a) - \\epsilon\\} . (V) Para f \\in C(K) , basta escrever f = u + iv , em que u e v tem imagem nos reais. Um conjunto F \\subseteq C(X,Y) \u00e9 equicont\u00ednuo se para todo \\epsilon > 0 , existe \\delta > 0 tal que para toda fun\u00e7\u00e3o f \\in F , se x, x' \\in X satisfazem d_X(x,x') < \\delta , ent\u00e3o d(f(x), f(x')) < \\epsilon . Com isso, um conjunto desses cont\u00e9m fun\u00e7\u00f5es uniformemente cont\u00ednuas e, al\u00e9m disso, o valor de \\delta \u00e9 selecionado independente da fun\u00e7\u00e3o escolhida. Teorema de Arzel\u00e0-Ascoli: Seja K compacto. Ent\u00e3o F \\subseteq C(K,Y) \u00e9 totalmente limitado se, e somente se, F K = \\{f(x) : f \\in F, x \\in K\\} \u00e9 totalmente limitado em Y e F \u00e9 equicont\u00ednuo. Uma vers\u00e3o um pouco simplificada que pode ser encontrada na literatura: Uma condi\u00e7\u00e3o suficiente e necess\u00e1ria para que F \\subseteq C([a,b], \\mathbb{R}) seja compacto em C[a,b] \u00e9 que F seja equicont\u00ednuo e uniformemente limitado , isto \u00e9, existe um M tal que |f(x)| < M para todo x \\in [a,b] e para todo f \\in F .","title":"Teorema de Arzel\u00e0-Ascoli"},{"location":"functional_analysis/arzela/#teorema-de-arzela-ascoli","text":"Considere (C[a,b], \\rho) o conjunto das fun\u00e7\u00f5es cont\u00ednuas definidas no intervalo [a,b] munido com a m\u00e9trica \\rho(f,g) = \\max_{x \\in [a,b]} |f(x)-g(x)|, f,g \\in C[a,b]. N\u00f3s provamos que esta tupla \u00e9 um espa\u00e7o m\u00e9trico completo na lista. Vamos estender esse espa\u00e7o para as fun\u00e7\u00f5es cont\u00ednuas f : X \\to Y , em que (X, d_X) e (Y, d_Y) s\u00e3o espa\u00e7os m\u00e9tricos. O teorema de Arzel\u00e0 apresenta um crit\u00e9rio de compacidade para subconjuntos de C(X,Y) . Seja f:X\\to Y uma fun\u00e7\u00e3o. Ela \u00e9 dita fun\u00e7\u00e3o limitada se im(f) \u00e9 um conjunto limitado em Y , isto \u00e9, \\exists M > 0 tal que \\forall a,b \\in X, d_Y(f(a), f(b)) \\le M . Teorema: O conjunto de fun\u00e7\u00f5es limitadas f:X \\to Y sob a m\u00e9trica d(f,g) = \\sup_{x \\in X} d_Y(f(x), g(x)) \u00e9 um espa\u00e7o m\u00e9trico. Al\u00e9m disso, se Y \u00e9 completo, ent\u00e3o o espa\u00e7o tamb\u00e9m ser\u00e1. Mostrar que d define uma m\u00e9trica n\u00e3o \u00e9 complicado pois ela herda as propriedades de d_Y . Al\u00e9m disso, ela \u00e9 limitada, pois d_Y(f(x), g(x)) \\le \\operatorname{diam}(im(f) \\cup im(g)), \\forall x . A completude vem do fato de que a converg\u00eancia ponto a ponto ocorre pela completude de Y e do fato de que a converg\u00eancia \u00e9 uniforme, dado que se o supremo \u00e9 limitado e, todo ponto \u00e9. Denotamos C_b(X,Y) o conjunto das fun\u00e7\u00f5es cont\u00ednuas limitadas de X a Y . Se Y = \\mathbb{C} , denotamos por C(X) . Se f_n \\to f em C_b(X,Y) , chamamos de converg\u00eancia uniforme . Teorema (Stone-Weierstra \\beta ): Os polin\u00f4mios (em z e \\bar{z} ) s\u00e3o densos em C(K) quando K \\subseteq \\mathbb{C} \u00e9 compacto. A demonstra\u00e7\u00e3o dada no livro do Muscat consiste em cinco etapas: (I) Existe uma sequ\u00eancia de polin\u00f4mios que aproxima |x| em x \\in [-1,1] . Defina q_{n+1}(x) = q_n(x) + (x^2 - q_n(x)^2) e q_0(x) = 0 . Primeiro mostra-se a converg\u00eancia ponto a ponto, depois a uniforme. (II) Seja f \\in C(K, \\mathbb{R}) com c = \\max_{x \\in K} |f(x)| > 0 . Ent\u00e3o |f/c| pode ser aproximado por q_n \\circ f/c . Al\u00e9m do mais, se p_n \\to f , ent\u00e3o cq_n \\circ (p_n/c) \\to |f| . (III) Se f e g podem ser aproximados por polin\u00f4mios, ent\u00e3o f+g e f-g tamb\u00e9m podem. Portanto |f-g| , \\max\\{f,g\\} e \\min\\{f,g\\} tamb\u00e9m podem ser aproximados. (IV) Polin\u00f4mios reais s\u00e3o densos em C(K, \\mathbb{R}) . Para isso, tome uma fun\u00e7\u00e3o f \\in C(K, \\mathbb{R}) e z \\neq w . Seja p_{w,z} a a reta que passa por f(w) e f(z) e defina U_{z,w} como o conjunto dos pontos a \\in K que satisfazem p_{w,z}(a) < f(a) + \\epsilon , claramente aberto e n\u00e3o-vazio, cuja uni\u00e3o sobre w \\neq z cobre K (que \u00e9 compacto e, portanto, admite subcobertura finita). Assim g_{z} := \\min\\{p_{z,w_1}, \\dots, p_{z,w_M}\\} < f + \\epsilon pode ser aproximada por polin\u00f4mios ( w_1, \\dots, w_M \u00e9 a subcobertura finita). Faz-se o mesmo com os conjuntos V_z = \\{a \\in K : g_z(a) > f(a) - \\epsilon\\} . (V) Para f \\in C(K) , basta escrever f = u + iv , em que u e v tem imagem nos reais. Um conjunto F \\subseteq C(X,Y) \u00e9 equicont\u00ednuo se para todo \\epsilon > 0 , existe \\delta > 0 tal que para toda fun\u00e7\u00e3o f \\in F , se x, x' \\in X satisfazem d_X(x,x') < \\delta , ent\u00e3o d(f(x), f(x')) < \\epsilon . Com isso, um conjunto desses cont\u00e9m fun\u00e7\u00f5es uniformemente cont\u00ednuas e, al\u00e9m disso, o valor de \\delta \u00e9 selecionado independente da fun\u00e7\u00e3o escolhida. Teorema de Arzel\u00e0-Ascoli: Seja K compacto. Ent\u00e3o F \\subseteq C(K,Y) \u00e9 totalmente limitado se, e somente se, F K = \\{f(x) : f \\in F, x \\in K\\} \u00e9 totalmente limitado em Y e F \u00e9 equicont\u00ednuo. Uma vers\u00e3o um pouco simplificada que pode ser encontrada na literatura: Uma condi\u00e7\u00e3o suficiente e necess\u00e1ria para que F \\subseteq C([a,b], \\mathbb{R}) seja compacto em C[a,b] \u00e9 que F seja equicont\u00ednuo e uniformemente limitado , isto \u00e9, existe um M tal que |f(x)| < M para todo x \\in [a,b] e para todo f \\in F .","title":"Teorema de Arzel\u00e0-Ascoli"},{"location":"functional_analysis/banach_spaces/","text":"Espa\u00e7os normados Um espa\u00e7o vetorial V sobre um corpo F \u00e9 um conjunto em que se define a soma + : V^2 \\to V e o produto escalar \\cdot : F \\times V \\to V , tal que a soma satis\u00e7a as propriedades: associatividade , comutatividade , exist\u00eancia de elemento zero e elemento inverso , e o produto seja linear, isto \u00e9, \\lambda (x + y) = \\lambda x + \\lambda y , (\\lambda + \\mu) x = \\lambda x + \\mu x e \\lambda (\\mu x) = (\\lambda \\mu) x . Defini\u00e7\u00e3o Um espa\u00e7o normado (linear) X \u00e9 um espa\u00e7o vetorial sobre F = \\mathbb{R} ou \\mathbb{C} com uma fun\u00e7\u00e3o \\|\\cdot\\| : X \\to \\mathbb{R}, tal que \\|x\\| = 0 \\iff x = 0 , \\|\\lambda x\\| = |\\lambda|\\|x\\| e \\|x+y\\| \\le \\|x\\| + \\|y\\| , para todo x,y \\in X e \\lambda \\in F . \ud83d\udcdd Exemplos O valor absoluto em \\mathbb{R} e \\mathbb{C} define uma norma. Os espa\u00e7os \\mathbb{R}^n e \\mathbb{C}^n com a norma \\|x\\|_2 := \\left(\\sum_{i=1} |x_i|^p \\right)^{1/p} para p \\ge 1 , definem espa\u00e7os normados. Quando p=2 , temos a norma Euclidiana. O espa\u00e7o l^1 = \\{(a_n)_{n \\in \\mathbb{N}} : \\sum |a_n| < +\\infty\\} com a m\u00e9trica \\|(a_n)\\| = \\sum |a_n| define um espa\u00e7o normado. O espa\u00e7o das fun\u00e7\u00f5es absolutamente integr\u00e1veis em A , L^1(A) , com a norma \\|f\\| = \\int_A |f(x)| \\, dx n\u00e3o forma um espa\u00e7o normado, pois \\|f\\| = 0 n\u00e3o implica que f = 0 . Mas temos que f = 0 quase certamente (isso demanda a necessidade de definir um espa\u00e7o de medida, mas isso n\u00e3o ser\u00e1 feito aqui). Com isso, o espa\u00e7o de Lebesgue \u00e9 definido pelas classes de equival\u00eancia de fun\u00e7\u00f5es iguais quase certamente. Esse espa\u00e7o \u00e9, de fato, espa\u00e7o normado. Note que um espa\u00e7o normado \u00e9 um espa\u00e7o m\u00e9trico com a m\u00e9trica d(x,y) = \\|x-y\\| . Logo, um espa\u00e7o normado \u00e9 completo se o espa\u00e7o m\u00e9trico induzido \u00e9 completo. Um espa\u00e7o normado completo \u00e9 um espa\u00e7o de Banach Em contrapartida, nem todo espa\u00e7o m\u00e9trico define um espa\u00e7o normado com a norma induzida. \ud83d\udcdd Exemplo Considere um espa\u00e7o vetorial X e d a m\u00e9trica trivial. Assim, d(\\lambda x, 0) = 1. Mas se existe uma norma tal que \\|x-y\\| = d(x,y) , ent\u00e3o ter\u00edamos que d(\\lambda x, 0) = \\lambda d(x,0) = \\lambda . Seja X um espa\u00e7o normado sobre o corpo F . Teorema: Os mapas (x,y) \\mapsto x + y , (\\alpha, x) \\mapsto \\alpha\\cdot x e x \\mapsto \\|x\\| s\u00e3o cont\u00ednuos. Desigualdades de H\u00f6lder e Minkowski O seguinte resultado \u00e9 uma aplica\u00e7\u00e3o de resultados de integral. Lemma: Sejam a,b, \\ge 0 e p, q > 1 tal que p^{-1} + q^{-1} = 1 . Ent\u00e3o ab \\le a^p/p + b^q/q . A ideia da demonstra\u00e7\u00e3o \u00e9 considerar as \u00e1reas A_1 = \\int_0^a x^{p-1} = \\frac{a^p}{p} \\text{ e } A_2 = \\int_0^b y^{q-1} = \\frac{b^q}{q}, de forma que y = x^{p-1} \\implies x = y^{q-1} e, portanto, ab \\le A_1 + A_2 . A partir dessa lema, chegamos no importante teorema de H\u00f6lder. Teorema (Holder): Seja p,q > 1 tal que p^{-1} + q^{-1} = 1 . Ent\u00e3o, vale que, se $x,y \\in \\mathbb{R} \\sum_{j=1}^n |x_i y_i| \\le \\left(\\sum_{j=1}^n |x_j|^p\\right)^{1/p}\\left(\\sum_{j=1}^n |y_j|^q\\right)^{1/q}. Esse resultado pode ser estendido para o espa\u00e7o das sequ\u00eancias definidas em l^p \\cap l^{q} . Al\u00e9m disso, o resultado se estende para o espa\u00e7o das fun\u00e7\u00f5es em L^p \\cap L^q , \\int_a^b |f(x) g(x)| \\, dx \\le \\left(\\int_a^b |f(x)|^p \\, dx\\right)^{1/p}\\left(\\int_a^b |g(x)|^q\\right)^{1/q} Outra desigualdade importante \u00e9 a de Minkowski, Teorema (Minkowski): Seja p \\ge 1 . Ent\u00e3o, \\left(\\sum_{j=1}^n |x_j + y_j|^p\\right)^{1/p} \\le \\left(\\sum_{j=1}^n |x_j|^p\\right)^{1/p} + \\left(\\sum_{j=1}^n |y_j|^p\\right)^{1/p}, que tamb\u00e9m pode ser estendido para s\u00e9ries e fun\u00e7\u00f5es. Completamento de um espa\u00e7o normado Seja (X, \\|\\cdot\\|) um espa\u00e7o normado e d(x,y) = \\|x-y\\| a m\u00e9trica induzida. Assim (X,d) \u00e9 um espa\u00e7o m\u00e9trico e existe um completamento (X^*, d^*) . Com uma defini\u00e7\u00e3o apropriada, X^* \u00e9 um espa\u00e7o normado (completo, claramente), com a propriedade de que X \u00e9 isomorfo e isom\u00e9trico a um subjunto denso de X^* . Al\u00e9m do mais, a norma de X^* vai estender a norma de X . Observa\u00e7\u00e3o: Uma fun\u00e7\u00e3o \u00e9 uma * congru\u00eancia** se \u00e9 uma isometria e um isomorfismo.* Ideia da prova: Sejam x^*, y^* \\in X^* (classes de equival\u00eancia de sequ\u00eancias Cauchy). Defina x^* + y^* = \\{x_n + y_n\\}_{n\\in\\mathbb{N}} , a sequ\u00eancia definida pela soma de suas respectivas representantes. \u00c9 claro que soma de sequ\u00eancias de Cauchy \u00e9 sequ\u00eancia de Cauchy e que se escolhessemos quaisquer outras representantes, ter\u00edamos sequ\u00eancias equivalentes. Al\u00e9m disso, \u00e9 f\u00e1cil ver tamb\u00e9m que \\{\\alpha x_n\\} \u00e9 sequ\u00eancia de Cauchy se \\{x_n\\} o for. Com isso, \u00e9 f\u00e1cil verificar que X^* \u00e9 um espa\u00e7o vetorial. Defina a norma \\|x^*\\|_* = \\lim \\|x_n\\| . Esse limite existe, pois \\{\\|x_n\\|\\} \u00e9 Cauchy e, portanto, convergente em \\mathbb{R} . Al\u00e9m disso, para qualquer outra sequ\u00eancia representante de x^* , o limite ser\u00e1 o mesmo pela equival\u00eancia. Ap\u00f3s verificar que \\|\\cdot\\|_* \u00e9 de fato uma norma, conclu\u00edmos que X^* \u00e9 espa\u00e7o normado. Observe que \\|x^* - y^*\\|_* = \\lim \\|x_n - y_n\\| = \\lim d(x_n, y_n ) = d^*(x^*, y^*) , portanto, d^* \u00e9 a m\u00e9trica induzida pela norma. Com essa m\u00e9trica, sabemos que o espa\u00e7o m\u00e9trico (X^*, d^*) \u00e9 completo e, portanto, X^* \u00e9 espa\u00e7o normado completo. Seja X_0 o espa\u00e7o das sequ\u00eancias constantes de X^* , que sabemos que \u00e9 denso em X^* e isom\u00e9trico a X . Falta verificar que o mapa A : X \\to X_0 tamb\u00e9m \u00e9 um isomorfismo, isto \u00e9, que ele \u00e9 bijetivo, o que ele de fato \u00e9, e que ele preserva combina\u00e7\u00f5es lineares, o que \u00e9 imediado. Por (1)-(4), verificamos que (X^*, \\|\\cdot\\|_*) \u00e9 um espa\u00e7o m\u00e9trico completo que cont\u00e9m um conjunto denso isom\u00e9trico e isomorfo a X . Subespa\u00e7os gerados e fechados Seja X um espa\u00e7o vetorial linear. Um conjunto S \\subseteq X \u00e9 um subespa\u00e7o quando S com as opera\u00e7\u00f5es de X forma um espa\u00e7o vetorial. Sendo S um subconjunto de X , o subespa\u00e7o gerado por S , dito [S] , \u00e9 a intersec\u00e7\u00e3o de todos os subespa\u00e7os de X que cont\u00e9m S . \u00c9 f\u00e1cil ver que a intersec\u00e7\u00e3o de subespa\u00e7os \u00e9 um subespa\u00e7o e que [S] \\subseteq X . De forma equivalente [S] \u00e9 o conjunto de todas as combina\u00e7\u00f5es lineares de elementos de S . Em espa\u00e7os normados de dimens\u00e3o finita, todos os subespa\u00e7os s\u00e3o fechados. Teorema: Se M \u00e9 subespa\u00e7o de um espa\u00e7o normado X , ent\u00e3o \\bar{M} \u00e9 subespa\u00e7o tamb\u00e9m. Para provar o teorema acima, \u00e9 necess\u00e1rio verificar que qualquer combina\u00e7\u00e3o linear de elementos de \\bar{M} pertence a \\bar{M} . Isso n\u00e3o \u00e9 dif\u00edcil de ver, pois se x_n \\to x e y_n \\to y , ent\u00e3o \\alpha x_n + \\beta y_n \\to \\alpha x + \\beta y . Para um conjunto S , definindo o menor subespa\u00e7o fechado que cont\u00e9m S , dito M , como a intersecc\u00e7\u00e3o desses subespa\u00e7os, \u00e9 f\u00e1cil ver que M = \\bar{[S]} , isto \u00e9, o fecho de [S] . Normas equivalentes Seja X um espa\u00e7o vetorial sobre um corpo F e sejam \\|\\cdot\\|_1 e \\|\\cdot\\|_2 normas em X . Dizemos que \\|\\cdot\\|_1 \\sim \\|\\cdot\\|_2 , isto \u00e9, as normas s\u00e3o equivalente, se existem a,b>0 tal que a\\|x\\|_1 \\le \\|x\\|_2 \\le b\\|x\\|_1, \\forall x \\in X, que forma uma rela\u00e7\u00e3o de equival\u00eancia. Com isso, sequ\u00eancia de Cauchy sobre uma norma, tamb\u00e9m ser\u00e1 sobre qualquer outra norma equivalente. Al\u00e9m disso, a classe de conjuntos abertos \u00e9 a mesma para normas equivalentes. Todas as normas s\u00e3o equivalente em espa\u00e7os finito-dimensionais Como equival\u00eancia de normas \u00e9 uma rela\u00e7\u00e3o de equival\u00eancia, \u00e9 suficiente mostrar que qualquer norma \u00e9 equivalente a uma espec\u00edfica. Em particular, seja x_1, \\dots, x_n uma base de X (espa\u00e7o normado finito-dimensional). Para cada x \\in X , defina \\|x\\|_0 := \\max_i |\\alpha_i|, \\text{ em que } x = \\sum_{i=1}^n \\alpha_i x_i, que \u00e9 bem definida, pois a escrita de um vetor em um base \u00e9 \u00fanica. Ideia da prova: Seja \\|\\cdot\\| uma norma em X . Assim, \\|x\\| = \\|\\sum_{i=1}^n \\alpha_i x_i \\| \\le \\sum_{i=1}^n |\\alpha_i|\\|x_i\\| \\le \\|x\\|_0 \\sum_{i=1}^n \\|x_i\\| = b\\|x\\|_0 , com b = \\sum_{i=1}^n \\|x_i\\| . Para a outra igualdade, se prova por indu\u00e7\u00e3o em n = \\operatorname{dim} X . Para n=1 , isso \u00e9 trivial, visto que \\|x\\| \\ge \\|x\\|_0 \\|x_1\\| = a\\|x\\|_0 . Suponha que o teorema valha para espa\u00e7os de dimens\u00e3o menor ou igual a n-1 , tome a base \\{x_1, \\dots, x_n\\} e defina M = [\\{x_1, \\dots, x_{n-1}\\}] . Ent\u00e3o \\|\\cdot \\| \\sim \\|\\cdot\\|_0 em M . Prova-se que M \u00e9 espa\u00e7o completo e, portanto, fechado. Considere o conjunto x_n + M , que \u00e9 fechado e note que 0 \\not \\in x_n + M . Como (x_n + M)^c \u00e9 aberto, existe uma bola aberta de centro 0 , tal que B_{c_n}(0) \\subseteq (x_n + M)^c . Nesse caso, \\|x\\| \\ge c_n para todo x \\in x_n + M . Com isso, \\|x\\| \\ge |\\alpha_n|c_n para todo x \\in X , em que \\alpha_n \u00e9 o coeficiente de x_n . Note que os fatos acima valeriam tamb\u00e9m para qualquer M_i = [\\{x_j\\}_{j=1}^n / x_i] . Nesse caso, \\|x\\| \\ge |\\alpha_i|c_i para todo x \\in X . Em particular, \\|x\\| \\ge \\min_i c_i \\|x\\|_0 , o que completa a prova. Como consequ\u00eancia temos os seguintes fatos: Se X \u00e9 espa\u00e7o normado finito-dimensional, ent\u00e3o ele \u00e9 completo. Se X \u00e9 espa\u00e7o normado e M subespa\u00e7o finito-dimensional, ent\u00e3o M \u00e9 fechado. Teorema de Riesz Seja M um subespa\u00e7o fechado pr\u00f3prio de um espa\u00e7o normado X e a \\in (0,1) . Ent\u00e3o existe um vetor x_a \\in X tal que \\|x_{a}\\| = 1 e \\|x - x_a\\| \\ge a para todo x \\in M . Ideia da prova: Tome x_1 \\in X / M e defina d = \\inf_{x \\in M} \\|x-x_1\\| > 0 , pois M \u00e9 fechado. Como d/a > d , existe x_0 \\in M tal que \\|x_0 - x_1\\| < d/a . Defina x_a = \\frac{x_1 - x_0}{\\|x_1 - x_0\\|}. Veja que para todo x \\in M , \\|x-x_a\\| \\ge \\frac{d}{\\|x_1 - x_0\\|} \\ge \\frac{d}{d/a} = a, pois (\\|x_1 - x_0\\|)x + x_0 \\in M e x - x_a \\propto (\\|x_1 - x_0\\|)x + x_0 - x_1 .","title":"Espa\u00e7os normados"},{"location":"functional_analysis/banach_spaces/#espacos-normados","text":"Um espa\u00e7o vetorial V sobre um corpo F \u00e9 um conjunto em que se define a soma + : V^2 \\to V e o produto escalar \\cdot : F \\times V \\to V , tal que a soma satis\u00e7a as propriedades: associatividade , comutatividade , exist\u00eancia de elemento zero e elemento inverso , e o produto seja linear, isto \u00e9, \\lambda (x + y) = \\lambda x + \\lambda y , (\\lambda + \\mu) x = \\lambda x + \\mu x e \\lambda (\\mu x) = (\\lambda \\mu) x .","title":"Espa\u00e7os normados"},{"location":"functional_analysis/banach_spaces/#definicao","text":"Um espa\u00e7o normado (linear) X \u00e9 um espa\u00e7o vetorial sobre F = \\mathbb{R} ou \\mathbb{C} com uma fun\u00e7\u00e3o \\|\\cdot\\| : X \\to \\mathbb{R}, tal que \\|x\\| = 0 \\iff x = 0 , \\|\\lambda x\\| = |\\lambda|\\|x\\| e \\|x+y\\| \\le \\|x\\| + \\|y\\| , para todo x,y \\in X e \\lambda \\in F . \ud83d\udcdd Exemplos O valor absoluto em \\mathbb{R} e \\mathbb{C} define uma norma. Os espa\u00e7os \\mathbb{R}^n e \\mathbb{C}^n com a norma \\|x\\|_2 := \\left(\\sum_{i=1} |x_i|^p \\right)^{1/p} para p \\ge 1 , definem espa\u00e7os normados. Quando p=2 , temos a norma Euclidiana. O espa\u00e7o l^1 = \\{(a_n)_{n \\in \\mathbb{N}} : \\sum |a_n| < +\\infty\\} com a m\u00e9trica \\|(a_n)\\| = \\sum |a_n| define um espa\u00e7o normado. O espa\u00e7o das fun\u00e7\u00f5es absolutamente integr\u00e1veis em A , L^1(A) , com a norma \\|f\\| = \\int_A |f(x)| \\, dx n\u00e3o forma um espa\u00e7o normado, pois \\|f\\| = 0 n\u00e3o implica que f = 0 . Mas temos que f = 0 quase certamente (isso demanda a necessidade de definir um espa\u00e7o de medida, mas isso n\u00e3o ser\u00e1 feito aqui). Com isso, o espa\u00e7o de Lebesgue \u00e9 definido pelas classes de equival\u00eancia de fun\u00e7\u00f5es iguais quase certamente. Esse espa\u00e7o \u00e9, de fato, espa\u00e7o normado. Note que um espa\u00e7o normado \u00e9 um espa\u00e7o m\u00e9trico com a m\u00e9trica d(x,y) = \\|x-y\\| . Logo, um espa\u00e7o normado \u00e9 completo se o espa\u00e7o m\u00e9trico induzido \u00e9 completo. Um espa\u00e7o normado completo \u00e9 um espa\u00e7o de Banach Em contrapartida, nem todo espa\u00e7o m\u00e9trico define um espa\u00e7o normado com a norma induzida. \ud83d\udcdd Exemplo Considere um espa\u00e7o vetorial X e d a m\u00e9trica trivial. Assim, d(\\lambda x, 0) = 1. Mas se existe uma norma tal que \\|x-y\\| = d(x,y) , ent\u00e3o ter\u00edamos que d(\\lambda x, 0) = \\lambda d(x,0) = \\lambda . Seja X um espa\u00e7o normado sobre o corpo F . Teorema: Os mapas (x,y) \\mapsto x + y , (\\alpha, x) \\mapsto \\alpha\\cdot x e x \\mapsto \\|x\\| s\u00e3o cont\u00ednuos.","title":"Defini\u00e7\u00e3o"},{"location":"functional_analysis/banach_spaces/#desigualdades-de-holder-e-minkowski","text":"O seguinte resultado \u00e9 uma aplica\u00e7\u00e3o de resultados de integral. Lemma: Sejam a,b, \\ge 0 e p, q > 1 tal que p^{-1} + q^{-1} = 1 . Ent\u00e3o ab \\le a^p/p + b^q/q . A ideia da demonstra\u00e7\u00e3o \u00e9 considerar as \u00e1reas A_1 = \\int_0^a x^{p-1} = \\frac{a^p}{p} \\text{ e } A_2 = \\int_0^b y^{q-1} = \\frac{b^q}{q}, de forma que y = x^{p-1} \\implies x = y^{q-1} e, portanto, ab \\le A_1 + A_2 . A partir dessa lema, chegamos no importante teorema de H\u00f6lder. Teorema (Holder): Seja p,q > 1 tal que p^{-1} + q^{-1} = 1 . Ent\u00e3o, vale que, se $x,y \\in \\mathbb{R} \\sum_{j=1}^n |x_i y_i| \\le \\left(\\sum_{j=1}^n |x_j|^p\\right)^{1/p}\\left(\\sum_{j=1}^n |y_j|^q\\right)^{1/q}. Esse resultado pode ser estendido para o espa\u00e7o das sequ\u00eancias definidas em l^p \\cap l^{q} . Al\u00e9m disso, o resultado se estende para o espa\u00e7o das fun\u00e7\u00f5es em L^p \\cap L^q , \\int_a^b |f(x) g(x)| \\, dx \\le \\left(\\int_a^b |f(x)|^p \\, dx\\right)^{1/p}\\left(\\int_a^b |g(x)|^q\\right)^{1/q} Outra desigualdade importante \u00e9 a de Minkowski, Teorema (Minkowski): Seja p \\ge 1 . Ent\u00e3o, \\left(\\sum_{j=1}^n |x_j + y_j|^p\\right)^{1/p} \\le \\left(\\sum_{j=1}^n |x_j|^p\\right)^{1/p} + \\left(\\sum_{j=1}^n |y_j|^p\\right)^{1/p}, que tamb\u00e9m pode ser estendido para s\u00e9ries e fun\u00e7\u00f5es.","title":"Desigualdades de H\u00f6lder e Minkowski"},{"location":"functional_analysis/banach_spaces/#completamento-de-um-espaco-normado","text":"Seja (X, \\|\\cdot\\|) um espa\u00e7o normado e d(x,y) = \\|x-y\\| a m\u00e9trica induzida. Assim (X,d) \u00e9 um espa\u00e7o m\u00e9trico e existe um completamento (X^*, d^*) . Com uma defini\u00e7\u00e3o apropriada, X^* \u00e9 um espa\u00e7o normado (completo, claramente), com a propriedade de que X \u00e9 isomorfo e isom\u00e9trico a um subjunto denso de X^* . Al\u00e9m do mais, a norma de X^* vai estender a norma de X . Observa\u00e7\u00e3o: Uma fun\u00e7\u00e3o \u00e9 uma * congru\u00eancia** se \u00e9 uma isometria e um isomorfismo.* Ideia da prova: Sejam x^*, y^* \\in X^* (classes de equival\u00eancia de sequ\u00eancias Cauchy). Defina x^* + y^* = \\{x_n + y_n\\}_{n\\in\\mathbb{N}} , a sequ\u00eancia definida pela soma de suas respectivas representantes. \u00c9 claro que soma de sequ\u00eancias de Cauchy \u00e9 sequ\u00eancia de Cauchy e que se escolhessemos quaisquer outras representantes, ter\u00edamos sequ\u00eancias equivalentes. Al\u00e9m disso, \u00e9 f\u00e1cil ver tamb\u00e9m que \\{\\alpha x_n\\} \u00e9 sequ\u00eancia de Cauchy se \\{x_n\\} o for. Com isso, \u00e9 f\u00e1cil verificar que X^* \u00e9 um espa\u00e7o vetorial. Defina a norma \\|x^*\\|_* = \\lim \\|x_n\\| . Esse limite existe, pois \\{\\|x_n\\|\\} \u00e9 Cauchy e, portanto, convergente em \\mathbb{R} . Al\u00e9m disso, para qualquer outra sequ\u00eancia representante de x^* , o limite ser\u00e1 o mesmo pela equival\u00eancia. Ap\u00f3s verificar que \\|\\cdot\\|_* \u00e9 de fato uma norma, conclu\u00edmos que X^* \u00e9 espa\u00e7o normado. Observe que \\|x^* - y^*\\|_* = \\lim \\|x_n - y_n\\| = \\lim d(x_n, y_n ) = d^*(x^*, y^*) , portanto, d^* \u00e9 a m\u00e9trica induzida pela norma. Com essa m\u00e9trica, sabemos que o espa\u00e7o m\u00e9trico (X^*, d^*) \u00e9 completo e, portanto, X^* \u00e9 espa\u00e7o normado completo. Seja X_0 o espa\u00e7o das sequ\u00eancias constantes de X^* , que sabemos que \u00e9 denso em X^* e isom\u00e9trico a X . Falta verificar que o mapa A : X \\to X_0 tamb\u00e9m \u00e9 um isomorfismo, isto \u00e9, que ele \u00e9 bijetivo, o que ele de fato \u00e9, e que ele preserva combina\u00e7\u00f5es lineares, o que \u00e9 imediado. Por (1)-(4), verificamos que (X^*, \\|\\cdot\\|_*) \u00e9 um espa\u00e7o m\u00e9trico completo que cont\u00e9m um conjunto denso isom\u00e9trico e isomorfo a X .","title":"Completamento de um espa\u00e7o normado"},{"location":"functional_analysis/banach_spaces/#subespacos-gerados-e-fechados","text":"Seja X um espa\u00e7o vetorial linear. Um conjunto S \\subseteq X \u00e9 um subespa\u00e7o quando S com as opera\u00e7\u00f5es de X forma um espa\u00e7o vetorial. Sendo S um subconjunto de X , o subespa\u00e7o gerado por S , dito [S] , \u00e9 a intersec\u00e7\u00e3o de todos os subespa\u00e7os de X que cont\u00e9m S . \u00c9 f\u00e1cil ver que a intersec\u00e7\u00e3o de subespa\u00e7os \u00e9 um subespa\u00e7o e que [S] \\subseteq X . De forma equivalente [S] \u00e9 o conjunto de todas as combina\u00e7\u00f5es lineares de elementos de S . Em espa\u00e7os normados de dimens\u00e3o finita, todos os subespa\u00e7os s\u00e3o fechados. Teorema: Se M \u00e9 subespa\u00e7o de um espa\u00e7o normado X , ent\u00e3o \\bar{M} \u00e9 subespa\u00e7o tamb\u00e9m. Para provar o teorema acima, \u00e9 necess\u00e1rio verificar que qualquer combina\u00e7\u00e3o linear de elementos de \\bar{M} pertence a \\bar{M} . Isso n\u00e3o \u00e9 dif\u00edcil de ver, pois se x_n \\to x e y_n \\to y , ent\u00e3o \\alpha x_n + \\beta y_n \\to \\alpha x + \\beta y . Para um conjunto S , definindo o menor subespa\u00e7o fechado que cont\u00e9m S , dito M , como a intersecc\u00e7\u00e3o desses subespa\u00e7os, \u00e9 f\u00e1cil ver que M = \\bar{[S]} , isto \u00e9, o fecho de [S] .","title":"Subespa\u00e7os gerados e fechados"},{"location":"functional_analysis/banach_spaces/#normas-equivalentes","text":"Seja X um espa\u00e7o vetorial sobre um corpo F e sejam \\|\\cdot\\|_1 e \\|\\cdot\\|_2 normas em X . Dizemos que \\|\\cdot\\|_1 \\sim \\|\\cdot\\|_2 , isto \u00e9, as normas s\u00e3o equivalente, se existem a,b>0 tal que a\\|x\\|_1 \\le \\|x\\|_2 \\le b\\|x\\|_1, \\forall x \\in X, que forma uma rela\u00e7\u00e3o de equival\u00eancia. Com isso, sequ\u00eancia de Cauchy sobre uma norma, tamb\u00e9m ser\u00e1 sobre qualquer outra norma equivalente. Al\u00e9m disso, a classe de conjuntos abertos \u00e9 a mesma para normas equivalentes.","title":"Normas equivalentes"},{"location":"functional_analysis/banach_spaces/#todas-as-normas-sao-equivalente-em-espacos-finito-dimensionais","text":"Como equival\u00eancia de normas \u00e9 uma rela\u00e7\u00e3o de equival\u00eancia, \u00e9 suficiente mostrar que qualquer norma \u00e9 equivalente a uma espec\u00edfica. Em particular, seja x_1, \\dots, x_n uma base de X (espa\u00e7o normado finito-dimensional). Para cada x \\in X , defina \\|x\\|_0 := \\max_i |\\alpha_i|, \\text{ em que } x = \\sum_{i=1}^n \\alpha_i x_i, que \u00e9 bem definida, pois a escrita de um vetor em um base \u00e9 \u00fanica. Ideia da prova: Seja \\|\\cdot\\| uma norma em X . Assim, \\|x\\| = \\|\\sum_{i=1}^n \\alpha_i x_i \\| \\le \\sum_{i=1}^n |\\alpha_i|\\|x_i\\| \\le \\|x\\|_0 \\sum_{i=1}^n \\|x_i\\| = b\\|x\\|_0 , com b = \\sum_{i=1}^n \\|x_i\\| . Para a outra igualdade, se prova por indu\u00e7\u00e3o em n = \\operatorname{dim} X . Para n=1 , isso \u00e9 trivial, visto que \\|x\\| \\ge \\|x\\|_0 \\|x_1\\| = a\\|x\\|_0 . Suponha que o teorema valha para espa\u00e7os de dimens\u00e3o menor ou igual a n-1 , tome a base \\{x_1, \\dots, x_n\\} e defina M = [\\{x_1, \\dots, x_{n-1}\\}] . Ent\u00e3o \\|\\cdot \\| \\sim \\|\\cdot\\|_0 em M . Prova-se que M \u00e9 espa\u00e7o completo e, portanto, fechado. Considere o conjunto x_n + M , que \u00e9 fechado e note que 0 \\not \\in x_n + M . Como (x_n + M)^c \u00e9 aberto, existe uma bola aberta de centro 0 , tal que B_{c_n}(0) \\subseteq (x_n + M)^c . Nesse caso, \\|x\\| \\ge c_n para todo x \\in x_n + M . Com isso, \\|x\\| \\ge |\\alpha_n|c_n para todo x \\in X , em que \\alpha_n \u00e9 o coeficiente de x_n . Note que os fatos acima valeriam tamb\u00e9m para qualquer M_i = [\\{x_j\\}_{j=1}^n / x_i] . Nesse caso, \\|x\\| \\ge |\\alpha_i|c_i para todo x \\in X . Em particular, \\|x\\| \\ge \\min_i c_i \\|x\\|_0 , o que completa a prova. Como consequ\u00eancia temos os seguintes fatos: Se X \u00e9 espa\u00e7o normado finito-dimensional, ent\u00e3o ele \u00e9 completo. Se X \u00e9 espa\u00e7o normado e M subespa\u00e7o finito-dimensional, ent\u00e3o M \u00e9 fechado.","title":"Todas as normas s\u00e3o equivalente em espa\u00e7os finito-dimensionais"},{"location":"functional_analysis/banach_spaces/#teorema-de-riesz","text":"Seja M um subespa\u00e7o fechado pr\u00f3prio de um espa\u00e7o normado X e a \\in (0,1) . Ent\u00e3o existe um vetor x_a \\in X tal que \\|x_{a}\\| = 1 e \\|x - x_a\\| \\ge a para todo x \\in M . Ideia da prova: Tome x_1 \\in X / M e defina d = \\inf_{x \\in M} \\|x-x_1\\| > 0 , pois M \u00e9 fechado. Como d/a > d , existe x_0 \\in M tal que \\|x_0 - x_1\\| < d/a . Defina x_a = \\frac{x_1 - x_0}{\\|x_1 - x_0\\|}. Veja que para todo x \\in M , \\|x-x_a\\| \\ge \\frac{d}{\\|x_1 - x_0\\|} \\ge \\frac{d}{d/a} = a, pois (\\|x_1 - x_0\\|)x + x_0 \\in M e x - x_a \\propto (\\|x_1 - x_0\\|)x + x_0 - x_1 .","title":"Teorema de Riesz"},{"location":"functional_analysis/compactness/","text":"Compacidade Defini\u00e7\u00e3o: Uma sequ\u00eancia de conjuntos encaixados \u00e9 uma sequ\u00eancia \\{F_n\\}_{n\\in\\mathbb{N}} se F_{n+1} \\subseteq F_n para todo n \\in \\mathbb{N} . Tamb\u00e9m definimos o di\u00e2metro de um conjunto A \\subseteq X como d(A) = \\sup_{a,b \\in A} d(a,b) . Teorema: Uma espa\u00e7o m\u00e9trico (X,d) \u00e9 completo se, e somente se, para toda sequ\u00eancia de conjuntos encaixados fechados e n\u00e3o vazios \\{F_n\\}, n \\ge 1 com d(F_n) \\to 0 , vale que \\cap_{n\\in\\mathbb{N}} F_n \\neq \\emptyset. Cobertura: Seja I um conjunto de \u00edndices. Dizemos que \\{G_{\\alpha}\\}_{\\alpha \\in I} \u00e9 uma cobertura de A se A \\subseteq \\cup_{\\alpha \\in I} G_{\\alpha} . Se I \u00e9 finito, chamamos de cobertura finita . Se G_{\\alpha} \u00e9 aberto para todo \\alpha \\in I , chamamos de cobertura aberta . Conjuntos limitados Um subconjunto A do espa\u00e7o m\u00e9trico (X,d) \u00e9 limitado se existe M \\in \\mathbb{R} tal que d(A) \\le M . A dist\u00e2ncia entre um ponto e um conjunto \u00e9 dado por d(x,B) = \\inf_{y \\in B} d(x,y) e a dist\u00e2ncia entre conjuntos \u00e9 d(B,C) = \\inf_{x \\in B, y \\in C} d(x,y). A uni\u00e3o finita de conjuntos limitados \u00e9 limitada. Outra rela\u00e7\u00e3o \u00e9 que um conjunto B \u00e9 limitado se existe uma bola que o cont\u00e9m. Conjuntos totalmente limitados Um conjunto B \\subseteq X \u00e9 totalmente limitado se pode ser coberto por um n\u00famero finito de bolas de raio \\epsilon , isto \u00e9, \\forall \\epsilon > 0, \\text{ existem pontos } a_1, \\dots, a_n \\text{ tal que } B \\subseteq \\cup_{i=1}^n B_{\\epsilon}(a_n). Temos que todo conjunto totalmente limitado \u00e9 limitado. \ud83d\udcdd Exemplo Tome A = [0,1] em (\\mathbb{R}, d) . A \u00e9 totalmente limitado, pois, para todo \\epsilon > 0 , A \\subseteq \\cup_{i=0}^n ((i-1)\\epsilon, (i+1)\\epsilon) = \\cup_{i=0}^n B_{\\epsilon}(i\\epsilon), tomando n \\in \\mathbb{N} de forma que (n+1)\\epsilon > 1 . \ud83d\udcdd Nem todo conjunto limitado \u00e9 totalmente limitado Considere a m\u00e9trica trivial d(x,y) = 1 \\iff x \\neq y e X = \\mathbb{N} . O conjunto A dos n\u00fameros pares \u00e9 limitado, pois, d(A) = \\sup_{m,n \\in A} d(m,n) = 1. Mas, para \\epsilon < 1 , B_{\\epsilon}(x) = \\{x\\} para todo x \\in \\mathbb{N} . Logo A n\u00e3o pode ser totalmente limitado. Proposi\u00e7\u00e3o: Uma fun\u00e7\u00e3o uniformemente cont\u00ednua mapeia conjuntos totalmente limitados em conjuntos totalmente limitados. Compacidade Um conjunto A \u00e9 compacto se para toda cobertura aberta, existe uma subcobertura finita. Para mostrar que um conjunto n\u00e3o \u00e9 compacto, basta selecionar uma cobertura aberta que n\u00e3o possua cobertura finita. \ud83d\udcdd Conjunto n\u00e3o compacto O conjunto I = (0,1) n\u00e3o \u00e9 compacto em \\mathbb{R} . Tome a cobertura \\{I_n\\} com I_n = (1/n, 1) . De fato, se x \\in (0,1) , para n > x^{-1} vale que x \\in (1/n, 1) , o que mostra que \\{I_n\\} \u00e9 de fato uma cobertura aberta. Observe que I_{n} \\subseteq I_{n+1} . Com isso, \\cup_{i=1}^k I_{n_i} = (1/\\max_{i=1,\\dots,k} n_i, 1) , que n\u00e3o pode cobrir (0,1) . Teorema: Seja (X,d) um espa\u00e7o m\u00e9trico. Se A \u00e9 conjunto compacto, ent\u00e3o A \u00e9 fechado e limitado. Ideias da demonstra\u00e7\u00e3o: (1) A \u00e9 fechado: tome y \\in A^c e mostra-se que existe B aberto contendo y com A \\cap B = \\emptyset , isto \u00e9, y \\not \\in \\bar{A} . Isso mostra que A = \\bar{A} . Para definir B , para cada x \\in A , sabemos que existe \\epsilon_x de forma que B_{\\epsilon_x}(x) \\cap B_{\\epsilon_x}(y) = \\emptyset . Al\u00e9m disso, \\{B_{\\epsilon_x}(x)\\} define uma cobertura aberta de A que possui subcobertura finita \\{B_{\\epsilon_{x_i}}(x_i)\\}_{i=1}^n . Defina B = \\cap_{i=1}^n B_{\\epsilon_{x_i}}(x_i) . (2) A \u00e9 limitado: Note que A \\subseteq \\cup_{x \\in A} B_1(x) e, portanto, existe subcobertura finita \\{B_1(x_i)\\}_{i=1}^n . Defina a = \\max d(x_i, x_j) . Se A fosse ilimitado, existiriam x,y \\in A de forma que d(x,y) > a + 2 . Por\u00e9m, x \\in B_1(x_i) e y \\in B_1(x_j) para alguns i, j . Portanto, a + 2 < d(x,y) \\le d(x, x_i) + d(x_i, x_j) + d(x_j, y) < a + 2, um absurdo, que indica que A \u00e9 limitado. Proposi\u00e7\u00e3o: Fun\u00e7\u00f5es cont\u00ednuas mapeiam conjuntos compactos em compactos. Com isso, a imagem de conjuntos compactos tem m\u00ednimo e m\u00e1ximo. \ud83d\udcdd Intervalo fechado e limitado O conjunto A = [a,b] \u00e9 compacto em \\mathbb{R} . Seja \\{A_i\\} uma cobertura aberta de A . Suponha que n\u00e3o exista subcobertura finita. Portanto, [a,(a+b)/2] ou [(a+b)/2, b] n\u00e3o admite subcobertura finita: chame de [a_1, b_1] . Mesmo assim, [a_1,(a_1+b_1)/2] ou [(a_1+b_1)/2, b_1] n\u00e3o admite subcobertura finita: chame de [a_2, b_2] . Assim, estamos definido um sequ\u00eancia de intervalos fechados encaixados [a_n, b_n] com d([a_n, b_n]) \\to 0 . As sequ\u00eancias \\{a_n\\} e \\{b_n\\} s\u00e3o sequ\u00eancias mon\u00f3tonas limitadas e, portanto convergentes, com \\lim a_n = \\lim b_n = x , pois d(a_n, b_n) \\to 0 . \u00c9 claro que x \\in A , pois A \u00e9 fechado. Nesse caso, x \\in A_i para algum i e, por ser aberto, x \\in B_{r}(x) \\subseteq A_i . Para n suficientemente grande, a_n, b_n \\in B_r(x) , o que implica que [a_n, b_n] \\subseteq B_r(x) , o que contradiz o fato de ser uma sequ\u00eancia que n\u00e3o pode ser coberta de forma finita. Essa contradi\u00e7\u00e3o mostra que existe subcobertura finita e, portanto, [a,b] \u00e9 fechado. Compacidade relativa e \\epsilon -net Um conjunto \u00e9 relativamente compacto se \\bar{A} \u00e9 compacto. Um exemplo \u00e9 I=(0,1) , que n\u00e3o \u00e9 compacto, mas [0,1] \u00e9. Tamb\u00e9m \u00e9 f\u00e1cil ver que um conjunto compacto \u00e9 relativamente compacto, dado que \u00e9 necessariamente fechado e \\bar{A} = A . Um conjunto de pontos N \u00e9 uma \\epsilon -net com respeito a um conjunto A se, para todo x \\in A , existe y \\in N tal que d(x,y) < \\epsilon . A ideia de uma \\epsilon -net \u00e9 que um conjunto de pontos est\u00e3o \\epsilon pr\u00f3ximos de qualquer ponto de A . Teorema: Seja A um subconjunto em um espa\u00e7o m\u00e9trico. Se para toda sequ\u00eancia de pontos de A , existe uma subsequ\u00eancia convergente, ent\u00e3o A \u00e9 totalmente limitado. Compacidade cont\u00e1vel e sequencial Um conjunto A \u00e9 compacto cont\u00e1vel se todo subconjunto infinito de A tem um ponto limite em A . Teorema: Compacidade implica compacidade cont\u00e1vel. A ideia da prova \u00e9 mostrar que um conjunto n\u00e3o compacto cont\u00e1vel, tamb\u00e9m n\u00e3o \u00e9 compacto. Para isso, deve existir M = \\{x_1, x_2, \\dots\\} \\subseteq A infinito enumer\u00e1vel que n\u00e3o tenha pontos limites de A . Mas com isso, podemos fazer uma sequ\u00eancia de conjuntos abertos de forma que E_n \\cap M = \\{x_n\\} . Se x \\in A / M , ent\u00e3o existe um aberto E(x) com E(x) \\cap M = \\emptyset . A uni\u00e3o dos conjuntos E(x) para x \\in A / M n\u00e3o cont\u00e9m pontos de M. Al\u00e9m disso, todo ponto de E_n s\u00f3 cont\u00e9m um ponto de M , mas A \\subseteq \\cup_{n\\in\\mathbb{N}} E_n \\cup \\cup_{x \\in A/M} E(x), mas \u00e9 imposs\u00edvel existir subcobertura finita e, portanto, A n\u00e3o \u00e9 compacto, como quer\u00edamos verificar. Um conjunto A \u00e9 compacto sequencialmente se, para toda sequ\u00eancia em A , existe uma subsequ\u00eancia convergente com limite em A . Teorema: Compacidade e compacidade sequencial s\u00e3o equivalentes em espa\u00e7os m\u00e9tricos. Que compacidade implica compacidade sequencial, prova-se que compacidade cont\u00e1vel implica sequencial. N\u00e3o \u00e9 complicado, todavia, porque a partir de uma sequ\u00eancia em A , usamos que o conjunto desses pontos (quando infinito) tem ponto limite em A . Justamente, isso implica que existe subsequ\u00eancia convergente em A . A volta \u00e9 um pouco mais complicada e parte de uma cobertura aberta \\{G_{\\alpha}\\} e define-se \\delta_0 = \\inf\\{\\sup \\{a : \\exists \\alpha; B_a(x) \\subseteq G_{\\alpha}\\} | x \\in A\\}, que est\u00e1 bem definido, pois G_{\\alpha} \u00e9 aberto e cobre A . Toma-se uma sequ\u00eancia \\delta(x_n) \\to \\delta_0 para se provar que \\delta_0 > 0 . Para isso, usamos o fato que a sequ\u00eancia \\{x_n\\} tem subsequ\u00eancia convergente com limite em A , isto \u00e9, x_{n_k} \\to x_0 \\in A . Em particular, prova-se que \\delta_0 \\ge \\delta(x_0)/4 > 0 com um pouco de \u00e1lgebra. Como toda sequ\u00eancia tem subsequ\u00eancia convergente, vale que o conjunto \u00e9 totalmente limitado. Da\u00ed para \\delta_0 > \\epsilon > 0 , existe \\epsilon -net finita com centros y_1, \\dots, y_n \\in A . Isso implica que B_{\\epsilon}(y_i) \\subseteq G_i e, portanto, \\{G_1, \\dots, G_n\\} formam uma subcobertura finita. Teorema: Em um espa\u00e7o m\u00e9trico, compacidade relativa implica totalmente limitada. Se ele for completo, totalmente limitada implica compacidade relativa. Observa\u00e7\u00f5es finais: Fun\u00e7\u00f5es cont\u00ednuas preservam compacidade; Fun\u00e7\u00f5es uniformemente cont\u00ednuas preservam totalmente limitados; Fun\u00e7\u00f5es Lipschitz cont\u00ednuas preservam limitados. Compacidade, compacidade cont\u00e1vel e compacidade sequencial s\u00e3o conceitos equivalentes em espa\u00e7os m\u00e9tricos. Se A \u00e9 compacto, ent\u00e3o A \u00e9 fechado e totalmente limitado. Se o espa\u00e7o m\u00e9trico \u00e9 completo, ent\u00e3o A ser fechado e totalmente limitado implica que A \u00e9 compacto.","title":"Compacidade"},{"location":"functional_analysis/compactness/#compacidade","text":"Defini\u00e7\u00e3o: Uma sequ\u00eancia de conjuntos encaixados \u00e9 uma sequ\u00eancia \\{F_n\\}_{n\\in\\mathbb{N}} se F_{n+1} \\subseteq F_n para todo n \\in \\mathbb{N} . Tamb\u00e9m definimos o di\u00e2metro de um conjunto A \\subseteq X como d(A) = \\sup_{a,b \\in A} d(a,b) . Teorema: Uma espa\u00e7o m\u00e9trico (X,d) \u00e9 completo se, e somente se, para toda sequ\u00eancia de conjuntos encaixados fechados e n\u00e3o vazios \\{F_n\\}, n \\ge 1 com d(F_n) \\to 0 , vale que \\cap_{n\\in\\mathbb{N}} F_n \\neq \\emptyset. Cobertura: Seja I um conjunto de \u00edndices. Dizemos que \\{G_{\\alpha}\\}_{\\alpha \\in I} \u00e9 uma cobertura de A se A \\subseteq \\cup_{\\alpha \\in I} G_{\\alpha} . Se I \u00e9 finito, chamamos de cobertura finita . Se G_{\\alpha} \u00e9 aberto para todo \\alpha \\in I , chamamos de cobertura aberta .","title":"Compacidade"},{"location":"functional_analysis/compactness/#conjuntos-limitados","text":"Um subconjunto A do espa\u00e7o m\u00e9trico (X,d) \u00e9 limitado se existe M \\in \\mathbb{R} tal que d(A) \\le M . A dist\u00e2ncia entre um ponto e um conjunto \u00e9 dado por d(x,B) = \\inf_{y \\in B} d(x,y) e a dist\u00e2ncia entre conjuntos \u00e9 d(B,C) = \\inf_{x \\in B, y \\in C} d(x,y). A uni\u00e3o finita de conjuntos limitados \u00e9 limitada. Outra rela\u00e7\u00e3o \u00e9 que um conjunto B \u00e9 limitado se existe uma bola que o cont\u00e9m.","title":"Conjuntos limitados"},{"location":"functional_analysis/compactness/#conjuntos-totalmente-limitados","text":"Um conjunto B \\subseteq X \u00e9 totalmente limitado se pode ser coberto por um n\u00famero finito de bolas de raio \\epsilon , isto \u00e9, \\forall \\epsilon > 0, \\text{ existem pontos } a_1, \\dots, a_n \\text{ tal que } B \\subseteq \\cup_{i=1}^n B_{\\epsilon}(a_n). Temos que todo conjunto totalmente limitado \u00e9 limitado. \ud83d\udcdd Exemplo Tome A = [0,1] em (\\mathbb{R}, d) . A \u00e9 totalmente limitado, pois, para todo \\epsilon > 0 , A \\subseteq \\cup_{i=0}^n ((i-1)\\epsilon, (i+1)\\epsilon) = \\cup_{i=0}^n B_{\\epsilon}(i\\epsilon), tomando n \\in \\mathbb{N} de forma que (n+1)\\epsilon > 1 . \ud83d\udcdd Nem todo conjunto limitado \u00e9 totalmente limitado Considere a m\u00e9trica trivial d(x,y) = 1 \\iff x \\neq y e X = \\mathbb{N} . O conjunto A dos n\u00fameros pares \u00e9 limitado, pois, d(A) = \\sup_{m,n \\in A} d(m,n) = 1. Mas, para \\epsilon < 1 , B_{\\epsilon}(x) = \\{x\\} para todo x \\in \\mathbb{N} . Logo A n\u00e3o pode ser totalmente limitado. Proposi\u00e7\u00e3o: Uma fun\u00e7\u00e3o uniformemente cont\u00ednua mapeia conjuntos totalmente limitados em conjuntos totalmente limitados.","title":"Conjuntos totalmente limitados"},{"location":"functional_analysis/compactness/#compacidade_1","text":"Um conjunto A \u00e9 compacto se para toda cobertura aberta, existe uma subcobertura finita. Para mostrar que um conjunto n\u00e3o \u00e9 compacto, basta selecionar uma cobertura aberta que n\u00e3o possua cobertura finita. \ud83d\udcdd Conjunto n\u00e3o compacto O conjunto I = (0,1) n\u00e3o \u00e9 compacto em \\mathbb{R} . Tome a cobertura \\{I_n\\} com I_n = (1/n, 1) . De fato, se x \\in (0,1) , para n > x^{-1} vale que x \\in (1/n, 1) , o que mostra que \\{I_n\\} \u00e9 de fato uma cobertura aberta. Observe que I_{n} \\subseteq I_{n+1} . Com isso, \\cup_{i=1}^k I_{n_i} = (1/\\max_{i=1,\\dots,k} n_i, 1) , que n\u00e3o pode cobrir (0,1) . Teorema: Seja (X,d) um espa\u00e7o m\u00e9trico. Se A \u00e9 conjunto compacto, ent\u00e3o A \u00e9 fechado e limitado. Ideias da demonstra\u00e7\u00e3o: (1) A \u00e9 fechado: tome y \\in A^c e mostra-se que existe B aberto contendo y com A \\cap B = \\emptyset , isto \u00e9, y \\not \\in \\bar{A} . Isso mostra que A = \\bar{A} . Para definir B , para cada x \\in A , sabemos que existe \\epsilon_x de forma que B_{\\epsilon_x}(x) \\cap B_{\\epsilon_x}(y) = \\emptyset . Al\u00e9m disso, \\{B_{\\epsilon_x}(x)\\} define uma cobertura aberta de A que possui subcobertura finita \\{B_{\\epsilon_{x_i}}(x_i)\\}_{i=1}^n . Defina B = \\cap_{i=1}^n B_{\\epsilon_{x_i}}(x_i) . (2) A \u00e9 limitado: Note que A \\subseteq \\cup_{x \\in A} B_1(x) e, portanto, existe subcobertura finita \\{B_1(x_i)\\}_{i=1}^n . Defina a = \\max d(x_i, x_j) . Se A fosse ilimitado, existiriam x,y \\in A de forma que d(x,y) > a + 2 . Por\u00e9m, x \\in B_1(x_i) e y \\in B_1(x_j) para alguns i, j . Portanto, a + 2 < d(x,y) \\le d(x, x_i) + d(x_i, x_j) + d(x_j, y) < a + 2, um absurdo, que indica que A \u00e9 limitado. Proposi\u00e7\u00e3o: Fun\u00e7\u00f5es cont\u00ednuas mapeiam conjuntos compactos em compactos. Com isso, a imagem de conjuntos compactos tem m\u00ednimo e m\u00e1ximo. \ud83d\udcdd Intervalo fechado e limitado O conjunto A = [a,b] \u00e9 compacto em \\mathbb{R} . Seja \\{A_i\\} uma cobertura aberta de A . Suponha que n\u00e3o exista subcobertura finita. Portanto, [a,(a+b)/2] ou [(a+b)/2, b] n\u00e3o admite subcobertura finita: chame de [a_1, b_1] . Mesmo assim, [a_1,(a_1+b_1)/2] ou [(a_1+b_1)/2, b_1] n\u00e3o admite subcobertura finita: chame de [a_2, b_2] . Assim, estamos definido um sequ\u00eancia de intervalos fechados encaixados [a_n, b_n] com d([a_n, b_n]) \\to 0 . As sequ\u00eancias \\{a_n\\} e \\{b_n\\} s\u00e3o sequ\u00eancias mon\u00f3tonas limitadas e, portanto convergentes, com \\lim a_n = \\lim b_n = x , pois d(a_n, b_n) \\to 0 . \u00c9 claro que x \\in A , pois A \u00e9 fechado. Nesse caso, x \\in A_i para algum i e, por ser aberto, x \\in B_{r}(x) \\subseteq A_i . Para n suficientemente grande, a_n, b_n \\in B_r(x) , o que implica que [a_n, b_n] \\subseteq B_r(x) , o que contradiz o fato de ser uma sequ\u00eancia que n\u00e3o pode ser coberta de forma finita. Essa contradi\u00e7\u00e3o mostra que existe subcobertura finita e, portanto, [a,b] \u00e9 fechado.","title":"Compacidade"},{"location":"functional_analysis/compactness/#compacidade-relativa-e-epsilon-net","text":"Um conjunto \u00e9 relativamente compacto se \\bar{A} \u00e9 compacto. Um exemplo \u00e9 I=(0,1) , que n\u00e3o \u00e9 compacto, mas [0,1] \u00e9. Tamb\u00e9m \u00e9 f\u00e1cil ver que um conjunto compacto \u00e9 relativamente compacto, dado que \u00e9 necessariamente fechado e \\bar{A} = A . Um conjunto de pontos N \u00e9 uma \\epsilon -net com respeito a um conjunto A se, para todo x \\in A , existe y \\in N tal que d(x,y) < \\epsilon . A ideia de uma \\epsilon -net \u00e9 que um conjunto de pontos est\u00e3o \\epsilon pr\u00f3ximos de qualquer ponto de A . Teorema: Seja A um subconjunto em um espa\u00e7o m\u00e9trico. Se para toda sequ\u00eancia de pontos de A , existe uma subsequ\u00eancia convergente, ent\u00e3o A \u00e9 totalmente limitado.","title":"Compacidade relativa e \\epsilon-net"},{"location":"functional_analysis/compactness/#compacidade-contavel-e-sequencial","text":"Um conjunto A \u00e9 compacto cont\u00e1vel se todo subconjunto infinito de A tem um ponto limite em A . Teorema: Compacidade implica compacidade cont\u00e1vel. A ideia da prova \u00e9 mostrar que um conjunto n\u00e3o compacto cont\u00e1vel, tamb\u00e9m n\u00e3o \u00e9 compacto. Para isso, deve existir M = \\{x_1, x_2, \\dots\\} \\subseteq A infinito enumer\u00e1vel que n\u00e3o tenha pontos limites de A . Mas com isso, podemos fazer uma sequ\u00eancia de conjuntos abertos de forma que E_n \\cap M = \\{x_n\\} . Se x \\in A / M , ent\u00e3o existe um aberto E(x) com E(x) \\cap M = \\emptyset . A uni\u00e3o dos conjuntos E(x) para x \\in A / M n\u00e3o cont\u00e9m pontos de M. Al\u00e9m disso, todo ponto de E_n s\u00f3 cont\u00e9m um ponto de M , mas A \\subseteq \\cup_{n\\in\\mathbb{N}} E_n \\cup \\cup_{x \\in A/M} E(x), mas \u00e9 imposs\u00edvel existir subcobertura finita e, portanto, A n\u00e3o \u00e9 compacto, como quer\u00edamos verificar. Um conjunto A \u00e9 compacto sequencialmente se, para toda sequ\u00eancia em A , existe uma subsequ\u00eancia convergente com limite em A . Teorema: Compacidade e compacidade sequencial s\u00e3o equivalentes em espa\u00e7os m\u00e9tricos. Que compacidade implica compacidade sequencial, prova-se que compacidade cont\u00e1vel implica sequencial. N\u00e3o \u00e9 complicado, todavia, porque a partir de uma sequ\u00eancia em A , usamos que o conjunto desses pontos (quando infinito) tem ponto limite em A . Justamente, isso implica que existe subsequ\u00eancia convergente em A . A volta \u00e9 um pouco mais complicada e parte de uma cobertura aberta \\{G_{\\alpha}\\} e define-se \\delta_0 = \\inf\\{\\sup \\{a : \\exists \\alpha; B_a(x) \\subseteq G_{\\alpha}\\} | x \\in A\\}, que est\u00e1 bem definido, pois G_{\\alpha} \u00e9 aberto e cobre A . Toma-se uma sequ\u00eancia \\delta(x_n) \\to \\delta_0 para se provar que \\delta_0 > 0 . Para isso, usamos o fato que a sequ\u00eancia \\{x_n\\} tem subsequ\u00eancia convergente com limite em A , isto \u00e9, x_{n_k} \\to x_0 \\in A . Em particular, prova-se que \\delta_0 \\ge \\delta(x_0)/4 > 0 com um pouco de \u00e1lgebra. Como toda sequ\u00eancia tem subsequ\u00eancia convergente, vale que o conjunto \u00e9 totalmente limitado. Da\u00ed para \\delta_0 > \\epsilon > 0 , existe \\epsilon -net finita com centros y_1, \\dots, y_n \\in A . Isso implica que B_{\\epsilon}(y_i) \\subseteq G_i e, portanto, \\{G_1, \\dots, G_n\\} formam uma subcobertura finita. Teorema: Em um espa\u00e7o m\u00e9trico, compacidade relativa implica totalmente limitada. Se ele for completo, totalmente limitada implica compacidade relativa.","title":"Compacidade cont\u00e1vel e sequencial"},{"location":"functional_analysis/compactness/#observacoes-finais","text":"Fun\u00e7\u00f5es cont\u00ednuas preservam compacidade; Fun\u00e7\u00f5es uniformemente cont\u00ednuas preservam totalmente limitados; Fun\u00e7\u00f5es Lipschitz cont\u00ednuas preservam limitados. Compacidade, compacidade cont\u00e1vel e compacidade sequencial s\u00e3o conceitos equivalentes em espa\u00e7os m\u00e9tricos. Se A \u00e9 compacto, ent\u00e3o A \u00e9 fechado e totalmente limitado. Se o espa\u00e7o m\u00e9trico \u00e9 completo, ent\u00e3o A ser fechado e totalmente limitado implica que A \u00e9 compacto.","title":"Observa\u00e7\u00f5es finais:"},{"location":"functional_analysis/hahn_banach/","text":"Teorema de Hahn Banach Queremos generalizar um pouco mais a ideia de norma para um funcional convexo . Ser\u00e1 f\u00e1cil verificar que toda norma \u00e9 um funcional convexo e, portanto, resultados para esses funcionais ser\u00e3o v\u00e1lidos para normas em geral. Um funcional linear \u00e9 um mapa f : X \\to \\mathbb{R} que \u00e9 uma transforma\u00e7\u00e3o linear. Um funcional convexo \u00e9 um mapa p : X \\to \\mathbb{R} que satisfaz (I) p(x) \\ge 0 para todo x \\in X ; (II) p(x+y) \\le p(x) + p(y) para todo x,y\\in X ; (II) p(\\alpha x) = \\alpha p(x) para todo x \\in X , \\alpha \\ge 0 . Se o item (I) n\u00e3o \u00e9 satisfeito, chamamos p de funcional sublinear . Lema: Seja M subespa\u00e7o pr\u00f3prio de X e x_0 \\in M^c . Defina N = [M \\cup \\{x_0\\}] , isto \u00e9, o menor subespa\u00e7o que cont\u00e9m M \\cup \\{x_0\\} . Suponha que f \u00e9 um funcional linear em M , p \u00e9 um funcional sublinear em X e f(x) \\le p(x) para todo x \\in M . Assim, existe um funcional linear F em N que estende f e satisfaz F(x) \\le p(x) para todo x \\in N . Em resumo, ao incluir uma dimens\u00e3o em nosso subespa\u00e7o, conseguimos estender um funcional linear que ainda \u00e9 limitado por um sublinear. Ideia da prova: Note que para y_1, y_2 \\in M , tem-se que -p(-y_2 -x_0) - f(y_2) \\le p(y_1+x_0) - f(y_1), o que implica que existe c_0 entre o supremo do lado esquerdo em y_2 e o \u00ednfimo do lado direito em y_1 . Qualquer elemento de N \u00e9 unicamente escrito como x = y + \\alpha x_0. Assim, podemos definir F(y + \\alpha x_0) = f(y) + \\alpha c_0. \u00c9 claro que F \u00e9 linear e que F estende f . Falta verificar se \\alpha \\neq 0 , ent\u00e3o F(x) \\le p(x) . Isso \u00e9 verific\u00e1vel usando a desigualdade em (1). A partir desse lema, podemos obter o seguinte famoso resultado. Teorema (Hahn-Banach): Seja M um subespa\u00e7o de X , p um sublinear e f um funcional linear definido em M , tal que f(x) \\le p(x) para todo x \\in M . Assim, existe um funcional linear F que estende f em X e satisfaz F(x) \\le p(x) para todo x \\in X . Ideia da prova: Defina S a classe de todos os funcionais lineares que estendem x e que satisfazem f(x) \\le p(x) para x no seu dom\u00ednio D_f . \u00c9 claro que S n\u00e3o \u00e9 vazio, j\u00e1 que f \\in S para D_f = M . Defina a seguinte ordem: f_1 < f_2 \\in S se f_2 estende f_1 , que induz uma ordena\u00e7\u00e3o parcial em S . Precisamos provar que S \u00e9 indutivamente ordenado, isto \u00e9, todo subconjunto de S totalmente ordenado tem um limite superior. A partir de um conjunto totalmente ordenado \\{f_{\\alpha}\\} , basta considerarmos uma fun\u00e7\u00e3o cujo dom\u00ednio seja \\cup_{\\alpha} D_{f_{\\alpha}} . Nesse caso, define-se que f(x) = f_{\\alpha}(x) quando x \\in D_{f_{\\alpha}} . Precisamos verificar que essa uni\u00e3o \u00e9 um subespa\u00e7o e que f \u00e9 bem definida, mas isso \u00e9 consequ\u00eancia do conjunto ser totalmente ordenado. O Lema de Zorn implica a exist\u00eancia de um elemento maximal F de S . Falta provar que D_F = X , mas isso \u00e9 resultado do lema anterior, pois caso n\u00e3o fosse, poder\u00edamos estender esse funcional, caindo em contradi\u00e7\u00e3o. Funcionais lineares limitados Em espa\u00e7os normados, o conceito de continuidade para funcionais lineares \u00e9 mais simples. Se f \u00e9 um funcional linear cont\u00ednuo em x \\in X , ent\u00e3o f ser\u00e1 cont\u00ednuo em todo ponto do espa\u00e7o. Mais do que isso, continuidade e limita\u00e7\u00e3o de f s\u00e3o equivalentes em espa\u00e7os normados. Um funcional linear f em um espa\u00e7o normado \u00e9 limitado se existe uma constante k tal que f(x) \\le k\\|x\\| para todo x \\in X . Teorema: Seja X um espa\u00e7o normado e f um funcional linear. Ent\u00e3o f \u00e9 cont\u00ednuo se, e s\u00f3 se, f \u00e9 limitado. Espa\u00e7o conjugado Em um espa\u00e7o normado, se f \u00e9 um funcional linear limitado, defina \\|f\\| = \\sup_{x \\neq 0} \\frac{|f(x)|}{\\|x\\|} = \\sup_{\\|x\\|=1} |f(x)|. O espa\u00e7o de todos os funcionais limitados sobre X com essa norma \u00e9 chamado de espa\u00e7o conjugado \\tilde{X} . Podemos demonstrar que \\tilde{X} \u00e9 espa\u00e7o de Banach. Teorema: Se X tem dimens\u00e3o finita, todos os funcionais lineares s\u00e3o limitados (logo, cont\u00ednuos), isto \u00e9, \\tilde{X} cont\u00e9m todos os funcionais lineares. Consequ\u00eancias do teorema de Hahn-Banach Teorema da Representa\u00e7\u00e3o de Riesz","title":"Teorema de Hahn Banach"},{"location":"functional_analysis/hahn_banach/#teorema-de-hahn-banach","text":"Queremos generalizar um pouco mais a ideia de norma para um funcional convexo . Ser\u00e1 f\u00e1cil verificar que toda norma \u00e9 um funcional convexo e, portanto, resultados para esses funcionais ser\u00e3o v\u00e1lidos para normas em geral. Um funcional linear \u00e9 um mapa f : X \\to \\mathbb{R} que \u00e9 uma transforma\u00e7\u00e3o linear. Um funcional convexo \u00e9 um mapa p : X \\to \\mathbb{R} que satisfaz (I) p(x) \\ge 0 para todo x \\in X ; (II) p(x+y) \\le p(x) + p(y) para todo x,y\\in X ; (II) p(\\alpha x) = \\alpha p(x) para todo x \\in X , \\alpha \\ge 0 . Se o item (I) n\u00e3o \u00e9 satisfeito, chamamos p de funcional sublinear . Lema: Seja M subespa\u00e7o pr\u00f3prio de X e x_0 \\in M^c . Defina N = [M \\cup \\{x_0\\}] , isto \u00e9, o menor subespa\u00e7o que cont\u00e9m M \\cup \\{x_0\\} . Suponha que f \u00e9 um funcional linear em M , p \u00e9 um funcional sublinear em X e f(x) \\le p(x) para todo x \\in M . Assim, existe um funcional linear F em N que estende f e satisfaz F(x) \\le p(x) para todo x \\in N . Em resumo, ao incluir uma dimens\u00e3o em nosso subespa\u00e7o, conseguimos estender um funcional linear que ainda \u00e9 limitado por um sublinear. Ideia da prova: Note que para y_1, y_2 \\in M , tem-se que -p(-y_2 -x_0) - f(y_2) \\le p(y_1+x_0) - f(y_1), o que implica que existe c_0 entre o supremo do lado esquerdo em y_2 e o \u00ednfimo do lado direito em y_1 . Qualquer elemento de N \u00e9 unicamente escrito como x = y + \\alpha x_0. Assim, podemos definir F(y + \\alpha x_0) = f(y) + \\alpha c_0. \u00c9 claro que F \u00e9 linear e que F estende f . Falta verificar se \\alpha \\neq 0 , ent\u00e3o F(x) \\le p(x) . Isso \u00e9 verific\u00e1vel usando a desigualdade em (1). A partir desse lema, podemos obter o seguinte famoso resultado. Teorema (Hahn-Banach): Seja M um subespa\u00e7o de X , p um sublinear e f um funcional linear definido em M , tal que f(x) \\le p(x) para todo x \\in M . Assim, existe um funcional linear F que estende f em X e satisfaz F(x) \\le p(x) para todo x \\in X . Ideia da prova: Defina S a classe de todos os funcionais lineares que estendem x e que satisfazem f(x) \\le p(x) para x no seu dom\u00ednio D_f . \u00c9 claro que S n\u00e3o \u00e9 vazio, j\u00e1 que f \\in S para D_f = M . Defina a seguinte ordem: f_1 < f_2 \\in S se f_2 estende f_1 , que induz uma ordena\u00e7\u00e3o parcial em S . Precisamos provar que S \u00e9 indutivamente ordenado, isto \u00e9, todo subconjunto de S totalmente ordenado tem um limite superior. A partir de um conjunto totalmente ordenado \\{f_{\\alpha}\\} , basta considerarmos uma fun\u00e7\u00e3o cujo dom\u00ednio seja \\cup_{\\alpha} D_{f_{\\alpha}} . Nesse caso, define-se que f(x) = f_{\\alpha}(x) quando x \\in D_{f_{\\alpha}} . Precisamos verificar que essa uni\u00e3o \u00e9 um subespa\u00e7o e que f \u00e9 bem definida, mas isso \u00e9 consequ\u00eancia do conjunto ser totalmente ordenado. O Lema de Zorn implica a exist\u00eancia de um elemento maximal F de S . Falta provar que D_F = X , mas isso \u00e9 resultado do lema anterior, pois caso n\u00e3o fosse, poder\u00edamos estender esse funcional, caindo em contradi\u00e7\u00e3o.","title":"Teorema de Hahn Banach"},{"location":"functional_analysis/hahn_banach/#funcionais-lineares-limitados","text":"Em espa\u00e7os normados, o conceito de continuidade para funcionais lineares \u00e9 mais simples. Se f \u00e9 um funcional linear cont\u00ednuo em x \\in X , ent\u00e3o f ser\u00e1 cont\u00ednuo em todo ponto do espa\u00e7o. Mais do que isso, continuidade e limita\u00e7\u00e3o de f s\u00e3o equivalentes em espa\u00e7os normados. Um funcional linear f em um espa\u00e7o normado \u00e9 limitado se existe uma constante k tal que f(x) \\le k\\|x\\| para todo x \\in X . Teorema: Seja X um espa\u00e7o normado e f um funcional linear. Ent\u00e3o f \u00e9 cont\u00ednuo se, e s\u00f3 se, f \u00e9 limitado.","title":"Funcionais lineares limitados"},{"location":"functional_analysis/hahn_banach/#espaco-conjugado","text":"Em um espa\u00e7o normado, se f \u00e9 um funcional linear limitado, defina \\|f\\| = \\sup_{x \\neq 0} \\frac{|f(x)|}{\\|x\\|} = \\sup_{\\|x\\|=1} |f(x)|. O espa\u00e7o de todos os funcionais limitados sobre X com essa norma \u00e9 chamado de espa\u00e7o conjugado \\tilde{X} . Podemos demonstrar que \\tilde{X} \u00e9 espa\u00e7o de Banach. Teorema: Se X tem dimens\u00e3o finita, todos os funcionais lineares s\u00e3o limitados (logo, cont\u00ednuos), isto \u00e9, \\tilde{X} cont\u00e9m todos os funcionais lineares.","title":"Espa\u00e7o conjugado"},{"location":"functional_analysis/hahn_banach/#consequencias-do-teorema-de-hahn-banach","text":"","title":"Consequ\u00eancias do teorema de Hahn-Banach"},{"location":"functional_analysis/hahn_banach/#teorema-da-representacao-de-riesz","text":"","title":"Teorema da Representa\u00e7\u00e3o de Riesz"},{"location":"functional_analysis/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de An\u00e1lise Funcional: Fundamentos (Doutorado) correspondente ao per\u00edodo de 2022.4. Hor\u00e1rio: Quintas-feiras, 14h. Link 14h-15h , Link 15h-16h T\u00f3picos Os t\u00f3picos cobertos na disciplina est\u00e3o separados em tr\u00eas partes: Parte 1 Espa\u00e7os m\u00e9tricos Isometrias e completamento Compacidade em espa\u00e7os m\u00e9tricos Teorema de Arzel\u00e0-Ascoli Parte 2 Espa\u00e7os normados Teorema de Hahn-Banach Exerc\u00edcios Os exerc\u00edcios resolvidos est\u00e3o dispon\u00edveis: Lista T\u00f3picos \u00daltima atualiza\u00e7\u00e3o Para entregar Solu\u00e7\u00e3o 1 Espa\u00e7os m\u00e9tricos 13/07/2022 4,8,27,29,30 1 2 Espa\u00e7os normados 12/08/2022 None 2 Notas Monitoria Itens discutidos Arquivo 07/07/2022 Conceitos da lista 1 ver arquivo 26/07/2022 Arzel\u00e0-Ascoli, lista 1 e espa\u00e7o l^{\\infty} ver arquivo Documentos Adicionais","title":"An\u00e1lise Funcional"},{"location":"functional_analysis/info/#informacoes-gerais","text":"Monitoria de An\u00e1lise Funcional: Fundamentos (Doutorado) correspondente ao per\u00edodo de 2022.4. Hor\u00e1rio: Quintas-feiras, 14h. Link 14h-15h , Link 15h-16h","title":"Informa\u00e7\u00f5es Gerais"},{"location":"functional_analysis/info/#topicos","text":"Os t\u00f3picos cobertos na disciplina est\u00e3o separados em tr\u00eas partes: Parte 1 Espa\u00e7os m\u00e9tricos Isometrias e completamento Compacidade em espa\u00e7os m\u00e9tricos Teorema de Arzel\u00e0-Ascoli Parte 2 Espa\u00e7os normados Teorema de Hahn-Banach","title":"T\u00f3picos"},{"location":"functional_analysis/info/#exercicios","text":"Os exerc\u00edcios resolvidos est\u00e3o dispon\u00edveis: Lista T\u00f3picos \u00daltima atualiza\u00e7\u00e3o Para entregar Solu\u00e7\u00e3o 1 Espa\u00e7os m\u00e9tricos 13/07/2022 4,8,27,29,30 1 2 Espa\u00e7os normados 12/08/2022 None 2","title":"Exerc\u00edcios"},{"location":"functional_analysis/info/#notas","text":"Monitoria Itens discutidos Arquivo 07/07/2022 Conceitos da lista 1 ver arquivo 26/07/2022 Arzel\u00e0-Ascoli, lista 1 e espa\u00e7o l^{\\infty} ver arquivo","title":"Notas"},{"location":"functional_analysis/info/#documentos-adicionais","text":"","title":"Documentos Adicionais"},{"location":"functional_analysis/isometries/","text":"Isometrias e Homeomorfismos Um conceito importante sobre espa\u00e7os m\u00e9tricos \u00e9 a ideia de isometria , isto \u00e9, fun\u00e7\u00f5es que mant\u00e9m propriedades relacionadas \u00e0 m\u00e9trica do espa\u00e7o (iso = igual, metria = m\u00e9trica). Sejam (X,d) e (Y,d') dois espa\u00e7os m\u00e9tricos e f:X \\to Y uma fun\u00e7\u00e3o bijetiva. Dizemos que f \u00e9 um homeomorfismo se f e f^{-1} s\u00e3o cont\u00ednuas. Al\u00e9m do mais, se existe um homeomorfismo entre X e Y , dizemos que X e Y s\u00e3o homeomorfos . Um homeomorfismo preserva conjuntos abertos e fechados, al\u00e9m de outras propriedades, como o conceito de ponto limite, isto \u00e9, se x \\in A' , ent\u00e3o f(x) \\in f(A)' . Se f:X \\to Y \u00e9 bijetiva e para quaisquer x,y \\in X , vale que d(x,y) = d'(f(x), f(y)), ent\u00e3o f \u00e9 dita isometria . Se existe uma isometria entre X e Y , eles s\u00e3o ditos isom\u00e9tricos . Toda isometria \u00e9 um homeomorfismo, pois se d(x_n, x) \\to 0 , ent\u00e3o d'(f(x_n), f(x)) \\to 0 . Em contrapartida, se d'(f(x_n), f(x)) \\to 0 , \u00e9 claro que d(f^{-1}(f(x_n)), f^{-1}(f(x))) = d(x_n, x) \\to 0 . Sequ\u00eancia de Cauchy Considere a sequ\u00eancia \\{x_n\\} no espa\u00e7o m\u00e9trico (X,d) . Ela \u00e9 dita de Cauchy se para todo \\epsilon > 0 , existe N \\in \\mathbb{N} tal que para todo n, m> N , vale que d(x_n, x_m) < \\epsilon . Intuitivamente, a dist\u00e2ncia entre um elemento da sequ\u00eancia x_m e seus subsequentes vai assintoticamente diminuindo. Observe que isso n\u00e3o garante converg\u00eancia de forma geral, mas toda sequ\u00eancia convergente \u00e9 de Cauchy, visto que d(x_n, x_m) < d(x_n, x) + d(x, x_m), quando x \u00e9 o limite dessa sequ\u00eancia. Dizemos que um espa\u00e7o \u00e9 completo se toda sequ\u00eancia de Cauchy \u00e9 convergente. \ud83d\udcdd Completude dos n\u00fameros reais O espa\u00e7o X=\\mathbb{R}^n com a m\u00e9trica d_2(x,y) = \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} \u00e9 completo. Em particular, os reais formam um espa\u00e7o completo com a dist\u00e2ncia dada pelo valor absoluto da diferen\u00e7a. \ud83d\udcdd M\u00e9trica trivial Qualquer conjunto dotado da m\u00e9trica trivial forma um espa\u00e7o completo, pois a sequ\u00eancia s\u00f3 ser\u00e1 de Cauchy se existir N \\in \\mathbb{N} tal que n > N \\implies x_n = x_N . \ud83d\udcdd Os n\u00fameros racionais Considere X= \\mathbb{Q} e d(x,y) = |x-y| . \u00c9 f\u00e1cil ver que (\\mathbb{Q}, d) formam um espa\u00e7o m\u00e9trico n\u00e3o completo. Para isso, considere a sequ\u00eancia x_{n+1} = \\frac{x_n}{2} + \\frac{1}{x_n}, x_1 = 2, que est\u00e1 definida em X , pois soma e divis\u00e3o s\u00e3o opera\u00e7\u00f5es fechadas nos racionais. Note que \\frac{x_{n+1}}{x_n} = \\frac{1}{2} + \\frac{1}{x_n^2} \\le 1 \\iff x_n^2 \\ge 2. Note que 0 \\le (x_{n+1} - x_n)^2 = x_{n+1}^2 + x_n^2 - 2x_{n+1}x_n = x_{n+1}^2 + x_n^2 - x_{n}^2 -2 = x_{n+1}^2 - 2, o que implica que x_n^2 \\ge 1 para todo n e, x_{n+1} \\le x_n pela desigualdade inicial. Com isso, temos uma sequ\u00eancia decrescente limitada e, portanto, convergente em \\mathbb{R} e, portanto de Cauchy em \\mathbb{R} e, por consequ\u00eancia, de Cauchy em X . Seja L o limite. Assim, L = L/2 + 1/L \\implies 2L^2 = L^2 +2 \\implies L = \\sqrt{2}. Como L \\not \\in X , ent\u00e3o a sequ\u00eancia n\u00e3o converge em X e X n\u00e3o \u00e9 completo. Proposi\u00e7\u00e3o: Duas sequ\u00eancias s\u00e3o assint\u00f3ticas quando d(x_n, y_n) \\to 0 . Isso cria uma rela\u00e7\u00e3o de equival\u00eancia no espa\u00e7o das sequ\u00eancias. Al\u00e9m do mais, se \\{x_n\\} \u00e9 Cauchy /converge para x , \\{y_n\\} tamb\u00e9m ser\u00e1 / converge. Completamento Seja (X,d) um espa\u00e7o m\u00e9trico. O espa\u00e7o m\u00e9trico completo (X^*, d^*) \u00e9 um completamento de (X,d) se (X,d) \u00e9 isom\u00e9trico a um subespa\u00e7o (X_0, d^*) denso em (X^*, d^*) , isto \u00e9, que satisfaz, \\bar{X}_0 = X^* . Nesse caso, todo ponto de X^* \u00e9 ponto de ader\u00eancia de X_0 , que \u00e9 equivalente a X no sentido de preservar a m\u00e9trica. Teorema: Todo espa\u00e7o m\u00e9trico (X,d) tem um completamento (X^*, d^*) e, al\u00e9m do mais, se (X^{**}, d^{**}) \u00e9 um completamento de (X,d) , ent\u00e3o (X^*, d^*) \u00e9 isom\u00e9trico a (X^{**}, d^{**}) . \ud83d\udcdd Os n\u00fameros racionais (continua\u00e7\u00e3o) Vimos que \\{\\mathbb{Q}, d\\} \u00e9 um espa\u00e7o m\u00e9trico n\u00e3o completo para d(x,y) = |x,y| . Pelo Teorema acima, existe um espa\u00e7o m\u00e9trico (\\mathbb{Q}^*, d^*\\} completo de forma que \\mathbb{Q} \u00e9 isom\u00e9trico a um subconjunto denso de \\mathbb{Q}^* . Al\u00e9m disso, sabemos que ele \u00e9 \u00fanico a menos de uma isometria. Essa \u00e9 uma forma de construir os n\u00fameros reais: o completamento dos n\u00fameros racionais. Para isso, basta definir um n\u00famero real como a classe de equival\u00eancia das sequ\u00eancias de Cauchy nos racionais com a rela\u00e7\u00e3o de duas sequ\u00eancias estarem na mesma classe se s\u00e3o assint\u00f3ticas. Mais detalhes dessa constru\u00e7\u00e3o, consulte o Exerc\u00edcio 31 da lista . Proposi\u00e7\u00e3o: Seja A \\subseteq X em que (X,d) seja um espa\u00e7o m\u00e9trico completo. Ent\u00e3o (A,d) \u00e9 um espa\u00e7o m\u00e9trico com (\\bar{A}, d) sendo seu completamento. \ud83d\udcdd Homeomorfismo, n\u00e3o isometria! Note que \\mathbb{R} \u00e9 homeomorfo a (0,1) atrav\u00e9s ta transforma\u00e7\u00e3o f(x) = \\frac{1}{\\pi}tan^{-1}(x) + 1/2 . Todavia, um \u00e9 completo, enquanto o outro n\u00e3o \u00e9. Teorema de Baire Seja (X,d) um espa\u00e7o m\u00e9trico. Um subconjunto S \\subseteq X \u00e9 dito denso em lugar algum se S n\u00e3o \u00e9 denso em nenhum subconjunto aberto n\u00e3o vazio U \\subseteq X . Teorema: Um espa\u00e7o m\u00e9trico completo n\u00e3o pode ser coberto por um n\u00famero enumer\u00e1vel de conjuntos densos em lugar algum. Espa\u00e7os Separ\u00e1veis Temos os seguintes tipos de espa\u00e7os m\u00e9tricos, cada um \"maior\" do que o anterior. Espa\u00e7o m\u00e9trico finito: existe um n\u00famero finito de dist\u00e2ncia para calcular entre os pontos do espa\u00e7o. Espa\u00e7o m\u00e9trico enumer\u00e1vel: com um n\u00famero infinito de pontos, um algoritmo pode calcular dist\u00e2ncias precisamente, mas isso pode tomar tempo. Espa\u00e7o m\u00e9trico separ\u00e1vel: pontos podem ser aproximados por um de um n\u00famero cont\u00e1vel de pontos. Qualquer dist\u00e2ncia pode ser calculada de forma aproximada. Espa\u00e7o m\u00e9trico n\u00e3o separ\u00e1vel: Pode n\u00e3o existir algoritmo para calcular dist\u00e2ncia entre pontos gen\u00e9ricos. Um espa\u00e7o m\u00e9trico (X,d) \u00e9 separ\u00e1vel quando ele cont\u00e9m um subconjunto denso enumer\u00e1vel. Espa\u00e7os m\u00e9tricos enumer\u00e1veis s\u00e3o separ\u00e1veis por defini\u00e7\u00e3o. Temos que \\mathbb{R} \u00e9 separ\u00e1vel, pois \\mathbb{Q} \u00e9 denso em \\mathbb{R} . Proposi\u00e7\u00e3o: O produto de dois espa\u00e7os separ\u00e1veis \u00e9 separ\u00e1vel.","title":"Isometrias e Homeomorfismos"},{"location":"functional_analysis/isometries/#isometrias-e-homeomorfismos","text":"Um conceito importante sobre espa\u00e7os m\u00e9tricos \u00e9 a ideia de isometria , isto \u00e9, fun\u00e7\u00f5es que mant\u00e9m propriedades relacionadas \u00e0 m\u00e9trica do espa\u00e7o (iso = igual, metria = m\u00e9trica). Sejam (X,d) e (Y,d') dois espa\u00e7os m\u00e9tricos e f:X \\to Y uma fun\u00e7\u00e3o bijetiva. Dizemos que f \u00e9 um homeomorfismo se f e f^{-1} s\u00e3o cont\u00ednuas. Al\u00e9m do mais, se existe um homeomorfismo entre X e Y , dizemos que X e Y s\u00e3o homeomorfos . Um homeomorfismo preserva conjuntos abertos e fechados, al\u00e9m de outras propriedades, como o conceito de ponto limite, isto \u00e9, se x \\in A' , ent\u00e3o f(x) \\in f(A)' . Se f:X \\to Y \u00e9 bijetiva e para quaisquer x,y \\in X , vale que d(x,y) = d'(f(x), f(y)), ent\u00e3o f \u00e9 dita isometria . Se existe uma isometria entre X e Y , eles s\u00e3o ditos isom\u00e9tricos . Toda isometria \u00e9 um homeomorfismo, pois se d(x_n, x) \\to 0 , ent\u00e3o d'(f(x_n), f(x)) \\to 0 . Em contrapartida, se d'(f(x_n), f(x)) \\to 0 , \u00e9 claro que d(f^{-1}(f(x_n)), f^{-1}(f(x))) = d(x_n, x) \\to 0 .","title":"Isometrias e Homeomorfismos"},{"location":"functional_analysis/isometries/#sequencia-de-cauchy","text":"Considere a sequ\u00eancia \\{x_n\\} no espa\u00e7o m\u00e9trico (X,d) . Ela \u00e9 dita de Cauchy se para todo \\epsilon > 0 , existe N \\in \\mathbb{N} tal que para todo n, m> N , vale que d(x_n, x_m) < \\epsilon . Intuitivamente, a dist\u00e2ncia entre um elemento da sequ\u00eancia x_m e seus subsequentes vai assintoticamente diminuindo. Observe que isso n\u00e3o garante converg\u00eancia de forma geral, mas toda sequ\u00eancia convergente \u00e9 de Cauchy, visto que d(x_n, x_m) < d(x_n, x) + d(x, x_m), quando x \u00e9 o limite dessa sequ\u00eancia. Dizemos que um espa\u00e7o \u00e9 completo se toda sequ\u00eancia de Cauchy \u00e9 convergente. \ud83d\udcdd Completude dos n\u00fameros reais O espa\u00e7o X=\\mathbb{R}^n com a m\u00e9trica d_2(x,y) = \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} \u00e9 completo. Em particular, os reais formam um espa\u00e7o completo com a dist\u00e2ncia dada pelo valor absoluto da diferen\u00e7a. \ud83d\udcdd M\u00e9trica trivial Qualquer conjunto dotado da m\u00e9trica trivial forma um espa\u00e7o completo, pois a sequ\u00eancia s\u00f3 ser\u00e1 de Cauchy se existir N \\in \\mathbb{N} tal que n > N \\implies x_n = x_N . \ud83d\udcdd Os n\u00fameros racionais Considere X= \\mathbb{Q} e d(x,y) = |x-y| . \u00c9 f\u00e1cil ver que (\\mathbb{Q}, d) formam um espa\u00e7o m\u00e9trico n\u00e3o completo. Para isso, considere a sequ\u00eancia x_{n+1} = \\frac{x_n}{2} + \\frac{1}{x_n}, x_1 = 2, que est\u00e1 definida em X , pois soma e divis\u00e3o s\u00e3o opera\u00e7\u00f5es fechadas nos racionais. Note que \\frac{x_{n+1}}{x_n} = \\frac{1}{2} + \\frac{1}{x_n^2} \\le 1 \\iff x_n^2 \\ge 2. Note que 0 \\le (x_{n+1} - x_n)^2 = x_{n+1}^2 + x_n^2 - 2x_{n+1}x_n = x_{n+1}^2 + x_n^2 - x_{n}^2 -2 = x_{n+1}^2 - 2, o que implica que x_n^2 \\ge 1 para todo n e, x_{n+1} \\le x_n pela desigualdade inicial. Com isso, temos uma sequ\u00eancia decrescente limitada e, portanto, convergente em \\mathbb{R} e, portanto de Cauchy em \\mathbb{R} e, por consequ\u00eancia, de Cauchy em X . Seja L o limite. Assim, L = L/2 + 1/L \\implies 2L^2 = L^2 +2 \\implies L = \\sqrt{2}. Como L \\not \\in X , ent\u00e3o a sequ\u00eancia n\u00e3o converge em X e X n\u00e3o \u00e9 completo. Proposi\u00e7\u00e3o: Duas sequ\u00eancias s\u00e3o assint\u00f3ticas quando d(x_n, y_n) \\to 0 . Isso cria uma rela\u00e7\u00e3o de equival\u00eancia no espa\u00e7o das sequ\u00eancias. Al\u00e9m do mais, se \\{x_n\\} \u00e9 Cauchy /converge para x , \\{y_n\\} tamb\u00e9m ser\u00e1 / converge.","title":"Sequ\u00eancia de Cauchy"},{"location":"functional_analysis/isometries/#completamento","text":"Seja (X,d) um espa\u00e7o m\u00e9trico. O espa\u00e7o m\u00e9trico completo (X^*, d^*) \u00e9 um completamento de (X,d) se (X,d) \u00e9 isom\u00e9trico a um subespa\u00e7o (X_0, d^*) denso em (X^*, d^*) , isto \u00e9, que satisfaz, \\bar{X}_0 = X^* . Nesse caso, todo ponto de X^* \u00e9 ponto de ader\u00eancia de X_0 , que \u00e9 equivalente a X no sentido de preservar a m\u00e9trica. Teorema: Todo espa\u00e7o m\u00e9trico (X,d) tem um completamento (X^*, d^*) e, al\u00e9m do mais, se (X^{**}, d^{**}) \u00e9 um completamento de (X,d) , ent\u00e3o (X^*, d^*) \u00e9 isom\u00e9trico a (X^{**}, d^{**}) . \ud83d\udcdd Os n\u00fameros racionais (continua\u00e7\u00e3o) Vimos que \\{\\mathbb{Q}, d\\} \u00e9 um espa\u00e7o m\u00e9trico n\u00e3o completo para d(x,y) = |x,y| . Pelo Teorema acima, existe um espa\u00e7o m\u00e9trico (\\mathbb{Q}^*, d^*\\} completo de forma que \\mathbb{Q} \u00e9 isom\u00e9trico a um subconjunto denso de \\mathbb{Q}^* . Al\u00e9m disso, sabemos que ele \u00e9 \u00fanico a menos de uma isometria. Essa \u00e9 uma forma de construir os n\u00fameros reais: o completamento dos n\u00fameros racionais. Para isso, basta definir um n\u00famero real como a classe de equival\u00eancia das sequ\u00eancias de Cauchy nos racionais com a rela\u00e7\u00e3o de duas sequ\u00eancias estarem na mesma classe se s\u00e3o assint\u00f3ticas. Mais detalhes dessa constru\u00e7\u00e3o, consulte o Exerc\u00edcio 31 da lista . Proposi\u00e7\u00e3o: Seja A \\subseteq X em que (X,d) seja um espa\u00e7o m\u00e9trico completo. Ent\u00e3o (A,d) \u00e9 um espa\u00e7o m\u00e9trico com (\\bar{A}, d) sendo seu completamento. \ud83d\udcdd Homeomorfismo, n\u00e3o isometria! Note que \\mathbb{R} \u00e9 homeomorfo a (0,1) atrav\u00e9s ta transforma\u00e7\u00e3o f(x) = \\frac{1}{\\pi}tan^{-1}(x) + 1/2 . Todavia, um \u00e9 completo, enquanto o outro n\u00e3o \u00e9.","title":"Completamento"},{"location":"functional_analysis/isometries/#teorema-de-baire","text":"Seja (X,d) um espa\u00e7o m\u00e9trico. Um subconjunto S \\subseteq X \u00e9 dito denso em lugar algum se S n\u00e3o \u00e9 denso em nenhum subconjunto aberto n\u00e3o vazio U \\subseteq X . Teorema: Um espa\u00e7o m\u00e9trico completo n\u00e3o pode ser coberto por um n\u00famero enumer\u00e1vel de conjuntos densos em lugar algum.","title":"Teorema de Baire"},{"location":"functional_analysis/isometries/#espacos-separaveis","text":"Temos os seguintes tipos de espa\u00e7os m\u00e9tricos, cada um \"maior\" do que o anterior. Espa\u00e7o m\u00e9trico finito: existe um n\u00famero finito de dist\u00e2ncia para calcular entre os pontos do espa\u00e7o. Espa\u00e7o m\u00e9trico enumer\u00e1vel: com um n\u00famero infinito de pontos, um algoritmo pode calcular dist\u00e2ncias precisamente, mas isso pode tomar tempo. Espa\u00e7o m\u00e9trico separ\u00e1vel: pontos podem ser aproximados por um de um n\u00famero cont\u00e1vel de pontos. Qualquer dist\u00e2ncia pode ser calculada de forma aproximada. Espa\u00e7o m\u00e9trico n\u00e3o separ\u00e1vel: Pode n\u00e3o existir algoritmo para calcular dist\u00e2ncia entre pontos gen\u00e9ricos. Um espa\u00e7o m\u00e9trico (X,d) \u00e9 separ\u00e1vel quando ele cont\u00e9m um subconjunto denso enumer\u00e1vel. Espa\u00e7os m\u00e9tricos enumer\u00e1veis s\u00e3o separ\u00e1veis por defini\u00e7\u00e3o. Temos que \\mathbb{R} \u00e9 separ\u00e1vel, pois \\mathbb{Q} \u00e9 denso em \\mathbb{R} . Proposi\u00e7\u00e3o: O produto de dois espa\u00e7os separ\u00e1veis \u00e9 separ\u00e1vel.","title":"Espa\u00e7os Separ\u00e1veis"},{"location":"functional_analysis/metric_spaces/","text":"Espa\u00e7os m\u00e9tricos Queremos introduzir o conceito de dist\u00e2ncia entre pontos em um espa\u00e7o. Com isso bem definido, a no\u00e7\u00e3o de limite, fundamental para a \u00e1rea da An\u00e1lise (e talvez um dos pontos que mais a distancie da \u00c1lgebra), fica bem entendida, j\u00e1 que estamos dizendo que uma sequ\u00eancia de pontos se aproxima cada vez mais de um ponto do espa\u00e7o. A ideia de se aproximar se conecta naturalmente com a dist\u00e2ncia entre os pontos diminuir. Defini\u00e7\u00e3o: Seja X um conjunto qualquer e d : X \\times X \\to \\mathbb{R} uma fun\u00e7\u00e3o que satisfaz, para quaisquer x,y,z \\in X , as seguintes propriedades: (1) d(x,y) \\ge 0 e d(x,y) = 0 se, e somente se, x=y , isto \u00e9, a dist\u00e2ncia \u00e9 sempre positiva, a menos quando a medidos de um ponto a ele mesmo, o que naturalmente queremos que seja zero; (2) d(x,y) = d(y,x) , isto \u00e9, a dist\u00e2ncia entre dois pontos independe da ordem com que se inicia a medida; (3) d(x,y) \\le d(x,z) + d(z,y) , a desigualdade triangular. Chamamos a fun\u00e7\u00e3o d de m\u00e9trica e o par (X,d) de espa\u00e7o m\u00e9trico . \ud83d\udcdd Exemplo (M\u00e9trica trivial) Considere X um conjunto qualquer e defina para x,y \\in X , d(x,y) = \\begin{cases} 1 &\\text{se } x \\neq y \\\\ 0 &\\text{se } x=y. \\end{cases} \u00c9 claro que as condi\u00e7\u00f5es (1) e (2) s\u00e3o satisfeitas. A condi\u00e7\u00e3o (3) tamb\u00e9m \u00e9 trivial no caso em que x = y . Se x \\neq y , note que d(x,z) + d(z,y) \\ge 1 , pois d(x,z) = 1 ou d(y,z) = 1 , dado que z n\u00e3o pode ser simultaneamente igual a x e y . \ud83d\udcdd Exemplo (M\u00e9trica - Norma 2) Seja X = \\mathbb{R}^n e considere d para x,y \\in X , d(x,y) = ||x-y||_2 := \\sqrt{(x_1-y_1)^2 + \\dots + (x_n+y_n)^2}, conhecida como norma-2 no estudo de \u00c1lgebra Linear. Mais uma vez, as condi\u00e7\u00f5es (1) e (2) s\u00e3o triviais. J\u00e1 a condi\u00e7\u00e3o (3) \u00e9 um pouco mais complicada e pode ser demonstrada usando a desigualdade de Cauchy-Schwartz. Outras m\u00e9tricas podem ser definidas nesse espa\u00e7o como a norma 1 ou a norma infinito. \ud83d\udcdd Exemplo (M\u00e9trica - Norma infinito para fun\u00e7\u00f5es cont\u00ednuas) Seja X = C[a,b] , o espa\u00e7o das fun\u00e7\u00f5es cont\u00ednuas definidas no intervalo [a,b] e considere d para f,g \\in X , d(f,g) = \\max_{x \\in [a,b]} |f(x) - g(x)|. O primeiro passo nessa m\u00e9trica \u00e9 demonstrar que ela est\u00e1 bem definida, isto \u00e9, ser\u00e1 que a fun\u00e7\u00e3o h(x) = |f(x) - g(x)| tem m\u00e1ximo no intervalo [a,b] ? Sabemos que sim pelo Teorema do valor extremo . As condi\u00e7\u00f5es (1) e (2) para que d seja m\u00e9trica s\u00e3o trivialmente satisfeitas. J\u00e1 a terceira condi\u00e7\u00e3o \u00e9 consequ\u00eancia da desigualdade triangular da m\u00e9trica d(x,y) = |x-y| definida nos reais. O que acontece com fun\u00e7\u00f5es n\u00e3o cont\u00ednuas? E se trocarmos o m\u00e1ximo pelo supremo? \ud83d\udcdd Exemplo (Pseudo-m\u00e9trica) Seja X = C[a,b] e defina para f,g \\in X , d(f,g) = |f(a) - g(a)|. \u00c9 claro que d satisfaz as condi\u00e7\u00f5es (2) e (3) e que d(f,g) \\ge 0 . Todavia, d(f,g) = 0 n\u00e3o implica que f = g , s\u00f3 fazer g \\equiv f(a) < f(b) para verificar esse fato. Chamamos fun\u00e7\u00f5es que satisfazem (2) e (3), mas n\u00e3o satisfazem essa parte da primeira propriedade de pseudo-m\u00e9trica. No\u00e7\u00f5es topol\u00f3gicas Seja (X,d) um espa\u00e7o m\u00e9trico. Vamos definir alguns conceitos importantes de topologia segundo o ponto de vista de espa\u00e7os m\u00e9tricos: Bola aberta: Tome x \\in X e r > 0 , ent\u00e3o a bola aberta de centro x e raio r \u00e9 o conjunto B_r(x) = \\{y \\in X | d(x,y) < r\\} . Ponto interior: Tome A \\subseteq X . Dizemos que x \\in A \u00e9 ponto interior de A se para algum r > 0 , B_r(x) \\subseteq A . Definimos o interior de A como o conjunto \\mathring{A} = \\{x \\in A | x \\text{ \u00e9 ponto interior}\\} . Conjunto aberto: A \u00e9 conjunto aberto se A = \\mathring{A} . Seja \\mathcal{O} a fam\u00edlia de conjuntos abertos de X . Ent\u00e3o \\mathcal{O} satisfaz: (i) \\emptyset \\in \\mathcal{O} e X \\in \\mathcal{O} . (ii) Se O_{\\alpha} \\in \\mathcal{O} para \\alpha \\in \\Alpha sendo um conjunto de \u00edndices arbitr\u00e1rio, ent\u00e3o \\cup_{\\alpha \\in \\Alpha} O_{\\alpha} \\in \\mathcal{O} . (iii) Se O_1, \\dots, O_n \\in \\mathcal{O} para n \\in \\mathbb{N} , ent\u00e3o \\cap_{i=1}^n O_i \\in \\mathcal{O} . Observa\u00e7\u00e3o: Podemos definir uma Topologia partindo de (i)-(iii) como axiomas. Observa\u00e7\u00e3o 2: Se considerarmos o item (iii) com n = \\infty , a propriedade n\u00e3o vale! Para isso, basta considerar O_i = (-i^{-1}, i^{-1}) \\subseteq \\mathbb{R} . Ponto de ader\u00eancia: Um ponto x \u00e9 um ponto de ader\u00eancia de um conjunto A \\subseteq X se para qualquer r > 0 , B_r(x) \\cap A \\neq \\emptyset . O conjunto desses pontos \u00e9 chamado de fecho e \u00e9 denotado por \\bar{A} . Em particular, a partir dessa defini\u00e7\u00e3o, podemos concluir que (i) A \\subseteq \\bar{A} , (ii) se A \\subseteq B , ent\u00e3o \\bar{A} \\subseteq \\bar{B} , e (iii) \\overline{A \\cup B} = \\bar{A} \\cup \\bar{B} . \ud83d\udcdd Exemplo (Espa\u00e7o L^2 e fun\u00e7\u00f5es cont\u00ednuas) Seja X = L^2(0,T) , o espa\u00e7o de fun\u00e7\u00f5es Lebesgue mensur\u00e1veis definidas em [0,T] que satisfa\u00e7am \\int_X |f(x)|^2 \\, dx < +\\infty com m\u00e9trica para f,g \\in X definida por d(f,g) = \\left(\\int_X (f(x) - g(x))^2 \\, dx\\right)^{1/2}. Se A = C[0,T] \u00e9 o conjunto das fun\u00e7\u00f5es cont\u00ednuas em [0,T] , sabemos que A \\subset X . Al\u00e9m disso, \\bar{A} = X . Dizemos que A \u00e9 denso em X (veja esse link ). Ponto limite (acumula\u00e7\u00e3o): O ponto x \u00e9 ponto limite de A \\subseteq X se, e somente se, para todo r > 0 , B_r(x) \\cap A cont\u00e9m infinitos pontos. O conjunto desses pontos \u00e9 o conjunto derivado e \u00e9 denotado por A ' . Um ponto x \\in A que n\u00e3o \u00e9 limite \u00e9 chamado de ponto isolado . Converg\u00eancia: Seja \\{x_n\\}_{n \\in \\mathbb{N}} \\subseteq X uma sequ\u00eancia de pontos. Dizemos que \\{x_n\\} converge para o ponto x se para todo r > 0 , existe N(r) tal que n > N(r) implica x_n \\in S_r(x) . Nesse caso, denotamos por x_n \\to x . Logo, a defini\u00e7\u00e3o de converg\u00eancia \u00e9 equivalente a afirmar que d(x,x_n) \\to 0 nos reais. Uma consequ\u00eancia dessa defini\u00e7\u00e3o \u00e9 que x \\in \\bar{A} se, e somente se, existe uma sequ\u00eancia \\{x_n\\} tal que x_n \\in A para todo n \\in \\mathbb{N} e x_n \\to x . Conjunto fechado: Dizemos que A \u00e9 fechado se A cont\u00e9m todos os seus pontos limite, isto \u00e9, A' \\subseteq A . De forma equivalente A = \\bar{A} . Teorema (rela\u00e7\u00e3o entre conjuntos abertos e fechados): A \u00e9 fechado se, e somente se, A^c \u00e9 aberto. Esse teorema permite usar as propriedades de abertos para fechados atrav\u00e9s da lei de DeMorgan . \"How often have I said to you that when you have eliminated the impossible, whatever remains, however improbable, must be the truth?\" - Sherlock Holmes. (Quantas vezes eu falei para voc\u00ea que quando voc\u00ea eliminou o imposs\u00edvel, qualquer coisa que sobra, mesmo que improv\u00e1vel, deve ser verdadeiro? - tradu\u00e7\u00e3o livre) Continuidade Sejam (X,d) e (Y,d') dois espa\u00e7os m\u00e9tricos e f: X \\to Y uma fun\u00e7\u00e3o. f \u00e9 dita cont\u00ednua no ponto x_0 \\in X se para qualquer \\epsilon > 0 , existe \\delta > 0 tal que f(B_{\\delta}(x_0)) \\subseteq B_{\\epsilon}(f(x_0)). A fun\u00e7\u00e3o \u00e9 cont\u00ednua em X se for cont\u00ednua em todos os pontos de X . Intuitivamente, estamos dizendo que a fun\u00e7\u00e3o f mapeia uma vizinhan\u00e7a de x_0 suficientemente pequena em uma vizinhan\u00e7a arbitrariamente pequena de f(x_0) , isto \u00e9, por mais pequena que seja a vizinhan\u00e7a tomada de f(x_0) , a fun\u00e7\u00e3o f \u00e9 t\u00e3o \"regular\", que ela encontra uma vizinhan\u00e7a de x_0 para mapear na de f(x_0) . Por vizinhan\u00e7a de x , queremos dizer um conjunto aberto que contenha x . De forma equivalente, uma bola aberta de centro x . Teorema: f: X \\to Y \u00e9 cont\u00ednua no ponto x \\in X se, e somente se, para todo sequ\u00eancia de pontos \\{x_n\\} com x_n \\to x , vale que f(x_n) \\to f(x) . Um rascunho para a demonstra\u00e7\u00e3o \u00e9 o seguinte: (Necessidade): Suponha f cont\u00ednua e tome x_n \\to x . Nesse caso, para todo \\epsilon > 0 , existe \\delta tal que f(B_{\\delta}(x)) \\subseteq B_{\\epsilon}(f(x)) . Para n suficientemente grande, teremos que x_n \\in B_{\\delta}(x_0) e, portanto, f(x_n) \\in B_{\\epsilon}(f(x)) . Como \\epsilon \u00e9 arbitrariamente pequeno, f(x_n) \\to f(x) . (Sufici\u00eancia) Tome \\epsilon > 0 . Suponha que para todo \\delta_n = 1/n , exista y_n tal que d(f(y_n), f(x)) \\ge \\epsilon, isto \u00e9, y_n \\in B_{\\delta_n}(x) , mas f(y_n) \\not \\in B_{\\epsilon}(f(x)) . Est\u00e1 claro que y_n \\to x , mas f(y_n) \\not \\to f(x) , o que implica que f n\u00e3o \u00e9 cont\u00ednua em x . Provamos pela contrapositiva. Outra equival\u00eancia com continuidade \u00e9 a seguinte: Teorema: A fun\u00e7\u00e3o f: X \\to Y \u00e9 cont\u00ednua se, e somente se, para todo conjunto F \\subseteq Y fechado (aberto), f^{-1}(F) \u00e9 fechado (aberto) em X .","title":"Espa\u00e7os m\u00e9tricos"},{"location":"functional_analysis/metric_spaces/#espacos-metricos","text":"Queremos introduzir o conceito de dist\u00e2ncia entre pontos em um espa\u00e7o. Com isso bem definido, a no\u00e7\u00e3o de limite, fundamental para a \u00e1rea da An\u00e1lise (e talvez um dos pontos que mais a distancie da \u00c1lgebra), fica bem entendida, j\u00e1 que estamos dizendo que uma sequ\u00eancia de pontos se aproxima cada vez mais de um ponto do espa\u00e7o. A ideia de se aproximar se conecta naturalmente com a dist\u00e2ncia entre os pontos diminuir. Defini\u00e7\u00e3o: Seja X um conjunto qualquer e d : X \\times X \\to \\mathbb{R} uma fun\u00e7\u00e3o que satisfaz, para quaisquer x,y,z \\in X , as seguintes propriedades: (1) d(x,y) \\ge 0 e d(x,y) = 0 se, e somente se, x=y , isto \u00e9, a dist\u00e2ncia \u00e9 sempre positiva, a menos quando a medidos de um ponto a ele mesmo, o que naturalmente queremos que seja zero; (2) d(x,y) = d(y,x) , isto \u00e9, a dist\u00e2ncia entre dois pontos independe da ordem com que se inicia a medida; (3) d(x,y) \\le d(x,z) + d(z,y) , a desigualdade triangular. Chamamos a fun\u00e7\u00e3o d de m\u00e9trica e o par (X,d) de espa\u00e7o m\u00e9trico . \ud83d\udcdd Exemplo (M\u00e9trica trivial) Considere X um conjunto qualquer e defina para x,y \\in X , d(x,y) = \\begin{cases} 1 &\\text{se } x \\neq y \\\\ 0 &\\text{se } x=y. \\end{cases} \u00c9 claro que as condi\u00e7\u00f5es (1) e (2) s\u00e3o satisfeitas. A condi\u00e7\u00e3o (3) tamb\u00e9m \u00e9 trivial no caso em que x = y . Se x \\neq y , note que d(x,z) + d(z,y) \\ge 1 , pois d(x,z) = 1 ou d(y,z) = 1 , dado que z n\u00e3o pode ser simultaneamente igual a x e y . \ud83d\udcdd Exemplo (M\u00e9trica - Norma 2) Seja X = \\mathbb{R}^n e considere d para x,y \\in X , d(x,y) = ||x-y||_2 := \\sqrt{(x_1-y_1)^2 + \\dots + (x_n+y_n)^2}, conhecida como norma-2 no estudo de \u00c1lgebra Linear. Mais uma vez, as condi\u00e7\u00f5es (1) e (2) s\u00e3o triviais. J\u00e1 a condi\u00e7\u00e3o (3) \u00e9 um pouco mais complicada e pode ser demonstrada usando a desigualdade de Cauchy-Schwartz. Outras m\u00e9tricas podem ser definidas nesse espa\u00e7o como a norma 1 ou a norma infinito. \ud83d\udcdd Exemplo (M\u00e9trica - Norma infinito para fun\u00e7\u00f5es cont\u00ednuas) Seja X = C[a,b] , o espa\u00e7o das fun\u00e7\u00f5es cont\u00ednuas definidas no intervalo [a,b] e considere d para f,g \\in X , d(f,g) = \\max_{x \\in [a,b]} |f(x) - g(x)|. O primeiro passo nessa m\u00e9trica \u00e9 demonstrar que ela est\u00e1 bem definida, isto \u00e9, ser\u00e1 que a fun\u00e7\u00e3o h(x) = |f(x) - g(x)| tem m\u00e1ximo no intervalo [a,b] ? Sabemos que sim pelo Teorema do valor extremo . As condi\u00e7\u00f5es (1) e (2) para que d seja m\u00e9trica s\u00e3o trivialmente satisfeitas. J\u00e1 a terceira condi\u00e7\u00e3o \u00e9 consequ\u00eancia da desigualdade triangular da m\u00e9trica d(x,y) = |x-y| definida nos reais. O que acontece com fun\u00e7\u00f5es n\u00e3o cont\u00ednuas? E se trocarmos o m\u00e1ximo pelo supremo? \ud83d\udcdd Exemplo (Pseudo-m\u00e9trica) Seja X = C[a,b] e defina para f,g \\in X , d(f,g) = |f(a) - g(a)|. \u00c9 claro que d satisfaz as condi\u00e7\u00f5es (2) e (3) e que d(f,g) \\ge 0 . Todavia, d(f,g) = 0 n\u00e3o implica que f = g , s\u00f3 fazer g \\equiv f(a) < f(b) para verificar esse fato. Chamamos fun\u00e7\u00f5es que satisfazem (2) e (3), mas n\u00e3o satisfazem essa parte da primeira propriedade de pseudo-m\u00e9trica.","title":"Espa\u00e7os m\u00e9tricos"},{"location":"functional_analysis/metric_spaces/#nocoes-topologicas","text":"Seja (X,d) um espa\u00e7o m\u00e9trico. Vamos definir alguns conceitos importantes de topologia segundo o ponto de vista de espa\u00e7os m\u00e9tricos: Bola aberta: Tome x \\in X e r > 0 , ent\u00e3o a bola aberta de centro x e raio r \u00e9 o conjunto B_r(x) = \\{y \\in X | d(x,y) < r\\} . Ponto interior: Tome A \\subseteq X . Dizemos que x \\in A \u00e9 ponto interior de A se para algum r > 0 , B_r(x) \\subseteq A . Definimos o interior de A como o conjunto \\mathring{A} = \\{x \\in A | x \\text{ \u00e9 ponto interior}\\} . Conjunto aberto: A \u00e9 conjunto aberto se A = \\mathring{A} . Seja \\mathcal{O} a fam\u00edlia de conjuntos abertos de X . Ent\u00e3o \\mathcal{O} satisfaz: (i) \\emptyset \\in \\mathcal{O} e X \\in \\mathcal{O} . (ii) Se O_{\\alpha} \\in \\mathcal{O} para \\alpha \\in \\Alpha sendo um conjunto de \u00edndices arbitr\u00e1rio, ent\u00e3o \\cup_{\\alpha \\in \\Alpha} O_{\\alpha} \\in \\mathcal{O} . (iii) Se O_1, \\dots, O_n \\in \\mathcal{O} para n \\in \\mathbb{N} , ent\u00e3o \\cap_{i=1}^n O_i \\in \\mathcal{O} . Observa\u00e7\u00e3o: Podemos definir uma Topologia partindo de (i)-(iii) como axiomas. Observa\u00e7\u00e3o 2: Se considerarmos o item (iii) com n = \\infty , a propriedade n\u00e3o vale! Para isso, basta considerar O_i = (-i^{-1}, i^{-1}) \\subseteq \\mathbb{R} . Ponto de ader\u00eancia: Um ponto x \u00e9 um ponto de ader\u00eancia de um conjunto A \\subseteq X se para qualquer r > 0 , B_r(x) \\cap A \\neq \\emptyset . O conjunto desses pontos \u00e9 chamado de fecho e \u00e9 denotado por \\bar{A} . Em particular, a partir dessa defini\u00e7\u00e3o, podemos concluir que (i) A \\subseteq \\bar{A} , (ii) se A \\subseteq B , ent\u00e3o \\bar{A} \\subseteq \\bar{B} , e (iii) \\overline{A \\cup B} = \\bar{A} \\cup \\bar{B} . \ud83d\udcdd Exemplo (Espa\u00e7o L^2 e fun\u00e7\u00f5es cont\u00ednuas) Seja X = L^2(0,T) , o espa\u00e7o de fun\u00e7\u00f5es Lebesgue mensur\u00e1veis definidas em [0,T] que satisfa\u00e7am \\int_X |f(x)|^2 \\, dx < +\\infty com m\u00e9trica para f,g \\in X definida por d(f,g) = \\left(\\int_X (f(x) - g(x))^2 \\, dx\\right)^{1/2}. Se A = C[0,T] \u00e9 o conjunto das fun\u00e7\u00f5es cont\u00ednuas em [0,T] , sabemos que A \\subset X . Al\u00e9m disso, \\bar{A} = X . Dizemos que A \u00e9 denso em X (veja esse link ). Ponto limite (acumula\u00e7\u00e3o): O ponto x \u00e9 ponto limite de A \\subseteq X se, e somente se, para todo r > 0 , B_r(x) \\cap A cont\u00e9m infinitos pontos. O conjunto desses pontos \u00e9 o conjunto derivado e \u00e9 denotado por A ' . Um ponto x \\in A que n\u00e3o \u00e9 limite \u00e9 chamado de ponto isolado . Converg\u00eancia: Seja \\{x_n\\}_{n \\in \\mathbb{N}} \\subseteq X uma sequ\u00eancia de pontos. Dizemos que \\{x_n\\} converge para o ponto x se para todo r > 0 , existe N(r) tal que n > N(r) implica x_n \\in S_r(x) . Nesse caso, denotamos por x_n \\to x . Logo, a defini\u00e7\u00e3o de converg\u00eancia \u00e9 equivalente a afirmar que d(x,x_n) \\to 0 nos reais. Uma consequ\u00eancia dessa defini\u00e7\u00e3o \u00e9 que x \\in \\bar{A} se, e somente se, existe uma sequ\u00eancia \\{x_n\\} tal que x_n \\in A para todo n \\in \\mathbb{N} e x_n \\to x . Conjunto fechado: Dizemos que A \u00e9 fechado se A cont\u00e9m todos os seus pontos limite, isto \u00e9, A' \\subseteq A . De forma equivalente A = \\bar{A} . Teorema (rela\u00e7\u00e3o entre conjuntos abertos e fechados): A \u00e9 fechado se, e somente se, A^c \u00e9 aberto. Esse teorema permite usar as propriedades de abertos para fechados atrav\u00e9s da lei de DeMorgan . \"How often have I said to you that when you have eliminated the impossible, whatever remains, however improbable, must be the truth?\" - Sherlock Holmes. (Quantas vezes eu falei para voc\u00ea que quando voc\u00ea eliminou o imposs\u00edvel, qualquer coisa que sobra, mesmo que improv\u00e1vel, deve ser verdadeiro? - tradu\u00e7\u00e3o livre)","title":"No\u00e7\u00f5es topol\u00f3gicas"},{"location":"functional_analysis/metric_spaces/#continuidade","text":"Sejam (X,d) e (Y,d') dois espa\u00e7os m\u00e9tricos e f: X \\to Y uma fun\u00e7\u00e3o. f \u00e9 dita cont\u00ednua no ponto x_0 \\in X se para qualquer \\epsilon > 0 , existe \\delta > 0 tal que f(B_{\\delta}(x_0)) \\subseteq B_{\\epsilon}(f(x_0)). A fun\u00e7\u00e3o \u00e9 cont\u00ednua em X se for cont\u00ednua em todos os pontos de X . Intuitivamente, estamos dizendo que a fun\u00e7\u00e3o f mapeia uma vizinhan\u00e7a de x_0 suficientemente pequena em uma vizinhan\u00e7a arbitrariamente pequena de f(x_0) , isto \u00e9, por mais pequena que seja a vizinhan\u00e7a tomada de f(x_0) , a fun\u00e7\u00e3o f \u00e9 t\u00e3o \"regular\", que ela encontra uma vizinhan\u00e7a de x_0 para mapear na de f(x_0) . Por vizinhan\u00e7a de x , queremos dizer um conjunto aberto que contenha x . De forma equivalente, uma bola aberta de centro x . Teorema: f: X \\to Y \u00e9 cont\u00ednua no ponto x \\in X se, e somente se, para todo sequ\u00eancia de pontos \\{x_n\\} com x_n \\to x , vale que f(x_n) \\to f(x) . Um rascunho para a demonstra\u00e7\u00e3o \u00e9 o seguinte: (Necessidade): Suponha f cont\u00ednua e tome x_n \\to x . Nesse caso, para todo \\epsilon > 0 , existe \\delta tal que f(B_{\\delta}(x)) \\subseteq B_{\\epsilon}(f(x)) . Para n suficientemente grande, teremos que x_n \\in B_{\\delta}(x_0) e, portanto, f(x_n) \\in B_{\\epsilon}(f(x)) . Como \\epsilon \u00e9 arbitrariamente pequeno, f(x_n) \\to f(x) . (Sufici\u00eancia) Tome \\epsilon > 0 . Suponha que para todo \\delta_n = 1/n , exista y_n tal que d(f(y_n), f(x)) \\ge \\epsilon, isto \u00e9, y_n \\in B_{\\delta_n}(x) , mas f(y_n) \\not \\in B_{\\epsilon}(f(x)) . Est\u00e1 claro que y_n \\to x , mas f(y_n) \\not \\to f(x) , o que implica que f n\u00e3o \u00e9 cont\u00ednua em x . Provamos pela contrapositiva. Outra equival\u00eancia com continuidade \u00e9 a seguinte: Teorema: A fun\u00e7\u00e3o f: X \\to Y \u00e9 cont\u00ednua se, e somente se, para todo conjunto F \\subseteq Y fechado (aberto), f^{-1}(F) \u00e9 fechado (aberto) em X .","title":"Continuidade"},{"location":"infestatistica_BSc/SufficientStatistics/","text":"Estat\u00edsticas Suficientes A ideia por tra\u015b da estat\u00edstica \u00e9, como o nome diz, ser suficiente. Uma estat\u00edstica \u00e9 uma fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias, como, por exemplo, T = r(X_1, ..., X_n) . M\u00e9dia amostral, vari\u00e2ncia amostral, valor m\u00e1ximo, s\u00e3o todos exemplos. Imagine que temos um problema como o seguinte: Vamos imaginar que um estat\u00edstico d\u00e1 um trabalho para seu estagi\u00e1rio para organizar os dados de forma mais eficiente poss\u00edvel, enquanto ele pensa no modelo. O estagi\u00e1rio de forma muito ing\u00eanua cria uma lista em seu Jupyter Notebook e salva o notebook com os dados na sua lista. Depois ele salva num arquivo .txt e vai para casa tranquilo que o trabalho acabou mais cedo. Ser\u00e1 que era necess\u00e1rio ter salvo todos os dados? O estat\u00edstico no dia seuinte diz que n\u00e3o! E manda o estagi\u00e1rio estudar novamente estat\u00edstica. Ele disse para estudar Estat\u00edstica Suficientes . Defini\u00e7\u00e3o Unidimensional Seja X_1, ..., X_n uma amostra aleat\u00f3ria de distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta . Suponha que para todo valor que \\theta assume e para todo valor que T assume (vamos chamar de t = r(x_1, ..., x_n) , nesse caso j\u00e1 observamos o processo e calculamos t ), a distribui\u00e7\u00e3o conjunta condicional de X_1, ...., X_n dado T=t e \\theta , isto \u00e9, dado que voc\u00ea observou uma estat\u00edstica (a m\u00e9dia de temperaturas, por exemplo) depende apenas de t , mas n\u00e3o de \\theta . Isso significa que a distribui\u00e7\u00e3o \u00e9 constante para todos os valores de \\theta . Chamaremos essa estat\u00edstica T de suficiente para \\theta . Obs.: Para quem estudou fun\u00e7\u00f5es mensur\u00e1veis, podemos definir estat\u00edstica como fun\u00e7\u00e3o mensur\u00e1vel dos dados. Seja (\\mathbb{T}, \\mathbb{C}) um espa\u00e7o mensur\u00e1vel tal que \\mathbb{C} cont\u00e9m todos os conjuntos unit\u00e1rios. Se T : \\mathbb{X} \\to \\mathbb{T} \u00e9 mensu\u00e1vel, ent\u00e3o \u00e9 uma estat\u00edstica. Seja \\mathbb{P}_0 uma familia param\u00e9trica de distribui\u00e7\u00f5es em (\\mathbb{X}, \\mathbb{B}) . Seja (\\Omega, \\tau) um espa\u00e7o dos par\u00e2metros e \\Theta: \\mathbb{P}_0 \\to \\Omega um par\u00e2metro. Seja T uma estat\u00edstica. Ela \u00e9 suficiente para \\Theta se para toda priori \\mu_{\\Theta} , existem vers\u00f5es da posteriori \\mu_{\\Theta|X} e \\mu_{\\Theta|T} tal que \\forall B \\in \\tau, \\mu_{\\Theta|X}(B|x) = \\mu_{\\Theta|T}(B|T(x)) , quase certamente convergente para [\\mu_X] onde \\mu_X \u00e9 distribui\u00e7\u00e3o marginal de X . Crit\u00e9rio de Fatoriza\u00e7\u00e3o Teorema atribu\u00eddo a Neyman-Fisher. X_1,...X_n amostra aleat\u00f3ria com pdf ou pmf f(x|\\theta) , onde \\theta \u00e9 desconhecido. Uma estat\u00edstica T = r(X) para \\theta \u00e9 suficiente se, e somente se, a distribui\u00e7\u00e3o conjunta f_n(x|\\theta) pode ser fatorada para todo valor x \\in \\mathbb{R}^n da seguinte forma: f_n(x|\\theta) = u(x)v[r(x), \\theta] Onde u e v s\u00e3o n\u00e3o negativas, u n\u00e3o depende de \\theta e v s\u00f3 depende dos dados atrav\u00e9s da estat\u00edstica. Isto \u00e9, n\u00e3o adianta voc\u00ea encontrar qualquer fun\u00e7\u00e3o de x , tem que encontrar a estat\u00edstica T em v . Estat\u00edsticas Conjuntas Suficientes Suponha que para cada \\theta , vetor, e cada valor das estat\u00edsticas (T_1, ..., T_k) = (t_1, ..., t_k) a distribui\u00e7\u00e3o conjunta condicional dos dados dadas as estat\u00edsticas n\u00e3o depende de \\theta . Veja que nesse caso, a diferen\u00e7a \u00e9 que condiciono em k estat\u00edsticas, k \\geq 1 . Crit\u00e9rio de Fatoriza\u00e7\u00e3o Sejam r_1, ..., r_k fun\u00e7\u00f5es de n vari\u00e1veis. A estat\u00edsticas T_i = r_i(X) s\u00e3o estat\u00edsticas suficientes conjuntas para \\theta se, e somene se, a pdf conjunta f_n(x|\\theta) pode ser fatorado como f_n(x|\\theta) = u(x)v[r_1(x), ..., r_k(x),\\theta], para todos os valores x \\in \\mathbb{R}^n e \\theta \\in \\Omega Obs.: Podemos mostrar que qualquer fun\u00e7\u00e3o injetiva de uma estat\u00edstica suficiente \u00e9 uma estat\u00edstica suficiente. Estat\u00edstica Suficiente M\u00ednima Estat\u00edstica de Ordem Considere uma amostra aleat\u00f3ria e a ordene. Diremos que a nova amostra, ordenada, \u00e9 uma estat\u00edstica de ordem. Observe que ela funciona como uma matrix de \"shifts\" que opera trocando as linhas do vetor de lugar. Por isso ela \u00e9 uma fun\u00e7\u00e3o. Essa estat\u00edstica \u00e9 sufciente conjunta para \\theta . O interessante que podemos ver isso dado que o produt\u00f3rio n\u00e3o importa a ordem. Estat\u00edstica Suficiente M\u00ednima \u00c9 uma estat\u00edstica T suficiente e, al\u00e9m disso, \u00e9 fun\u00e7\u00e3o de todas as outras estat\u00edsticas suficientes. MLE e Estat\u00edstica Suficiente Seja T uma estat\u00edstica suficiente para \\theta . Ent\u00e3o o estimador de m\u00e1xima verossimilhan\u00e7a \\hat{\\theta} depende das observa\u00e7\u00f5es somente atrav\u00e9s da estat\u00edstica T . Al\u00e9m disso, se \\hat{\\theta} \u00e9 suficiente, ent\u00e3o \u00e9 m\u00ednimo. Estat\u00edsticas Suficientes e Estimador de Bayes T = r(X) estat\u00edstica suficiente para \\theta . Ent\u00e3o todo estimador de Bayes \\hat{\\theta} depende nas observa\u00e7\u00f5es X_1, ..., X_n apenas atrav\u00e9s da estat\u00edstica T . Al\u00e9m do mais, se for suficiente, ser\u00e1 suficiente m\u00ednimo. Defini\u00e7\u00f5es Adicionais Considere uma amostra aleat\u00f3ria X_1,...,X_n Estat\u00edstica Completa Seja t = T(X) estat\u00edstica. Se E[g(T(X))|\\theta] = 0, \\forall \\theta \\implies P[g(T(X)) = 0] = 1, ent\u00e3o ela \u00e9 dita completa. Estat\u00edstica Ancillary Suponha que queremos estimar \\theta e f_n(x|\\theta) seja a pdf conjunta. Seja A(X) uma estat\u00edstica. Se a sua distribui\u00e7\u00e3o n\u00e3o depende de \\theta , ent\u00e3o ser\u00e1 uma estat\u00edstica ancillary (auxiliar?) Por exemplo, se X_1, X_2 \\sim N(\\mu, \\sigma^2) e \\mu \u00e9 desconhecido, temos que X_1 - X_2 \\sim N(0, 2\\sigma^2) \u00e9 uma estat\u00edstica auxiliar. Melhorando um Estimador Suponha que temos uma amostra aleat\u00f3ria X = (X_1, ..., X_n) cuja pdf \u00e9 f(x|\\theta) e \\theta \\in \\Omega desconhecido, tal que queremos estimar h(\\theta) para alguma fun\u00e7\u00e3o h . Seja Z = g(X_1, ..., X_n) . E_{\\theta}(Z) = \\int_{-\\infty}^{\\infty}...\\int_{-\\infty}^{\\infty} g(x)f_n(x|\\theta)dx_1, ..., dx_n Para cada estimado \\delta(X) e para todo valor de \\theta , definimos o MSE (Erro M\u00e9dio Quadr\u00e1tico) R(\\theta, \\delta) = E_{\\theta}\\{[\\delta(X) - h(\\theta)]^2\\} Quando n\u00e3o atribu\u00edmos uma priori para \\theta , ent\u00e3o queremos encontrar um estimador para que o MSE seja pequeno para v\u00e1rios valores de \\theta . Seja T uma estat\u00edstica suficiente conhecida. Definimos \\delta_0(T) = E_{\\theta}\\{\\delta(X)|T\\} \\overset{1}{=} E\\{\\delta(X)|T\\} (1) Agora, por que podemos chamar \\delta_0 de estimador se depende de \\theta ? Como T \u00e9 uma estat\u00edstica suficiente, a distribui\u00e7\u00e3o condicionada em T e em \\theta da amostra X_1, ..., X_n n\u00e3o depende de \\theta !!! Em particular o valor esperado do estimador \\delta(T) . Logo, como esse valor esperado n\u00e3o depende de \\theta , podemos dizer sim que ele \u00e9 um estimador. Teorema Rao - Blackwell Teorema 7.9.1 do livro. Seja \\delta(X) um estimador e T uma estat\u00edstica suficiente para \\theta . O estimador \\delta_0(T) definido acima, para todo valor \\theta \\in \\Omega \u00e9: R(\\theta, \\delta_0) \\leq R(\\theta, \\delta), isto \u00e9, \u00e9 um estimador com menor erro quadr\u00e1tico m\u00e9dio (MSE). Em particular se R(\\theta, \\delta) < \\infty , a desigualdade se torna estrita, a menos que \\delta(X) seja um afun\u00e7\u00e3o de T , isto \u00e9, se \\delta(X) n\u00e3o for fun\u00e7\u00e3o de T , ent\u00e3o a desigualdade ser\u00e1 estrita. Por desigualdade estrita entenda < . Obs.: Chamamos o processo de melhorar um estimador com esse teorema de \"Rao-Blackwelliation\". Obs.2: Podemos generalizar um pouco mais. Para isso, pesquise sobre Conjuntos Convexos e sobre Fun\u00e7\u00f5es Convexas . Em um conjunto convexo, se a nossa fun\u00e7\u00e3o de perda n\u00e3o for o MSE, mas for uma fun\u00e7\u00e3o convexa, o teorema tamb\u00e9m valer\u00e1. Uma suposi\u00e7\u00e3o interessante que o Livro n\u00e3o imp\u00f5e \u00e9 que E[||\\delta(X)||) < \\infty . Inadmissibilidade Suponha que R(\\theta, \\delta) \u00e9 MSE. O estimador \\delta \u00e9 inadimiss\u00edvel se existe outro estimador \\delta_0 tal que R(\\theta, \\delta_0) \\leq R(\\theta, \\delta) para todo valor de \\theta e existe a desigualdade estrita em, pelo menos um valor de \\theta . Dizemos nesse caso que \\delta_0 domina o estimador \\delta . Um estimador \\delta_0 \u00e9 admiss\u00edvel se n\u00e3o existe outro estimador que o domine.","title":"Estat\u00edsticas Suficientes"},{"location":"infestatistica_BSc/SufficientStatistics/#estatisticas-suficientes","text":"A ideia por tra\u015b da estat\u00edstica \u00e9, como o nome diz, ser suficiente. Uma estat\u00edstica \u00e9 uma fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias, como, por exemplo, T = r(X_1, ..., X_n) . M\u00e9dia amostral, vari\u00e2ncia amostral, valor m\u00e1ximo, s\u00e3o todos exemplos. Imagine que temos um problema como o seguinte: Vamos imaginar que um estat\u00edstico d\u00e1 um trabalho para seu estagi\u00e1rio para organizar os dados de forma mais eficiente poss\u00edvel, enquanto ele pensa no modelo. O estagi\u00e1rio de forma muito ing\u00eanua cria uma lista em seu Jupyter Notebook e salva o notebook com os dados na sua lista. Depois ele salva num arquivo .txt e vai para casa tranquilo que o trabalho acabou mais cedo. Ser\u00e1 que era necess\u00e1rio ter salvo todos os dados? O estat\u00edstico no dia seuinte diz que n\u00e3o! E manda o estagi\u00e1rio estudar novamente estat\u00edstica. Ele disse para estudar Estat\u00edstica Suficientes .","title":"Estat\u00edsticas Suficientes"},{"location":"infestatistica_BSc/SufficientStatistics/#definicao-unidimensional","text":"Seja X_1, ..., X_n uma amostra aleat\u00f3ria de distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta . Suponha que para todo valor que \\theta assume e para todo valor que T assume (vamos chamar de t = r(x_1, ..., x_n) , nesse caso j\u00e1 observamos o processo e calculamos t ), a distribui\u00e7\u00e3o conjunta condicional de X_1, ...., X_n dado T=t e \\theta , isto \u00e9, dado que voc\u00ea observou uma estat\u00edstica (a m\u00e9dia de temperaturas, por exemplo) depende apenas de t , mas n\u00e3o de \\theta . Isso significa que a distribui\u00e7\u00e3o \u00e9 constante para todos os valores de \\theta . Chamaremos essa estat\u00edstica T de suficiente para \\theta . Obs.: Para quem estudou fun\u00e7\u00f5es mensur\u00e1veis, podemos definir estat\u00edstica como fun\u00e7\u00e3o mensur\u00e1vel dos dados. Seja (\\mathbb{T}, \\mathbb{C}) um espa\u00e7o mensur\u00e1vel tal que \\mathbb{C} cont\u00e9m todos os conjuntos unit\u00e1rios. Se T : \\mathbb{X} \\to \\mathbb{T} \u00e9 mensu\u00e1vel, ent\u00e3o \u00e9 uma estat\u00edstica. Seja \\mathbb{P}_0 uma familia param\u00e9trica de distribui\u00e7\u00f5es em (\\mathbb{X}, \\mathbb{B}) . Seja (\\Omega, \\tau) um espa\u00e7o dos par\u00e2metros e \\Theta: \\mathbb{P}_0 \\to \\Omega um par\u00e2metro. Seja T uma estat\u00edstica. Ela \u00e9 suficiente para \\Theta se para toda priori \\mu_{\\Theta} , existem vers\u00f5es da posteriori \\mu_{\\Theta|X} e \\mu_{\\Theta|T} tal que \\forall B \\in \\tau, \\mu_{\\Theta|X}(B|x) = \\mu_{\\Theta|T}(B|T(x)) , quase certamente convergente para [\\mu_X] onde \\mu_X \u00e9 distribui\u00e7\u00e3o marginal de X .","title":"Defini\u00e7\u00e3o Unidimensional"},{"location":"infestatistica_BSc/SufficientStatistics/#criterio-de-fatorizacao","text":"Teorema atribu\u00eddo a Neyman-Fisher. X_1,...X_n amostra aleat\u00f3ria com pdf ou pmf f(x|\\theta) , onde \\theta \u00e9 desconhecido. Uma estat\u00edstica T = r(X) para \\theta \u00e9 suficiente se, e somente se, a distribui\u00e7\u00e3o conjunta f_n(x|\\theta) pode ser fatorada para todo valor x \\in \\mathbb{R}^n da seguinte forma: f_n(x|\\theta) = u(x)v[r(x), \\theta] Onde u e v s\u00e3o n\u00e3o negativas, u n\u00e3o depende de \\theta e v s\u00f3 depende dos dados atrav\u00e9s da estat\u00edstica. Isto \u00e9, n\u00e3o adianta voc\u00ea encontrar qualquer fun\u00e7\u00e3o de x , tem que encontrar a estat\u00edstica T em v .","title":"Crit\u00e9rio de Fatoriza\u00e7\u00e3o"},{"location":"infestatistica_BSc/SufficientStatistics/#estatisticas-conjuntas-suficientes","text":"Suponha que para cada \\theta , vetor, e cada valor das estat\u00edsticas (T_1, ..., T_k) = (t_1, ..., t_k) a distribui\u00e7\u00e3o conjunta condicional dos dados dadas as estat\u00edsticas n\u00e3o depende de \\theta . Veja que nesse caso, a diferen\u00e7a \u00e9 que condiciono em k estat\u00edsticas, k \\geq 1 .","title":"Estat\u00edsticas Conjuntas Suficientes"},{"location":"infestatistica_BSc/SufficientStatistics/#criterio-de-fatorizacao_1","text":"Sejam r_1, ..., r_k fun\u00e7\u00f5es de n vari\u00e1veis. A estat\u00edsticas T_i = r_i(X) s\u00e3o estat\u00edsticas suficientes conjuntas para \\theta se, e somene se, a pdf conjunta f_n(x|\\theta) pode ser fatorado como f_n(x|\\theta) = u(x)v[r_1(x), ..., r_k(x),\\theta], para todos os valores x \\in \\mathbb{R}^n e \\theta \\in \\Omega Obs.: Podemos mostrar que qualquer fun\u00e7\u00e3o injetiva de uma estat\u00edstica suficiente \u00e9 uma estat\u00edstica suficiente.","title":"Crit\u00e9rio de Fatoriza\u00e7\u00e3o"},{"location":"infestatistica_BSc/SufficientStatistics/#estatistica-suficiente-minima","text":"","title":"Estat\u00edstica Suficiente M\u00ednima"},{"location":"infestatistica_BSc/SufficientStatistics/#estatistica-de-ordem","text":"Considere uma amostra aleat\u00f3ria e a ordene. Diremos que a nova amostra, ordenada, \u00e9 uma estat\u00edstica de ordem. Observe que ela funciona como uma matrix de \"shifts\" que opera trocando as linhas do vetor de lugar. Por isso ela \u00e9 uma fun\u00e7\u00e3o. Essa estat\u00edstica \u00e9 sufciente conjunta para \\theta . O interessante que podemos ver isso dado que o produt\u00f3rio n\u00e3o importa a ordem.","title":"Estat\u00edstica de Ordem"},{"location":"infestatistica_BSc/SufficientStatistics/#estatistica-suficiente-minima_1","text":"\u00c9 uma estat\u00edstica T suficiente e, al\u00e9m disso, \u00e9 fun\u00e7\u00e3o de todas as outras estat\u00edsticas suficientes.","title":"Estat\u00edstica Suficiente M\u00ednima"},{"location":"infestatistica_BSc/SufficientStatistics/#mle-e-estatistica-suficiente","text":"Seja T uma estat\u00edstica suficiente para \\theta . Ent\u00e3o o estimador de m\u00e1xima verossimilhan\u00e7a \\hat{\\theta} depende das observa\u00e7\u00f5es somente atrav\u00e9s da estat\u00edstica T . Al\u00e9m disso, se \\hat{\\theta} \u00e9 suficiente, ent\u00e3o \u00e9 m\u00ednimo.","title":"MLE e Estat\u00edstica Suficiente"},{"location":"infestatistica_BSc/SufficientStatistics/#estatisticas-suficientes-e-estimador-de-bayes","text":"T = r(X) estat\u00edstica suficiente para \\theta . Ent\u00e3o todo estimador de Bayes \\hat{\\theta} depende nas observa\u00e7\u00f5es X_1, ..., X_n apenas atrav\u00e9s da estat\u00edstica T . Al\u00e9m do mais, se for suficiente, ser\u00e1 suficiente m\u00ednimo.","title":"Estat\u00edsticas Suficientes e Estimador de Bayes"},{"location":"infestatistica_BSc/SufficientStatistics/#definicoes-adicionais","text":"Considere uma amostra aleat\u00f3ria X_1,...,X_n","title":"Defini\u00e7\u00f5es Adicionais"},{"location":"infestatistica_BSc/SufficientStatistics/#estatistica-completa","text":"Seja t = T(X) estat\u00edstica. Se E[g(T(X))|\\theta] = 0, \\forall \\theta \\implies P[g(T(X)) = 0] = 1, ent\u00e3o ela \u00e9 dita completa.","title":"Estat\u00edstica Completa"},{"location":"infestatistica_BSc/SufficientStatistics/#estatistica-ancillary","text":"Suponha que queremos estimar \\theta e f_n(x|\\theta) seja a pdf conjunta. Seja A(X) uma estat\u00edstica. Se a sua distribui\u00e7\u00e3o n\u00e3o depende de \\theta , ent\u00e3o ser\u00e1 uma estat\u00edstica ancillary (auxiliar?) Por exemplo, se X_1, X_2 \\sim N(\\mu, \\sigma^2) e \\mu \u00e9 desconhecido, temos que X_1 - X_2 \\sim N(0, 2\\sigma^2) \u00e9 uma estat\u00edstica auxiliar.","title":"Estat\u00edstica Ancillary"},{"location":"infestatistica_BSc/SufficientStatistics/#melhorando-um-estimador","text":"Suponha que temos uma amostra aleat\u00f3ria X = (X_1, ..., X_n) cuja pdf \u00e9 f(x|\\theta) e \\theta \\in \\Omega desconhecido, tal que queremos estimar h(\\theta) para alguma fun\u00e7\u00e3o h . Seja Z = g(X_1, ..., X_n) . E_{\\theta}(Z) = \\int_{-\\infty}^{\\infty}...\\int_{-\\infty}^{\\infty} g(x)f_n(x|\\theta)dx_1, ..., dx_n Para cada estimado \\delta(X) e para todo valor de \\theta , definimos o MSE (Erro M\u00e9dio Quadr\u00e1tico) R(\\theta, \\delta) = E_{\\theta}\\{[\\delta(X) - h(\\theta)]^2\\} Quando n\u00e3o atribu\u00edmos uma priori para \\theta , ent\u00e3o queremos encontrar um estimador para que o MSE seja pequeno para v\u00e1rios valores de \\theta . Seja T uma estat\u00edstica suficiente conhecida. Definimos \\delta_0(T) = E_{\\theta}\\{\\delta(X)|T\\} \\overset{1}{=} E\\{\\delta(X)|T\\} (1) Agora, por que podemos chamar \\delta_0 de estimador se depende de \\theta ? Como T \u00e9 uma estat\u00edstica suficiente, a distribui\u00e7\u00e3o condicionada em T e em \\theta da amostra X_1, ..., X_n n\u00e3o depende de \\theta !!! Em particular o valor esperado do estimador \\delta(T) . Logo, como esse valor esperado n\u00e3o depende de \\theta , podemos dizer sim que ele \u00e9 um estimador.","title":"Melhorando um Estimador"},{"location":"infestatistica_BSc/SufficientStatistics/#teorema-rao-blackwell","text":"Teorema 7.9.1 do livro. Seja \\delta(X) um estimador e T uma estat\u00edstica suficiente para \\theta . O estimador \\delta_0(T) definido acima, para todo valor \\theta \\in \\Omega \u00e9: R(\\theta, \\delta_0) \\leq R(\\theta, \\delta), isto \u00e9, \u00e9 um estimador com menor erro quadr\u00e1tico m\u00e9dio (MSE). Em particular se R(\\theta, \\delta) < \\infty , a desigualdade se torna estrita, a menos que \\delta(X) seja um afun\u00e7\u00e3o de T , isto \u00e9, se \\delta(X) n\u00e3o for fun\u00e7\u00e3o de T , ent\u00e3o a desigualdade ser\u00e1 estrita. Por desigualdade estrita entenda < . Obs.: Chamamos o processo de melhorar um estimador com esse teorema de \"Rao-Blackwelliation\". Obs.2: Podemos generalizar um pouco mais. Para isso, pesquise sobre Conjuntos Convexos e sobre Fun\u00e7\u00f5es Convexas . Em um conjunto convexo, se a nossa fun\u00e7\u00e3o de perda n\u00e3o for o MSE, mas for uma fun\u00e7\u00e3o convexa, o teorema tamb\u00e9m valer\u00e1. Uma suposi\u00e7\u00e3o interessante que o Livro n\u00e3o imp\u00f5e \u00e9 que E[||\\delta(X)||) < \\infty .","title":"Teorema Rao - Blackwell"},{"location":"infestatistica_BSc/SufficientStatistics/#inadmissibilidade","text":"Suponha que R(\\theta, \\delta) \u00e9 MSE. O estimador \\delta \u00e9 inadimiss\u00edvel se existe outro estimador \\delta_0 tal que R(\\theta, \\delta_0) \\leq R(\\theta, \\delta) para todo valor de \\theta e existe a desigualdade estrita em, pelo menos um valor de \\theta . Dizemos nesse caso que \\delta_0 domina o estimador \\delta . Um estimador \\delta_0 \u00e9 admiss\u00edvel se n\u00e3o existe outro estimador que o domine.","title":"Inadmissibilidade"},{"location":"infestatistica_BSc/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de Infer\u00eancia Estat\u00edstica correspondente ao per\u00edodo de 2020.2. Notebooks : Voc\u00ea pode rodar esses notebooks tamb\u00e9m! GitHub Professor T\u00f3picos Nessa se\u00e7\u00e3o, os t\u00f3picos discutidos nas aulas s\u00e3o apresentados conjuntamente a exemplos. Todos os textos foram desenvolvidos em Jupyter Notebook , com a Linguagem de Programa\u00e7\u00e3o Python. Conceitos Introdut\u00f3rios Grandes Amostras Introdu\u00e7\u00e3o Priori e Posteriori Distribui\u00e7\u00e3o Amostral Estimadores Distribui\u00e7\u00f5es Conjugada e Estimador de Bayes Estimador de M\u00e1xima Verossimilhan\u00e7a Estat\u00edsticas Suficientes Intervalos de Confian\u00e7a An\u00e1lise Bayesiana da Normal Estimadores n\u00e3o enviesados Informa\u00e7\u00e3o de Fisher Testes de Hip\u00f3tese Teste de Hip\u00f3teses Teste de Hip\u00f3teses II Exemplos B\u00e1sicos Testes de Hip\u00f3tese Testes Uniformemente mais Poderosos Modelos Lineares Modelos Lineares Simples Exerc\u00edcios Os exerc\u00edcios resolvidos dispon\u00edveis est\u00e3o lincados ao t\u00edtulo da se\u00e7\u00e3o. A1 Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios 7.2 Priori e Posteriori 2,3,10 7.3 Fam\u00edlias Conjugadas 2,17,19,21 7.4 Estimador de Bayes 2,4,7,11,14 7.5 Estimador de M\u00e1xima Verossimilhan\u00e7a 1,4,9,10 7.6 Propriedades EMV 3,5,11,20,22,23 7.7 Estat\u00edstica Suficiente 4,7,13,16 7.8 Estat\u00edstica Suficiente Conjunta 3,8,12,16 7.9 Melhorando um Estimador 2,3,6,9,10 8.7 Estimadores n\u00e3o enviesados 4,6,11,13 8.8 Informa\u00e7\u00e3o de Fisher 5,7,10 8.1 Estimadores de Distribui\u00e7\u00f5es Amostrais 1,2,3,9 8.2 Distribui\u00e7\u00e3o Chi-Quadrada 4,7,10,13 - Exerc\u00edcios de Revis\u00e3o A1 - A2 Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios 8.3 Distribui\u00e7\u00e3o conjunta da m\u00e9dia e vari\u00e2ncia amostral 8 8.4 A distribui\u00e7\u00e3o t Derivar a distribui\u00e7\u00e3o 8.5 Intervalos de Confian\u00e7a 1,4,5,6 9.1 Problemas de Teste de Hip\u00f3teses 3,8,13,19,21 9.5 Teste t 4,5,8 9.6 Comparando as m\u00e9dias de Normais - 9.7 Teste F Derivar a distribui\u00e7\u00e3o e teste de raz\u00e3o de verossimilhan\u00e7as 11.1 M\u00e9todo de M\u00ednimos Quadrados 3 11.2 Regress\u00e3o 2,3,6,19 11.3 Infer\u00eancia Estat\u00edstica sobre Regress\u00e3o Linear Simples - - Exerc\u00edcios de Revis\u00e3o A2 - Resumos Probability and Statistics (Morris H. DeGroot) Cap\u00edtulo 7 Cap\u00edtulo 8 - parte 1 Cap\u00edtulo 8 - parte 2 Cap\u00edtulo 9 - Defini\u00e7\u00f5es Cap\u00edtulo 9 - Testes Documentos Adicionais Theory of Statistical Estimation (Ronald Fisher) : problema da estima\u00e7\u00e3o \u00e9 abordado e as t\u00e9cnicas apresentadas por Ronald Fisher. Ele se debru\u00e7a sobre estat\u00edsticas eficientes e suficientes. Mathematical Foudations Statistics (Ronald Fisher) : refer\u00eancia em estat\u00edstica com principais conceitos da mat\u00e9ria. Digital TextBook Statlect Bayesian x Frequentist Inference","title":"Infer\u00eancia Estat\u00edstica"},{"location":"infestatistica_BSc/info/#informacoes-gerais","text":"Monitoria de Infer\u00eancia Estat\u00edstica correspondente ao per\u00edodo de 2020.2. Notebooks : Voc\u00ea pode rodar esses notebooks tamb\u00e9m! GitHub Professor","title":"Informa\u00e7\u00f5es Gerais"},{"location":"infestatistica_BSc/info/#topicos","text":"Nessa se\u00e7\u00e3o, os t\u00f3picos discutidos nas aulas s\u00e3o apresentados conjuntamente a exemplos. Todos os textos foram desenvolvidos em Jupyter Notebook , com a Linguagem de Programa\u00e7\u00e3o Python. Conceitos Introdut\u00f3rios Grandes Amostras Introdu\u00e7\u00e3o Priori e Posteriori Distribui\u00e7\u00e3o Amostral Estimadores Distribui\u00e7\u00f5es Conjugada e Estimador de Bayes Estimador de M\u00e1xima Verossimilhan\u00e7a Estat\u00edsticas Suficientes Intervalos de Confian\u00e7a An\u00e1lise Bayesiana da Normal Estimadores n\u00e3o enviesados Informa\u00e7\u00e3o de Fisher Testes de Hip\u00f3tese Teste de Hip\u00f3teses Teste de Hip\u00f3teses II Exemplos B\u00e1sicos Testes de Hip\u00f3tese Testes Uniformemente mais Poderosos Modelos Lineares Modelos Lineares Simples","title":"T\u00f3picos"},{"location":"infestatistica_BSc/info/#exercicios","text":"Os exerc\u00edcios resolvidos dispon\u00edveis est\u00e3o lincados ao t\u00edtulo da se\u00e7\u00e3o.","title":"Exerc\u00edcios"},{"location":"infestatistica_BSc/info/#a1","text":"Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios 7.2 Priori e Posteriori 2,3,10 7.3 Fam\u00edlias Conjugadas 2,17,19,21 7.4 Estimador de Bayes 2,4,7,11,14 7.5 Estimador de M\u00e1xima Verossimilhan\u00e7a 1,4,9,10 7.6 Propriedades EMV 3,5,11,20,22,23 7.7 Estat\u00edstica Suficiente 4,7,13,16 7.8 Estat\u00edstica Suficiente Conjunta 3,8,12,16 7.9 Melhorando um Estimador 2,3,6,9,10 8.7 Estimadores n\u00e3o enviesados 4,6,11,13 8.8 Informa\u00e7\u00e3o de Fisher 5,7,10 8.1 Estimadores de Distribui\u00e7\u00f5es Amostrais 1,2,3,9 8.2 Distribui\u00e7\u00e3o Chi-Quadrada 4,7,10,13 - Exerc\u00edcios de Revis\u00e3o A1 -","title":"A1"},{"location":"infestatistica_BSc/info/#a2","text":"Se\u00e7\u00e3o T\u00edtulo Exerc\u00edcios 8.3 Distribui\u00e7\u00e3o conjunta da m\u00e9dia e vari\u00e2ncia amostral 8 8.4 A distribui\u00e7\u00e3o t Derivar a distribui\u00e7\u00e3o 8.5 Intervalos de Confian\u00e7a 1,4,5,6 9.1 Problemas de Teste de Hip\u00f3teses 3,8,13,19,21 9.5 Teste t 4,5,8 9.6 Comparando as m\u00e9dias de Normais - 9.7 Teste F Derivar a distribui\u00e7\u00e3o e teste de raz\u00e3o de verossimilhan\u00e7as 11.1 M\u00e9todo de M\u00ednimos Quadrados 3 11.2 Regress\u00e3o 2,3,6,19 11.3 Infer\u00eancia Estat\u00edstica sobre Regress\u00e3o Linear Simples - - Exerc\u00edcios de Revis\u00e3o A2 -","title":"A2"},{"location":"infestatistica_BSc/info/#resumos","text":"Probability and Statistics (Morris H. DeGroot) Cap\u00edtulo 7 Cap\u00edtulo 8 - parte 1 Cap\u00edtulo 8 - parte 2 Cap\u00edtulo 9 - Defini\u00e7\u00f5es Cap\u00edtulo 9 - Testes","title":"Resumos"},{"location":"infestatistica_BSc/info/#documentos-adicionais","text":"Theory of Statistical Estimation (Ronald Fisher) : problema da estima\u00e7\u00e3o \u00e9 abordado e as t\u00e9cnicas apresentadas por Ronald Fisher. Ele se debru\u00e7a sobre estat\u00edsticas eficientes e suficientes. Mathematical Foudations Statistics (Ronald Fisher) : refer\u00eancia em estat\u00edstica com principais conceitos da mat\u00e9ria. Digital TextBook Statlect Bayesian x Frequentist Inference","title":"Documentos Adicionais"},{"location":"infestatistica_BSc/BayesianAnalysisNormal/BayesianAnalysisNormal/","text":"An\u00e1lise Bayesiana de amostras da distribui\u00e7\u00e3o Normal Refer\u00eancias [1] Andrew Gelman. Bayesian Data Analysis. Parte I, Se\u00e7\u00e3o 3.3: essa refer\u00eancia usa a vari\u00e2ncia ao inv\u00e9s da precis\u00e3o. [2] Kevin P. Murphy. Conjugate Bayesian analysis of the Gaussian distribution: N\u00e3o muito simples, mas possui todas as contas. Precis\u00e3o da distribui\u00e7\u00e3o normal Definimos \\tau := 1/\\sigma^2 como a precis\u00e3o da distribui\u00e7\u00e3o normal. A fun\u00e7\u00e3o da densidade de probabilidade da distribui\u00e7\u00e3o normal f(x|\\mu,\\tau) \u00e9, -\\infty < x < \\infty : f(x|\\mu,\\tau) = \\left(\\frac{\\tau}{2\\pi}\\right)^{1/2}\\exp\\left[-\\frac{1}{2}\\tau(x-\\mu)^2\\right] Teorema Suponha que X_1,...,X_n \\overset{iid}{\\sim} N_2(\\mu, \\tau) , desconhecidos. Suponha que \\mu|\\tau \\sim N_2(\\mu_0, \\lambda_0 \\tau) \\tau \\sim \\text{Gamma}(\\alpha_0, \\beta_0) Ent\u00e3o a distribui\u00e7\u00e3o conjunta de \\mu e \\tau a posteriori \u00e9 dada por: \\mu|\\tau, X_1,...,X_n \\sim N_2(\\mu_1, \\lambda_1\\tau) \\tau|X_1,...,X_n \\sim \\text{Gamma}(\\alpha_1, \\beta_1), onde Par\u00e2metro Valor a posteriori do par\u00e2metro \\mu_1 \\frac{\\lambda_0 \\mu_0 + n\\bar{x}_n}{\\lambda_0 + n} \\lambda_1 \\lambda_0 + n \\alpha_1 \\alpha_0 + n/2 \\beta_1 \\beta_0 + s_n^2/2 + \\frac{n\\lambda_0(\\bar{x}_n - \\mu_0)^2}{2(\\lambda_0 + n)} Fam\u00edlia Normal-Gamma Sejam \\mu e \\tau vari\u00e1veis aleat\u00f3rias. Suponha que a distribui\u00e7\u00e3o condicional de \\mu dado \\tau \u00e9 normal com m\u00e9dia \\mu_0 e precis\u00e3o \\lambda_0 \\tau e que a distribui\u00e7\u00e3o marginal de \\tau seja gamma com par\u00e2metros \\alpha_0, \\beta_0 . Ent\u00e3o, falamos que a distribui\u00e7\u00e3o conjunta de \\mu e \\tau \u00e9 a distribui\u00e7\u00e3o normal-gamma com hiperpar\u00e2meteros \\mu_0, \\lambda_0, \\alpha_0 e \\beta_0 . A distribui\u00e7\u00e3o marginal da m\u00e9dia \\mu Suponha que a distribui\u00e7\u00e3o a priori \\mu e \\tau seja normal-gamma com hiperpar\u00e2metros \\mu_0, \\lambda_0, \\alpha_0 e \\beta_0 . Ent\u00e3o \\left(\\frac{\\lambda_0\\alpha_0}{\\beta_0}\\right)^{1/2}(\\mu - \\mu_0) tem a distribui\u00e7\u00e3o t com 2\\alpha_0 graus de liberdade. Nesse caso, se \\alpha_0 > 1/2 , E(\\mu) = \\mu_0 . Se \\alpha_0 > 1 , Var(\\mu) \\frac{\\beta_0}{\\lambda_0(\\alpha_0 - 1)} Obs.: As condi\u00e7\u00f5es sobre \\alpha_0 , vem da exist\u00eancia do k momento somente se o grau de liberdade da distribui\u00e7\u00e3o t \u00e9 maior do que k , isto \u00e9, 2\\alpha_0 > k . Compara\u00e7\u00e3o com Intervalos de Confian\u00e7a Podemos construir intervalos de confian\u00e7a para \\mu no mundo Bayesiano, pois ela \u00e9 uma vari\u00e1vel aleat\u00f3ria. Nesse caso, podemos fazer da seguinte maneira: \\begin{split} &P(-c < \\left(\\frac{\\lambda_0\\alpha_0}{\\beta_0}\\right)^{1/2}(\\mu - \\mu_0) < c) = \\gamma \\\\ &P(\\mu_0 - c\\left(\\frac{\\beta_0}{\\lambda_0\\alpha_0}\\right)^{1/2} < \\mu < \\mu_0 + c\\left(\\frac{\\beta_0}{\\lambda_0\\alpha_0}\\right)^{1/2}) &= \\gamma \\end{split} Ou seja, ganhamos um intervalo de confian\u00e7a de gra\u00e7a! Implementa\u00e7\u00e3o Quando temos dados, X_1, ..., X_n \\sim N(\\mu, \\sigma^2) , podemos fazer \\hat{\\mu} = \\bar{X}_n que \u00e9 uma pontual (s\u00f3 para um valor de \\mu ). Mas isso nem sempre \u00e9 o melhor, e \u00e0s vezes nem t\u00e3o prazeroso. Por isso precisamos trazer Bayes para nossa an\u00e1lise. import numpy as np from scipy.stats import norm, gamma,t import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set() Nesse exemplos, vamos estudar o exemplo Casas de Enfermagem em Novo M\u00e9xico , tema estudado por Howard L. Smith, Neill F. Piland e Nancy Fisher . Nesse trabalho explora os desafios da enfermagem na \u00e1rea rural, para que se mantenha a viabilidade. Vamos utilizar o banco de dados deles, que incluem as seguintes informa\u00e7\u00f5es: BED: n\u00fameros de camas na casa. MCDAYS: dias anuais de interna\u00e7\u00e3o m\u00e9dica (centenas) TDAYS: total anual de pacientes dias (centenas) PCREV: receita anual total de atendimento ao paciente (centenas) NSAL: sal\u00e1rio das enfermeiras anual (centenas) FEXP: despesas anuais com instala\u00e7\u00f5es (centenas) RURAL: 1, se rural, 0, se urbano Os dados se encontram nesse site . table = [] with open('../data/nursinghome.txt', 'r') as f: line = f.readline() table.append(line.split()) line = f.readline() while line != '': table.append([int(i) for i in line.split()]) line = f.readline() nurse_df = pd.DataFrame(data = table[1:], columns = table[0], dtype = np.int) Vamos considerar nesse exemplo a coluna MCDAYS, restrita \u00e0s casas urbanas, que denotaremos por X . Antes de observarmos os dados, vamos modelar o valor de X para cada casa como uma vari\u00e1vel aleat\u00f3ria normal com m\u00e9dia \\mu e precis\u00e3o \\tau . urban_nurse_df = nurse_df[nurse_df.RURAL == 0] plt.bar(x = range(len(urban_nurse_df)), height = urban_nurse_df.MCDAYS) plt.title('MCDAYS por casa urbana de enfermeiras') plt.show() Para calcular nossa priori, dever\u00edamos conversar com especialistas. Como n\u00e3o \u00e9 o caso, vamos usar as informa\u00e7\u00f5es de camas. Temos: print('M\u00e9dia: {:.1f}, Desvio-Padr\u00e3o: {:.1f}'. format(np.mean(urban_nurse_df.BED), np.std(urban_nurse_df.BED))) M\u00e9dia: 111.4, Desvio-Padr\u00e3o: 42.3 Podemos supor, a priori, que a taxa de ocupa\u00e7\u00e3o \u00e9 de 50%. Logo, em um ano, podemos obter que os dias anuais de interna\u00e7\u00e3o m\u00e9dica s\u00e3o: media = 0.5*365*np.mean(urban_nurse_df.BED)/100 # unidade em centenas std = 0.5*365*np.std(urban_nurse_df.BED)/100 Agora precisamos mapear esses valores para os hiperpar\u00e2metros a priori \\alpha_0, \\beta_0, \\lambda_0, \\mu_0 . Vamos dibidir a vari\u00e2ncia obtida acima (usando o n\u00famero de camas), como incerteza, e dividiremos igualmente essa incerteza sobre a m\u00e9dia e a precis\u00e3o, isto \u00e9, Var(\\mu) = std^2/2 E(\\tau) = 1/(std^2/2) Escolhemos \\alpha_0 = 2 (arbitr\u00e1rio, mas prefer\u00edvel a ser pequeno, porque esse par\u00e2metro tem a interpreta\u00e7\u00e3o de ser o conhecimento sobre o valor. Logo E[\\tau] = \\alpha_0/\\beta_0 \\implies \\beta_0 = \\alpha_0/E[\\tau] E[\\mu] = \\mu_0 Var(\\mu) = \\frac{\\beta_0}{\\lambda_0(\\alpha_0 - 1)} \\implies \\lambda_0 = \\frac{\\beta_0}{Var(\\mu)(\\alpha_0 - 1)} alpha0 = 2 beta0 = alpha*std**2/2 mu0 = media lambda0 = beta*2/(std**2*(alpha - 1)) Agora conseguimos expressar nosso conhecimento a priori para construir o primeiro intervalo de confian\u00e7a. Vamos usar a express\u00e3o constru\u00edda anteriormente. Primeiro, vamos ver, numericamente, a distribui\u00e7\u00e3o de \\mu def draw_samples(alpha0, beta0, lambda0, mu0, seed = 10000): r = np.random.RandomState(seed) tau = r.gamma(shape = alpha0, scale = 1/beta0, size = 100000) mu = r.normal(mu0, scale = np.sqrt(1/(lambda0*tau))) return mu, tau mu_samples, tau_samples = draw_samples(alpha0, beta0, lambda0, mu0) fig, ax = plt.subplots(1,2, figsize = (10,5)) sns.histplot(mu_samples, ax = ax[0]) sns.histplot(tau_samples, ax = ax[1]) ax[0].set_title(r'$\\mu$ a priori') ax[1].set_title(r'$\\tau$ a priori') ax[0].set_xlim((0,500)) plt.show() ci_mu = (np.quantile(mu_samples, 0.025), np.quantile(mu_samples, 0.975)) print(ci_mu) (95.85458277508882, 309.0974651995673) Agora, vamos observar os dados e calcular nossa posteriori! Vamos atualizar como demonstrado no Teorema. Par\u00e2metro Valor a posteriori do par\u00e2metro \\mu_1 \\frac{\\lambda_0 \\mu_0 + n\\bar{x}_n}{\\lambda_0 + n} \\lambda_1 \\lambda_0 + n \\alpha_1 \\alpha_0 + n/2 \\beta_1 \\beta_0 + s_n^2/2 + \\frac{n\\lambda_0(\\bar{x}_n - \\mu_0)^2}{2(\\lambda_0 + n)} n = len(urban_nurse_df) mom1 = np.mean(urban_nurse_df.MCDAYS) mom2 = np.var(urban_nurse_df.MCDAYS)*n mu1 = (lambda0*mu0 + n*mom1)/(lambda0 + n) lambda1 = lambda0 + n alpha1 = alpha0 + n/2 beta1 = beta0 + mom2/2 + (n*lambda0*(mom1 - mu0)**2)/(2*(lambda0 + n)) mu_samples_post, tau_samples_post = draw_samples(alpha1, beta1, lambda1, mu1) fig, ax = plt.subplots(1,2, figsize = (10,5)) sns.histplot(mu_samples_post, ax = ax[0]) sns.histplot(tau_samples_post, ax = ax[1]) ax[0].set_title(r'$\\mu$ a posteriori') ax[1].set_title(r'$\\tau$ a posteriori') ax[0].set_xlim((100,300)) plt.show() ci_mu = (np.quantile(mu_samples_post, 0.025), np.quantile(mu_samples_post, 0.975)) print(ci_mu) (152.61993663355443, 215.58303707725233) Observe como o intervalo de confian\u00e7a est\u00e1 bem menor! Ou seja, os dados aumentaram nossa certeza sobre o par\u00e2metro. Vamos comparar os valores. fig, ax = plt.subplots(figsize = (10,5)) sns.histplot(mu_samples_post, ax = ax, kde = True, stat = 'density', label = 'Posteriori') sns.histplot(mu_samples, ax = ax, kde = True, stat = 'density', label = 'Priori', color = 'red') ax.set_title(r'$\\mu$') ax.set_xlim((-200,400)) ax.legend() plt.show() Por fim, vamos comparar o estimador de Bayes com o de M\u00e1xima Verossimilhan\u00e7a: mle = mom1 eb = np.mean(mu_samples_post) print('MLE: {}'.format(mle)) print('Bayes Estimator: {}'.format(eb)) MLE: 182.16666666666666 Bayes Estimator: 184.21760983937554","title":"An\u00e1lise Bayesiana de amostras da distribui\u00e7\u00e3o Normal"},{"location":"infestatistica_BSc/BayesianAnalysisNormal/BayesianAnalysisNormal/#analise-bayesiana-de-amostras-da-distribuicao-normal","text":"","title":"An\u00e1lise Bayesiana de amostras da distribui\u00e7\u00e3o Normal"},{"location":"infestatistica_BSc/BayesianAnalysisNormal/BayesianAnalysisNormal/#referencias","text":"[1] Andrew Gelman. Bayesian Data Analysis. Parte I, Se\u00e7\u00e3o 3.3: essa refer\u00eancia usa a vari\u00e2ncia ao inv\u00e9s da precis\u00e3o. [2] Kevin P. Murphy. Conjugate Bayesian analysis of the Gaussian distribution: N\u00e3o muito simples, mas possui todas as contas.","title":"Refer\u00eancias"},{"location":"infestatistica_BSc/BayesianAnalysisNormal/BayesianAnalysisNormal/#precisao-da-distribuicao-normal","text":"Definimos \\tau := 1/\\sigma^2 como a precis\u00e3o da distribui\u00e7\u00e3o normal. A fun\u00e7\u00e3o da densidade de probabilidade da distribui\u00e7\u00e3o normal f(x|\\mu,\\tau) \u00e9, -\\infty < x < \\infty : f(x|\\mu,\\tau) = \\left(\\frac{\\tau}{2\\pi}\\right)^{1/2}\\exp\\left[-\\frac{1}{2}\\tau(x-\\mu)^2\\right]","title":"Precis\u00e3o da distribui\u00e7\u00e3o normal"},{"location":"infestatistica_BSc/BayesianAnalysisNormal/BayesianAnalysisNormal/#teorema","text":"Suponha que X_1,...,X_n \\overset{iid}{\\sim} N_2(\\mu, \\tau) , desconhecidos. Suponha que \\mu|\\tau \\sim N_2(\\mu_0, \\lambda_0 \\tau) \\tau \\sim \\text{Gamma}(\\alpha_0, \\beta_0) Ent\u00e3o a distribui\u00e7\u00e3o conjunta de \\mu e \\tau a posteriori \u00e9 dada por: \\mu|\\tau, X_1,...,X_n \\sim N_2(\\mu_1, \\lambda_1\\tau) \\tau|X_1,...,X_n \\sim \\text{Gamma}(\\alpha_1, \\beta_1), onde Par\u00e2metro Valor a posteriori do par\u00e2metro \\mu_1 \\frac{\\lambda_0 \\mu_0 + n\\bar{x}_n}{\\lambda_0 + n} \\lambda_1 \\lambda_0 + n \\alpha_1 \\alpha_0 + n/2 \\beta_1 \\beta_0 + s_n^2/2 + \\frac{n\\lambda_0(\\bar{x}_n - \\mu_0)^2}{2(\\lambda_0 + n)}","title":"Teorema"},{"location":"infestatistica_BSc/BayesianAnalysisNormal/BayesianAnalysisNormal/#familia-normal-gamma","text":"Sejam \\mu e \\tau vari\u00e1veis aleat\u00f3rias. Suponha que a distribui\u00e7\u00e3o condicional de \\mu dado \\tau \u00e9 normal com m\u00e9dia \\mu_0 e precis\u00e3o \\lambda_0 \\tau e que a distribui\u00e7\u00e3o marginal de \\tau seja gamma com par\u00e2metros \\alpha_0, \\beta_0 . Ent\u00e3o, falamos que a distribui\u00e7\u00e3o conjunta de \\mu e \\tau \u00e9 a distribui\u00e7\u00e3o normal-gamma com hiperpar\u00e2meteros \\mu_0, \\lambda_0, \\alpha_0 e \\beta_0 .","title":"Fam\u00edlia Normal-Gamma"},{"location":"infestatistica_BSc/BayesianAnalysisNormal/BayesianAnalysisNormal/#a-distribuicao-marginal-da-media-mu","text":"Suponha que a distribui\u00e7\u00e3o a priori \\mu e \\tau seja normal-gamma com hiperpar\u00e2metros \\mu_0, \\lambda_0, \\alpha_0 e \\beta_0 . Ent\u00e3o \\left(\\frac{\\lambda_0\\alpha_0}{\\beta_0}\\right)^{1/2}(\\mu - \\mu_0) tem a distribui\u00e7\u00e3o t com 2\\alpha_0 graus de liberdade. Nesse caso, se \\alpha_0 > 1/2 , E(\\mu) = \\mu_0 . Se \\alpha_0 > 1 , Var(\\mu) \\frac{\\beta_0}{\\lambda_0(\\alpha_0 - 1)} Obs.: As condi\u00e7\u00f5es sobre \\alpha_0 , vem da exist\u00eancia do k momento somente se o grau de liberdade da distribui\u00e7\u00e3o t \u00e9 maior do que k , isto \u00e9, 2\\alpha_0 > k .","title":"A distribui\u00e7\u00e3o marginal da m\u00e9dia \\mu"},{"location":"infestatistica_BSc/BayesianAnalysisNormal/BayesianAnalysisNormal/#comparacao-com-intervalos-de-confianca","text":"Podemos construir intervalos de confian\u00e7a para \\mu no mundo Bayesiano, pois ela \u00e9 uma vari\u00e1vel aleat\u00f3ria. Nesse caso, podemos fazer da seguinte maneira: \\begin{split} &P(-c < \\left(\\frac{\\lambda_0\\alpha_0}{\\beta_0}\\right)^{1/2}(\\mu - \\mu_0) < c) = \\gamma \\\\ &P(\\mu_0 - c\\left(\\frac{\\beta_0}{\\lambda_0\\alpha_0}\\right)^{1/2} < \\mu < \\mu_0 + c\\left(\\frac{\\beta_0}{\\lambda_0\\alpha_0}\\right)^{1/2}) &= \\gamma \\end{split} Ou seja, ganhamos um intervalo de confian\u00e7a de gra\u00e7a!","title":"Compara\u00e7\u00e3o com Intervalos de Confian\u00e7a"},{"location":"infestatistica_BSc/BayesianAnalysisNormal/BayesianAnalysisNormal/#implementacao","text":"Quando temos dados, X_1, ..., X_n \\sim N(\\mu, \\sigma^2) , podemos fazer \\hat{\\mu} = \\bar{X}_n que \u00e9 uma pontual (s\u00f3 para um valor de \\mu ). Mas isso nem sempre \u00e9 o melhor, e \u00e0s vezes nem t\u00e3o prazeroso. Por isso precisamos trazer Bayes para nossa an\u00e1lise. import numpy as np from scipy.stats import norm, gamma,t import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set() Nesse exemplos, vamos estudar o exemplo Casas de Enfermagem em Novo M\u00e9xico , tema estudado por Howard L. Smith, Neill F. Piland e Nancy Fisher . Nesse trabalho explora os desafios da enfermagem na \u00e1rea rural, para que se mantenha a viabilidade. Vamos utilizar o banco de dados deles, que incluem as seguintes informa\u00e7\u00f5es: BED: n\u00fameros de camas na casa. MCDAYS: dias anuais de interna\u00e7\u00e3o m\u00e9dica (centenas) TDAYS: total anual de pacientes dias (centenas) PCREV: receita anual total de atendimento ao paciente (centenas) NSAL: sal\u00e1rio das enfermeiras anual (centenas) FEXP: despesas anuais com instala\u00e7\u00f5es (centenas) RURAL: 1, se rural, 0, se urbano Os dados se encontram nesse site . table = [] with open('../data/nursinghome.txt', 'r') as f: line = f.readline() table.append(line.split()) line = f.readline() while line != '': table.append([int(i) for i in line.split()]) line = f.readline() nurse_df = pd.DataFrame(data = table[1:], columns = table[0], dtype = np.int) Vamos considerar nesse exemplo a coluna MCDAYS, restrita \u00e0s casas urbanas, que denotaremos por X . Antes de observarmos os dados, vamos modelar o valor de X para cada casa como uma vari\u00e1vel aleat\u00f3ria normal com m\u00e9dia \\mu e precis\u00e3o \\tau . urban_nurse_df = nurse_df[nurse_df.RURAL == 0] plt.bar(x = range(len(urban_nurse_df)), height = urban_nurse_df.MCDAYS) plt.title('MCDAYS por casa urbana de enfermeiras') plt.show() Para calcular nossa priori, dever\u00edamos conversar com especialistas. Como n\u00e3o \u00e9 o caso, vamos usar as informa\u00e7\u00f5es de camas. Temos: print('M\u00e9dia: {:.1f}, Desvio-Padr\u00e3o: {:.1f}'. format(np.mean(urban_nurse_df.BED), np.std(urban_nurse_df.BED))) M\u00e9dia: 111.4, Desvio-Padr\u00e3o: 42.3 Podemos supor, a priori, que a taxa de ocupa\u00e7\u00e3o \u00e9 de 50%. Logo, em um ano, podemos obter que os dias anuais de interna\u00e7\u00e3o m\u00e9dica s\u00e3o: media = 0.5*365*np.mean(urban_nurse_df.BED)/100 # unidade em centenas std = 0.5*365*np.std(urban_nurse_df.BED)/100 Agora precisamos mapear esses valores para os hiperpar\u00e2metros a priori \\alpha_0, \\beta_0, \\lambda_0, \\mu_0 . Vamos dibidir a vari\u00e2ncia obtida acima (usando o n\u00famero de camas), como incerteza, e dividiremos igualmente essa incerteza sobre a m\u00e9dia e a precis\u00e3o, isto \u00e9, Var(\\mu) = std^2/2 E(\\tau) = 1/(std^2/2) Escolhemos \\alpha_0 = 2 (arbitr\u00e1rio, mas prefer\u00edvel a ser pequeno, porque esse par\u00e2metro tem a interpreta\u00e7\u00e3o de ser o conhecimento sobre o valor. Logo E[\\tau] = \\alpha_0/\\beta_0 \\implies \\beta_0 = \\alpha_0/E[\\tau] E[\\mu] = \\mu_0 Var(\\mu) = \\frac{\\beta_0}{\\lambda_0(\\alpha_0 - 1)} \\implies \\lambda_0 = \\frac{\\beta_0}{Var(\\mu)(\\alpha_0 - 1)} alpha0 = 2 beta0 = alpha*std**2/2 mu0 = media lambda0 = beta*2/(std**2*(alpha - 1)) Agora conseguimos expressar nosso conhecimento a priori para construir o primeiro intervalo de confian\u00e7a. Vamos usar a express\u00e3o constru\u00edda anteriormente. Primeiro, vamos ver, numericamente, a distribui\u00e7\u00e3o de \\mu def draw_samples(alpha0, beta0, lambda0, mu0, seed = 10000): r = np.random.RandomState(seed) tau = r.gamma(shape = alpha0, scale = 1/beta0, size = 100000) mu = r.normal(mu0, scale = np.sqrt(1/(lambda0*tau))) return mu, tau mu_samples, tau_samples = draw_samples(alpha0, beta0, lambda0, mu0) fig, ax = plt.subplots(1,2, figsize = (10,5)) sns.histplot(mu_samples, ax = ax[0]) sns.histplot(tau_samples, ax = ax[1]) ax[0].set_title(r'$\\mu$ a priori') ax[1].set_title(r'$\\tau$ a priori') ax[0].set_xlim((0,500)) plt.show() ci_mu = (np.quantile(mu_samples, 0.025), np.quantile(mu_samples, 0.975)) print(ci_mu) (95.85458277508882, 309.0974651995673) Agora, vamos observar os dados e calcular nossa posteriori! Vamos atualizar como demonstrado no Teorema. Par\u00e2metro Valor a posteriori do par\u00e2metro \\mu_1 \\frac{\\lambda_0 \\mu_0 + n\\bar{x}_n}{\\lambda_0 + n} \\lambda_1 \\lambda_0 + n \\alpha_1 \\alpha_0 + n/2 \\beta_1 \\beta_0 + s_n^2/2 + \\frac{n\\lambda_0(\\bar{x}_n - \\mu_0)^2}{2(\\lambda_0 + n)} n = len(urban_nurse_df) mom1 = np.mean(urban_nurse_df.MCDAYS) mom2 = np.var(urban_nurse_df.MCDAYS)*n mu1 = (lambda0*mu0 + n*mom1)/(lambda0 + n) lambda1 = lambda0 + n alpha1 = alpha0 + n/2 beta1 = beta0 + mom2/2 + (n*lambda0*(mom1 - mu0)**2)/(2*(lambda0 + n)) mu_samples_post, tau_samples_post = draw_samples(alpha1, beta1, lambda1, mu1) fig, ax = plt.subplots(1,2, figsize = (10,5)) sns.histplot(mu_samples_post, ax = ax[0]) sns.histplot(tau_samples_post, ax = ax[1]) ax[0].set_title(r'$\\mu$ a posteriori') ax[1].set_title(r'$\\tau$ a posteriori') ax[0].set_xlim((100,300)) plt.show() ci_mu = (np.quantile(mu_samples_post, 0.025), np.quantile(mu_samples_post, 0.975)) print(ci_mu) (152.61993663355443, 215.58303707725233) Observe como o intervalo de confian\u00e7a est\u00e1 bem menor! Ou seja, os dados aumentaram nossa certeza sobre o par\u00e2metro. Vamos comparar os valores. fig, ax = plt.subplots(figsize = (10,5)) sns.histplot(mu_samples_post, ax = ax, kde = True, stat = 'density', label = 'Posteriori') sns.histplot(mu_samples, ax = ax, kde = True, stat = 'density', label = 'Priori', color = 'red') ax.set_title(r'$\\mu$') ax.set_xlim((-200,400)) ax.legend() plt.show() Por fim, vamos comparar o estimador de Bayes com o de M\u00e1xima Verossimilhan\u00e7a: mle = mom1 eb = np.mean(mu_samples_post) print('MLE: {}'.format(mle)) print('Bayes Estimator: {}'.format(eb)) MLE: 182.16666666666666 Bayes Estimator: 184.21760983937554","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica_BSc/ConfidenceIntervals/ConfidenceIntervals/","text":"Intervalos de Confian\u00e7a Esse tema procura responder qu\u00e3o confian\u00e7a dever\u00edamos por em um estimador. \u00c9 claro que essa pergunta tem que ser um pouco melhor descrita matematicamente. A ideia \u00e9 frequentista e tem a ideia a seguinte forma: O intervalo [a,b] , uma realiza\u00e7\u00e3o de [A,B] , tem 95% de confian\u00e7a se em 95% do tempo, o par\u00e2metro procurado est\u00e1 entre a e b . Veja que a ideia \u00e9 basicamente frequentista, dado que a interpreta\u00e7\u00e3o est\u00e1 ligada \u00e0 frequ\u00eancia quando o n\u00famero de experimentos tende para infinito. (Cuidado: N\u00e3o vamos falar da probabilidade do par\u00e2metro estar em [a,b] , isso n\u00e3o faz sentido, pois \\theta n\u00e3o \u00e9 uma vari\u00e1vel aleat\u00f3ria, e sim um valor fixo). Defini\u00e7\u00e3o Seja X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Sejam A \\leq B duas estat\u00edsticas que possuem a propriedade, para todo \\theta , P(A < g(\\theta) < B) \\geq \\gamma Chamamos (A,B) de intevalo de confian\u00e7a para g(\\theta) com coeficiente \\gamma . O intervalo \u00e9 chamado de exato se ao inv\u00e9s da desigualdade, tivermos uma igualdade. Ap\u00f3s observarmos os valores de X_1, ..., X_n e computarmos A = a e B = b , o intervalo (a,b) \u00e9 chamado de valor observado do intervalo de confian\u00e7a. Intervalo de Confian\u00e7a para a m\u00e9dia de N(\\mu, \\sigma^2) Seja X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Para cada 0 < \\gamma < 1 , o intervalo (A,B) \u00e9 intervalo de confian\u00e7a exato para \\mu com coeficiente \\gamma , em que: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} onde T_{n-1} denota a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. O interessante \u00e9 que isso \u00e9 implica\u00e7\u00e3o direta da distribui\u00e7\u00e3o de U = \\frac{n^{1/2}(\\bar{X}_n - \\mu)}{\\sigma '} que inferimos no cap\u00edtulo 8.4, nesse caso, simplemente fizemos a transforma\u00e7\u00e3o: \\gamma = P(-c < U < c) = P(A < \\mu < B) e c \u00e9 escolhido de acordo com \\gamma . Implementa\u00e7\u00e3o Vamos rever a informa\u00e7\u00e3o sobre caf\u00e9 que usamos cap\u00edtulos antes para ver como isso acontece na pr\u00e1tica. Considere dados sobre pesos de beb\u00eas logo ao nascer. bwt: peso do beb\u00ea ao nascer. gestation: dura\u00e7\u00e3o em dias da gesta\u00e7\u00e3o. parity: primeiro filho ou n\u00e3o. age: idade da m\u00e3e. height: altura da m\u00e3e em polegadas. weight: peso da m\u00e3e em pounds. smoke: se a m\u00e3e \u00e9 fumante ou n\u00e3o. # Importando bibliotecas import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import t birth_df = pd.read_csv(\"http://people.reed.edu/~jones/141/Bwt.dat\") birth_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bwt gestation parity age height weight smoke 0 120 284 0 27 62 100 0 1 113 282 0 33 64 135 0 2 128 279 0 28 64 115 1 3 108 282 0 23 67 125 1 4 136 286 0 25 62 93 0 sns.histplot(data = birth_df.bwt, kde = True) plt.title('Histograma dos pesos dos beb\u00eas') plt.show() birth_df[birth_df.smoke == 0].bwt.hist(density = True, label = 'N\u00e3o fumante') birth_df[birth_df.smoke == 1].bwt.hist(density = True, label = 'Fumante', alpha = 0.6) plt.xlabel('Peso') plt.legend() plt.show() Sabemos que essa \u00e9 uma extra\u00e7\u00e3o de uma popula\u00e7\u00e3o maior. Para conseguirmos mais amostras, vamos usar um procedimento chamado bootstrap . A ideia desse procedimento \u00e9 criar um novas amostras a partir de uma amostra inicial, usando replace = True como diferencial. Vou fazer esse procedimento diversar vezes e ir calculando a m\u00e9dia amostral. Como a m\u00e9dia amostral \u00e9 uma vari\u00e1vel aleat\u00f3ria, vamos obter um histograma das realiza\u00e7\u00f5es. Vamos supor que o peso W_i da crian\u00e7a i vem de uma distribui\u00e7\u00e3o com par\u00e2metros \\mu e \\sigma^2 desconhecidos. Nesse caso, \\bar{W}_i vir\u00e1 de uma distribui\u00e7\u00e3o normal com par\u00e2metros \\mu e \\sigma^2/n . ite = 10000 n = 200 bootstrap_means = np.zeros(ite) for i in range(ite): bootstrap_sample = birth_df.sample(n = n, replace = True, random_state=i) bootstrap_means[i] = bootstrap_sample.bwt.mean() sns.histplot(bootstrap_means, kde = True) plt.title(\"M\u00e9dias das amostras\") plt.xlabel('Peso') plt.show() Vamos calcular o nosso intervalo de confian\u00e7a com \\gamma = 0.95 . Temos que: gamma = 0.95 A = lambda x: np.mean(x) - t.ppf(q = (1 + gamma)/2, df = len(x) - 1)*np.std(x, ddof = 1)/len(x)**(1/2) B = lambda x: np.mean(x) + t.ppf(q = (1 + gamma)/2, df = len(x) - 1)*np.std(x, ddof = 1)/len(x)**(1/2) ite = 100 n = 500 bootstrap_intervals = np.zeros((ite,2)) for i in range(ite): bootstrap_sample = birth_df.sample(n = n, replace = True, random_state=i) bootstrap_intervals[i,0] = A(bootstrap_sample.bwt) bootstrap_intervals[i,1] = B(bootstrap_sample.bwt) out_values = np.where((bootstrap_intervals[:,0] > 119.5) | (bootstrap_intervals[:,1] < 119.5)) plt.figure(figsize = (6,10)) plt.scatter(bootstrap_intervals[:,0], np.arange(0,ite), color = 'red', label = 'a') plt.scatter(bootstrap_intervals[:,1], np.arange(0,ite), color = 'green', label = 'b') plt.scatter(bootstrap_intervals[out_values[0],0], out_values[0], color = 'black', label = 'Fora') plt.scatter(bootstrap_intervals[out_values[0],1], out_values[0], color = 'black') plt.vlines(119.5, ymin = 0, ymax = ite, linestyle = '--', alpha = 0.6, label = 'M\u00e9dia real') plt.legend() plt.show() Interpreta\u00e7\u00e3o Estamos fazer uma afirma\u00e7\u00e3o probabil\u00edstica sobre o intervalo (A,B) antes de observar os dados. Ap\u00f3s observarmos os dados, n\u00e3o podemos interpretar (a,b) como um intervalo em que temos 95% de confian\u00e7a de g(\\theta) estar no intervalo. Antes de observarmos as amostras, temos a confian\u00e7a de que 95% dos intervalos conter\u00e3o \\mu . Sem simetria Construimos anteriormente um intervalo sim\u00e9trico, onde a estat\u00edstica U acima mencionada estaria entre -c e c com probabilidade \\gamma . Mas podemos desenvolver intervalos n\u00e3o sim\u00e9tricos tamb\u00e9m. Uma forma que podemos fazer isso \u00e9 escolhendo \\gamma_1 e \\gamma_2 , tal que \\gamma_2 - \\gamma_1 = \\gamma . Assim: P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) = \\gamma Talv\u00e9m vc esteja se perguntando: porque escolher \\gamma_1, \\gamma_2 dessa forma? Bom: \\begin{split} \\gamma &= P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) \\\\ &= P\\left(U < T_{n-1}^{-1}(\\gamma_2)\\right) - P\\left(U \\leq T_{n-1}^{-1}(\\gamma_1)\\right) \\\\ &= \\gamma_2 - \\gamma_1 \\end{split} Intervalos de Confian\u00e7a Unilateral Defini\u00e7\u00e3o Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Sejam A e B duas estat\u00edsticas tais que: P(A < g(\\theta)) \\geq \\gamma P(B > g(\\theta)) \\geq \\gamma Ent\u00e3o (A, \\infty) e (-\\infty, B) s\u00e3o chamados de intervalos de confia\u00e7a unilaterais para g(\\theta) de coeficiente \\gamma ou percentil 100\\gamma . No caso de A 100\\gamma porcento abaixo e no caso de B a cima. Se vale a igualdade, dizemos que o intervalor \u00e9 exato. Intervalo unilateral para a m\u00e9dia de N(\\mu,\\sigma^2) Nas mesma condi\u00e7\u00f5es do teorema anterior, mas as estat\u00edsticas para baixo e para cima com coeficiente \\gamma para \\mu s\u00e3o: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}} Pivotal Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Seja V(\\vec{X},\\theta) uma vari\u00e1vel aleat\u00f3ria cuja distribui\u00e7\u00e3o \u00e9 a mesma para \\theta . Chamamos V de quantidade pivotal . Teorema Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Suponha que Exista V pivotal. A cdf G de V \u00e9 cont\u00ednua. Exista fun\u00e7\u00e3o r tal r(V(X,\\theta), X) = g(\\theta) , ou seja, \u00e9 uma esp\u00e9cie de \"inversa\". r(v,x) (3) \u00e9 uma fun\u00e7\u00e3o estritamente crescente em v para todo x . Ent\u00e3o A = r(G^{-1}(\\gamma_1), X) B = r(G^{-1}(\\gamma_2), X) s\u00e3o os pontos extremos do intervalo de confian\u00e7a exato para g(\\theta) de coeficiente \\gamma = \\gamma_2 - \\gamma_1 . Se r \u00e9 estritamente decrescente, invertemos A e B . Obs.: Ainda podemos usar o Teorema Central do Limite para obter intervalos de confian\u00e7a assint\u00f3ticos. Exemplo com Regress\u00e3o Linear O dataset que utilizei anteriormente n\u00e3o \u00e9 muito bom para esse exemplo, mas eu vou usar, de qualquer forma, para entendermos o processo e como pode nos ajudar o intervalo de confian\u00e7a. Em uma Regress\u00e3o Linear, queremos dizer aferir uma rela\u00e7\u00e3o linear entre duas vari\u00e1veis, isto \u00e9, queremos dizer que uma vari\u00e1vel pode ser obtida pela outra atrav\u00e9s de uma reta, mais um erro aleat\u00f3rio. Suponha que queremos estimar Y o peso da crian\u00e7a ao nascer, sabendo a informa\u00e7\u00e3o do tempo de gesta\u00e7\u00e3o X e que Y = aX + b + E, onde E \\sim N(0,\\sigma^2) . Nesse caso, estamos dizendo que Y|X \\sim N(aX + b, \\sigma^2) . Queremos estimar a e b de forma que tenhamos o melhor ajuste poss\u00edvel. Esse tema em espec\u00edfico n\u00e3o me interessa. Entretanto, podemos dizer que queremos estimar aX + b , a m\u00e9dia de uma normal, mas que muda para cada X = x observado. sns.lmplot(x = 'gestation', y = 'bwt', data = birth_df, height = 5, ci = 95) <seaborn.axisgrid.FacetGrid at 0x7fc517f55c18> O resultado n\u00e3o foi muito bom (na verdade eu j\u00e1 imaginava isso). Mas o interessante \u00e9 tentar refletir o que essas bandas significam? Por que os pontos n\u00e3o est\u00e3o nela? Esper\u00e1vamos que estiv\u00e9sse? E por que ela diminui a vari\u00e2ncia com o n\u00famero de pontos? Essas perguntas v\u00e3o ser devidamente respondidas no pr\u00f3ximo curso de Estat\u00edstica! Mas eu j\u00e1 vou adiantando que esse intervalo de confian\u00e7a \u00e9 para a m\u00e9dia estimada.","title":"Intervalos de Confian\u00e7a"},{"location":"infestatistica_BSc/ConfidenceIntervals/ConfidenceIntervals/#intervalos-de-confianca","text":"Esse tema procura responder qu\u00e3o confian\u00e7a dever\u00edamos por em um estimador. \u00c9 claro que essa pergunta tem que ser um pouco melhor descrita matematicamente. A ideia \u00e9 frequentista e tem a ideia a seguinte forma: O intervalo [a,b] , uma realiza\u00e7\u00e3o de [A,B] , tem 95% de confian\u00e7a se em 95% do tempo, o par\u00e2metro procurado est\u00e1 entre a e b . Veja que a ideia \u00e9 basicamente frequentista, dado que a interpreta\u00e7\u00e3o est\u00e1 ligada \u00e0 frequ\u00eancia quando o n\u00famero de experimentos tende para infinito. (Cuidado: N\u00e3o vamos falar da probabilidade do par\u00e2metro estar em [a,b] , isso n\u00e3o faz sentido, pois \\theta n\u00e3o \u00e9 uma vari\u00e1vel aleat\u00f3ria, e sim um valor fixo).","title":"Intervalos de Confian\u00e7a"},{"location":"infestatistica_BSc/ConfidenceIntervals/ConfidenceIntervals/#definicao","text":"Seja X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Sejam A \\leq B duas estat\u00edsticas que possuem a propriedade, para todo \\theta , P(A < g(\\theta) < B) \\geq \\gamma Chamamos (A,B) de intevalo de confian\u00e7a para g(\\theta) com coeficiente \\gamma . O intervalo \u00e9 chamado de exato se ao inv\u00e9s da desigualdade, tivermos uma igualdade. Ap\u00f3s observarmos os valores de X_1, ..., X_n e computarmos A = a e B = b , o intervalo (a,b) \u00e9 chamado de valor observado do intervalo de confian\u00e7a.","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica_BSc/ConfidenceIntervals/ConfidenceIntervals/#intervalo-de-confianca-para-a-media-de-nmu-sigma2","text":"Seja X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Para cada 0 < \\gamma < 1 , o intervalo (A,B) \u00e9 intervalo de confian\u00e7a exato para \\mu com coeficiente \\gamma , em que: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} onde T_{n-1} denota a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. O interessante \u00e9 que isso \u00e9 implica\u00e7\u00e3o direta da distribui\u00e7\u00e3o de U = \\frac{n^{1/2}(\\bar{X}_n - \\mu)}{\\sigma '} que inferimos no cap\u00edtulo 8.4, nesse caso, simplemente fizemos a transforma\u00e7\u00e3o: \\gamma = P(-c < U < c) = P(A < \\mu < B) e c \u00e9 escolhido de acordo com \\gamma .","title":"Intervalo de Confian\u00e7a para a m\u00e9dia de N(\\mu, \\sigma^2)"},{"location":"infestatistica_BSc/ConfidenceIntervals/ConfidenceIntervals/#implementacao","text":"Vamos rever a informa\u00e7\u00e3o sobre caf\u00e9 que usamos cap\u00edtulos antes para ver como isso acontece na pr\u00e1tica. Considere dados sobre pesos de beb\u00eas logo ao nascer. bwt: peso do beb\u00ea ao nascer. gestation: dura\u00e7\u00e3o em dias da gesta\u00e7\u00e3o. parity: primeiro filho ou n\u00e3o. age: idade da m\u00e3e. height: altura da m\u00e3e em polegadas. weight: peso da m\u00e3e em pounds. smoke: se a m\u00e3e \u00e9 fumante ou n\u00e3o. # Importando bibliotecas import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import t birth_df = pd.read_csv(\"http://people.reed.edu/~jones/141/Bwt.dat\") birth_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bwt gestation parity age height weight smoke 0 120 284 0 27 62 100 0 1 113 282 0 33 64 135 0 2 128 279 0 28 64 115 1 3 108 282 0 23 67 125 1 4 136 286 0 25 62 93 0 sns.histplot(data = birth_df.bwt, kde = True) plt.title('Histograma dos pesos dos beb\u00eas') plt.show() birth_df[birth_df.smoke == 0].bwt.hist(density = True, label = 'N\u00e3o fumante') birth_df[birth_df.smoke == 1].bwt.hist(density = True, label = 'Fumante', alpha = 0.6) plt.xlabel('Peso') plt.legend() plt.show() Sabemos que essa \u00e9 uma extra\u00e7\u00e3o de uma popula\u00e7\u00e3o maior. Para conseguirmos mais amostras, vamos usar um procedimento chamado bootstrap . A ideia desse procedimento \u00e9 criar um novas amostras a partir de uma amostra inicial, usando replace = True como diferencial. Vou fazer esse procedimento diversar vezes e ir calculando a m\u00e9dia amostral. Como a m\u00e9dia amostral \u00e9 uma vari\u00e1vel aleat\u00f3ria, vamos obter um histograma das realiza\u00e7\u00f5es. Vamos supor que o peso W_i da crian\u00e7a i vem de uma distribui\u00e7\u00e3o com par\u00e2metros \\mu e \\sigma^2 desconhecidos. Nesse caso, \\bar{W}_i vir\u00e1 de uma distribui\u00e7\u00e3o normal com par\u00e2metros \\mu e \\sigma^2/n . ite = 10000 n = 200 bootstrap_means = np.zeros(ite) for i in range(ite): bootstrap_sample = birth_df.sample(n = n, replace = True, random_state=i) bootstrap_means[i] = bootstrap_sample.bwt.mean() sns.histplot(bootstrap_means, kde = True) plt.title(\"M\u00e9dias das amostras\") plt.xlabel('Peso') plt.show() Vamos calcular o nosso intervalo de confian\u00e7a com \\gamma = 0.95 . Temos que: gamma = 0.95 A = lambda x: np.mean(x) - t.ppf(q = (1 + gamma)/2, df = len(x) - 1)*np.std(x, ddof = 1)/len(x)**(1/2) B = lambda x: np.mean(x) + t.ppf(q = (1 + gamma)/2, df = len(x) - 1)*np.std(x, ddof = 1)/len(x)**(1/2) ite = 100 n = 500 bootstrap_intervals = np.zeros((ite,2)) for i in range(ite): bootstrap_sample = birth_df.sample(n = n, replace = True, random_state=i) bootstrap_intervals[i,0] = A(bootstrap_sample.bwt) bootstrap_intervals[i,1] = B(bootstrap_sample.bwt) out_values = np.where((bootstrap_intervals[:,0] > 119.5) | (bootstrap_intervals[:,1] < 119.5)) plt.figure(figsize = (6,10)) plt.scatter(bootstrap_intervals[:,0], np.arange(0,ite), color = 'red', label = 'a') plt.scatter(bootstrap_intervals[:,1], np.arange(0,ite), color = 'green', label = 'b') plt.scatter(bootstrap_intervals[out_values[0],0], out_values[0], color = 'black', label = 'Fora') plt.scatter(bootstrap_intervals[out_values[0],1], out_values[0], color = 'black') plt.vlines(119.5, ymin = 0, ymax = ite, linestyle = '--', alpha = 0.6, label = 'M\u00e9dia real') plt.legend() plt.show()","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica_BSc/ConfidenceIntervals/ConfidenceIntervals/#interpretacao","text":"Estamos fazer uma afirma\u00e7\u00e3o probabil\u00edstica sobre o intervalo (A,B) antes de observar os dados. Ap\u00f3s observarmos os dados, n\u00e3o podemos interpretar (a,b) como um intervalo em que temos 95% de confian\u00e7a de g(\\theta) estar no intervalo. Antes de observarmos as amostras, temos a confian\u00e7a de que 95% dos intervalos conter\u00e3o \\mu .","title":"Interpreta\u00e7\u00e3o"},{"location":"infestatistica_BSc/ConfidenceIntervals/ConfidenceIntervals/#sem-simetria","text":"Construimos anteriormente um intervalo sim\u00e9trico, onde a estat\u00edstica U acima mencionada estaria entre -c e c com probabilidade \\gamma . Mas podemos desenvolver intervalos n\u00e3o sim\u00e9tricos tamb\u00e9m. Uma forma que podemos fazer isso \u00e9 escolhendo \\gamma_1 e \\gamma_2 , tal que \\gamma_2 - \\gamma_1 = \\gamma . Assim: P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) = \\gamma Talv\u00e9m vc esteja se perguntando: porque escolher \\gamma_1, \\gamma_2 dessa forma? Bom: \\begin{split} \\gamma &= P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) \\\\ &= P\\left(U < T_{n-1}^{-1}(\\gamma_2)\\right) - P\\left(U \\leq T_{n-1}^{-1}(\\gamma_1)\\right) \\\\ &= \\gamma_2 - \\gamma_1 \\end{split}","title":"Sem simetria"},{"location":"infestatistica_BSc/ConfidenceIntervals/ConfidenceIntervals/#intervalos-de-confianca-unilateral","text":"","title":"Intervalos de Confian\u00e7a Unilateral"},{"location":"infestatistica_BSc/ConfidenceIntervals/ConfidenceIntervals/#definicao_1","text":"Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Sejam A e B duas estat\u00edsticas tais que: P(A < g(\\theta)) \\geq \\gamma P(B > g(\\theta)) \\geq \\gamma Ent\u00e3o (A, \\infty) e (-\\infty, B) s\u00e3o chamados de intervalos de confia\u00e7a unilaterais para g(\\theta) de coeficiente \\gamma ou percentil 100\\gamma . No caso de A 100\\gamma porcento abaixo e no caso de B a cima. Se vale a igualdade, dizemos que o intervalor \u00e9 exato.","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica_BSc/ConfidenceIntervals/ConfidenceIntervals/#intervalo-unilateral-para-a-media-de-nmusigma2","text":"Nas mesma condi\u00e7\u00f5es do teorema anterior, mas as estat\u00edsticas para baixo e para cima com coeficiente \\gamma para \\mu s\u00e3o: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}}","title":"Intervalo unilateral para a m\u00e9dia de N(\\mu,\\sigma^2)"},{"location":"infestatistica_BSc/ConfidenceIntervals/ConfidenceIntervals/#pivotal","text":"Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Seja V(\\vec{X},\\theta) uma vari\u00e1vel aleat\u00f3ria cuja distribui\u00e7\u00e3o \u00e9 a mesma para \\theta . Chamamos V de quantidade pivotal .","title":"Pivotal"},{"location":"infestatistica_BSc/ConfidenceIntervals/ConfidenceIntervals/#teorema","text":"Seja X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Suponha que Exista V pivotal. A cdf G de V \u00e9 cont\u00ednua. Exista fun\u00e7\u00e3o r tal r(V(X,\\theta), X) = g(\\theta) , ou seja, \u00e9 uma esp\u00e9cie de \"inversa\". r(v,x) (3) \u00e9 uma fun\u00e7\u00e3o estritamente crescente em v para todo x . Ent\u00e3o A = r(G^{-1}(\\gamma_1), X) B = r(G^{-1}(\\gamma_2), X) s\u00e3o os pontos extremos do intervalo de confian\u00e7a exato para g(\\theta) de coeficiente \\gamma = \\gamma_2 - \\gamma_1 . Se r \u00e9 estritamente decrescente, invertemos A e B . Obs.: Ainda podemos usar o Teorema Central do Limite para obter intervalos de confian\u00e7a assint\u00f3ticos.","title":"Teorema"},{"location":"infestatistica_BSc/ConfidenceIntervals/ConfidenceIntervals/#exemplo-com-regressao-linear","text":"O dataset que utilizei anteriormente n\u00e3o \u00e9 muito bom para esse exemplo, mas eu vou usar, de qualquer forma, para entendermos o processo e como pode nos ajudar o intervalo de confian\u00e7a. Em uma Regress\u00e3o Linear, queremos dizer aferir uma rela\u00e7\u00e3o linear entre duas vari\u00e1veis, isto \u00e9, queremos dizer que uma vari\u00e1vel pode ser obtida pela outra atrav\u00e9s de uma reta, mais um erro aleat\u00f3rio. Suponha que queremos estimar Y o peso da crian\u00e7a ao nascer, sabendo a informa\u00e7\u00e3o do tempo de gesta\u00e7\u00e3o X e que Y = aX + b + E, onde E \\sim N(0,\\sigma^2) . Nesse caso, estamos dizendo que Y|X \\sim N(aX + b, \\sigma^2) . Queremos estimar a e b de forma que tenhamos o melhor ajuste poss\u00edvel. Esse tema em espec\u00edfico n\u00e3o me interessa. Entretanto, podemos dizer que queremos estimar aX + b , a m\u00e9dia de uma normal, mas que muda para cada X = x observado. sns.lmplot(x = 'gestation', y = 'bwt', data = birth_df, height = 5, ci = 95) <seaborn.axisgrid.FacetGrid at 0x7fc517f55c18> O resultado n\u00e3o foi muito bom (na verdade eu j\u00e1 imaginava isso). Mas o interessante \u00e9 tentar refletir o que essas bandas significam? Por que os pontos n\u00e3o est\u00e3o nela? Esper\u00e1vamos que estiv\u00e9sse? E por que ela diminui a vari\u00e2ncia com o n\u00famero de pontos? Essas perguntas v\u00e3o ser devidamente respondidas no pr\u00f3ximo curso de Estat\u00edstica! Mas eu j\u00e1 vou adiantando que esse intervalo de confian\u00e7a \u00e9 para a m\u00e9dia estimada.","title":"Exemplo com Regress\u00e3o Linear"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/","text":"Distribui\u00e7\u00f5es de Prioris Conjugadas Se a distribui\u00e7\u00e3o a priori \u00e9 membro de uma fam\u00edlia e a distribui\u00e7\u00e3o a posteriori tamb\u00e9m pertence a mesma a fam\u00edlia, essa fam\u00edlia de distribui\u00e7\u00f5es \u00e9 chamada de fam\u00edlia conjugada . A principal consequ\u00eancia de usar prioris de uma fam\u00edlia conjugada \u00e9 que as contas ficam muito mais simples. Principais Fam\u00edlias Conjugadas Teorema Suponha que X_1, ..., X_n \\overset{iid}{\\sim} \\text{Bernoulli}(\\theta) , 0 < \\theta < 1 desconhecido. Suponha que \\theta \\sim \\text{Beta}(\\alpha, \\beta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori de \\theta \u00e9 a distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n - \\sum_{i=1}^n x_i . Teorema Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Poisson}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n . Teorema Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Normal}(\\theta, \\sigma^2) , onde \\theta \u00e9 desconhecido e \\sigma \u00e9 conhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma normal com m\u00e9dia \\mu_0 e vari\u00e2ncia v_0^2 . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 normal com m\u00e9dia \\mu_1 e vari\u00e2ncia v_1^2 , onde: \\mu_1 = \\frac{\\sigma^2\\mu_0 + nv_0^2\\bar{x}_n}{\\sigma^2 + nv_0^2} v_1^2 = \\frac{\\sigma^2v_0^2}{\\sigma^2 + nv_0^2} Teorema Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + n e \\beta + \\sum_{i=1}^n x_i . import numpy as np from scipy import stats import matplotlib.pyplot as plt from matplotlib import animation, cm from IPython.display import HTML %matplotlib inline Suponha que \\theta seja a probabilidade de um item ser defeituoso em uma s\u00e9rie de items. Suponha que nossa priori em \\theta \u00e9 uma distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha e \\beta . S\u00e3o selecionados n items por vez para o teste. Sabemos que nossa posteriori ser\u00e1 uma Beta com os par\u00e2metros acima. Vejamos graficamente esse processo. theta_real = 0.1 n = 100 np.random.seed(10) XMIN = 0 XMAX = 1 YMIN = 0 YMAX = 5 alpha = [1] beta = [1] x = np.linspace(0.001,1,1000) # Definindo cores cmap = cm.autumn # Esta fun\u00e7\u00e3o permite plotar o backgroud def init(): line.set_data([], []) return (line,) # Definindo o espa\u00e7o da imagem fig, ax = plt.subplots() # Definindo caracter\u00edsticas do background ax.set_xlim((XMIN, XMAX)) ax.set_ylim((YMIN, YMAX)) ax.set_title('Evolu\u00e7\u00e3o da posteriori a cada itera\u00e7\u00e3o') ax.vlines(theta_real, ymin = YMIN, ymax = YMAX, linestyle = '--', color = 'grey') # Definindo plots vari\u00e1veis line, = ax.plot([], [], lw=2) line2, _ = ax.plot(XMIN, XMAX, YMIN, YMAX, linestyle = ':') def animate(i, alpha, beta, x, n): # Amostro da distribui\u00e7\u00e3o sample = np.random.binomial(1, p = theta_real) # Junto a lista que guarda os alphas e betas de cada itera\u00e7\u00e3o alpha.append(alpha[-1] + sample) beta.append(beta[-1] + 1 - sample) # Calculo a posteriori posteriori = stats.beta(a = alpha[-1], b = beta[-1]) line.set_data(x, posteriori.pdf(x)) line.set_color(cmap(1 - i/n)) line2.set_data([posteriori.mean(), posteriori.mean()], [YMIN, YMAX]) line2.set_color(cmap(1 - i/n)) return (line,line2) anim = animation.FuncAnimation(fig, animate, frames = n, init_func = init, interval = 100, blit = True, fargs=(alpha, beta, x, n), repeat = False) HTML(anim.to_html5_video()) Your browser does not support the video tag. Hiperpar\u00e2metros Seja \\Psi uma fam\u00edlia de distribui\u00e7\u00f5es poss\u00edveis sobre um espa\u00e7o de par\u00eamtros \\Omega . Suponha que independente da distribui\u00e7\u00e3o a priori dessa fam\u00edlia e n\u00e3o importando as observa\u00e7\u00f5es (quais s\u00e3o ou quantas s\u00e3o), a distribui\u00e7\u00e3o a posteriori seja da mesma fam\u00edlia. Chamamos \\Psi de fam\u00edlia conjunda de distribui\u00e7\u00f5es a priori. Os par\u00e2metros associados a essa fam\u00edlia s\u00e3o chamados de hiperpar\u00e2metros. Distribui\u00e7\u00f5es a Priori Impr\u00f3prias Seja \\xi uma fun\u00e7\u00e3o n\u00e3o negativa tal que \\Omega \u00e9 subconjunto de seu dom\u00ednio. Suponha que \\int \\xi(\\theta) d\\theta = \\infty . Se \\xi(\\theta) \u00e9 priori de \\theta , ela \u00e9 chamada de prioori impr\u00f3pria. Podemos gerar limites de distribui\u00e7\u00f5es, como, por exemplo, a distribui\u00e7\u00e3o uniforme [0,1] com intervalo sendo a reta, agora. Estimador de Bayes Estimador Seja X_1, ..., X_n dados observador cuja distribui\u00e7\u00e3o conjunta \u00e9 inndexada pelo par\u00e2metro \\theta . Um estimador do par\u00e2metro \\theta \u00e9 uma fun\u00e7\u00e3o real X_1, ..., X_n \\mapsto \\delta(X_1, ..., X_n) . Se X_i = x_i \u00e9 observado, \\delta(x_1,...,x_n) \u00e9 uma estimativa. Fun\u00e7\u00e3o de Perda \u00c9 uma fun\u00e7\u00e3o real L(\\theta, a) onde \\theta \\in \\Omega e a \\in \\mathbb{R} . Essa fun\u00e7\u00e3o procura indicar, para cada escoolha de \\theta , a perda do estat\u00edstico. Seja \\xi(\\theta) priori de \\theta . O valor esperado da perda \u00e9 dado por: E[L(\\theta, a)] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta) d\\theta, \\text{ a priori} E[L(\\theta, a)|x] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta|x) d\\theta, \\text{ a posteriori} Estimador de Bayes Seja L(\\theta, a) fun\u00e7\u00e3o de perda. Seja \\delta^*(x) o valor de a tal que E[L(\\theta, a)|x] \u00e9 minimizado. Ent\u00e3o \\delta^* \u00e9 o estimador de Bayes de \\theta . E[L(\\theta, \\delta^*(x))|x] = \\min_{a \\in \\mathbb{R}}E[L(\\theta, a)|x] Fun\u00e7\u00f5es de Perda: Exemplos Erro quadr\u00e1tico: L(\\theta, a) = (\\theta - a)^2 Queremos minimizar E[(\\theta - a)^2|x] \\delta^*(X) = E(\\theta| X) P\u00e1gina 260 (DeGroot) Erro absoluto: L(\\theta, a) = |\\theta - a| Queremos minimizar E[|\\theta - a||x] \\delta^*(X) = \\text{mediana (quartil 0.5)} P\u00e1gina 245 (DeGroot) Estimador Consistente Uma sequ\u00eancia de estimadores que converge em probabilidade para um valor desconhecido de um par\u00e2metro a ser estimafo \u00e9 chamado de sequ\u00eancia consistente de estimadores. Essa consist\u00eancia fala que em grandes amostras, o estimador estar\u00e1 pr\u00f3ximo o suficiente do valor desconhecido de \\theta . O estimador de Bayes, sob algumas condi\u00e7\u00f5es, forma uma sequ\u00eancia de estimadores consistentes. Limita\u00e7\u00f5es De acordo com a teoria Bayesiana , esse estimador \u00e9 o \u00fanico coerente que pode ser constru\u00eddo. \u00c9 importante que tenha-se definido uma fun\u00e7\u00e3o de perda e uma distribui\u00e7\u00e3o a priori para os par\u00e2metros. Quando \\theta \u00e9 um vetor, precisamos definit uma priori multivariada, mesmo que n\u00e3o queiramos estimar todos os par\u00e2metros. Exemplo 7.4.7 Quetelet reportou medidas do peito de 5732 homens militares. Os dados foram retirados desse site . import requests from bs4 import BeautifulSoup import pandas as pd Obtendo os dados direto do site Eu uso essas tr\u00eas bibliotecas, onde as duas primeiras s\u00e3o usadas para retirar informa\u00e7\u00e3o do site desejado. Veja que n\u00e3o coloco verifica\u00e7\u00e3o, pois o site tem esse problema. Depois eu coloco numa estrutura chamada DataFrame que \u00e9 basicamente uma tabela onde tem cada item nas linhas e cada caracter\u00edstica nas colunas. website = requests.get('https://www.stat.cmu.edu/StatDat/Datafiles/MilitiamenChests.html', verify = False) soup = BeautifulSoup(website.content) data = soup.pre.text.strip().split('\\n') chest_data = {'Chest': [], 'Count': []} for item in data[1:]: co, ch = item.split('\\t') chest_data['Chest'].append(int(ch)) chest_data['Count'].append(int(co)) chest_df = pd.DataFrame(chest_data) chest_df.head() /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Chest Count 0 33 3 1 34 18 2 35 81 3 36 185 4 37 420 plt.bar(chest_df['Chest'], chest_df['Count']) mean = sum(chest_df['Chest']*chest_df['Count'])/chest_df['Count'].sum() plt.vlines(mean, ymin = 0, ymax = 1200, color = 'black', linestyle = '--', label = 'M\u00e9dia={:.2f}'.format(mean)) plt.title('Histograma do tamanho do ppeito de militares escoseses') plt.xlabel('Medida de peitorais') plt.legend() plt.show() Vamos modelar as medidas do peitoral, como uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal com m\u00e9dia \\theta e vari\u00e2ncia \\sigma^2 , conhecido. Temos que a m\u00e9dia \u00e9 39.83 das amostras. Se \\theta \\sim N(\\mu_0, v_0^2) \u00e9 uma priori para \\theta , podemos calcular o estimador de Bayes a posteriori. Sabemos que a posteriori ser\u00e1 uma normal (conjugada) com m\u00e9dia e vari\u00e2ncia: \\mu_1 = \\frac{\\sigma^2 + 5732\\cdot v_0^2 \\cdot 39.83}{\\sigma^2 + 5732\\cdot v_0^2} v_1^2 = \\frac{\\sigma^2 v_0^2}{\\sigma^2 + 5732\\cdot v_0^2} O estimador de Bayes, segundo a perda quadr\u00e1tica, \u00e9 a m\u00e9dia a posteriori, portanto \\delta(x) = \\mu_1 Priori para \\theta \\mu_0 = 39.83 v_0^2 = 4","title":"Distribui\u00e7\u00f5es de Prioris Conjugadas"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#distribuicoes-de-prioris-conjugadas","text":"Se a distribui\u00e7\u00e3o a priori \u00e9 membro de uma fam\u00edlia e a distribui\u00e7\u00e3o a posteriori tamb\u00e9m pertence a mesma a fam\u00edlia, essa fam\u00edlia de distribui\u00e7\u00f5es \u00e9 chamada de fam\u00edlia conjugada . A principal consequ\u00eancia de usar prioris de uma fam\u00edlia conjugada \u00e9 que as contas ficam muito mais simples.","title":"Distribui\u00e7\u00f5es de Prioris Conjugadas"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#principais-familias-conjugadas","text":"","title":"Principais Fam\u00edlias Conjugadas"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#teorema","text":"Suponha que X_1, ..., X_n \\overset{iid}{\\sim} \\text{Bernoulli}(\\theta) , 0 < \\theta < 1 desconhecido. Suponha que \\theta \\sim \\text{Beta}(\\alpha, \\beta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori de \\theta \u00e9 a distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n - \\sum_{i=1}^n x_i .","title":"Teorema"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#teorema_1","text":"Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Poisson}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + \\sum_{i=1}^n x_i e \\beta + n .","title":"Teorema"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#teorema_2","text":"Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Normal}(\\theta, \\sigma^2) , onde \\theta \u00e9 desconhecido e \\sigma \u00e9 conhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma normal com m\u00e9dia \\mu_0 e vari\u00e2ncia v_0^2 . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 normal com m\u00e9dia \\mu_1 e vari\u00e2ncia v_1^2 , onde: \\mu_1 = \\frac{\\sigma^2\\mu_0 + nv_0^2\\bar{x}_n}{\\sigma^2 + nv_0^2} v_1^2 = \\frac{\\sigma^2v_0^2}{\\sigma^2 + nv_0^2}","title":"Teorema"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#teorema_3","text":"Suponha que X_1, ...., X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta) , onde \\theta \u00e9 desconhecido. Suponha tamb\u00e9m que a distribui\u00e7\u00e3o a priori de \\theta \u00e9 uma gamma com par\u00e2metros \\alpha e \\beta . Ent\u00e3o a distribui\u00e7\u00e3o a posteriori de \\theta sej\u00e1 gamma com par\u00e2metros \\alpha + n e \\beta + \\sum_{i=1}^n x_i . import numpy as np from scipy import stats import matplotlib.pyplot as plt from matplotlib import animation, cm from IPython.display import HTML %matplotlib inline Suponha que \\theta seja a probabilidade de um item ser defeituoso em uma s\u00e9rie de items. Suponha que nossa priori em \\theta \u00e9 uma distribui\u00e7\u00e3o beta com par\u00e2metros \\alpha e \\beta . S\u00e3o selecionados n items por vez para o teste. Sabemos que nossa posteriori ser\u00e1 uma Beta com os par\u00e2metros acima. Vejamos graficamente esse processo. theta_real = 0.1 n = 100 np.random.seed(10) XMIN = 0 XMAX = 1 YMIN = 0 YMAX = 5 alpha = [1] beta = [1] x = np.linspace(0.001,1,1000) # Definindo cores cmap = cm.autumn # Esta fun\u00e7\u00e3o permite plotar o backgroud def init(): line.set_data([], []) return (line,) # Definindo o espa\u00e7o da imagem fig, ax = plt.subplots() # Definindo caracter\u00edsticas do background ax.set_xlim((XMIN, XMAX)) ax.set_ylim((YMIN, YMAX)) ax.set_title('Evolu\u00e7\u00e3o da posteriori a cada itera\u00e7\u00e3o') ax.vlines(theta_real, ymin = YMIN, ymax = YMAX, linestyle = '--', color = 'grey') # Definindo plots vari\u00e1veis line, = ax.plot([], [], lw=2) line2, _ = ax.plot(XMIN, XMAX, YMIN, YMAX, linestyle = ':') def animate(i, alpha, beta, x, n): # Amostro da distribui\u00e7\u00e3o sample = np.random.binomial(1, p = theta_real) # Junto a lista que guarda os alphas e betas de cada itera\u00e7\u00e3o alpha.append(alpha[-1] + sample) beta.append(beta[-1] + 1 - sample) # Calculo a posteriori posteriori = stats.beta(a = alpha[-1], b = beta[-1]) line.set_data(x, posteriori.pdf(x)) line.set_color(cmap(1 - i/n)) line2.set_data([posteriori.mean(), posteriori.mean()], [YMIN, YMAX]) line2.set_color(cmap(1 - i/n)) return (line,line2) anim = animation.FuncAnimation(fig, animate, frames = n, init_func = init, interval = 100, blit = True, fargs=(alpha, beta, x, n), repeat = False) HTML(anim.to_html5_video()) Your browser does not support the video tag.","title":"Teorema"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#hiperparametros","text":"Seja \\Psi uma fam\u00edlia de distribui\u00e7\u00f5es poss\u00edveis sobre um espa\u00e7o de par\u00eamtros \\Omega . Suponha que independente da distribui\u00e7\u00e3o a priori dessa fam\u00edlia e n\u00e3o importando as observa\u00e7\u00f5es (quais s\u00e3o ou quantas s\u00e3o), a distribui\u00e7\u00e3o a posteriori seja da mesma fam\u00edlia. Chamamos \\Psi de fam\u00edlia conjunda de distribui\u00e7\u00f5es a priori. Os par\u00e2metros associados a essa fam\u00edlia s\u00e3o chamados de hiperpar\u00e2metros.","title":"Hiperpar\u00e2metros"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#distribuicoes-a-priori-improprias","text":"Seja \\xi uma fun\u00e7\u00e3o n\u00e3o negativa tal que \\Omega \u00e9 subconjunto de seu dom\u00ednio. Suponha que \\int \\xi(\\theta) d\\theta = \\infty . Se \\xi(\\theta) \u00e9 priori de \\theta , ela \u00e9 chamada de prioori impr\u00f3pria. Podemos gerar limites de distribui\u00e7\u00f5es, como, por exemplo, a distribui\u00e7\u00e3o uniforme [0,1] com intervalo sendo a reta, agora.","title":"Distribui\u00e7\u00f5es a Priori Impr\u00f3prias"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#estimador-de-bayes","text":"","title":"Estimador de Bayes"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#estimador","text":"Seja X_1, ..., X_n dados observador cuja distribui\u00e7\u00e3o conjunta \u00e9 inndexada pelo par\u00e2metro \\theta . Um estimador do par\u00e2metro \\theta \u00e9 uma fun\u00e7\u00e3o real X_1, ..., X_n \\mapsto \\delta(X_1, ..., X_n) . Se X_i = x_i \u00e9 observado, \\delta(x_1,...,x_n) \u00e9 uma estimativa.","title":"Estimador"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#funcao-de-perda","text":"\u00c9 uma fun\u00e7\u00e3o real L(\\theta, a) onde \\theta \\in \\Omega e a \\in \\mathbb{R} . Essa fun\u00e7\u00e3o procura indicar, para cada escoolha de \\theta , a perda do estat\u00edstico. Seja \\xi(\\theta) priori de \\theta . O valor esperado da perda \u00e9 dado por: E[L(\\theta, a)] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta) d\\theta, \\text{ a priori} E[L(\\theta, a)|x] = \\int_{\\Omega} L(\\theta, a)\\xi(\\theta|x) d\\theta, \\text{ a posteriori}","title":"Fun\u00e7\u00e3o de Perda"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#estimador-de-bayes_1","text":"Seja L(\\theta, a) fun\u00e7\u00e3o de perda. Seja \\delta^*(x) o valor de a tal que E[L(\\theta, a)|x] \u00e9 minimizado. Ent\u00e3o \\delta^* \u00e9 o estimador de Bayes de \\theta . E[L(\\theta, \\delta^*(x))|x] = \\min_{a \\in \\mathbb{R}}E[L(\\theta, a)|x]","title":"Estimador de Bayes"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#funcoes-de-perda-exemplos","text":"Erro quadr\u00e1tico: L(\\theta, a) = (\\theta - a)^2 Queremos minimizar E[(\\theta - a)^2|x] \\delta^*(X) = E(\\theta| X) P\u00e1gina 260 (DeGroot) Erro absoluto: L(\\theta, a) = |\\theta - a| Queremos minimizar E[|\\theta - a||x] \\delta^*(X) = \\text{mediana (quartil 0.5)} P\u00e1gina 245 (DeGroot)","title":"Fun\u00e7\u00f5es de Perda: Exemplos"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#estimador-consistente","text":"Uma sequ\u00eancia de estimadores que converge em probabilidade para um valor desconhecido de um par\u00e2metro a ser estimafo \u00e9 chamado de sequ\u00eancia consistente de estimadores. Essa consist\u00eancia fala que em grandes amostras, o estimador estar\u00e1 pr\u00f3ximo o suficiente do valor desconhecido de \\theta . O estimador de Bayes, sob algumas condi\u00e7\u00f5es, forma uma sequ\u00eancia de estimadores consistentes.","title":"Estimador Consistente"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#limitacoes","text":"De acordo com a teoria Bayesiana , esse estimador \u00e9 o \u00fanico coerente que pode ser constru\u00eddo. \u00c9 importante que tenha-se definido uma fun\u00e7\u00e3o de perda e uma distribui\u00e7\u00e3o a priori para os par\u00e2metros. Quando \\theta \u00e9 um vetor, precisamos definit uma priori multivariada, mesmo que n\u00e3o queiramos estimar todos os par\u00e2metros.","title":"Limita\u00e7\u00f5es"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#exemplo-747","text":"Quetelet reportou medidas do peito de 5732 homens militares. Os dados foram retirados desse site . import requests from bs4 import BeautifulSoup import pandas as pd","title":"Exemplo 7.4.7"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#obtendo-os-dados-direto-do-site","text":"Eu uso essas tr\u00eas bibliotecas, onde as duas primeiras s\u00e3o usadas para retirar informa\u00e7\u00e3o do site desejado. Veja que n\u00e3o coloco verifica\u00e7\u00e3o, pois o site tem esse problema. Depois eu coloco numa estrutura chamada DataFrame que \u00e9 basicamente uma tabela onde tem cada item nas linhas e cada caracter\u00edstica nas colunas. website = requests.get('https://www.stat.cmu.edu/StatDat/Datafiles/MilitiamenChests.html', verify = False) soup = BeautifulSoup(website.content) data = soup.pre.text.strip().split('\\n') chest_data = {'Chest': [], 'Count': []} for item in data[1:]: co, ch = item.split('\\t') chest_data['Chest'].append(int(ch)) chest_data['Count'].append(int(co)) chest_df = pd.DataFrame(chest_data) chest_df.head() /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Chest Count 0 33 3 1 34 18 2 35 81 3 36 185 4 37 420 plt.bar(chest_df['Chest'], chest_df['Count']) mean = sum(chest_df['Chest']*chest_df['Count'])/chest_df['Count'].sum() plt.vlines(mean, ymin = 0, ymax = 1200, color = 'black', linestyle = '--', label = 'M\u00e9dia={:.2f}'.format(mean)) plt.title('Histograma do tamanho do ppeito de militares escoseses') plt.xlabel('Medida de peitorais') plt.legend() plt.show() Vamos modelar as medidas do peitoral, como uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal com m\u00e9dia \\theta e vari\u00e2ncia \\sigma^2 , conhecido. Temos que a m\u00e9dia \u00e9 39.83 das amostras. Se \\theta \\sim N(\\mu_0, v_0^2) \u00e9 uma priori para \\theta , podemos calcular o estimador de Bayes a posteriori. Sabemos que a posteriori ser\u00e1 uma normal (conjugada) com m\u00e9dia e vari\u00e2ncia: \\mu_1 = \\frac{\\sigma^2 + 5732\\cdot v_0^2 \\cdot 39.83}{\\sigma^2 + 5732\\cdot v_0^2} v_1^2 = \\frac{\\sigma^2 v_0^2}{\\sigma^2 + 5732\\cdot v_0^2} O estimador de Bayes, segundo a perda quadr\u00e1tica, \u00e9 a m\u00e9dia a posteriori, portanto \\delta(x) = \\mu_1","title":"Obtendo os dados direto do site"},{"location":"infestatistica_BSc/ConjugateDistributions/ConjugateDistributions/#priori-para-theta","text":"\\mu_0 = 39.83 v_0^2 = 4","title":"Priori para \\theta"},{"location":"infestatistica_BSc/ExampleHypothesesTesting/ExampleHypothesesTesting/","text":"Simples Exemplos em Teste de Hip\u00f3teses Refer\u00eancia: Dr. Tirthajyoti Sarkar , Fremont, 94536 Vamos lembrar que nosso objetivo \u00e9 testar hip\u00f3teses cient\u00edficas com uma formula\u00e7\u00e3o estat\u00edstica, a fim de fazer alguma infer\u00eancia sobre os par\u00e2metros de algum modelo. Os testes que ser\u00e3o discutidos nesse notebook s\u00e3o: Propor\u00e7\u00e3o de uma popula\u00e7\u00e3o. Deferen\u00e7a entre propor\u00e7\u00e3o de popula\u00e7\u00f5es M\u00e9dia de uma popula\u00e7\u00e3o. Diferen\u00e7a enre m\u00e9dias de popula\u00e7\u00f5es. import statsmodels.api as sm import numpy as np import matplotlib.pyplot as plt import pandas as pd Propor\u00e7\u00e3o de uma Popula\u00e7\u00e3o Quest\u00e3o : Em anos anteriores, 52% dos pais acreditavam que a falta de sono era causada por eletr\u00f4nicos e as m\u00e9dias sociais em seus filhos e filhas adolescentes. E agora, como essa propor\u00e7\u00e3o se encontra? Popula\u00e7\u00e3o : Pais com filhos e filhas adolescentes de 13 a 18 anos. Par\u00e2metro de interesse: p H_0: p \\le 0.52 e H_1: p > 0.52 . Dados: Pesquisa entre 1018 pessoas: 56% acreditam agora. Teste Z para propor\u00e7\u00f5es Vamos usar um teste chamado teste Z, considerando que $X_1 , ..., X_n \\sim Bernoulli(p) Z = \\frac{\\bar{X}_n - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} Onde \\pi_0 \u00e9 o valor limiar, no caso 0.52. Observe que quando p = \\pi_0 , a estat\u00edstica Z \u00e9 aproximadamente normal padr\u00e3o quando n cresce. O que esse teste mensura? No denominador, temos o desvio padr\u00e3o de quando p = \\pi_0 . Portanto, estamos medindo a dist\u00e2ncia entre a m\u00e9dia amostral e o limiar em unidades de desvio padr\u00e3o. Nosso procedimento de teste ser\u00e1, portanto, se Z \\ge c , rejeitamos H_0 , mas com n\u00edvel de signific\u00e2ncia \\alpha_0 , isto \u00e9: P(Z \\ge c|p \\le \\pi_0) \\le \\alpha_0 Podemos conferir que \\begin{split} P(Z \\ge c) &= P\\left(\\frac{\\bar{X}_n - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} \\ge c\\right) \\\\ &= P\\left(\\frac{\\bar{X}_n - p}{\\sqrt{(p(1 - p)/n)}}\\frac{\\sqrt{(p(1 - p)/n)}}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} + \\frac{p - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} \\ge c\\right) \\\\ &= P\\left(\\frac{\\bar{X}_n - p}{\\sqrt{(p(1 - p)/n)}} \\ge c\\frac{\\sqrt{\\pi_0(1 - \\pi_0)}}{\\sqrt{p(1 - p)}} - \\frac{p - \\pi_0}{\\sqrt{(p(1 - p)/n)}}\\right) \\\\ &\\approx 1 - \\Phi\\left(c\\frac{\\sqrt{\\pi_0(1 - \\pi_0)}}{\\sqrt{p(1 - p)}} - \\frac{p - \\pi_0}{\\sqrt{(p(1 - p)/n)}}\\right), \\text{ pelo Teo. Central do Limite} \\end{split} Assim podemos encontrar \\alpha_0 a partir do m\u00e1ximo que essa quantidade assume (Mas com uma obseeva\u00e7\u00e3o bem detalhada, quando n \u00e9 grande, p = 0.52 \u00e9 o maximizador) Mas como isso funciona na pr\u00e1tica, propriamente dito. Tenho que escolher um valor limiar para o p-valor, isto \u00e9, se p-valor for menor do que esse limiar, eu rejeito a hip\u00f3tese. Vou ficar esse limiar em 0.05, mas isso \u00e9 arbitr\u00e1rio, apesar da literatura costumar us\u00e1-lo. n = 1018 pnull = .52 phat = .56 ztest, pvalue = sm.stats.proportions_ztest(count = phat*n, #n\u00famero de sucessos nobs = n, #n\u00famero de observa\u00e7\u00f5es value = pnull, #pi_0 alternative = 'larger') #hip\u00f3tese alternativa print('O valor da estat\u00edstica de teste foi {} e o p-valor {}'.format(ztest, pvalue)) O valor da estat\u00edstica de teste foi 0.6392739759907055 e o p-valor 0.26132235751888716 Conclus\u00e3o do teste de hip\u00f3teses Como o p-valor foi menor do que nosso limiar, n\u00f3s temos evid\u00eancia para rejeitar a hip\u00f3tese nula que dizia que a prpor\u00e7\u00e3o teria permanecido ou at\u00e9 diminu\u00eddo. Isso n\u00e3o significa que aceitamos a hip\u00f3tese alternativa, apenas que temos evid\u00eancia para acreditar que a propor\u00e7\u00e3o seja maior do que 0.52. n = 1018 pnull = .52 phat = .53 ztest, pvalue = sm.stats.proportions_ztest(count = phat*n, #n\u00famero de sucessos nobs = n, #n\u00famero de observa\u00e7\u00f5es value = pnull, #pi_0 alternative = 'larger') #hip\u00f3tese alternativa print('O valor da estat\u00edstica de teste foi {} e o p-valor {}'.format(ztest, pvalue)) O valor da estat\u00edstica de teste foi 0.6392739759907055 e o p-valor 0.26132235751888716 Diferen\u00e7a entre propo\u00e7\u00f5es de duas popula\u00e7\u00f5es Quest\u00e3o : Existe diferen\u00e7a significativa entre pais ingleses e pais alem\u00e3es que reportaram que seus filhos e filhas tiveram aulas de nata\u00e7\u00e3o? Popula\u00e7\u00e3o : Pais com filhos e filhas ingleses e alem\u00e3es. Par\u00e2metro de interesse: p_{ingleses} = p_i e p_{alemaes} = p_a . H_0: p_i - p_a = 0 e H_1: p_i - p_a \\neq 0 . Dados: 247 pais ingleses responderam e dentre eles 36.8% reportaram que sim. Pais alem\u00e3es foram 308, 38.9% que disseram sim. Teste T para propor\u00e7\u00f5es Na verdade, poder\u00edamos usar o teste Z, com a mesma ideia, s\u00f3 que nesse caso, ter\u00edamos que tomar um pouco de cuidado com o denominador, dado que agora existem duas m\u00e9dias, ent\u00e3o nosso estimador para o desvio padr\u00e3o deve levar em conta esses dois fatores e n deve ser suficientemente grande para que n\u00e3o tenhamos problema. Para evitar isso, vamos usar o T teste. A estat\u00edstica de teste \u00e9 a seguinte, tratando como X e Y as duas amostras consideradas. t = \\frac{\\bar{X}_n - \\bar{Y}_m}{SE} Nesse caso SE \u00e9 o erro entre a diferen\u00e7a entre a m\u00e9dias: isso tem um pequeno problema quando m \\neq n . Ent\u00e3o fazer as contas no papel n\u00e3o \u00e9 trivial. Confira aqui as defini\u00e7\u00f5es de SE precisas. Como nossa inten\u00e7\u00e3o \u00e9 apenas usar esse teste, vamos mostrar como isso pode ser pr\u00e1tico. n = 247 pi = .368 m = 308 pa = .389 # Gerando as popula\u00e7\u00f5es england = np.random.binomial(n = 1, p = pi, size = n) germany = np.random.binomial(n = 1, p = pa, size = m) _, p_value, _ = sm.stats.ttest_ind(england, germany) print('O p-valor foi {}'.format(p_value)) O p-valor foi 0.2615435082780627 Conclus\u00e3o sobre o teste de hip\u00f3teses Dado que o p-valor \u00e9 maior do que nosso limiar, n\u00e3o podemos rejeitar a hip\u00f3tese nula. Nesse caso, a diferen\u00e7a das propor\u00e7\u00f5es nas popu\u00e7a\u00e7\u00f5es n\u00e3o foi nada mais do que meramente uma aleatoriedade. Mas o que acontece se essas propor\u00e7\u00f5es se mantiveram para mais pessoas? n = 5000 pi = .37 m = 5000 pa = .389 england = np.random.binomial(n = 1, p = pi, size = n) germany = np.random.binomial(n = 1, p = pa, size = m) _, p_value, _ = sm.stats.ttest_ind(england, germany) print('O p-valor foi {}'.format(p_value)) O p-valor foi 0.027721099791980015 Diferen\u00e7a entre m\u00e9dias de popula\u00e7\u00f5es Quest\u00e3o : Considerando os adultos nos dados da NHAMES , homens tem maior m\u00e9dia de \u00cdndice de Massa Corp\u00f3rea do que mulheres? Popula\u00e7\u00e3o : Adultos na base NHAMES. Par\u00e2metro de interesse: \\mu_{homens} = \\mu_h e \\mu_{mulheres} = \\mu_m . H_0: \\mu_1 = \\mu_2 e H_1: \\mu_1 \\neq \\mu_2 . Dados: 2976 mulheres adultas \\hat{\\mu}_m = 29.94 \\hat{\\sigma}_m = 7.75 2759 homens adultos \\hat{\\mu}_h = 28.78 \\hat{\\sigma}_h = 6.25 url = \"https://raw.githubusercontent.com/kshedden/statswpy/master/NHANES/merged/nhanes_2015_2016.csv\" da = pd.read_csv(url) da.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SEQN ALQ101 ALQ110 ALQ130 SMQ020 RIAGENDR RIDAGEYR RIDRETH1 DMDCITZN DMDEDUC2 ... BPXSY2 BPXDI2 BMXWT BMXHT BMXBMI BMXLEG BMXARML BMXARMC BMXWAIST HIQ210 0 83732 1.0 NaN 1.0 1 1 62 3 1.0 5.0 ... 124.0 64.0 94.8 184.5 27.8 43.3 43.6 35.9 101.1 2.0 1 83733 1.0 NaN 6.0 1 1 53 3 2.0 3.0 ... 140.0 88.0 90.4 171.4 30.8 38.0 40.0 33.2 107.9 NaN 2 83734 1.0 NaN NaN 1 1 78 3 1.0 3.0 ... 132.0 44.0 83.4 170.1 28.8 35.6 37.0 31.0 116.5 2.0 3 83735 2.0 1.0 1.0 2 2 56 3 1.0 5.0 ... 134.0 68.0 109.8 160.9 42.4 38.5 37.7 38.3 110.1 2.0 4 83736 2.0 1.0 1.0 2 2 42 4 1.0 4.0 ... 114.0 54.0 55.2 164.9 20.3 37.4 36.0 27.2 80.4 2.0 5 rows \u00d7 28 columns females = da[da[\"RIAGENDR\"] == 2] male = da[da[\"RIAGENDR\"] == 1] n_m = len(females) mu_m = females[\"BMXBMI\"].mean() sd_m = females[\"BMXBMI\"].std() (n_m, mu_m, sd_m) (2976, 29.939945652173996, 7.75331880954568) n_h = len(male) mu_h = male[\"BMXBMI\"].mean() sd_h = male[\"BMXBMI\"].std() (n_h, mu_h, sd_h) (2759, 28.778072111846985, 6.252567616801485) fig, ax = plt.subplots(1,2,figsize=(15,5)) ax[0].set_title(\"Histogram BMI para mulheres\",fontsize=16) ax[0].hist(females[\"BMXBMI\"].dropna(), edgecolor='k', color='darkred',bins=25) ax[1].set_title(\"Histogram BMI para homens\", fontsize=16) ax[1].hist(male[\"BMXBMI\"].dropna(), edgecolor='k', color='green', bins=25) plt.show() Vamos usar o t-test descrito em Welch . Temos que us\u00e1-lo porque n\u00e3o conhecemos a vari\u00e2ncia. Se conhecessemos, poder\u00edamos usar o teste normal mesmo e se soub\u00e9ssemos que s\u00e3o iguais, mas desconhecessemos, poder\u00edamos usar o ttest ques estudamos no cap\u00edtulo 9.5 . sm.stats.ttest_ind(x1 = females[\"BMXBMI\"].dropna(), x2 = male[\"BMXBMI\"].dropna(), alternative='two-sided', value = 0) # diferen\u00e7a na hip\u00f3tese nula (6.175593353138302, 7.050275578095374e-10, 5660.0) Conclus\u00e3o no teste de hip\u00f3teses Como o p-valor \u00e9 bem pequeno, n\u00f3s podemos rejeitar a hip\u00f3tese nula, o que significa que existe diferen\u00e7a estat\u00edstica entre as m\u00e9dias. Isso n\u00e3o responde se a o \u00edndice \u00e9 mais alto para homens, mas podemos fazer o teste unilateral e perceber que de fato isso de fato acontece segundo os dados. sm.stats.ttest_ind(x1 = females[\"BMXBMI\"].dropna(), x2 = male[\"BMXBMI\"].dropna(), alternative='larger', value = 0) # diferen\u00e7a na hip\u00f3tese nula (6.175593353138302, 3.525137789047687e-10, 5660.0)","title":"Simples Exemplos em Teste de Hip\u00f3teses"},{"location":"infestatistica_BSc/ExampleHypothesesTesting/ExampleHypothesesTesting/#simples-exemplos-em-teste-de-hipoteses","text":"","title":"Simples Exemplos em Teste de Hip\u00f3teses"},{"location":"infestatistica_BSc/ExampleHypothesesTesting/ExampleHypothesesTesting/#referencia-dr-tirthajyoti-sarkar-fremont-94536","text":"Vamos lembrar que nosso objetivo \u00e9 testar hip\u00f3teses cient\u00edficas com uma formula\u00e7\u00e3o estat\u00edstica, a fim de fazer alguma infer\u00eancia sobre os par\u00e2metros de algum modelo. Os testes que ser\u00e3o discutidos nesse notebook s\u00e3o: Propor\u00e7\u00e3o de uma popula\u00e7\u00e3o. Deferen\u00e7a entre propor\u00e7\u00e3o de popula\u00e7\u00f5es M\u00e9dia de uma popula\u00e7\u00e3o. Diferen\u00e7a enre m\u00e9dias de popula\u00e7\u00f5es. import statsmodels.api as sm import numpy as np import matplotlib.pyplot as plt import pandas as pd","title":"Refer\u00eancia:  Dr. Tirthajyoti Sarkar, Fremont, 94536"},{"location":"infestatistica_BSc/ExampleHypothesesTesting/ExampleHypothesesTesting/#proporcao-de-uma-populacao","text":"Quest\u00e3o : Em anos anteriores, 52% dos pais acreditavam que a falta de sono era causada por eletr\u00f4nicos e as m\u00e9dias sociais em seus filhos e filhas adolescentes. E agora, como essa propor\u00e7\u00e3o se encontra? Popula\u00e7\u00e3o : Pais com filhos e filhas adolescentes de 13 a 18 anos. Par\u00e2metro de interesse: p H_0: p \\le 0.52 e H_1: p > 0.52 . Dados: Pesquisa entre 1018 pessoas: 56% acreditam agora.","title":"Propor\u00e7\u00e3o de uma Popula\u00e7\u00e3o"},{"location":"infestatistica_BSc/ExampleHypothesesTesting/ExampleHypothesesTesting/#teste-z-para-proporcoes","text":"Vamos usar um teste chamado teste Z, considerando que $X_1 , ..., X_n \\sim Bernoulli(p) Z = \\frac{\\bar{X}_n - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} Onde \\pi_0 \u00e9 o valor limiar, no caso 0.52. Observe que quando p = \\pi_0 , a estat\u00edstica Z \u00e9 aproximadamente normal padr\u00e3o quando n cresce. O que esse teste mensura? No denominador, temos o desvio padr\u00e3o de quando p = \\pi_0 . Portanto, estamos medindo a dist\u00e2ncia entre a m\u00e9dia amostral e o limiar em unidades de desvio padr\u00e3o. Nosso procedimento de teste ser\u00e1, portanto, se Z \\ge c , rejeitamos H_0 , mas com n\u00edvel de signific\u00e2ncia \\alpha_0 , isto \u00e9: P(Z \\ge c|p \\le \\pi_0) \\le \\alpha_0 Podemos conferir que \\begin{split} P(Z \\ge c) &= P\\left(\\frac{\\bar{X}_n - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} \\ge c\\right) \\\\ &= P\\left(\\frac{\\bar{X}_n - p}{\\sqrt{(p(1 - p)/n)}}\\frac{\\sqrt{(p(1 - p)/n)}}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} + \\frac{p - \\pi_0}{\\sqrt{(\\pi_0(1 - \\pi_0)/n)}} \\ge c\\right) \\\\ &= P\\left(\\frac{\\bar{X}_n - p}{\\sqrt{(p(1 - p)/n)}} \\ge c\\frac{\\sqrt{\\pi_0(1 - \\pi_0)}}{\\sqrt{p(1 - p)}} - \\frac{p - \\pi_0}{\\sqrt{(p(1 - p)/n)}}\\right) \\\\ &\\approx 1 - \\Phi\\left(c\\frac{\\sqrt{\\pi_0(1 - \\pi_0)}}{\\sqrt{p(1 - p)}} - \\frac{p - \\pi_0}{\\sqrt{(p(1 - p)/n)}}\\right), \\text{ pelo Teo. Central do Limite} \\end{split} Assim podemos encontrar \\alpha_0 a partir do m\u00e1ximo que essa quantidade assume (Mas com uma obseeva\u00e7\u00e3o bem detalhada, quando n \u00e9 grande, p = 0.52 \u00e9 o maximizador) Mas como isso funciona na pr\u00e1tica, propriamente dito. Tenho que escolher um valor limiar para o p-valor, isto \u00e9, se p-valor for menor do que esse limiar, eu rejeito a hip\u00f3tese. Vou ficar esse limiar em 0.05, mas isso \u00e9 arbitr\u00e1rio, apesar da literatura costumar us\u00e1-lo. n = 1018 pnull = .52 phat = .56 ztest, pvalue = sm.stats.proportions_ztest(count = phat*n, #n\u00famero de sucessos nobs = n, #n\u00famero de observa\u00e7\u00f5es value = pnull, #pi_0 alternative = 'larger') #hip\u00f3tese alternativa print('O valor da estat\u00edstica de teste foi {} e o p-valor {}'.format(ztest, pvalue)) O valor da estat\u00edstica de teste foi 0.6392739759907055 e o p-valor 0.26132235751888716","title":"Teste Z para propor\u00e7\u00f5es"},{"location":"infestatistica_BSc/ExampleHypothesesTesting/ExampleHypothesesTesting/#conclusao-do-teste-de-hipoteses","text":"Como o p-valor foi menor do que nosso limiar, n\u00f3s temos evid\u00eancia para rejeitar a hip\u00f3tese nula que dizia que a prpor\u00e7\u00e3o teria permanecido ou at\u00e9 diminu\u00eddo. Isso n\u00e3o significa que aceitamos a hip\u00f3tese alternativa, apenas que temos evid\u00eancia para acreditar que a propor\u00e7\u00e3o seja maior do que 0.52. n = 1018 pnull = .52 phat = .53 ztest, pvalue = sm.stats.proportions_ztest(count = phat*n, #n\u00famero de sucessos nobs = n, #n\u00famero de observa\u00e7\u00f5es value = pnull, #pi_0 alternative = 'larger') #hip\u00f3tese alternativa print('O valor da estat\u00edstica de teste foi {} e o p-valor {}'.format(ztest, pvalue)) O valor da estat\u00edstica de teste foi 0.6392739759907055 e o p-valor 0.26132235751888716","title":"Conclus\u00e3o do teste de hip\u00f3teses"},{"location":"infestatistica_BSc/ExampleHypothesesTesting/ExampleHypothesesTesting/#diferenca-entre-propocoes-de-duas-populacoes","text":"Quest\u00e3o : Existe diferen\u00e7a significativa entre pais ingleses e pais alem\u00e3es que reportaram que seus filhos e filhas tiveram aulas de nata\u00e7\u00e3o? Popula\u00e7\u00e3o : Pais com filhos e filhas ingleses e alem\u00e3es. Par\u00e2metro de interesse: p_{ingleses} = p_i e p_{alemaes} = p_a . H_0: p_i - p_a = 0 e H_1: p_i - p_a \\neq 0 . Dados: 247 pais ingleses responderam e dentre eles 36.8% reportaram que sim. Pais alem\u00e3es foram 308, 38.9% que disseram sim.","title":"Diferen\u00e7a entre propo\u00e7\u00f5es de duas popula\u00e7\u00f5es"},{"location":"infestatistica_BSc/ExampleHypothesesTesting/ExampleHypothesesTesting/#teste-t-para-proporcoes","text":"Na verdade, poder\u00edamos usar o teste Z, com a mesma ideia, s\u00f3 que nesse caso, ter\u00edamos que tomar um pouco de cuidado com o denominador, dado que agora existem duas m\u00e9dias, ent\u00e3o nosso estimador para o desvio padr\u00e3o deve levar em conta esses dois fatores e n deve ser suficientemente grande para que n\u00e3o tenhamos problema. Para evitar isso, vamos usar o T teste. A estat\u00edstica de teste \u00e9 a seguinte, tratando como X e Y as duas amostras consideradas. t = \\frac{\\bar{X}_n - \\bar{Y}_m}{SE} Nesse caso SE \u00e9 o erro entre a diferen\u00e7a entre a m\u00e9dias: isso tem um pequeno problema quando m \\neq n . Ent\u00e3o fazer as contas no papel n\u00e3o \u00e9 trivial. Confira aqui as defini\u00e7\u00f5es de SE precisas. Como nossa inten\u00e7\u00e3o \u00e9 apenas usar esse teste, vamos mostrar como isso pode ser pr\u00e1tico. n = 247 pi = .368 m = 308 pa = .389 # Gerando as popula\u00e7\u00f5es england = np.random.binomial(n = 1, p = pi, size = n) germany = np.random.binomial(n = 1, p = pa, size = m) _, p_value, _ = sm.stats.ttest_ind(england, germany) print('O p-valor foi {}'.format(p_value)) O p-valor foi 0.2615435082780627","title":"Teste T para propor\u00e7\u00f5es"},{"location":"infestatistica_BSc/ExampleHypothesesTesting/ExampleHypothesesTesting/#conclusao-sobre-o-teste-de-hipoteses","text":"Dado que o p-valor \u00e9 maior do que nosso limiar, n\u00e3o podemos rejeitar a hip\u00f3tese nula. Nesse caso, a diferen\u00e7a das propor\u00e7\u00f5es nas popu\u00e7a\u00e7\u00f5es n\u00e3o foi nada mais do que meramente uma aleatoriedade.","title":"Conclus\u00e3o sobre o teste de hip\u00f3teses"},{"location":"infestatistica_BSc/ExampleHypothesesTesting/ExampleHypothesesTesting/#mas-o-que-acontece-se-essas-proporcoes-se-mantiveram-para-mais-pessoas","text":"n = 5000 pi = .37 m = 5000 pa = .389 england = np.random.binomial(n = 1, p = pi, size = n) germany = np.random.binomial(n = 1, p = pa, size = m) _, p_value, _ = sm.stats.ttest_ind(england, germany) print('O p-valor foi {}'.format(p_value)) O p-valor foi 0.027721099791980015","title":"Mas o que acontece se essas propor\u00e7\u00f5es se mantiveram para mais pessoas?"},{"location":"infestatistica_BSc/ExampleHypothesesTesting/ExampleHypothesesTesting/#diferenca-entre-medias-de-populacoes","text":"Quest\u00e3o : Considerando os adultos nos dados da NHAMES , homens tem maior m\u00e9dia de \u00cdndice de Massa Corp\u00f3rea do que mulheres? Popula\u00e7\u00e3o : Adultos na base NHAMES. Par\u00e2metro de interesse: \\mu_{homens} = \\mu_h e \\mu_{mulheres} = \\mu_m . H_0: \\mu_1 = \\mu_2 e H_1: \\mu_1 \\neq \\mu_2 . Dados: 2976 mulheres adultas \\hat{\\mu}_m = 29.94 \\hat{\\sigma}_m = 7.75 2759 homens adultos \\hat{\\mu}_h = 28.78 \\hat{\\sigma}_h = 6.25 url = \"https://raw.githubusercontent.com/kshedden/statswpy/master/NHANES/merged/nhanes_2015_2016.csv\" da = pd.read_csv(url) da.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SEQN ALQ101 ALQ110 ALQ130 SMQ020 RIAGENDR RIDAGEYR RIDRETH1 DMDCITZN DMDEDUC2 ... BPXSY2 BPXDI2 BMXWT BMXHT BMXBMI BMXLEG BMXARML BMXARMC BMXWAIST HIQ210 0 83732 1.0 NaN 1.0 1 1 62 3 1.0 5.0 ... 124.0 64.0 94.8 184.5 27.8 43.3 43.6 35.9 101.1 2.0 1 83733 1.0 NaN 6.0 1 1 53 3 2.0 3.0 ... 140.0 88.0 90.4 171.4 30.8 38.0 40.0 33.2 107.9 NaN 2 83734 1.0 NaN NaN 1 1 78 3 1.0 3.0 ... 132.0 44.0 83.4 170.1 28.8 35.6 37.0 31.0 116.5 2.0 3 83735 2.0 1.0 1.0 2 2 56 3 1.0 5.0 ... 134.0 68.0 109.8 160.9 42.4 38.5 37.7 38.3 110.1 2.0 4 83736 2.0 1.0 1.0 2 2 42 4 1.0 4.0 ... 114.0 54.0 55.2 164.9 20.3 37.4 36.0 27.2 80.4 2.0 5 rows \u00d7 28 columns females = da[da[\"RIAGENDR\"] == 2] male = da[da[\"RIAGENDR\"] == 1] n_m = len(females) mu_m = females[\"BMXBMI\"].mean() sd_m = females[\"BMXBMI\"].std() (n_m, mu_m, sd_m) (2976, 29.939945652173996, 7.75331880954568) n_h = len(male) mu_h = male[\"BMXBMI\"].mean() sd_h = male[\"BMXBMI\"].std() (n_h, mu_h, sd_h) (2759, 28.778072111846985, 6.252567616801485) fig, ax = plt.subplots(1,2,figsize=(15,5)) ax[0].set_title(\"Histogram BMI para mulheres\",fontsize=16) ax[0].hist(females[\"BMXBMI\"].dropna(), edgecolor='k', color='darkred',bins=25) ax[1].set_title(\"Histogram BMI para homens\", fontsize=16) ax[1].hist(male[\"BMXBMI\"].dropna(), edgecolor='k', color='green', bins=25) plt.show()","title":"Diferen\u00e7a entre m\u00e9dias de popula\u00e7\u00f5es"},{"location":"infestatistica_BSc/ExampleHypothesesTesting/ExampleHypothesesTesting/#vamos-usar-o-t-test-descrito-em-welch","text":"Temos que us\u00e1-lo porque n\u00e3o conhecemos a vari\u00e2ncia. Se conhecessemos, poder\u00edamos usar o teste normal mesmo e se soub\u00e9ssemos que s\u00e3o iguais, mas desconhecessemos, poder\u00edamos usar o ttest ques estudamos no cap\u00edtulo 9.5 . sm.stats.ttest_ind(x1 = females[\"BMXBMI\"].dropna(), x2 = male[\"BMXBMI\"].dropna(), alternative='two-sided', value = 0) # diferen\u00e7a na hip\u00f3tese nula (6.175593353138302, 7.050275578095374e-10, 5660.0)","title":"Vamos usar o t-test descrito em Welch."},{"location":"infestatistica_BSc/ExampleHypothesesTesting/ExampleHypothesesTesting/#conclusao-no-teste-de-hipoteses","text":"Como o p-valor \u00e9 bem pequeno, n\u00f3s podemos rejeitar a hip\u00f3tese nula, o que significa que existe diferen\u00e7a estat\u00edstica entre as m\u00e9dias. Isso n\u00e3o responde se a o \u00edndice \u00e9 mais alto para homens, mas podemos fazer o teste unilateral e perceber que de fato isso de fato acontece segundo os dados. sm.stats.ttest_ind(x1 = females[\"BMXBMI\"].dropna(), x2 = male[\"BMXBMI\"].dropna(), alternative='larger', value = 0) # diferen\u00e7a na hip\u00f3tese nula (6.175593353138302, 3.525137789047687e-10, 5660.0)","title":"Conclus\u00e3o no teste de hip\u00f3teses"},{"location":"infestatistica_BSc/FisherInformation/FisherInformation/","text":"Informa\u00e7\u00e3o de Fisher Seja X uma amostra aleat\u00f3ria cuja distribui\u00e7\u00e3o depende de \\theta e tem valores em (a,b) \\subset \\mathbb{R} . Seja f_n(x|\\theta) a pdf conjunta de X . Assuma que S = {x | f(x|\\theta) > 0} \u00e9 o mesmo para todo \\theta . E \\lambda_n(x|\\theta) = \\log f_n(x|\\theta) \u00e9 duas vezes diferenci\u00e1vel em \\theta . A informa\u00e7\u00e3o \u00e9: I_n(\\theta) = E_{\\theta}\\{[\\lambda_n '(X|\\theta)]^2\\} Agora assuma que duas derivadas de \\int_S f_n(x|\\theta)dx com respeito a \\theta podemos inverter a ordem de integra\u00e7\u00e3o e diferencia\u00e7\u00e3o . Ent\u00e3o: I_n(\\theta) = - E_{\\theta}[\\lambda_n ''(X|\\theta)] Teorema I_n(\\theta) = nI(\\theta) Obs.: Estamos tratando da informa\u00e7\u00e3o de Fisher para o caso unidimensional. Para o caso em que temos \\Omega \\subset \\mathbb{R}^k , a informa\u00e7\u00e3o de Fisher ser\u00e1 uma matriz de tamanho k \\times k onde I_{n,i,j} = Cov_{\\theta}\\left[\\frac{\\partial}{\\partial \\theta_i}\\lambda_n'(X|\\theta), \\frac{\\partial}{\\partial \\theta_j}\\lambda_n'(X|\\theta)\\right] import numpy as np from scipy.stats import norm from scipy.misc import derivative from scipy.optimize import curve_fit import matplotlib.pyplot as plt from seaborn import violinplot import inspect Exemplo Construtivo Vamos pensar num caso bem simples: amostra aleat\u00f3ria X_1, ..., X_n \\sim \\text{Normal}(\\mu, \\sigma^2) , onde o par\u00e2metro \\sigma^2 \u00e9 conhecido e \\mu n\u00e3o. De forma direta, poder\u00edamos perguntar qual a Informa\u00e7\u00e3o de Fisher (ou Informa\u00e7\u00e3o Diferencial) da amostra aleat\u00f3ria sobre o par\u00e2metro desconhecido \\mu . Vamos encontrar a distribui\u00e7\u00e3o conjunta: f(x|\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2}\\right] \\begin{split} f_n(x|\\mu) &= \\prod_{i=1}^n f(x_i|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i^2 - 2x_i\\mu + \\mu^2)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^n x_i^2 - 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\end{split} Vamos encontrar a verossimilhan\u00e7a: \u00e9 a distribui\u00e7\u00e3o conjunta como fun\u00e7\u00e3o do par\u00e2metro! f_n(x|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] Vamos comparar para \\sigma = 1 e \\sigma = 5 loglikelihood = lambda mu, sigma, x: np.sum(np.log([norm(loc = mu, scale = sigma).pdf(xi) for xi in x]), axis = 0) sigmas = [1,3,5,10] mu_true = 5 mu_range = np.linspace(0,10,1000) fig,ax = plt.subplots(2,2,figsize = (16, 10)) fig.suptitle('Comparando Log-verossimilhan\u00e7as da Distribui\u00e7\u00e3o Normal') def generate_curves(sigma, ax, n = 20, n_times = 50): for i in range(n_times): x = np.random.normal(loc = mu_true, scale = sigma, size = n) logvalues = loglikelihood(mu_range, sigma, x) ax.plot(mu_range, logvalues, color = 'blue', alpha = 0.2) ax.vlines(mu_true, ymin = ax.get_ylim()[0], ymax = ax.get_ylim()[1], linestyle = '--') ax.set_title(r'$\\sigma =$ {}'.format(sigma)) ax.set_xlabel(r'$\\mu$') generate_curves(sigmas[0], ax[0][0]) generate_curves(sigmas[1], ax[0][1]) generate_curves(sigmas[2], ax[1][0]) generate_curves(sigmas[3], ax[1][1]) Vamos ver como se comporta derivada. Esse \u00e9 o score: \\lambda '_n(y|\\mu) = \\frac{1}{\\sigma^2}\\left(n\\bar{x}_n - \\mu\\right) score = lambda mu, sigma, x: derivative(loglikelihood, mu, dx = 1e-5, args = (sigma, x)) fig,ax = plt.subplots(2,2,figsize = (16, 10)) fig.suptitle('Comparando Scores da Distribui\u00e7\u00e3o Normal') def generate_curves(sigma, ax, n = 20, n_times = 50): for i in range(n_times): x = np.random.normal(loc = mu_true, scale = sigma, size = n) scorevalues = score(mu_range, sigma, x) ax.plot(mu_range, scorevalues, color = 'blue', alpha = 0.2) ax.vlines(mu_true, ymin = ax.get_ylim()[0], ymax = ax.get_ylim()[1], linestyle = '--') ax.set_title(r'$\\sigma =$ {}'.format(sigma)) ax.set_xlabel(r'$\\mu$') ax.set_ylim((-10,10)) generate_curves(sigmas[0], ax[0][0]) generate_curves(sigmas[1], ax[0][1]) generate_curves(sigmas[2], ax[1][0]) generate_curves(sigmas[3], ax[1][1]) fig,ax = plt.subplots(2,2,figsize = (16, 10)) fig.suptitle('Comparando Histogramas dos Scores para mu') def generate_histograms(mu, sigma, ax, n = 15, n_times = 100): scorevalues = [] for i in range(n_times): x = np.random.normal(loc = mu_true, scale = sigma, size = n) scorevalues.append(score(mu, sigma, x)) violinplot(scorevalues, ax = ax) ax.set_title(r'$\\sigma =$ {}'.format(sigma)) ax.set_xlabel('score') generate_histograms(5, sigmas[0], ax[0][0]) generate_histograms(5, sigmas[1], ax[0][1]) generate_histograms(5, sigmas[2], ax[1][0]) generate_histograms(5, sigmas[3], ax[1][1]) A informa\u00e7\u00e3o de Fisher \u00e9 a Vari\u00e2ncia da fun\u00e7\u00e3o score em X , isto \u00e9: \\begin{split} I_n(\\mu) &= Var(\\lambda '_n(x|p)) = E[(\\lambda '_n(x|p))^2] - E[\\lambda '_n(x|p)]^2\\\\ &= \\frac{1}{\\sigma^4}Var\\left[n\\bar{x}_n - \\mu\\right] \\\\ &= \\frac{n^2}{\\sigma^4}Var(\\bar{x}_n) \\\\ &= \\frac{n^2\\sigma^2}{n\\sigma^4} \\\\ &= \\frac{n}{\\sigma^2} \\end{split} Desigualdade de Cram\u00e9r-Rao Seja X uma amostra aleat\u00f3ria com pdf f(x| \\theta) . Suponha as hip\u00f3teses acima acerca dessa distribui\u00e7\u00e3o. Seja T = r(X) com vari\u00e2ncia finita e m(\\theta) = E_{\\theta}(T) \u00e9 diferenci\u00e1vel. Assim: Var_{\\theta}(T) \\geq \\frac{[m'(\\theta)]^2}{nI(\\theta)} A igualdade vale se, e somente se, existem fun\u00e7\u00f5es u(\\theta) e v(\\theta) que podem depender em \\theta mas n\u00e3o de X tal que: T = u(\\theta)\\lambda_n'(X|\\theta) + v(\\theta) Se T for n\u00e3o enviesado m(\\theta) = \\theta \\implies m'(\\theta) = 1 Exemplo Num\u00e9rico do limite de Cram\u00e9r-Rao Refer\u00eancia Considere um sinal (como uma m\u00fasica) com tr\u00eas par\u00e2metros, amplitude, frequ\u00eancia e fase inicia. Saberemos o n\u00famero de amostras que sera 100Hz com n\u00edvel de ru\u00eddo de 0.1 s = lambda t,a,f,ph: a*np.sin(2*np.pi*f*t + ph) # fun\u00e7\u00e3o que representa o sinal p0 = [2,8,0] # Amplitude, frequ\u00eancia e fase inicial para testar noise = 0.1 T = np.linspace(0,1,100) #100 valores entre 0 e 1 igualmente espa\u00e7ados plt.plot(T, s(T, *p0), '.-k') plt.xlabel('Tempo (s)') plt.title('Sinal') plt.show() Vamos usar inspect para nos ajudar a pegar labels das fun\u00e7\u00f5es, isto \u00e9, os par\u00e2metros necess\u00e1rios das fun\u00e7\u00f5es. Essa biblioteca fornece v\u00e1rias fun\u00e7\u00f5es de ajuda desse tipo. D\u00ea uma olhada. parameters = str(inspect.signature(s)).strip('()').replace(' ', '').split(',')[1:] p0dict = dict(zip(parameters, p0)) p0dict {'a': 2, 'f': 8, 'ph': 0} No caso geral, calcular a Matriz de Informa\u00e7\u00e3o de Fisher n\u00e3o \u00e9 trivial. Por isso, vamos calcular para o caso em que as medi\u00e7\u00f5es s\u00e3o de uma amostra com distribui\u00e7\u00e3o multivariada normal, isto \u00e9, \u00e9 uma distribui\u00e7\u00e3o normal, s\u00f3 que em mais dimens\u00f5es, em particular, 441 dimens\u00f5es (n\u00famero de pontos no tempo) Se calcularmos a informa\u00e7\u00e3o de Fisher, podemos ver que: \\mathcal{I}_{mn} = \\frac{1}{\\sigma^2} \\frac{\\partial \\mu^\\mathrm{T}}{\\partial \\theta_m} \\frac{\\partial \\mu}{\\partial \\theta_n} = \\frac{1}{\\sigma^2} \\sum_k \\frac{\\partial \\mu_k}{\\partial \\theta_m} \\frac{\\partial \\mu_k}{\\partial \\theta_n} onde \\theta = [a,f,ph]^T , \\mu = \\mu(\\theta) \u00e9 o vetor m\u00e9dia da normal multivariada e \\sigma^2 \u00e9 a vari\u00e2ncia de cada marginal da normal. N\u00e3o se assuste. Na multivariada, temos uma matriz para indicar as vari\u00e2ncias (ela se chama Matriz de Covari\u00e2ncias, na verdade). O que estou dizendo \u00e9 que ela \u00e9 \\sigma^2 vezes a identidade. \u00c9 bom conhecer essa distribui\u00e7\u00e3o! Por enquando acredite em mim! Ou no Wikipedia . Vou chamar D_{ik} = \\frac{\\partial \\mu_k}{\\partial \\theta_i} # Usamos ** para desempacotar elementos de um dicion\u00e1rio. string = \"a: {a} f: {f} ph: {ph}\".format(**p0dict) print(string) a: 2 f: 8 ph: 0 D = np.zeros((len(p0), len(T))) # para cada par\u00e2metro for i, parameter in enumerate(parameters): # para cada ponto no tempo for k, t in enumerate(T): func = lambda x: s(t, **dict(p0dict, **{parameter: x})) # Calculamos a derivada com respeito a x, que nesse caso \u00e9 o valor do parametro D[i,k] = derivative(func, p0dict[parameter], dx = 1e-4) Veja que o tamanho de D \u00e9 o seguinte: D.shape (3, 100) plt.plot(T, s(T, *p0), '--k', lw=2, label='Sinal') for Di, parameter in zip(D, parameters): # Estamos acessando Di = linha_i(D) plt.plot(T, Di, '.-', label=parameter) plt.legend() plt.xlabel('Tempo (s)') plt.show() O que D_{ik} indica? \u00c9 a derivada da k-\u00e9sima m\u00e9dia com respeito ao i-\u00e9simo par\u00e2metro. Logo indica o quanto o quando a amostra k afeta o par\u00e2metro i . Veja que quando temos picos no seno, teremos pico na amplitude,. Tamb\u00e9m vemos que a fase inicial n\u00e3o tem essa relev\u00e2ncia. Vemos tamb\u00e9m que o sinal se torna mais e mais sens\u00edvel \u00e0 frequ\u00eancia. Assim, podemos calular a informa\u00e7\u00e3o de fisher, usando einsum I = 1/noise**2*np.einsum('mk,nk', D, D) print(I) [[ 4.95000000e+03 -5.64643569e+02 -3.43706036e-09] [-5.64643569e+02 2.68635205e+05 6.34601694e+04] [-3.43706036e-09 6.34601694e+04 2.01999999e+04]] Podemos calcular o limite de Cram\u00e9r-Rao para qualquer estimador n\u00e3o enviesado. Nesse caso, veja aqui para mais detalhes. Mas n\u00e3o se incomode com os detalhes, se preferir. iI = np.linalg.inv(I) print('Cram\u00e9r-Rao Limite Inferior') for parameter, variance in zip(parameters, iI.diagonal()): print('{}: {:.2g}'.format(parameter, np.sqrt(variance))) Cram\u00e9r-Rao Limite Inferior a: 0.014 f: 0.0038 ph: 0.014 Estimador Eficiente T \u00e9 um estimador eficiente de sua esperan\u00e7a m(\\theta) se, para todo \\theta , vale a igualdade em Cram\u00e9r-Rao. Mas nem sempre vale a igualdade, inclusive conhecemos uma consdi\u00e7\u00e3o necess\u00e1ria e suficiente para isso, que est\u00e1 logo acima. Estimadores n\u00e3o enviesados com vari\u00e2ncia m\u00ednima Suponha que T seja um estimador eficiente de sua esperan\u00e7a m(\\theta) e T_1 outro estimador n\u00e3o enviesado. Ent\u00e3o para todo valor \\theta \\in \\Omega , Var_{\\theta}(T) ser\u00e1 igual ao limite inferior de Cram\u00e9r-Rao e Var_{\\theta}(T_1) ser\u00e1 pelo menos maior ou igual. Portanto Var_{\\theta}(T) \\leq Var_{\\theta}(T_1), \\forall \\theta . Isto \u00e9, um estimado eficiente de m(\\theta) ter\u00e1 menor vari\u00e2ncia. Distribui\u00e7\u00e3o assint\u00f3tica de um estimador eficiente Assuma as hip\u00f3teses do teorema de Cram\u00e9r-Rao. Seja T um estimador eficiente para a sua m\u00e9dia m(\\theta) e m'(\\theta) \\neq 0 . Ent\u00e3o: \\frac{[nI(\\theta)]^{1/2}}{m'(\\theta)}[T - m(\\theta)] \\overset{d}{\\to} N(0,1) Distribui\u00e7\u00e3o assint\u00f3tica do MLE Suponha que obtemos \\hat{\\theta}_n resolvendo a equa\u00e7\u00e3o \\lambda_n'(x|\\theta) = 0 , isto \u00e9, maximizando a log-verossimilhan\u00e7a (MLE). E suponha que \\lambda_n'' e \\lambda_n''' existem e satisfazem certas condi\u00e7\u00f5es de regularidade. Ent\u00e3o [nI(\\theta)]^{1/2}(\\hat{\\theta}_n - \\theta) \\overset{d}{\\to} N(0,1) Como o MLE \u00e9 n\u00e3o enviesado, ent\u00e3o se ele for Eficiente, j\u00e1 sabemos que esse teorema \u00e9 verdade pelo anterior. (se ele \u00e9 n\u00e3o enviesado) Bayesiano Suponha que adotamos uma priori para \\theta com uma pdf diferenci\u00e1vel no intervalo. Sobre condi\u00e7\u00f5es de regularidade similares \u00e0quelas que garantem normalidade assint\u00f3tica para \\hat{\\theta}_n , pode-se mostrar que que a distribui\u00e7\u00e3o a posteriori de \\theta vai se aproximadamente uma normal com m\u00e9dia \\hat{\\theta}_n e vari\u00e2ncia 1/[nI(\\hat{\\theta}_n)] , onde \\hat{\\theta}_n \u00e9 o MLE.","title":"Informa\u00e7\u00e3o de Fisher"},{"location":"infestatistica_BSc/FisherInformation/FisherInformation/#informacao-de-fisher","text":"Seja X uma amostra aleat\u00f3ria cuja distribui\u00e7\u00e3o depende de \\theta e tem valores em (a,b) \\subset \\mathbb{R} . Seja f_n(x|\\theta) a pdf conjunta de X . Assuma que S = {x | f(x|\\theta) > 0} \u00e9 o mesmo para todo \\theta . E \\lambda_n(x|\\theta) = \\log f_n(x|\\theta) \u00e9 duas vezes diferenci\u00e1vel em \\theta . A informa\u00e7\u00e3o \u00e9: I_n(\\theta) = E_{\\theta}\\{[\\lambda_n '(X|\\theta)]^2\\} Agora assuma que duas derivadas de \\int_S f_n(x|\\theta)dx com respeito a \\theta podemos inverter a ordem de integra\u00e7\u00e3o e diferencia\u00e7\u00e3o . Ent\u00e3o: I_n(\\theta) = - E_{\\theta}[\\lambda_n ''(X|\\theta)]","title":"Informa\u00e7\u00e3o de Fisher"},{"location":"infestatistica_BSc/FisherInformation/FisherInformation/#teorema","text":"I_n(\\theta) = nI(\\theta) Obs.: Estamos tratando da informa\u00e7\u00e3o de Fisher para o caso unidimensional. Para o caso em que temos \\Omega \\subset \\mathbb{R}^k , a informa\u00e7\u00e3o de Fisher ser\u00e1 uma matriz de tamanho k \\times k onde I_{n,i,j} = Cov_{\\theta}\\left[\\frac{\\partial}{\\partial \\theta_i}\\lambda_n'(X|\\theta), \\frac{\\partial}{\\partial \\theta_j}\\lambda_n'(X|\\theta)\\right] import numpy as np from scipy.stats import norm from scipy.misc import derivative from scipy.optimize import curve_fit import matplotlib.pyplot as plt from seaborn import violinplot import inspect","title":"Teorema"},{"location":"infestatistica_BSc/FisherInformation/FisherInformation/#exemplo-construtivo","text":"Vamos pensar num caso bem simples: amostra aleat\u00f3ria X_1, ..., X_n \\sim \\text{Normal}(\\mu, \\sigma^2) , onde o par\u00e2metro \\sigma^2 \u00e9 conhecido e \\mu n\u00e3o. De forma direta, poder\u00edamos perguntar qual a Informa\u00e7\u00e3o de Fisher (ou Informa\u00e7\u00e3o Diferencial) da amostra aleat\u00f3ria sobre o par\u00e2metro desconhecido \\mu . Vamos encontrar a distribui\u00e7\u00e3o conjunta: f(x|\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2}\\right] \\begin{split} f_n(x|\\mu) &= \\prod_{i=1}^n f(x_i|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i^2 - 2x_i\\mu + \\mu^2)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^n x_i^2 - 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\end{split} Vamos encontrar a verossimilhan\u00e7a: \u00e9 a distribui\u00e7\u00e3o conjunta como fun\u00e7\u00e3o do par\u00e2metro! f_n(x|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] Vamos comparar para \\sigma = 1 e \\sigma = 5 loglikelihood = lambda mu, sigma, x: np.sum(np.log([norm(loc = mu, scale = sigma).pdf(xi) for xi in x]), axis = 0) sigmas = [1,3,5,10] mu_true = 5 mu_range = np.linspace(0,10,1000) fig,ax = plt.subplots(2,2,figsize = (16, 10)) fig.suptitle('Comparando Log-verossimilhan\u00e7as da Distribui\u00e7\u00e3o Normal') def generate_curves(sigma, ax, n = 20, n_times = 50): for i in range(n_times): x = np.random.normal(loc = mu_true, scale = sigma, size = n) logvalues = loglikelihood(mu_range, sigma, x) ax.plot(mu_range, logvalues, color = 'blue', alpha = 0.2) ax.vlines(mu_true, ymin = ax.get_ylim()[0], ymax = ax.get_ylim()[1], linestyle = '--') ax.set_title(r'$\\sigma =$ {}'.format(sigma)) ax.set_xlabel(r'$\\mu$') generate_curves(sigmas[0], ax[0][0]) generate_curves(sigmas[1], ax[0][1]) generate_curves(sigmas[2], ax[1][0]) generate_curves(sigmas[3], ax[1][1]) Vamos ver como se comporta derivada. Esse \u00e9 o score: \\lambda '_n(y|\\mu) = \\frac{1}{\\sigma^2}\\left(n\\bar{x}_n - \\mu\\right) score = lambda mu, sigma, x: derivative(loglikelihood, mu, dx = 1e-5, args = (sigma, x)) fig,ax = plt.subplots(2,2,figsize = (16, 10)) fig.suptitle('Comparando Scores da Distribui\u00e7\u00e3o Normal') def generate_curves(sigma, ax, n = 20, n_times = 50): for i in range(n_times): x = np.random.normal(loc = mu_true, scale = sigma, size = n) scorevalues = score(mu_range, sigma, x) ax.plot(mu_range, scorevalues, color = 'blue', alpha = 0.2) ax.vlines(mu_true, ymin = ax.get_ylim()[0], ymax = ax.get_ylim()[1], linestyle = '--') ax.set_title(r'$\\sigma =$ {}'.format(sigma)) ax.set_xlabel(r'$\\mu$') ax.set_ylim((-10,10)) generate_curves(sigmas[0], ax[0][0]) generate_curves(sigmas[1], ax[0][1]) generate_curves(sigmas[2], ax[1][0]) generate_curves(sigmas[3], ax[1][1]) fig,ax = plt.subplots(2,2,figsize = (16, 10)) fig.suptitle('Comparando Histogramas dos Scores para mu') def generate_histograms(mu, sigma, ax, n = 15, n_times = 100): scorevalues = [] for i in range(n_times): x = np.random.normal(loc = mu_true, scale = sigma, size = n) scorevalues.append(score(mu, sigma, x)) violinplot(scorevalues, ax = ax) ax.set_title(r'$\\sigma =$ {}'.format(sigma)) ax.set_xlabel('score') generate_histograms(5, sigmas[0], ax[0][0]) generate_histograms(5, sigmas[1], ax[0][1]) generate_histograms(5, sigmas[2], ax[1][0]) generate_histograms(5, sigmas[3], ax[1][1]) A informa\u00e7\u00e3o de Fisher \u00e9 a Vari\u00e2ncia da fun\u00e7\u00e3o score em X , isto \u00e9: \\begin{split} I_n(\\mu) &= Var(\\lambda '_n(x|p)) = E[(\\lambda '_n(x|p))^2] - E[\\lambda '_n(x|p)]^2\\\\ &= \\frac{1}{\\sigma^4}Var\\left[n\\bar{x}_n - \\mu\\right] \\\\ &= \\frac{n^2}{\\sigma^4}Var(\\bar{x}_n) \\\\ &= \\frac{n^2\\sigma^2}{n\\sigma^4} \\\\ &= \\frac{n}{\\sigma^2} \\end{split}","title":"Exemplo Construtivo"},{"location":"infestatistica_BSc/FisherInformation/FisherInformation/#desigualdade-de-cramer-rao","text":"Seja X uma amostra aleat\u00f3ria com pdf f(x| \\theta) . Suponha as hip\u00f3teses acima acerca dessa distribui\u00e7\u00e3o. Seja T = r(X) com vari\u00e2ncia finita e m(\\theta) = E_{\\theta}(T) \u00e9 diferenci\u00e1vel. Assim: Var_{\\theta}(T) \\geq \\frac{[m'(\\theta)]^2}{nI(\\theta)} A igualdade vale se, e somente se, existem fun\u00e7\u00f5es u(\\theta) e v(\\theta) que podem depender em \\theta mas n\u00e3o de X tal que: T = u(\\theta)\\lambda_n'(X|\\theta) + v(\\theta) Se T for n\u00e3o enviesado m(\\theta) = \\theta \\implies m'(\\theta) = 1","title":"Desigualdade de Cram\u00e9r-Rao"},{"location":"infestatistica_BSc/FisherInformation/FisherInformation/#exemplo-numerico-do-limite-de-cramer-rao","text":"Refer\u00eancia Considere um sinal (como uma m\u00fasica) com tr\u00eas par\u00e2metros, amplitude, frequ\u00eancia e fase inicia. Saberemos o n\u00famero de amostras que sera 100Hz com n\u00edvel de ru\u00eddo de 0.1 s = lambda t,a,f,ph: a*np.sin(2*np.pi*f*t + ph) # fun\u00e7\u00e3o que representa o sinal p0 = [2,8,0] # Amplitude, frequ\u00eancia e fase inicial para testar noise = 0.1 T = np.linspace(0,1,100) #100 valores entre 0 e 1 igualmente espa\u00e7ados plt.plot(T, s(T, *p0), '.-k') plt.xlabel('Tempo (s)') plt.title('Sinal') plt.show() Vamos usar inspect para nos ajudar a pegar labels das fun\u00e7\u00f5es, isto \u00e9, os par\u00e2metros necess\u00e1rios das fun\u00e7\u00f5es. Essa biblioteca fornece v\u00e1rias fun\u00e7\u00f5es de ajuda desse tipo. D\u00ea uma olhada. parameters = str(inspect.signature(s)).strip('()').replace(' ', '').split(',')[1:] p0dict = dict(zip(parameters, p0)) p0dict {'a': 2, 'f': 8, 'ph': 0} No caso geral, calcular a Matriz de Informa\u00e7\u00e3o de Fisher n\u00e3o \u00e9 trivial. Por isso, vamos calcular para o caso em que as medi\u00e7\u00f5es s\u00e3o de uma amostra com distribui\u00e7\u00e3o multivariada normal, isto \u00e9, \u00e9 uma distribui\u00e7\u00e3o normal, s\u00f3 que em mais dimens\u00f5es, em particular, 441 dimens\u00f5es (n\u00famero de pontos no tempo) Se calcularmos a informa\u00e7\u00e3o de Fisher, podemos ver que: \\mathcal{I}_{mn} = \\frac{1}{\\sigma^2} \\frac{\\partial \\mu^\\mathrm{T}}{\\partial \\theta_m} \\frac{\\partial \\mu}{\\partial \\theta_n} = \\frac{1}{\\sigma^2} \\sum_k \\frac{\\partial \\mu_k}{\\partial \\theta_m} \\frac{\\partial \\mu_k}{\\partial \\theta_n} onde \\theta = [a,f,ph]^T , \\mu = \\mu(\\theta) \u00e9 o vetor m\u00e9dia da normal multivariada e \\sigma^2 \u00e9 a vari\u00e2ncia de cada marginal da normal. N\u00e3o se assuste. Na multivariada, temos uma matriz para indicar as vari\u00e2ncias (ela se chama Matriz de Covari\u00e2ncias, na verdade). O que estou dizendo \u00e9 que ela \u00e9 \\sigma^2 vezes a identidade. \u00c9 bom conhecer essa distribui\u00e7\u00e3o! Por enquando acredite em mim! Ou no Wikipedia . Vou chamar D_{ik} = \\frac{\\partial \\mu_k}{\\partial \\theta_i} # Usamos ** para desempacotar elementos de um dicion\u00e1rio. string = \"a: {a} f: {f} ph: {ph}\".format(**p0dict) print(string) a: 2 f: 8 ph: 0 D = np.zeros((len(p0), len(T))) # para cada par\u00e2metro for i, parameter in enumerate(parameters): # para cada ponto no tempo for k, t in enumerate(T): func = lambda x: s(t, **dict(p0dict, **{parameter: x})) # Calculamos a derivada com respeito a x, que nesse caso \u00e9 o valor do parametro D[i,k] = derivative(func, p0dict[parameter], dx = 1e-4) Veja que o tamanho de D \u00e9 o seguinte: D.shape (3, 100) plt.plot(T, s(T, *p0), '--k', lw=2, label='Sinal') for Di, parameter in zip(D, parameters): # Estamos acessando Di = linha_i(D) plt.plot(T, Di, '.-', label=parameter) plt.legend() plt.xlabel('Tempo (s)') plt.show() O que D_{ik} indica? \u00c9 a derivada da k-\u00e9sima m\u00e9dia com respeito ao i-\u00e9simo par\u00e2metro. Logo indica o quanto o quando a amostra k afeta o par\u00e2metro i . Veja que quando temos picos no seno, teremos pico na amplitude,. Tamb\u00e9m vemos que a fase inicial n\u00e3o tem essa relev\u00e2ncia. Vemos tamb\u00e9m que o sinal se torna mais e mais sens\u00edvel \u00e0 frequ\u00eancia. Assim, podemos calular a informa\u00e7\u00e3o de fisher, usando einsum I = 1/noise**2*np.einsum('mk,nk', D, D) print(I) [[ 4.95000000e+03 -5.64643569e+02 -3.43706036e-09] [-5.64643569e+02 2.68635205e+05 6.34601694e+04] [-3.43706036e-09 6.34601694e+04 2.01999999e+04]] Podemos calcular o limite de Cram\u00e9r-Rao para qualquer estimador n\u00e3o enviesado. Nesse caso, veja aqui para mais detalhes. Mas n\u00e3o se incomode com os detalhes, se preferir. iI = np.linalg.inv(I) print('Cram\u00e9r-Rao Limite Inferior') for parameter, variance in zip(parameters, iI.diagonal()): print('{}: {:.2g}'.format(parameter, np.sqrt(variance))) Cram\u00e9r-Rao Limite Inferior a: 0.014 f: 0.0038 ph: 0.014","title":"Exemplo Num\u00e9rico do limite de Cram\u00e9r-Rao"},{"location":"infestatistica_BSc/FisherInformation/FisherInformation/#estimador-eficiente","text":"T \u00e9 um estimador eficiente de sua esperan\u00e7a m(\\theta) se, para todo \\theta , vale a igualdade em Cram\u00e9r-Rao. Mas nem sempre vale a igualdade, inclusive conhecemos uma consdi\u00e7\u00e3o necess\u00e1ria e suficiente para isso, que est\u00e1 logo acima.","title":"Estimador Eficiente"},{"location":"infestatistica_BSc/FisherInformation/FisherInformation/#estimadores-nao-enviesados-com-variancia-minima","text":"Suponha que T seja um estimador eficiente de sua esperan\u00e7a m(\\theta) e T_1 outro estimador n\u00e3o enviesado. Ent\u00e3o para todo valor \\theta \\in \\Omega , Var_{\\theta}(T) ser\u00e1 igual ao limite inferior de Cram\u00e9r-Rao e Var_{\\theta}(T_1) ser\u00e1 pelo menos maior ou igual. Portanto Var_{\\theta}(T) \\leq Var_{\\theta}(T_1), \\forall \\theta . Isto \u00e9, um estimado eficiente de m(\\theta) ter\u00e1 menor vari\u00e2ncia.","title":"Estimadores n\u00e3o enviesados com vari\u00e2ncia m\u00ednima"},{"location":"infestatistica_BSc/FisherInformation/FisherInformation/#distribuicao-assintotica-de-um-estimador-eficiente","text":"Assuma as hip\u00f3teses do teorema de Cram\u00e9r-Rao. Seja T um estimador eficiente para a sua m\u00e9dia m(\\theta) e m'(\\theta) \\neq 0 . Ent\u00e3o: \\frac{[nI(\\theta)]^{1/2}}{m'(\\theta)}[T - m(\\theta)] \\overset{d}{\\to} N(0,1)","title":"Distribui\u00e7\u00e3o assint\u00f3tica de um estimador eficiente"},{"location":"infestatistica_BSc/FisherInformation/FisherInformation/#distribuicao-assintotica-do-mle","text":"Suponha que obtemos \\hat{\\theta}_n resolvendo a equa\u00e7\u00e3o \\lambda_n'(x|\\theta) = 0 , isto \u00e9, maximizando a log-verossimilhan\u00e7a (MLE). E suponha que \\lambda_n'' e \\lambda_n''' existem e satisfazem certas condi\u00e7\u00f5es de regularidade. Ent\u00e3o [nI(\\theta)]^{1/2}(\\hat{\\theta}_n - \\theta) \\overset{d}{\\to} N(0,1) Como o MLE \u00e9 n\u00e3o enviesado, ent\u00e3o se ele for Eficiente, j\u00e1 sabemos que esse teorema \u00e9 verdade pelo anterior. (se ele \u00e9 n\u00e3o enviesado)","title":"Distribui\u00e7\u00e3o assint\u00f3tica do MLE"},{"location":"infestatistica_BSc/FisherInformation/FisherInformation/#bayesiano","text":"Suponha que adotamos uma priori para \\theta com uma pdf diferenci\u00e1vel no intervalo. Sobre condi\u00e7\u00f5es de regularidade similares \u00e0quelas que garantem normalidade assint\u00f3tica para \\hat{\\theta}_n , pode-se mostrar que que a distribui\u00e7\u00e3o a posteriori de \\theta vai se aproximadamente uma normal com m\u00e9dia \\hat{\\theta}_n e vari\u00e2ncia 1/[nI(\\hat{\\theta}_n)] , onde \\hat{\\theta}_n \u00e9 o MLE.","title":"Bayesiano"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/","text":"Grandes Amostras Desigualdade de Markov P(X \\geq t) \\leq \\frac{E[X^n]}{t^n}, ~dado~que~P(X \\geq 0) = 1, t > 0 Desigualdade de Chebyshev Seja X uma vari\u00e1vel aleat\u00f3ria em que o segundo momento \u00e9 finito. Ent\u00e3o, \\forall t > 0 . P(|X - E[X]| \\geq t) \\leq \\frac{Var[X]}{t^2} Propriedades Importantes X_1, ..., X_n amostra aleat\u00f3ria (por defini\u00e7\u00e3o mesma distribui\u00e7\u00e3o e independentes), com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 . Ent\u00e3o E[\\bar{X_n}] = \\mu e Var[\\bar{X_n}] = \\sigma^2/n . Importando bibliotecas import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set() Exemplo 6.2.2 Um engenheiro ambiental acredita que existam dois contaminantes na \u00e1gua: ars\u00eanico e chumbo. Suponha que ambas s\u00e3o vari\u00e1veis aleat\u00f3rias independentes X e Y , medidas na mesma unidade. O engenheiro est\u00e1 interessado em saber a propor\u00e7\u00e3o de contamina\u00e7\u00e3o por chumbo em m\u00e9dia, isto \u00e9, E[R] = E[Y/(X + Y)] . Como nem sempre conhecemos a distribui\u00e7\u00e3o de R, podemos aproximar o valor esperado atrav\u00e9s de uma m\u00e9dia amostral de R , atrav\u00e9s de observa\u00e7\u00f5es (X_1, Y_1), ..., (X_n, Y_n) . Usando a desigualdade de Chebyshev, (tente ver que Var[R] \\leq 1 ). P(|\\bar{R_n} - E[R]| \\geq \\epsilon) \\leq \\frac{1}{n\\epsilon^2} # Usando apenas Chebyshev: epsilon = 0.0005 prob = 0.95 # probabilidade m\u00ednima de que a diferen\u00e7a entre a # m\u00e9dia amostral e o valor esperado seja menor do que epsilon def get_number_simulations(epsilon, prob): assert prob >= 0 assert prob <= 1 # Queremos que P <= 1 - prob-> 1/(n*eps**2) <= 1 - prob min_n = 1/((1 - prob)*(epsilon**2)) min_n = np.ceil(min_n) print('----------------------------------------------------------------------') print('In order to have the sample mean at least {} close with probability {}: '.format(epsilon, prob)) print('The minimum number of simulations are: {}'. format(int(min_n))) print('----------------------------------------------------------------------') return min_n def get_epsilon(prob, n): assert prob >= 0 assert prob <= 1 # Queremos encontrar epsilon para que 1/(n*eps**2) = 1 - prob eps = np.sqrt(1/(n*(1 - prob))) return eps _ = get_number_simulations(epsilon, prob) ---------------------------------------------------------------------- In order to have the sample mean at least 0.0005 close with probability 0.95: The minimum number of simulations are: 80000000 ---------------------------------------------------------------------- # Testando com distribui\u00e7\u00e3o uniforme (X e Y tem distribui\u00e7\u00f5es uniformes) # Nesse caso, podemos provar que E[R] = 0.5 probs = [0.6, 0.75, 0.9, 0.95, 0.99] n_range = np.array([j*10**i for i in [2,3,4,5,6,7] for j in [1,2,3,4,5,6,7,8,9]]) E_R = [] for n in n_range: X = np.random.uniform(0,1,size = int(n)) #np = numpy Y = np.random.uniform(0,1,size = int(n)) R = Y/(X + Y) E_R.append(np.mean(R)) # E[R] = 0.5 chebyshev_interval = np.empty(shape = (len(probs), len(n_range))) for i, prob in enumerate(probs): chebyshev_interval[i, :] = get_epsilon(prob, n_range) # Plotando fig, ax = plt.subplots(1, 2, figsize = (16, 6)) for i in [0,1]: ax[i].plot(n_range, E_R, color = 'darkred') ax[i].hlines(0.5, xmin = min(n_range), xmax = max(n_range), linestyle = '--',alpha = 0.4, color = 'black') ax[i].set_xscale('log') colors = ['black', 'red', 'green', 'blue', 'pink'] for i in range(len(probs)): ax[1].fill_between(x = n_range, y1 = 0.5 + chebyshev_interval[i,:], y2 = 0.5 - chebyshev_interval[i,:], color = colors[i], alpha = 0.3 + 0.5*(len(probs) - i)/(len(probs)), label = probs[i]) ax[1].legend() ax[0].set_title('Different mean samples', fontsize = 15) ax[1].set_title('Chebyshev bound with prob', fontsize = 15) plt.show() Lei dos Grandes N\u00fameros Converg\u00eancia em Probabilidade \\forall \\epsilon > 0, \\lim_{n\\to\\infty} P[|Z_n - b| < \\epsilon] = 1 \\iff Z_n \\overset{p}{\\to} b Converg\u00eancia quase certa (Implica a anterior) P[\\lim_{n_\\to\\infty} Z_n = b] = 1 Vers\u00e3o Fraca X_1, \\dots, X_n amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o com m\u00e9dia \\mu e vari\u00e2ncia vinita. Se \\bar{X}_n \u00e9 a m\u00e9dia amostral. Ent\u00e3o \\bar{X}_n \\overset{p}{\\to} \\mu . Vers\u00e3o Forte P[\\lim_{n\\to\\infty} \\bar{X}_n = \\mu] = 1 Histogramas S\u00e3o usados para aproximar uma fun\u00e7\u00e3o de densidade de probabilidade de forma discreta. Seja X_1, X_2, \\dots vari\u00e1veis aleat\u00f3rias iid. Seja c_1 < c_2 constantes. Seja Y_i uma indicadora para c_1\\leq X_i < c_2 . Ent\u00e3o \\bar{Y}_n (propor\u00e7\u00e3o de valores X_1, ..., X_n no intervalo [c_1, c_2) e \\bar{Y}_n \\overset{p}{\\to} P[c_1 \\leq X_1 < c_2] . # Exemplo 6.2.4 lamda = 0.5 # N\u00e3o posso usar lambda beta = 1/lamda # Numpy usa esse par\u00e2metro t = np.arange(0.0001, 15, 0.01) X_true = lamda*np.exp(-lamda*t) fig, ax = plt.subplots(figsize = (14, 5)) for n in [1, 10, 100, 1000, 10000]: X = np.random.exponential(scale = beta, size = n) sns.distplot(X, ax = ax, kde = False, norm_hist = True, label = 'n='+str(n)) # area = 1 sns.lineplot(t, X_true, ax = ax, lw = 3) ax.set_title('Histograma da Distribui\u00e7\u00e3o Exponencial') ax.legend() plt.show() Teorema Central do Limite Se as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n formam uma amostra aleat\u00f3ria de tamanho n para uma dada distribui\u00e7\u00e3o de m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 finita, ent\u00e3o para cada n\u00famero x , lim_{n\\to\\infty} P[\\frac{\\bar{X}_n - \\mu}{\\sigma/n^{1/2}} \\leq x] = \\Phi(x), onde \\Phi \u00e9 a fun\u00e7\u00e3o de densidade acumulada da distribui\u00e7\u00e3o normal!!! coffee_df = pd.read_csv('../data/CoffeeAndCode.csv') display(coffee_df.head()) display(coffee_df.shape) display(coffee_df.describe()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay CoffeeTime CodingWithoutCoffee CoffeeType CoffeeSolveBugs Gender Country AgeRange 0 8 2 Before coding Yes Caff\u00e8 latte Sometimes Female Lebanon 18 to 29 1 3 2 Before coding Yes Americano Yes Female Lebanon 30 to 39 2 5 3 While coding No Nescafe Yes Female Lebanon 18 to 29 3 8 2 Before coding No Nescafe Yes Male Lebanon NaN 4 10 3 While coding Sometimes Turkish No Male Lebanon 18 to 29 (100, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay count 100.000000 100.000000 mean 6.410000 2.890000 std 2.644205 1.613673 min 1.000000 1.000000 25% 4.000000 2.000000 50% 7.000000 2.500000 75% 8.000000 4.000000 max 10.000000 8.000000 # Plotting fig, ax = plt.subplots(figsize = (7, 5)) sns.distplot(coffee_df.CoffeeCupsPerDay, ax = ax) ax.vlines(coffee_df.CoffeeCupsPerDay.mean(), ymin = 0, ymax = 1, linestyle = '--', color = 'black', label = 'black') ax.annotate('M\u00e9dia:'+str(coffee_df.CoffeeCupsPerDay.mean()), (coffee_df.CoffeeCupsPerDay.mean() + 0.5, 0.4)) ax.set_title('Histrograma de copos de caf\u00e9 bebidos por programadores') ax.set_ylabel('Frequ\u00eancia') ax.set_ylim((0,0.45)) plt.show() # Generating sample means samples = [10,50,150,300,500,1000] n_experiments = 500 experiments_coffe_cups = np.empty((n_experiments, len(samples))) for j, sample_size in enumerate(samples): sample = coffee_df.CoffeeCupsPerDay.sample(n = sample_size*n_experiments, replace = True) matrix = np.array(sample).reshape((n_experiments, sample_size)) experiments_coffe_cups[:,j] = matrix.mean(axis = 1) experiments_coffe_cups_df = pd.DataFrame(experiments_coffe_cups, columns = samples) fig, ax = plt.subplots(2,3, figsize = (20,10)) for index, column in enumerate(experiments_coffe_cups_df.columns): i = int(index/3) j = index % 3 sns.distplot(experiments_coffe_cups_df[column], ax = ax[i][j]) ax[i][j].set_title('M\u00e9dia amostral com {} amostras'.format(column)) ax[i][j].set_ylabel('Frequ\u00eancia') fig.suptitle('Histogramas com diferentes n\u00fameros de amostras') plt.show() M\u00e9todo Delta Seja Y_1, Y_2, \\dots uma sequ\u00eancia de v.a. e F uma fun\u00e7\u00e3o de densidade acumulada cont\u00ednua. Sejam \\theta \\in \\mathbb{R} e \\{a_n\\}_{n\\in\\mathbb{N}} que tende ao \\infty . Suponha que a_n(Y_n - \\theta) converge para F . Seja \\alpha uma fun\u00e7\u00e3o com derivada cont\u00ednua, tal que \\alpha '(\\theta) \\neq 0 . Ent\u00e3o a_n[\\alpha(Y_n) - \\alpha(\\theta)]/\\alpha '(\\theta) converge para a distribui\u00e7\u00e3o F . Teorema de Slutsky \\begin{align*} {X}^{(n)}& \\overset{d}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{d}{\\to} X,\\\\ {X}^{(n)}& \\overset{p}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{p}{\\to} X,\\\\ {X}^{(n)}& \\overset{as}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{as}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{as}{\\to} X. \\end{align*} Corol\u00e1rio Se f \u00e9 uma fun\u00e7\u00e3o cont\u00ednua: {X}^{(n)}\\overset{d}{\\to} X \\quad \\text{ e }\\quad {Y}^{(n)}\\overset{p}{\\to} c\\quad \\text{implica}\\quad f ({X}^{(n)},{Y}^{(n)}) \\overset{d}{\\to} f(X,c). Aproxima\u00e7\u00e3o de Taylor e M\u00e9todo Delta Refer\u00eancia de Probabilidade","title":"Grandes Amostras"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#grandes-amostras","text":"","title":"Grandes Amostras"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#desigualdade-de-markov","text":"P(X \\geq t) \\leq \\frac{E[X^n]}{t^n}, ~dado~que~P(X \\geq 0) = 1, t > 0","title":"Desigualdade de Markov"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#desigualdade-de-chebyshev","text":"Seja X uma vari\u00e1vel aleat\u00f3ria em que o segundo momento \u00e9 finito. Ent\u00e3o, \\forall t > 0 . P(|X - E[X]| \\geq t) \\leq \\frac{Var[X]}{t^2}","title":"Desigualdade de Chebyshev"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#propriedades-importantes","text":"X_1, ..., X_n amostra aleat\u00f3ria (por defini\u00e7\u00e3o mesma distribui\u00e7\u00e3o e independentes), com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 . Ent\u00e3o E[\\bar{X_n}] = \\mu e Var[\\bar{X_n}] = \\sigma^2/n .","title":"Propriedades Importantes"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#importando-bibliotecas","text":"import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set()","title":"Importando bibliotecas"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#exemplo-622","text":"Um engenheiro ambiental acredita que existam dois contaminantes na \u00e1gua: ars\u00eanico e chumbo. Suponha que ambas s\u00e3o vari\u00e1veis aleat\u00f3rias independentes X e Y , medidas na mesma unidade. O engenheiro est\u00e1 interessado em saber a propor\u00e7\u00e3o de contamina\u00e7\u00e3o por chumbo em m\u00e9dia, isto \u00e9, E[R] = E[Y/(X + Y)] . Como nem sempre conhecemos a distribui\u00e7\u00e3o de R, podemos aproximar o valor esperado atrav\u00e9s de uma m\u00e9dia amostral de R , atrav\u00e9s de observa\u00e7\u00f5es (X_1, Y_1), ..., (X_n, Y_n) . Usando a desigualdade de Chebyshev, (tente ver que Var[R] \\leq 1 ). P(|\\bar{R_n} - E[R]| \\geq \\epsilon) \\leq \\frac{1}{n\\epsilon^2} # Usando apenas Chebyshev: epsilon = 0.0005 prob = 0.95 # probabilidade m\u00ednima de que a diferen\u00e7a entre a # m\u00e9dia amostral e o valor esperado seja menor do que epsilon def get_number_simulations(epsilon, prob): assert prob >= 0 assert prob <= 1 # Queremos que P <= 1 - prob-> 1/(n*eps**2) <= 1 - prob min_n = 1/((1 - prob)*(epsilon**2)) min_n = np.ceil(min_n) print('----------------------------------------------------------------------') print('In order to have the sample mean at least {} close with probability {}: '.format(epsilon, prob)) print('The minimum number of simulations are: {}'. format(int(min_n))) print('----------------------------------------------------------------------') return min_n def get_epsilon(prob, n): assert prob >= 0 assert prob <= 1 # Queremos encontrar epsilon para que 1/(n*eps**2) = 1 - prob eps = np.sqrt(1/(n*(1 - prob))) return eps _ = get_number_simulations(epsilon, prob) ---------------------------------------------------------------------- In order to have the sample mean at least 0.0005 close with probability 0.95: The minimum number of simulations are: 80000000 ---------------------------------------------------------------------- # Testando com distribui\u00e7\u00e3o uniforme (X e Y tem distribui\u00e7\u00f5es uniformes) # Nesse caso, podemos provar que E[R] = 0.5 probs = [0.6, 0.75, 0.9, 0.95, 0.99] n_range = np.array([j*10**i for i in [2,3,4,5,6,7] for j in [1,2,3,4,5,6,7,8,9]]) E_R = [] for n in n_range: X = np.random.uniform(0,1,size = int(n)) #np = numpy Y = np.random.uniform(0,1,size = int(n)) R = Y/(X + Y) E_R.append(np.mean(R)) # E[R] = 0.5 chebyshev_interval = np.empty(shape = (len(probs), len(n_range))) for i, prob in enumerate(probs): chebyshev_interval[i, :] = get_epsilon(prob, n_range) # Plotando fig, ax = plt.subplots(1, 2, figsize = (16, 6)) for i in [0,1]: ax[i].plot(n_range, E_R, color = 'darkred') ax[i].hlines(0.5, xmin = min(n_range), xmax = max(n_range), linestyle = '--',alpha = 0.4, color = 'black') ax[i].set_xscale('log') colors = ['black', 'red', 'green', 'blue', 'pink'] for i in range(len(probs)): ax[1].fill_between(x = n_range, y1 = 0.5 + chebyshev_interval[i,:], y2 = 0.5 - chebyshev_interval[i,:], color = colors[i], alpha = 0.3 + 0.5*(len(probs) - i)/(len(probs)), label = probs[i]) ax[1].legend() ax[0].set_title('Different mean samples', fontsize = 15) ax[1].set_title('Chebyshev bound with prob', fontsize = 15) plt.show()","title":"Exemplo 6.2.2"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#lei-dos-grandes-numeros","text":"","title":"Lei dos Grandes N\u00fameros"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#convergencia-em-probabilidade","text":"\\forall \\epsilon > 0, \\lim_{n\\to\\infty} P[|Z_n - b| < \\epsilon] = 1 \\iff Z_n \\overset{p}{\\to} b","title":"Converg\u00eancia em Probabilidade"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#convergencia-quase-certa-implica-a-anterior","text":"P[\\lim_{n_\\to\\infty} Z_n = b] = 1","title":"Converg\u00eancia quase certa (Implica a anterior)"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#versao-fraca","text":"X_1, \\dots, X_n amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o com m\u00e9dia \\mu e vari\u00e2ncia vinita. Se \\bar{X}_n \u00e9 a m\u00e9dia amostral. Ent\u00e3o \\bar{X}_n \\overset{p}{\\to} \\mu .","title":"Vers\u00e3o Fraca"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#versao-forte","text":"P[\\lim_{n\\to\\infty} \\bar{X}_n = \\mu] = 1","title":"Vers\u00e3o Forte"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#histogramas","text":"S\u00e3o usados para aproximar uma fun\u00e7\u00e3o de densidade de probabilidade de forma discreta. Seja X_1, X_2, \\dots vari\u00e1veis aleat\u00f3rias iid. Seja c_1 < c_2 constantes. Seja Y_i uma indicadora para c_1\\leq X_i < c_2 . Ent\u00e3o \\bar{Y}_n (propor\u00e7\u00e3o de valores X_1, ..., X_n no intervalo [c_1, c_2) e \\bar{Y}_n \\overset{p}{\\to} P[c_1 \\leq X_1 < c_2] . # Exemplo 6.2.4 lamda = 0.5 # N\u00e3o posso usar lambda beta = 1/lamda # Numpy usa esse par\u00e2metro t = np.arange(0.0001, 15, 0.01) X_true = lamda*np.exp(-lamda*t) fig, ax = plt.subplots(figsize = (14, 5)) for n in [1, 10, 100, 1000, 10000]: X = np.random.exponential(scale = beta, size = n) sns.distplot(X, ax = ax, kde = False, norm_hist = True, label = 'n='+str(n)) # area = 1 sns.lineplot(t, X_true, ax = ax, lw = 3) ax.set_title('Histograma da Distribui\u00e7\u00e3o Exponencial') ax.legend() plt.show()","title":"Histogramas"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#teorema-central-do-limite","text":"Se as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n formam uma amostra aleat\u00f3ria de tamanho n para uma dada distribui\u00e7\u00e3o de m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 finita, ent\u00e3o para cada n\u00famero x , lim_{n\\to\\infty} P[\\frac{\\bar{X}_n - \\mu}{\\sigma/n^{1/2}} \\leq x] = \\Phi(x), onde \\Phi \u00e9 a fun\u00e7\u00e3o de densidade acumulada da distribui\u00e7\u00e3o normal!!! coffee_df = pd.read_csv('../data/CoffeeAndCode.csv') display(coffee_df.head()) display(coffee_df.shape) display(coffee_df.describe()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay CoffeeTime CodingWithoutCoffee CoffeeType CoffeeSolveBugs Gender Country AgeRange 0 8 2 Before coding Yes Caff\u00e8 latte Sometimes Female Lebanon 18 to 29 1 3 2 Before coding Yes Americano Yes Female Lebanon 30 to 39 2 5 3 While coding No Nescafe Yes Female Lebanon 18 to 29 3 8 2 Before coding No Nescafe Yes Male Lebanon NaN 4 10 3 While coding Sometimes Turkish No Male Lebanon 18 to 29 (100, 9) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CodingHours CoffeeCupsPerDay count 100.000000 100.000000 mean 6.410000 2.890000 std 2.644205 1.613673 min 1.000000 1.000000 25% 4.000000 2.000000 50% 7.000000 2.500000 75% 8.000000 4.000000 max 10.000000 8.000000 # Plotting fig, ax = plt.subplots(figsize = (7, 5)) sns.distplot(coffee_df.CoffeeCupsPerDay, ax = ax) ax.vlines(coffee_df.CoffeeCupsPerDay.mean(), ymin = 0, ymax = 1, linestyle = '--', color = 'black', label = 'black') ax.annotate('M\u00e9dia:'+str(coffee_df.CoffeeCupsPerDay.mean()), (coffee_df.CoffeeCupsPerDay.mean() + 0.5, 0.4)) ax.set_title('Histrograma de copos de caf\u00e9 bebidos por programadores') ax.set_ylabel('Frequ\u00eancia') ax.set_ylim((0,0.45)) plt.show() # Generating sample means samples = [10,50,150,300,500,1000] n_experiments = 500 experiments_coffe_cups = np.empty((n_experiments, len(samples))) for j, sample_size in enumerate(samples): sample = coffee_df.CoffeeCupsPerDay.sample(n = sample_size*n_experiments, replace = True) matrix = np.array(sample).reshape((n_experiments, sample_size)) experiments_coffe_cups[:,j] = matrix.mean(axis = 1) experiments_coffe_cups_df = pd.DataFrame(experiments_coffe_cups, columns = samples) fig, ax = plt.subplots(2,3, figsize = (20,10)) for index, column in enumerate(experiments_coffe_cups_df.columns): i = int(index/3) j = index % 3 sns.distplot(experiments_coffe_cups_df[column], ax = ax[i][j]) ax[i][j].set_title('M\u00e9dia amostral com {} amostras'.format(column)) ax[i][j].set_ylabel('Frequ\u00eancia') fig.suptitle('Histogramas com diferentes n\u00fameros de amostras') plt.show()","title":"Teorema Central do Limite"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#metodo-delta","text":"Seja Y_1, Y_2, \\dots uma sequ\u00eancia de v.a. e F uma fun\u00e7\u00e3o de densidade acumulada cont\u00ednua. Sejam \\theta \\in \\mathbb{R} e \\{a_n\\}_{n\\in\\mathbb{N}} que tende ao \\infty . Suponha que a_n(Y_n - \\theta) converge para F . Seja \\alpha uma fun\u00e7\u00e3o com derivada cont\u00ednua, tal que \\alpha '(\\theta) \\neq 0 . Ent\u00e3o a_n[\\alpha(Y_n) - \\alpha(\\theta)]/\\alpha '(\\theta) converge para a distribui\u00e7\u00e3o F .","title":"M\u00e9todo Delta"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#teorema-de-slutsky","text":"\\begin{align*} {X}^{(n)}& \\overset{d}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{d}{\\to} X,\\\\ {X}^{(n)}& \\overset{p}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{p}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{p}{\\to} X,\\\\ {X}^{(n)}& \\overset{as}{\\to} X\\quad \\text{ e }\\quad ({X}^{(n)}-{Y}^{(n)})\\overset{as}{\\to} 0 \\quad \\text{implica} \\quad {Y}^{(n)}\\overset{as}{\\to} X. \\end{align*}","title":"Teorema de Slutsky"},{"location":"infestatistica_BSc/LargeRandomSamples/LargeRandomSamples/#corolario","text":"Se f \u00e9 uma fun\u00e7\u00e3o cont\u00ednua: {X}^{(n)}\\overset{d}{\\to} X \\quad \\text{ e }\\quad {Y}^{(n)}\\overset{p}{\\to} c\\quad \\text{implica}\\quad f ({X}^{(n)},{Y}^{(n)}) \\overset{d}{\\to} f(X,c). Aproxima\u00e7\u00e3o de Taylor e M\u00e9todo Delta Refer\u00eancia de Probabilidade","title":"Corol\u00e1rio"},{"location":"infestatistica_BSc/LinearModel/LinearModel/","text":"Modelo Linear M\u00ednimos quadrados Sejam os pontos (x_1, y_1), ..., (x_n,y_n) . A reta que minimiza \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_i))^2 segundo \\beta_0 e \\beta_1 tem inclina\u00e7\u00e3o e intecepto \\hat{\\beta}_1 = \\dfrac{\\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x} A verifica\u00e7\u00e3o desse resultado \u00e9 direta ao derivar o funcional objetivo com respeito aos par\u00e2metros e igualando a 0. Al\u00e9m disso, observar que a Hessiana \u00e9 definida positiva, o que garante a exist\u00eancia do m\u00ednimo local (que nesse caso ser\u00e1 global). import numpy as np import matplotlib.pyplot as plt from scipy.stats import t from statsmodels.regression.linear_model import OLS plt.style.use('ggplot') ro = np.random.RandomState(10000) Vamos gerar os n pontos e calcular a reta que minimiza os m\u00ednimos quadrados. n = 20 x = ro.uniform(0,10, size = n) epsilon = ro.normal(0, scale = 3, size = n) beta0, beta1 = ro.uniform(0,10, size = 2) y = beta0 + beta1*x + epsilon plt.scatter(x,y) plt.title('Pontos gerados') plt.show() Estimando os coeficientes beta1_hat = np.dot(y - y.mean(), x - x.mean())/np.dot(x - x.mean(), x - x.mean()) beta0_hat = y.mean() - beta1_hat*x.mean() t = np.linspace(0,10,1000) plt.scatter(x,y, label = r'$\\beta_0$ = {:.2f}, $\\beta_1$ = {:.2f}'.format(beta0, beta1)) plt.plot(t,beta0_hat + beta1_hat*t, color = 'blue', label = r'$\\beta_0$ = {:.2f}, $\\beta_1$ = {:.2f}'.format(beta0_hat, beta1_hat)) plt.legend() plt.title('Reta m\u00ednimos quadrados') plt.show() Agora vamos usar numpy para a estima\u00e7\u00e3o. Observe que o resultado coincide com o calculado usando a f\u00f3rmula derivada. # adiciona coluna de 1 para lidar com o beta_0 xlinha = np.c_[np.ones(n),x] sol, _, _, _ = np.linalg.lstsq(xlinha, y, rcond = None) print(sol) [3.40285283 3.52323316] V\u00e1rias vari\u00e1veis Nesse caso, queremos encontrar um hiperplano que minimiza Q = \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_k x_{ik}))^2 Nesse caso teremos k+1 equa\u00e7\u00f5es para resolver. Regress\u00e3o Dizemos que X_1, \\dots, X_k s\u00e3o preditores e Y \u00e9 a resposta. A esperan\u00e7a condicional da resposta dados os preditores \u00e9 chamada de fun\u00e7\u00e3o regress\u00e3o. Assim a regress\u00e3o de Y sobre X_1, \\dots, X_k depende dos valores x_1, \\dots, x_k assumidos pelos preditores. Vamos assumir que estamos com uma regress\u00e3o linear e, portanto, E[Y|x_1,\\dots,x_k] = \\beta_0 + \\beta_1 x_1 + \\dots \\beta_k x_k onde os coeficientes \\beta_j s\u00e3o coeficientes de regress\u00e3o, que s\u00e3o desconhecidos (par\u00e2metros do modelo). Regress\u00e3o Linear Simples Nesse caso Y = \\beta_0 + \\beta_1 x + \\varepsilon para cada X = x e, em geral, \\varepsilon \\sim N(0, \\sigma^2) . Assumimos que Os preditores s\u00e3o conhecidos; A distribui\u00e7\u00e3o de Y condicionada em x_1, ..., x_n \u00e9 normal. A m\u00e9dia condicional \u00e9 linear com par\u00e2metros \\beta_0 e \\beta_1 . Homoscedasticidade, isto \u00e9, vari\u00e2ncia constante. Independ\u00eancia das respostas dadas as covari\u00e1veis. Teorema: Os estimadores de m\u00e1xima verossimilhan\u00e7a de \\beta_0 e \\beta_1 s\u00e3o os de m\u00ednimos quadrados e de \\sigma^2 \u00e9 \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2 Distribui\u00e7\u00e3o dos estimadores: Considere os estimadores de m\u00ednimos quadrados como fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias Y_1,...,Y_n dados os preditores x_1,...,x_n . \\hat{\\beta}_1 \\sim \\mathcal{N}(\\beta_1, \\sigma^2/s_x^2) \\hat{\\beta}_0 \\sim \\mathcal{N}\\left(\\beta_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{s_x^2}\\right)\\right) tal que Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\frac{\\bar{x}\\sigma^2}{s_x^2} . Em particular a distribui\u00e7\u00e3o conjunta desses par\u00e2metros \u00e9 uma normal bivariada com as m\u00e9dias, vai\u00e2ncias e covari\u00e2ncia especificadas acima. As distribui\u00e7\u00f5es s\u00e3o todas condicionadas em X_i = x_i . Al\u00e9m disso, se n \\ge 3, \\hat{\\sigma}^2 \u00e9 independente de (\\hat{\\beta}_0, \\hat{\\beta}_1) e n\\hat{\\sigma}^2/\\sigma^2 tem distribui\u00e7\u00e3o \\chi^2 com n-2 graus de liberdade. Infer\u00eancia sobre regress\u00e3o linear simples Teste de hip\u00f3teses sobre os coeficientes Defina S^2 = \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 \\sigma ' = \\left(\\dfrac{S^2}{n-2}\\right)^{1/2} Sejam c_0, c_1 e \\bar{c} n\u00fameros especificados onde c_i \\neq 0 para algum i e suponha que queiramos testar H_0: c_0 \\beta_0 + c_1 \\beta_1 = \\bar{c} H_1: c_0 \\beta_0 + c_1 \\beta_1 \\neq \\bar{c} Para cada \\alpha_0 \\in (0,1) um teste de hip\u00f3teses com n\u00edvel \\alpha_0 \u00e9 rejeitar H_0 se |U| \\ge T_{n-2}^{-1}(1 - \\alpha_0/2) , onde U = \\left[\\frac{c_0^2}{n} + \\frac{(c_0\\bar{x} - c_1)^2}{s_x^2}\\right]^{-1/2}\\left(\\frac{c_0\\hat{\\beta}_0 + c_1\\hat{\\beta}_1 - \\bar{c}}{\\sigma '}\\right) Para ver isso, basta olhar a vari\u00e1vel W que tem distribui\u00e7\u00e3o normal padr\u00e3o W = \\left[\\frac{c_0^2}{n} + \\frac{(c_0\\bar{x} - c_1)^2}{s_x^2}\\right]^{-1/2}\\left(\\frac{c_0\\hat{\\beta}_0 + c_1\\hat{\\beta}_1 - \\bar{c}}{\\sigma}\\right) e que S^2/\\sigma^2 \\sim \\chi^2_{n-2} A mesma deriva\u00e7\u00e3o pode ser usada para testes unilaterais. A diferen\u00e7a \u00e9 que rejeitaremos quando U \\ge T_{n-2}^{-1}(1 - \\alpha_0) quando a hip\u00f3tese nula \u00e9 uma desiguldade do tipo menor igual ou U \\le -T_{n-2}^{-1}(1 - \\alpha_0) quando \u00e9 do tipo maior ou igual. Exemplo: Vamos testar a hip\u00f3tese de que \\beta_1 = 0 em rela\u00e7\u00e3o a curva anterior. S\u00f3 para lembrar: t = np.linspace(0,10,1000) plt.scatter(x,y, label = r'$\\beta_0$ = {:.2f}, $\\beta_1$ = {:.2f}'.format(beta0, beta1)) plt.plot(t,beta0_hat + beta1_hat*t, color = 'blue', label = r'$\\beta_0$ = {:.2f}, $\\beta_1$ = {:.2f}'.format(beta0_hat, beta1_hat)) plt.legend() plt.title('Reta m\u00ednimos quadrados') plt.show() Nesse caso temos c_0 = \\bar{c} = 0 e c_1 = 1 . Assim: # O denominador da divis\u00e3o \u00e9 n - ddof sx2 = np.var(x, ddof = n - 1) S2 = sum((y - beta0_hat - beta1_hat*x)**2) sigma_prime = (S2/(n-2))**(1/2) # Estat\u00edstica de teste U = np.sqrt(sx2)*beta1_hat/sigma_prime Seja \\alpha_0 = 0.05 o n\u00edvel de signific\u00e2ncia alpha0 = 0.05 abs(U) >= t.ppf(1 - alpha0/2, df = n-2) True Como isso \u00e9 verdade, rejeitamos a hip\u00f3tese nula! Intervalos de confian\u00e7a Temos que c_0\\hat{\\beta}_0 + c_1\\hat{\\beta}_1 \\pm \\sigma '\\left[\\frac{c_0^2}{n} + \\frac{(c_0\\bar{x} - c_1)^2}{s_x^2}\\right]^{1/2}T_{n-2}^{-1}\\left(1 - \\frac{\\alpha_0}{2}\\right) \u00e9 un intervalo de confian\u00e7a 1 - \\alpha_0 para c_0\\beta_0 + c_1\\beta_1 . Para encontrar esse intervalo basta encontrar o conjunto de todos os valores \\bar{c} para que a hip\u00f3tese nula seja rejeitada a n\u00edvel de signific\u00e2ncia \\alpha_0 . No nosso exemplo, um intervalo para \\beta_1 \u00e9 [beta1_hat - sigma_prime*(1/np.sqrt(sx2))*t.ppf(1 - alpha0/2, df = n-2), beta1_hat + sigma_prime*(1/np.sqrt(sx2))*t.ppf(1 - alpha0/2, df = n-2)] [3.00233584703191, 4.044130469815932] Usando o pacote statsmodels model = OLS(y,xlinha) result = model.fit() print(result.params) [3.40285283 3.52323316] Observe que os par\u00e2metros estimados s\u00e3o os mesmo que encontramos com numpy e na m\u00e3o. Tamb\u00e9m observe que o tvalor calculado coincide com nossa estat\u00edstica U print('Nossa: {}'.format(U)) print('Statsmodels: {}'.format(result.tvalues[1])) Nossa: 14.210167788464632 Statsmodels: 14.21016778846464 Podemos colocar c_0 = 0 e c_1 = 1 e obteremos o intervalo de confian\u00e7a como desej\u00e1vamos! result.t_test([0,1]) <class 'statsmodels.stats.contrast.ContrastResults'> Test for Constraints ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ c0 3.5232 0.248 14.210 0.000 3.002 4.044 ==============================================================================","title":"Modelo Linear"},{"location":"infestatistica_BSc/LinearModel/LinearModel/#modelo-linear","text":"","title":"Modelo Linear"},{"location":"infestatistica_BSc/LinearModel/LinearModel/#minimos-quadrados","text":"Sejam os pontos (x_1, y_1), ..., (x_n,y_n) . A reta que minimiza \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_i))^2 segundo \\beta_0 e \\beta_1 tem inclina\u00e7\u00e3o e intecepto \\hat{\\beta}_1 = \\dfrac{\\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x} A verifica\u00e7\u00e3o desse resultado \u00e9 direta ao derivar o funcional objetivo com respeito aos par\u00e2metros e igualando a 0. Al\u00e9m disso, observar que a Hessiana \u00e9 definida positiva, o que garante a exist\u00eancia do m\u00ednimo local (que nesse caso ser\u00e1 global). import numpy as np import matplotlib.pyplot as plt from scipy.stats import t from statsmodels.regression.linear_model import OLS plt.style.use('ggplot') ro = np.random.RandomState(10000) Vamos gerar os n pontos e calcular a reta que minimiza os m\u00ednimos quadrados. n = 20 x = ro.uniform(0,10, size = n) epsilon = ro.normal(0, scale = 3, size = n) beta0, beta1 = ro.uniform(0,10, size = 2) y = beta0 + beta1*x + epsilon plt.scatter(x,y) plt.title('Pontos gerados') plt.show() Estimando os coeficientes beta1_hat = np.dot(y - y.mean(), x - x.mean())/np.dot(x - x.mean(), x - x.mean()) beta0_hat = y.mean() - beta1_hat*x.mean() t = np.linspace(0,10,1000) plt.scatter(x,y, label = r'$\\beta_0$ = {:.2f}, $\\beta_1$ = {:.2f}'.format(beta0, beta1)) plt.plot(t,beta0_hat + beta1_hat*t, color = 'blue', label = r'$\\beta_0$ = {:.2f}, $\\beta_1$ = {:.2f}'.format(beta0_hat, beta1_hat)) plt.legend() plt.title('Reta m\u00ednimos quadrados') plt.show() Agora vamos usar numpy para a estima\u00e7\u00e3o. Observe que o resultado coincide com o calculado usando a f\u00f3rmula derivada. # adiciona coluna de 1 para lidar com o beta_0 xlinha = np.c_[np.ones(n),x] sol, _, _, _ = np.linalg.lstsq(xlinha, y, rcond = None) print(sol) [3.40285283 3.52323316]","title":"M\u00ednimos quadrados"},{"location":"infestatistica_BSc/LinearModel/LinearModel/#varias-variaveis","text":"Nesse caso, queremos encontrar um hiperplano que minimiza Q = \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_k x_{ik}))^2 Nesse caso teremos k+1 equa\u00e7\u00f5es para resolver.","title":"V\u00e1rias vari\u00e1veis"},{"location":"infestatistica_BSc/LinearModel/LinearModel/#regressao","text":"Dizemos que X_1, \\dots, X_k s\u00e3o preditores e Y \u00e9 a resposta. A esperan\u00e7a condicional da resposta dados os preditores \u00e9 chamada de fun\u00e7\u00e3o regress\u00e3o. Assim a regress\u00e3o de Y sobre X_1, \\dots, X_k depende dos valores x_1, \\dots, x_k assumidos pelos preditores. Vamos assumir que estamos com uma regress\u00e3o linear e, portanto, E[Y|x_1,\\dots,x_k] = \\beta_0 + \\beta_1 x_1 + \\dots \\beta_k x_k onde os coeficientes \\beta_j s\u00e3o coeficientes de regress\u00e3o, que s\u00e3o desconhecidos (par\u00e2metros do modelo).","title":"Regress\u00e3o"},{"location":"infestatistica_BSc/LinearModel/LinearModel/#regressao-linear-simples","text":"Nesse caso Y = \\beta_0 + \\beta_1 x + \\varepsilon para cada X = x e, em geral, \\varepsilon \\sim N(0, \\sigma^2) . Assumimos que Os preditores s\u00e3o conhecidos; A distribui\u00e7\u00e3o de Y condicionada em x_1, ..., x_n \u00e9 normal. A m\u00e9dia condicional \u00e9 linear com par\u00e2metros \\beta_0 e \\beta_1 . Homoscedasticidade, isto \u00e9, vari\u00e2ncia constante. Independ\u00eancia das respostas dadas as covari\u00e1veis. Teorema: Os estimadores de m\u00e1xima verossimilhan\u00e7a de \\beta_0 e \\beta_1 s\u00e3o os de m\u00ednimos quadrados e de \\sigma^2 \u00e9 \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2 Distribui\u00e7\u00e3o dos estimadores: Considere os estimadores de m\u00ednimos quadrados como fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias Y_1,...,Y_n dados os preditores x_1,...,x_n . \\hat{\\beta}_1 \\sim \\mathcal{N}(\\beta_1, \\sigma^2/s_x^2) \\hat{\\beta}_0 \\sim \\mathcal{N}\\left(\\beta_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{s_x^2}\\right)\\right) tal que Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\frac{\\bar{x}\\sigma^2}{s_x^2} . Em particular a distribui\u00e7\u00e3o conjunta desses par\u00e2metros \u00e9 uma normal bivariada com as m\u00e9dias, vai\u00e2ncias e covari\u00e2ncia especificadas acima. As distribui\u00e7\u00f5es s\u00e3o todas condicionadas em X_i = x_i . Al\u00e9m disso, se n \\ge 3, \\hat{\\sigma}^2 \u00e9 independente de (\\hat{\\beta}_0, \\hat{\\beta}_1) e n\\hat{\\sigma}^2/\\sigma^2 tem distribui\u00e7\u00e3o \\chi^2 com n-2 graus de liberdade.","title":"Regress\u00e3o Linear Simples"},{"location":"infestatistica_BSc/LinearModel/LinearModel/#inferencia-sobre-regressao-linear-simples","text":"","title":"Infer\u00eancia sobre regress\u00e3o linear simples"},{"location":"infestatistica_BSc/LinearModel/LinearModel/#teste-de-hipoteses-sobre-os-coeficientes","text":"Defina S^2 = \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 \\sigma ' = \\left(\\dfrac{S^2}{n-2}\\right)^{1/2} Sejam c_0, c_1 e \\bar{c} n\u00fameros especificados onde c_i \\neq 0 para algum i e suponha que queiramos testar H_0: c_0 \\beta_0 + c_1 \\beta_1 = \\bar{c} H_1: c_0 \\beta_0 + c_1 \\beta_1 \\neq \\bar{c} Para cada \\alpha_0 \\in (0,1) um teste de hip\u00f3teses com n\u00edvel \\alpha_0 \u00e9 rejeitar H_0 se |U| \\ge T_{n-2}^{-1}(1 - \\alpha_0/2) , onde U = \\left[\\frac{c_0^2}{n} + \\frac{(c_0\\bar{x} - c_1)^2}{s_x^2}\\right]^{-1/2}\\left(\\frac{c_0\\hat{\\beta}_0 + c_1\\hat{\\beta}_1 - \\bar{c}}{\\sigma '}\\right) Para ver isso, basta olhar a vari\u00e1vel W que tem distribui\u00e7\u00e3o normal padr\u00e3o W = \\left[\\frac{c_0^2}{n} + \\frac{(c_0\\bar{x} - c_1)^2}{s_x^2}\\right]^{-1/2}\\left(\\frac{c_0\\hat{\\beta}_0 + c_1\\hat{\\beta}_1 - \\bar{c}}{\\sigma}\\right) e que S^2/\\sigma^2 \\sim \\chi^2_{n-2} A mesma deriva\u00e7\u00e3o pode ser usada para testes unilaterais. A diferen\u00e7a \u00e9 que rejeitaremos quando U \\ge T_{n-2}^{-1}(1 - \\alpha_0) quando a hip\u00f3tese nula \u00e9 uma desiguldade do tipo menor igual ou U \\le -T_{n-2}^{-1}(1 - \\alpha_0) quando \u00e9 do tipo maior ou igual. Exemplo: Vamos testar a hip\u00f3tese de que \\beta_1 = 0 em rela\u00e7\u00e3o a curva anterior. S\u00f3 para lembrar: t = np.linspace(0,10,1000) plt.scatter(x,y, label = r'$\\beta_0$ = {:.2f}, $\\beta_1$ = {:.2f}'.format(beta0, beta1)) plt.plot(t,beta0_hat + beta1_hat*t, color = 'blue', label = r'$\\beta_0$ = {:.2f}, $\\beta_1$ = {:.2f}'.format(beta0_hat, beta1_hat)) plt.legend() plt.title('Reta m\u00ednimos quadrados') plt.show() Nesse caso temos c_0 = \\bar{c} = 0 e c_1 = 1 . Assim: # O denominador da divis\u00e3o \u00e9 n - ddof sx2 = np.var(x, ddof = n - 1) S2 = sum((y - beta0_hat - beta1_hat*x)**2) sigma_prime = (S2/(n-2))**(1/2) # Estat\u00edstica de teste U = np.sqrt(sx2)*beta1_hat/sigma_prime Seja \\alpha_0 = 0.05 o n\u00edvel de signific\u00e2ncia alpha0 = 0.05 abs(U) >= t.ppf(1 - alpha0/2, df = n-2) True Como isso \u00e9 verdade, rejeitamos a hip\u00f3tese nula!","title":"Teste de hip\u00f3teses sobre os coeficientes"},{"location":"infestatistica_BSc/LinearModel/LinearModel/#intervalos-de-confianca","text":"Temos que c_0\\hat{\\beta}_0 + c_1\\hat{\\beta}_1 \\pm \\sigma '\\left[\\frac{c_0^2}{n} + \\frac{(c_0\\bar{x} - c_1)^2}{s_x^2}\\right]^{1/2}T_{n-2}^{-1}\\left(1 - \\frac{\\alpha_0}{2}\\right) \u00e9 un intervalo de confian\u00e7a 1 - \\alpha_0 para c_0\\beta_0 + c_1\\beta_1 . Para encontrar esse intervalo basta encontrar o conjunto de todos os valores \\bar{c} para que a hip\u00f3tese nula seja rejeitada a n\u00edvel de signific\u00e2ncia \\alpha_0 . No nosso exemplo, um intervalo para \\beta_1 \u00e9 [beta1_hat - sigma_prime*(1/np.sqrt(sx2))*t.ppf(1 - alpha0/2, df = n-2), beta1_hat + sigma_prime*(1/np.sqrt(sx2))*t.ppf(1 - alpha0/2, df = n-2)] [3.00233584703191, 4.044130469815932] Usando o pacote statsmodels model = OLS(y,xlinha) result = model.fit() print(result.params) [3.40285283 3.52323316] Observe que os par\u00e2metros estimados s\u00e3o os mesmo que encontramos com numpy e na m\u00e3o. Tamb\u00e9m observe que o tvalor calculado coincide com nossa estat\u00edstica U print('Nossa: {}'.format(U)) print('Statsmodels: {}'.format(result.tvalues[1])) Nossa: 14.210167788464632 Statsmodels: 14.21016778846464 Podemos colocar c_0 = 0 e c_1 = 1 e obteremos o intervalo de confian\u00e7a como desej\u00e1vamos! result.t_test([0,1]) <class 'statsmodels.stats.contrast.ContrastResults'> Test for Constraints ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ c0 3.5232 0.248 14.210 0.000 3.002 4.044 ==============================================================================","title":"Intervalos de confian\u00e7a"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/","text":"Estimador de M\u00e1xima Verossimilhan\u00e7a Introdu\u00e7\u00e3o \"Tradicionalmente a infer\u00eancia estat\u00edstica sobre a m\u00e9dia de uma popula\u00e7\u00e3o se apoia no Teorema Central do Limite para construir Intervalos de Confian\u00e7a ou testar hip\u00f3teses sobre o valor do par\u00e2metro. Esta abordagem da estat\u00edstica tradicional pode ser extendida para infer\u00eancias a respeito de qualquer par\u00e2metro, n\u00e3o s\u00f3 a m\u00e9dia. Da mesma forma que no caso da m\u00e9dia populacional se usa a distribui\u00e7\u00e3o t-Student ou a distribui\u00e7\u00e3o Normal Padr\u00e3o , no caso de outros par\u00e2metros se utiliza outras distribui\u00e7\u00f5es amostrais. Essas distribui\u00e7\u00f5es s\u00e3o chamadas amostrais porque representam o comportamento das estimativas baseado na repeti\u00e7\u00e3o incont\u00e1vel do processo de amostragem . Na pr\u00e1tica cient\u00edfica, no entanto, sempre se realiza uma \u00fanica amostragem , o que resulta em uma \u00fanica amostra. Assim, o conceito de distribui\u00e7\u00e3o amostral \u00e9 at\u00e9 certo ponto artificial, pois em pesquisa cient\u00edfica n\u00e3o raciocinamos em termos de repeti\u00e7\u00f5es incont\u00e1veis de experimentos ou processos de observa\u00e7\u00e3o . O resultado disto \u00e9 que o conceito de teste estat\u00edstico de hip\u00f3tese e de intervalo de confian\u00e7a s\u00e3o frequentemente mal compreendidos. O desenvolvimento da infer\u00eancia estat\u00edstica a partir do conceito de verossimilhan\u00e7a tem sido utilizado como uma alternativa \u00e0 abordagem estat\u00edstica frequentista e, segundo alguns autores (como por exemplo Royall, 1997), \u00e9 mais coerente com a pr\u00e1tica cient\u00edfica .\" (Batista, 2009) Site de Refer\u00eancia Fun\u00e7\u00e3o Verossimilhan\u00e7a Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x) Estimador de M\u00e1xima Verossimilhan\u00e7a (MLE) Para cada observa\u00e7\u00e3o x , seja \\delta(x) um valor de \\theta \\in \\Omega tal que a fun\u00e7\u00e3o de verossimilhna\u00e7a seja m\u00e1xima . Defina \\hat{\\theta} = \\delta(X) o estimador. \u00c9 importante observar que o m\u00e1ximo dessa fun\u00e7\u00e3o pode n\u00e3o estar em um ponto de \\Omega . Nesse caso, MLE n\u00e3o existe. Ele pode n\u00e3o estar unicamente definido, tamb\u00e9m. Limita\u00e7\u00f5es N\u00e3o exist\u00eancia em todos os casos, isso depende muito da fun\u00e7\u00e3o e do espa\u00e7o dos par\u00e2metros. N\u00e3o unicidade em todos os casos. N\u00e3o podemos interpretar MLE como o par\u00e2metro mais prov\u00e1vel, pois ter\u00edamos que ter um espa\u00e7o de probabilidade associado ao par\u00e2metro, o que n\u00e3o \u00e9 dado. Implementa\u00e7\u00e3o Como refer\u00eancia, estou utilizando este site . # importando bibliotecas import numpy as np, pandas as pd from matplotlib import pyplot as plt import seaborn as sns from scipy.optimize import minimize import scipy.stats as stats import pymc3 as pm3 import numdifftools as ndt import statsmodels.api as sm from statsmodels.base.model import GenericLikelihoodModel %matplotlib inline # Gerando os dados N = 100 x = np.linspace(0, 20, N) # gerando lista igualmente espa\u00e7ada beta1 = 3 beta0 = 0 sigma = 5 error = np.random.normal(0, sigma, size = N) y = beta1*x + beta0 + error data = pd.DataFrame({'y': y, 'x': x}) data['constant'] = 1 sns.regplot('x','y',data = data) # Essa reta \u00e9 uma estimativa dos dados feito por seaborn plt.title('Dados') plt.show() Y = \\beta_1 x + \\beta_0 + e Nesse exemplo, o nosso problema ser\u00e1 estimar a m\u00e9dia. Observe que os dados tem um comportamento linear. Sem nos concentrarmos muito na modelagem e os problemas que ela pode trazer, eu vou j\u00e1 supor que temos um problema de Regress\u00e3o Linear , onde os dados Y \\sim N(\\mu, \\sigma^2) , onde \\sigma^2 \u00e9 a vari\u00e2ncia do erro no processo, e \\mu = \\beta_0 + \\beta_1 x , isto \u00e9, depende de x, nesse caso. Essa \u00e9 uma dificuldade, as contas ficam mais dif\u00edceis e, por isso, vamos usar asrtif\u00edcios computacionais. Vamos supor que a vari\u00e2ncia \u00e9 conhecida . Al\u00e9m disso, vamos supor que temos uma amostra aleat\u00f3ria Y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2) Temos que a verossimilhan\u00e7a \u00e9 produto das pdfs(distribui\u00e7\u00e3o de densidade de probabilidade). Para otimizar podemos, entretanto, obter a soma dos logaritmos das pdfs . E por fim, vamos resolver um problema de minimizar o negativo desse valor. Veja que \u00e9 equivalente a maximixar a soma!! # Fun\u00e7\u00e3o de verossimilhan\u00e7a. Chamamos de Fun\u00e7\u00e3o de Perda def MLE(params): # Fun\u00e7\u00e3o Perda: - log-verossimilhan\u00e7a beta0, beta1 = params[0], params[1] # Modelo Linear yhat = beta0 + beta1*x #= mu #loc \u00e9 a m\u00e9dia e scale desvio padr\u00e3o. Note que sigma \u00e9 conhecido negLikelihood = - np.sum(stats.norm.logpdf(y, loc = yhat, scale = sigma)) return negLikelihood # Esse \u00e9 o chute inicial initial_guess = np.array([3, 6]) results = minimize(MLE, initial_guess, method='Nelder-Mead', options = {'disp': True}) Optimization terminated successfully. Current function value: 307.745486 Iterations: 56 Function evaluations: 107 print(results) final_simplex: (array([[-1.03428809, 3.11012856], [-1.0342294 , 3.110121 ], [-1.03433677, 3.11012912]]), array([293.95399071, 293.95399071, 293.95399071])) fun: 293.95399070678394 message: 'Optimization terminated successfully.' nfev: 103 nit: 55 status: 0 success: True x: array([-1.03428809, 3.11012856]) resultsdf = pd.DataFrame({'coef': results['x']}) resultsdf.index=[r'$\\beta_0$',r'$\\beta_1$'] np.round(resultsdf.head(2), 4) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef $\\beta_0$ -1.0343 $\\beta_1$ 3.1101 Vamos estimar usando a biblioteca OLS. Ela faz esse processo e muito mais internamente. results_ols = sm.OLS(data.y, data[['constant', 'x']]).fit() results_ols.summary() OLS Regression Results Dep. Variable: y R-squared: 0.941 Model: OLS Adj. R-squared: 0.941 Method: Least Squares F-statistic: 1568. Date: Wed, 26 Aug 2020 Prob (F-statistic): 4.22e-62 Time: 21:20:55 Log-Likelihood: -293.06 No. Observations: 100 AIC: 590.1 Df Residuals: 98 BIC: 595.3 Df Model: 1 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] constant -1.0343 0.909 -1.138 0.258 -2.839 0.770 x 3.1101 0.079 39.599 0.000 2.954 3.266 Omnibus: 1.778 Durbin-Watson: 2.306 Prob(Omnibus): 0.411 Jarque-Bera (JB): 1.423 Skew: -0.289 Prob(JB): 0.491 Kurtosis: 3.084 Cond. No. 23.1 Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Veja que a estima\u00e7\u00e3o dos coeficientes foi a mesma! Apesar de ambas estarem erradas p para \\beta_0 . Na verdade se olharmos o intervalo de confian\u00e7a que OLS nos d\u00e1, vemos que de fato 0 est\u00e1 nele. Mas ainda n\u00e3o esta na hora de voc\u00eas verem isso! Conclus\u00e3o Podemos usar uma fun\u00e7\u00e3o de perda (que no caso ser\u00e1 menos a log-verossimilhan\u00e7a) e usar um algoritmo de otimiza\u00e7\u00e3o! Propriedades Invari\u00e2ncia Se \\hat{\\theta} \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de \\theta e g \u00e9 uma fun\u00e7\u00e3o injetiva, ent\u00e3o g(\\hat{\\theta}) \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de g(\\theta) . Na verdade, podemos retirar condi\u00e7\u00e3o de injetividade. MLE de uma Fun\u00e7\u00e3o Seja g(\\theta) uma fun\u00e7\u00e3o arbitr\u00e1ria do par\u00e2metro e G = g(\\Omega) . Para cada t \\in G , definimos G_t := \\{\\theta : g(\\theta) = t\\} e L^*(t) := \\max_{\\theta \\in G_t} log f_n(x|\\theta) Definimos a ML.E.de g(\\theta) := arg\\,max_{t\\in G} L^*(t) Teorema Seja \\hat{\\theta} MLE de \\theta e g(\\theta) fun\u00e7\u00e3o de \\theta . Ent\u00e3o uma MLE de g(\\theta) \u00e9 g(\\hat{\\theta}) . Consist\u00eancia Suponha que para uma amostra suficientemente grantde, existe um MLE \u00fanico para \\theta . Ent\u00e3o, sob algumas condi\u00e7\u00f5es, a sequ\u00eancia de MLE \u00e9 uma sequ\u00eancia consistente de estimadores de \\theta . A seuq\u00eancia convergee em probabilidade para o valor desconhecido de \\theta . O mesmo acontece com o Estimador de Bayes, dadas condi\u00e7\u00f5es de regularidade. Fun\u00e7\u00e3o Digamma: \\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)} M\u00e9todo dos Momentos Assuma que a amostra aleat\u00f3ria X_1,...,X_n vem da distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta k-dimensional. Por exemplo, a distribui\u00e7\u00e3o normal tem k = 2 . Tamb\u00e9m suponha que pelo menos os k primeiros momentos ( E[X_i^k] < \\infty ) sejam finitos. Defina \\mu_j(\\theta) = E[X_1^j|\\theta], j = 1,...k . Suponha que a fun\u00e7\u00e3o: \\begin{split} \\mu : ~&\\Omega \\to \\mathbb{R}^k \\\\ &\\theta \\mapsto \\mu(\\theta) = (\\mu_1(\\theta), ..., \\mu_k(\\theta)), \\end{split} \u00e9 injetiva em \\theta . Seja M(\\mu_1,...,\\mu_k) a fun\u00e7\u00e3o inversa, isto \u00e9, \\theta = M(\\mu_1,...,\\mu_k) O m\u00e9todo dos momentos ser\u00e1 M(m_1,...,m_j) , onde m_j = \\frac{1}{n}\\sum_{i=1}^n X_i^j, j = 1,...,k De forma mais simplificada, basta que sesolvemos o sistema: m_j = \\mu_j(\\theta), isto \u00e9, os momentos amostrais iguais aos momentos da amostra, condicionados em \\theta . Teorema Suponha que \\{X_n\\}_{n\\in\\mathbb{N}} i.i.d com distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta , k -dimensional. Suponha que os primeiros k momentos existem e s\u00e3o finitos para todo \\theta . Suponha que a inversa M definida acima \u00e9 cont\u00ednua. Ent\u00e3o a sequ\u00eancia de estimadores do m\u00e9todo de momentos em X_1,...,X_n \u00e9 consistente. M.L.E e Estimador de Bayes Se tivermos condi\u00e7\u00f5es de suavidade em f(x|\\theta) , podemos provar que quando n \\to \\infty , teremos que: L(\\theta|x) \\to c(x)\\cdot \\exp\\{-\\frac{1}{2V_n(\\theta)/n}(\\theta - \\hat{\\theta})^2\\}, onde \\hat{\\theta} \u00e9 MLE e V_n(\\theta) \u00e9 uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias convergente. No caso de termos uma priori relativamente flat, a posteriori ser\u00e1 aproximadamente uma distribui\u00e7\u00e3o normal com m\u00e9dia \\hat{\\theta} e vari\u00e2ncia V_n(\\hat{\\theta})/n . Exemplo 7.6.12 (Mortes ex\u00e9rcito pr\u00fassio) Bortkiewicz contou o n\u00famero de soldados mortos por horsekick em 14 unidades do ex\u00e9rcito em 20 anos, com 280 contagens ao total. Das contagens temos Valor 0 1 2 3 4 Total Contagem 144 91 32 11 2 280 Modelamos X_1, ..., X_{280} como uma vari\u00e1vel de contagem. Considere a distribui\u00e7\u00e3o Poisson(\\theta) . Escolhemos a distribui\u00e7\u00e3o Gamma(\\alpha,\\beta) , dada que ela pertence \u00e0 familia conjungada. Em particular, a distribui\u00e7\u00e3o a posteriori ser\u00e1 Gamma(\\alpha + \\sum X_i, \\beta + n) , onde \\sum X_i = 196 . Se assumirmos \\alpha inteiro por simplicidade, vemos que a distribui\u00e7\u00e3o Gamma pode ser vista como a soma de \\alpha + \\sum X_i distribui\u00e7\u00f5es Exponencial(\\beta + n) . Logo a soma dessas vari\u00e1veis ser\u00e1 aproximadamente normal com m\u00e9dia 196/280 e vari\u00e2ncia 196/280^2 . import numpy as np import matplotlib.pyplot as plt from scipy.stats import gamma alpha = 1 beta = 1 # Esse \u00e9 o MLE, a m\u00e9dia. Vou supor que esse \u00e9 o par\u00e2metro verdadeiro s\u00f3 para mostrar. theta = 196/280 sum_xi = 196 fig, ax = plt.subplots(2,3,figsize = (18,6)) fig.suptitle('Avaliando a converg\u00eancia da distribui\u00e7\u00e3o Gamma') for index, n in enumerate([1,10,100,1000,10000,280]): i = int(index/3) j = index % 3 X = np.random.poisson(theta, size = n) if n != 280: T = X.sum() ax[i][j].set_title('n = {}'.format(n)) else: T = sum_xi #Valor dos dados ax[i][j].set_title('Dados Oficiais: n = {}'.format(n)) t = np.linspace(start = 0.00001, stop = 3 - i - 1, num = 1000) posteriori = gamma(alpha + T, scale = 1/(beta + n)) y = posteriori.pdf(t) ax[i][j].plot(t, y, color = 'darkblue') ax[i][j].grid(color = 'grey', alpha = 0.6, linestyle = '--') ax[i][j].vlines(theta, ymin = 0, ymax = max(y), color = 'black', linestyle = '--') Veja que com os dados reais, j\u00e1 temos uma boa aproxima\u00e7\u00e3o!","title":"Estimador de M\u00e1xima Verossimilhan\u00e7a"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#estimador-de-maxima-verossimilhanca","text":"","title":"Estimador de M\u00e1xima Verossimilhan\u00e7a"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#introducao","text":"\"Tradicionalmente a infer\u00eancia estat\u00edstica sobre a m\u00e9dia de uma popula\u00e7\u00e3o se apoia no Teorema Central do Limite para construir Intervalos de Confian\u00e7a ou testar hip\u00f3teses sobre o valor do par\u00e2metro. Esta abordagem da estat\u00edstica tradicional pode ser extendida para infer\u00eancias a respeito de qualquer par\u00e2metro, n\u00e3o s\u00f3 a m\u00e9dia. Da mesma forma que no caso da m\u00e9dia populacional se usa a distribui\u00e7\u00e3o t-Student ou a distribui\u00e7\u00e3o Normal Padr\u00e3o , no caso de outros par\u00e2metros se utiliza outras distribui\u00e7\u00f5es amostrais. Essas distribui\u00e7\u00f5es s\u00e3o chamadas amostrais porque representam o comportamento das estimativas baseado na repeti\u00e7\u00e3o incont\u00e1vel do processo de amostragem . Na pr\u00e1tica cient\u00edfica, no entanto, sempre se realiza uma \u00fanica amostragem , o que resulta em uma \u00fanica amostra. Assim, o conceito de distribui\u00e7\u00e3o amostral \u00e9 at\u00e9 certo ponto artificial, pois em pesquisa cient\u00edfica n\u00e3o raciocinamos em termos de repeti\u00e7\u00f5es incont\u00e1veis de experimentos ou processos de observa\u00e7\u00e3o . O resultado disto \u00e9 que o conceito de teste estat\u00edstico de hip\u00f3tese e de intervalo de confian\u00e7a s\u00e3o frequentemente mal compreendidos. O desenvolvimento da infer\u00eancia estat\u00edstica a partir do conceito de verossimilhan\u00e7a tem sido utilizado como uma alternativa \u00e0 abordagem estat\u00edstica frequentista e, segundo alguns autores (como por exemplo Royall, 1997), \u00e9 mais coerente com a pr\u00e1tica cient\u00edfica .\" (Batista, 2009) Site de Refer\u00eancia","title":"Introdu\u00e7\u00e3o"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#funcao-verossimilhanca","text":"Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x)","title":"Fun\u00e7\u00e3o Verossimilhan\u00e7a"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#estimador-de-maxima-verossimilhanca-mle","text":"Para cada observa\u00e7\u00e3o x , seja \\delta(x) um valor de \\theta \\in \\Omega tal que a fun\u00e7\u00e3o de verossimilhna\u00e7a seja m\u00e1xima . Defina \\hat{\\theta} = \\delta(X) o estimador. \u00c9 importante observar que o m\u00e1ximo dessa fun\u00e7\u00e3o pode n\u00e3o estar em um ponto de \\Omega . Nesse caso, MLE n\u00e3o existe. Ele pode n\u00e3o estar unicamente definido, tamb\u00e9m.","title":"Estimador de M\u00e1xima Verossimilhan\u00e7a (MLE)"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#limitacoes","text":"N\u00e3o exist\u00eancia em todos os casos, isso depende muito da fun\u00e7\u00e3o e do espa\u00e7o dos par\u00e2metros. N\u00e3o unicidade em todos os casos. N\u00e3o podemos interpretar MLE como o par\u00e2metro mais prov\u00e1vel, pois ter\u00edamos que ter um espa\u00e7o de probabilidade associado ao par\u00e2metro, o que n\u00e3o \u00e9 dado.","title":"Limita\u00e7\u00f5es"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#implementacao","text":"Como refer\u00eancia, estou utilizando este site . # importando bibliotecas import numpy as np, pandas as pd from matplotlib import pyplot as plt import seaborn as sns from scipy.optimize import minimize import scipy.stats as stats import pymc3 as pm3 import numdifftools as ndt import statsmodels.api as sm from statsmodels.base.model import GenericLikelihoodModel %matplotlib inline # Gerando os dados N = 100 x = np.linspace(0, 20, N) # gerando lista igualmente espa\u00e7ada beta1 = 3 beta0 = 0 sigma = 5 error = np.random.normal(0, sigma, size = N) y = beta1*x + beta0 + error data = pd.DataFrame({'y': y, 'x': x}) data['constant'] = 1 sns.regplot('x','y',data = data) # Essa reta \u00e9 uma estimativa dos dados feito por seaborn plt.title('Dados') plt.show() Y = \\beta_1 x + \\beta_0 + e Nesse exemplo, o nosso problema ser\u00e1 estimar a m\u00e9dia. Observe que os dados tem um comportamento linear. Sem nos concentrarmos muito na modelagem e os problemas que ela pode trazer, eu vou j\u00e1 supor que temos um problema de Regress\u00e3o Linear , onde os dados Y \\sim N(\\mu, \\sigma^2) , onde \\sigma^2 \u00e9 a vari\u00e2ncia do erro no processo, e \\mu = \\beta_0 + \\beta_1 x , isto \u00e9, depende de x, nesse caso. Essa \u00e9 uma dificuldade, as contas ficam mais dif\u00edceis e, por isso, vamos usar asrtif\u00edcios computacionais. Vamos supor que a vari\u00e2ncia \u00e9 conhecida . Al\u00e9m disso, vamos supor que temos uma amostra aleat\u00f3ria Y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2) Temos que a verossimilhan\u00e7a \u00e9 produto das pdfs(distribui\u00e7\u00e3o de densidade de probabilidade). Para otimizar podemos, entretanto, obter a soma dos logaritmos das pdfs . E por fim, vamos resolver um problema de minimizar o negativo desse valor. Veja que \u00e9 equivalente a maximixar a soma!! # Fun\u00e7\u00e3o de verossimilhan\u00e7a. Chamamos de Fun\u00e7\u00e3o de Perda def MLE(params): # Fun\u00e7\u00e3o Perda: - log-verossimilhan\u00e7a beta0, beta1 = params[0], params[1] # Modelo Linear yhat = beta0 + beta1*x #= mu #loc \u00e9 a m\u00e9dia e scale desvio padr\u00e3o. Note que sigma \u00e9 conhecido negLikelihood = - np.sum(stats.norm.logpdf(y, loc = yhat, scale = sigma)) return negLikelihood # Esse \u00e9 o chute inicial initial_guess = np.array([3, 6]) results = minimize(MLE, initial_guess, method='Nelder-Mead', options = {'disp': True}) Optimization terminated successfully. Current function value: 307.745486 Iterations: 56 Function evaluations: 107 print(results) final_simplex: (array([[-1.03428809, 3.11012856], [-1.0342294 , 3.110121 ], [-1.03433677, 3.11012912]]), array([293.95399071, 293.95399071, 293.95399071])) fun: 293.95399070678394 message: 'Optimization terminated successfully.' nfev: 103 nit: 55 status: 0 success: True x: array([-1.03428809, 3.11012856]) resultsdf = pd.DataFrame({'coef': results['x']}) resultsdf.index=[r'$\\beta_0$',r'$\\beta_1$'] np.round(resultsdf.head(2), 4) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } coef $\\beta_0$ -1.0343 $\\beta_1$ 3.1101 Vamos estimar usando a biblioteca OLS. Ela faz esse processo e muito mais internamente. results_ols = sm.OLS(data.y, data[['constant', 'x']]).fit() results_ols.summary() OLS Regression Results Dep. Variable: y R-squared: 0.941 Model: OLS Adj. R-squared: 0.941 Method: Least Squares F-statistic: 1568. Date: Wed, 26 Aug 2020 Prob (F-statistic): 4.22e-62 Time: 21:20:55 Log-Likelihood: -293.06 No. Observations: 100 AIC: 590.1 Df Residuals: 98 BIC: 595.3 Df Model: 1 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] constant -1.0343 0.909 -1.138 0.258 -2.839 0.770 x 3.1101 0.079 39.599 0.000 2.954 3.266 Omnibus: 1.778 Durbin-Watson: 2.306 Prob(Omnibus): 0.411 Jarque-Bera (JB): 1.423 Skew: -0.289 Prob(JB): 0.491 Kurtosis: 3.084 Cond. No. 23.1 Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Veja que a estima\u00e7\u00e3o dos coeficientes foi a mesma! Apesar de ambas estarem erradas p para \\beta_0 . Na verdade se olharmos o intervalo de confian\u00e7a que OLS nos d\u00e1, vemos que de fato 0 est\u00e1 nele. Mas ainda n\u00e3o esta na hora de voc\u00eas verem isso!","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#conclusao","text":"Podemos usar uma fun\u00e7\u00e3o de perda (que no caso ser\u00e1 menos a log-verossimilhan\u00e7a) e usar um algoritmo de otimiza\u00e7\u00e3o!","title":"Conclus\u00e3o"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#propriedades","text":"","title":"Propriedades"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#invariancia","text":"Se \\hat{\\theta} \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de \\theta e g \u00e9 uma fun\u00e7\u00e3o injetiva, ent\u00e3o g(\\hat{\\theta}) \u00e9 o estimador de m\u00e1xima verossimilhan\u00e7a de g(\\theta) . Na verdade, podemos retirar condi\u00e7\u00e3o de injetividade.","title":"Invari\u00e2ncia"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#mle-de-uma-funcao","text":"Seja g(\\theta) uma fun\u00e7\u00e3o arbitr\u00e1ria do par\u00e2metro e G = g(\\Omega) . Para cada t \\in G , definimos G_t := \\{\\theta : g(\\theta) = t\\} e L^*(t) := \\max_{\\theta \\in G_t} log f_n(x|\\theta) Definimos a ML.E.de g(\\theta) := arg\\,max_{t\\in G} L^*(t)","title":"MLE de uma Fun\u00e7\u00e3o"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#teorema","text":"Seja \\hat{\\theta} MLE de \\theta e g(\\theta) fun\u00e7\u00e3o de \\theta . Ent\u00e3o uma MLE de g(\\theta) \u00e9 g(\\hat{\\theta}) .","title":"Teorema"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#consistencia","text":"Suponha que para uma amostra suficientemente grantde, existe um MLE \u00fanico para \\theta . Ent\u00e3o, sob algumas condi\u00e7\u00f5es, a sequ\u00eancia de MLE \u00e9 uma sequ\u00eancia consistente de estimadores de \\theta . A seuq\u00eancia convergee em probabilidade para o valor desconhecido de \\theta . O mesmo acontece com o Estimador de Bayes, dadas condi\u00e7\u00f5es de regularidade.","title":"Consist\u00eancia"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#funcao-digamma","text":"\\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)}","title":"Fun\u00e7\u00e3o Digamma:"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#metodo-dos-momentos","text":"Assuma que a amostra aleat\u00f3ria X_1,...,X_n vem da distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta k-dimensional. Por exemplo, a distribui\u00e7\u00e3o normal tem k = 2 . Tamb\u00e9m suponha que pelo menos os k primeiros momentos ( E[X_i^k] < \\infty ) sejam finitos. Defina \\mu_j(\\theta) = E[X_1^j|\\theta], j = 1,...k . Suponha que a fun\u00e7\u00e3o: \\begin{split} \\mu : ~&\\Omega \\to \\mathbb{R}^k \\\\ &\\theta \\mapsto \\mu(\\theta) = (\\mu_1(\\theta), ..., \\mu_k(\\theta)), \\end{split} \u00e9 injetiva em \\theta . Seja M(\\mu_1,...,\\mu_k) a fun\u00e7\u00e3o inversa, isto \u00e9, \\theta = M(\\mu_1,...,\\mu_k) O m\u00e9todo dos momentos ser\u00e1 M(m_1,...,m_j) , onde m_j = \\frac{1}{n}\\sum_{i=1}^n X_i^j, j = 1,...,k De forma mais simplificada, basta que sesolvemos o sistema: m_j = \\mu_j(\\theta), isto \u00e9, os momentos amostrais iguais aos momentos da amostra, condicionados em \\theta .","title":"M\u00e9todo dos Momentos"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#teorema_1","text":"Suponha que \\{X_n\\}_{n\\in\\mathbb{N}} i.i.d com distribui\u00e7\u00e3o indexada pelo par\u00e2metro \\theta , k -dimensional. Suponha que os primeiros k momentos existem e s\u00e3o finitos para todo \\theta . Suponha que a inversa M definida acima \u00e9 cont\u00ednua. Ent\u00e3o a sequ\u00eancia de estimadores do m\u00e9todo de momentos em X_1,...,X_n \u00e9 consistente.","title":"Teorema"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#mle-e-estimador-de-bayes","text":"Se tivermos condi\u00e7\u00f5es de suavidade em f(x|\\theta) , podemos provar que quando n \\to \\infty , teremos que: L(\\theta|x) \\to c(x)\\cdot \\exp\\{-\\frac{1}{2V_n(\\theta)/n}(\\theta - \\hat{\\theta})^2\\}, onde \\hat{\\theta} \u00e9 MLE e V_n(\\theta) \u00e9 uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias convergente. No caso de termos uma priori relativamente flat, a posteriori ser\u00e1 aproximadamente uma distribui\u00e7\u00e3o normal com m\u00e9dia \\hat{\\theta} e vari\u00e2ncia V_n(\\hat{\\theta})/n .","title":"M.L.E e Estimador de Bayes"},{"location":"infestatistica_BSc/MaximumLikelihoodEstimator/MaximumLikelihoodEstimator/#exemplo-7612-mortes-exercito-prussio","text":"Bortkiewicz contou o n\u00famero de soldados mortos por horsekick em 14 unidades do ex\u00e9rcito em 20 anos, com 280 contagens ao total. Das contagens temos Valor 0 1 2 3 4 Total Contagem 144 91 32 11 2 280 Modelamos X_1, ..., X_{280} como uma vari\u00e1vel de contagem. Considere a distribui\u00e7\u00e3o Poisson(\\theta) . Escolhemos a distribui\u00e7\u00e3o Gamma(\\alpha,\\beta) , dada que ela pertence \u00e0 familia conjungada. Em particular, a distribui\u00e7\u00e3o a posteriori ser\u00e1 Gamma(\\alpha + \\sum X_i, \\beta + n) , onde \\sum X_i = 196 . Se assumirmos \\alpha inteiro por simplicidade, vemos que a distribui\u00e7\u00e3o Gamma pode ser vista como a soma de \\alpha + \\sum X_i distribui\u00e7\u00f5es Exponencial(\\beta + n) . Logo a soma dessas vari\u00e1veis ser\u00e1 aproximadamente normal com m\u00e9dia 196/280 e vari\u00e2ncia 196/280^2 . import numpy as np import matplotlib.pyplot as plt from scipy.stats import gamma alpha = 1 beta = 1 # Esse \u00e9 o MLE, a m\u00e9dia. Vou supor que esse \u00e9 o par\u00e2metro verdadeiro s\u00f3 para mostrar. theta = 196/280 sum_xi = 196 fig, ax = plt.subplots(2,3,figsize = (18,6)) fig.suptitle('Avaliando a converg\u00eancia da distribui\u00e7\u00e3o Gamma') for index, n in enumerate([1,10,100,1000,10000,280]): i = int(index/3) j = index % 3 X = np.random.poisson(theta, size = n) if n != 280: T = X.sum() ax[i][j].set_title('n = {}'.format(n)) else: T = sum_xi #Valor dos dados ax[i][j].set_title('Dados Oficiais: n = {}'.format(n)) t = np.linspace(start = 0.00001, stop = 3 - i - 1, num = 1000) posteriori = gamma(alpha + T, scale = 1/(beta + n)) y = posteriori.pdf(t) ax[i][j].plot(t, y, color = 'darkblue') ax[i][j].grid(color = 'grey', alpha = 0.6, linestyle = '--') ax[i][j].vlines(theta, ymin = 0, ymax = max(y), color = 'black', linestyle = '--') Veja que com os dados reais, j\u00e1 temos uma boa aproxima\u00e7\u00e3o!","title":"Exemplo 7.6.12 (Mortes ex\u00e9rcito pr\u00fassio)"},{"location":"infestatistica_BSc/PrioriPosteriori/PrioriPosteriori/","text":"Distribui\u00e7\u00f5es a Priori e a Posteriori Priori Tratamos \\theta de um modelo como uma vari\u00e1vel aleat\u00f3ria e atribuimos uma distribui\u00e7\u00e3o para esse par\u00e2metro. O nome ser\u00e1 distribui\u00e7\u00e3o a priori. Ao fazer modelagens, ela \u00e9 em geral pr\u00e9-definida pelo modelador, que \u00e9 em geral aconselhado por um especialista. Posteriori Sejam X_1, ..., X_n v.a. observadas e um par\u00e2metro \\theta desconhecido. A distribui\u00e7\u00e3o de \\theta condicionado nas vari\u00e1veis aleat\u00f3rias \u00e9 a distribui\u00e7\u00e3o a posteriori. Observe a rela\u00e7\u00e3o com o Teorema de Bayes. Teorema Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o f(x|\\theta) . Suponha que o par\u00e2metro seja desconhecido e que a distribui\u00e7\u00e3o da priori seja \\xi(\\theta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori \u00e9: \\xi(\\theta|x) = \\frac{f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta)}{g_n(x)}, \\theta \\in \\Omega Onde g_n \u00e9 a distribui\u00e7\u00e3o marginal conjunta de X_1,...,X_n Observe que, essencialmente \\xi(\\theta|x) \\propto f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta) , mas que sua integral seja 1 . Queremos que essa fun\u00e7\u00e3o seja integr\u00e1vel e a integral sobre o dom\u00ednio seja 1 . Fun\u00e7\u00e3o de Verossimilhan\u00e7a Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x) Observa\u00e7\u00f5es Sequenciais e Predi\u00e7\u00f5es Nesse caso a ordem das vari\u00e1veis X_1, ..., X_n importam (como uma s\u00e9rie temporal, por exemplo). Nesse caso, podemos, iterativamente fazer: \\xi(\\theta|x_1) \\propto f(x_1|\\theta)\\xi(\\theta) \\xi(\\theta|x_1,...,x_{n+1}) \\propto f(x_n|\\theta)\\xi(\\theta|x_1,....,x_n) Isso acontece dada a independ\u00eancia das vari\u00e1veis aleat\u00f3rias. Notebook de Refer\u00eancia Frequentistas Os dados observados s\u00e3o considerados aleat\u00f3rios, realidados de um processo aleat\u00f3rio. Os par\u00e2metros do modelo s\u00e3o fixos e desconhecidos Queremos derivas estimadores para os par\u00e2metros desconhecidos. Bayesianos Os dados s\u00e3o fixos, isto \u00e9, vieram de um processo aleat\u00f3rio, mas depois eles n\u00e3o se alteram. Os par\u00e2metros s\u00e3o usualmente representados por distribui\u00e7\u00f5es, s\u00e3o vari\u00e1veis aleat\u00f3rias. F\u00f3rmula de Bayes. Simples exemplo de infer\u00eancia Bayesiana Hemofilia \u00e9 uma disordem gen\u00e9tica que prejudica a coagula\u00e7\u00e3o em resposta a rupturas em vasos sangu\u00edneos. \u00c9 recessiva ligada ao cromossomo X. Isso implica que homens com 1 gene s\u00e3o afetados, enquanto as mulheres n\u00e3o s\u00e3o afetadas, mas portadoras. Considere uma mulher cuja m\u00e3e \u00e9 portadora e tem um irm\u00e3o afetado. Ela se casa com um homem n\u00e3o afetado. A mulher tem dois filhos consecutivos que n\u00e3o s\u00e3o afetados. Ser\u00e1 que a m\u00e3e \u00e9 portadora? A pergunra \u00e9 simples. Vamos tentar usar um pouco do que sabemos. Seja W = 1 se a mulher \u00e9 portadora e W = 0 se ela n\u00e3o for portadora. Queremos saber P(W = 1|s_1 = 0, s_2 = 0) , isto \u00e9, os filhos n\u00e3o s\u00e3o afetados. Que informa\u00e7\u00e3o n\u00f3s temos ? A m\u00e3e dela \u00e9 portadora, portanto uma priori interessante \u00e9: P(W = 1) = 0.5 \\Rightarrow O(W = 1) = \\frac{P(W=1)}{P(W=0)} = 1 \\text{ chances (odds) a priori } Podemos calcular a fun\u00e7\u00e3o de verossimilhan\u00e7a: L(W = 1|s_1 = 0, s_2 = 0) = F(s_1 = 0, s_2 = 0 | W = 1) = (0.5)(0.5) = 0.25 L(W = 0|s_1 = 0, s_2 = 0) = (1)(1) = 1 import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm priori = 0.5 # P(W = 1) p = 0.5 # prob de um filho ser afetado Likelihood = lambda w, s: np.prod([(1 - i, p**i*(1-p)**(1 - i))[w]for i in s]) s = [0,0] posteriori = Likelihood(1,s)*priori/(Likelihood(1,s)*priori + Likelihood(0,s)*(1 - priori)) print(\"A probabilidade da m\u00e3e portar \u00e9 {} com dois filhos n\u00e3o portadores.\".format(posteriori)) A probabilidade da m\u00e3e portar \u00e9 0.2 com dois filhos n\u00e3o portadores. s = [0] # terceiro filho priori = posteriori posteriori = Likelihood(1,s)*priori/(Likelihood(1,s)*priori + Likelihood(0,s)*(1 - priori)) print(\"A probabilidade da m\u00e3e portar \u00e9 {:.3f} com tr\u00eas filhos n\u00e3o portadores.\".format(posteriori)) A probabilidade da m\u00e3e portar \u00e9 0.111 com tr\u00eas filhos n\u00e3o portadores. priori = 0.5 p = 0.5 s = [0] posteriori = [] for i in range(50): posteriori.append(Likelihood(1,s)*priori/(Likelihood(1,s)*priori + Likelihood(0,s)*(1 - priori))) priori = posteriori[-1] priori = 0.5 posteriori2 = [] for i in range(50): posteriori2.append(Likelihood(1,s)*priori/(Likelihood(1,s)*priori + Likelihood(0,s)*(1 - priori))) priori = posteriori2[-1] s = [np.random.choice([0,1], p = (0.9, 0.1))] plt.plot(range(50), posteriori, label = 'Situa\u00e7\u00e3o 1') plt.plot(range(50), posteriori2, label = 'Situa\u00e7\u00e3o 2') plt.legend() plt.title('Probabilidade dado cada filho') plt.show() Princ\u00edpio de Verossimilhan\u00e7a Afirma que para uma infer\u00eancia sobre um par\u00e2metro \\theta , toda evid\u00eancia de qualquer observa\u00e7\u00e3o de uma vari\u00e1vel aleat\u00f3ria X = x com distribui\u00e7\u00e3o X \\sim f(x|\\theta) se encontra na fun\u00e7\u00e3o de verossimilhan\u00e7a L(\\theta|x) . A interpreta\u00e7\u00e3o \u00e9 de que qualquer observa\u00e7\u00e3o de X pode construir conclus\u00f5es sobre \\theta . Al\u00e9m disso, se pud\u00e9ssemos obter informa\u00e7\u00e3o de \\theta sobre outra vari\u00e1vel aleat\u00f3ria Y com verossimilhan\u00e7a \\tilde{L} , teremos que L(\\theta|x) = c\\cdot \\tilde{L}(\\theta|y) . Isto \u00e9, as conclus\u00f5es sobre o par\u00e2metro n\u00e3o dependem da observa\u00e7\u00e3o feita. Qual o problema? Jeffreys : \"An hypothesis that may be true is rejected because it has failed to predict observable results that have not occurred. \" import pymc3 as pm from pymc3 import Model, Normal, Slice from pymc3 import sample from pymc3 import traceplot from pymc3.distributions import Interpolated from scipy import stats import matplotlib as mpl plt.style.use('seaborn-darkgrid') Gerando os dados Y = \\alpha + \\beta_0\\cdot X_1 + \\beta_1\\cdot X_2 + \\text{erro} # True parameters alpha_true = 5 beta0_true = 7 beta1_true = 13 # Size of the dataset size = 100 # Random variables np.random.seed(1) X1 = np.random.randn(size) X2 = np.random.randn(size)*0.2 e = np.random.randn(size) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e Especificando o modelo Vamos fazer um modelo simples aqui, s\u00f3 para mostrar essa biblioteca nova. A ideia \u00e9 mostrar como funciona a ideia de priori e posteriori. Vamos dizer que Y \\sim N(\\mu, 1) , onde \\mu = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 model = Model() #cria um novo modelo, que em Python, \u00e9 um objeto with model: # Isso cria um contexto em python # Vamos dizer nossas prioris! alpha = Normal('alpha', mu = 0, sigma = 1) beta0 = Normal('beta0', mu = 12, sigma = 1) beta1 = Normal('beta1', mu = 18, sigma = 1) # Valor esperado da sa\u00edda mu = alpha + beta0*X1 + beta1*X2 # Verossimilhan\u00e7a das observa\u00e7\u00f5es Y_obs = Normal('Y_obs', mu = mu, sigma = 1, observed = Y) # Amostras da distribui\u00e7\u00e3o trace = sample(1000) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. A pr\u00f3xima fun\u00e7\u00e3o \u00e9 um plot de distribui\u00e7\u00e3o a priori amostrada (que \u00e9 basicamente um histograma) de cada par\u00e2metro. Veja que parece um chiado em torno da m\u00e9dia. traceplot(trace); Agora que n\u00f3s temos os dados gerados Y , vamos atualizar nosso conhecimento, nossa confian\u00e7a nos par\u00e2metros, atrav\u00e9s da distribui\u00e7\u00e3o a posteriori. Os dados devem ser independentes a ada intera\u00e7\u00e3o para que valha o que estudamos. Para isso, precisamos calcular a posteriori de cada par\u00e2metro. Nesse caso, vamos utilizar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o, utlizando aproxima\u00e7\u00e3o Kernel . N\u00e3o se preocupe com isso, \u00e9 s\u00f3 para mostrar que estamos calculado a posteriori def from_posterior(param, samples): smin, smax = np.min(samples), np.max(samples) width = smax - smin x = np.linspace(smin, smax, 100) y = stats.gaussian_kde(samples)(x) # what was never sampled should have a small probability but not 0, # so we'll extend the domain and use linear approximation of density on it x = np.concatenate([[x[0] - 3 * width], x, [x[-1] + 3 * width]]) y = np.concatenate([[0], y, [0]]) return Interpolated(param, x, y) Agora, vamos gerar mais dados e usar a F\u00f3rmula de Bayes e usaremos uma forma sequencial das observa\u00e7\u00f5es, isto \u00e9, a posteriori da itera\u00e7\u00e3o n-1 ser\u00e1 a priori da itera\u00e7\u00e3o n . traces = [trace] # salva os tra\u00e7os para que plotamos depois. for _ in range(10): # _ indica uma vari\u00e1vel que n\u00e3o \u00e9 usada # Gerando mais e mais dados! X1 = np.random.randn(size) X2 = np.random.randn(size) * 0.2 e = np.random.randn(size) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e model = Model() with model: # As novas prioris s\u00e3o as posterioris alpha = from_posterior('alpha', trace['alpha']) beta0 = from_posterior('beta0', trace['beta0']) beta1 = from_posterior('beta1', trace['beta1']) # EValor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Calculando a verossimilhan\u00e7a dos novos dados Y_obs = Normal('Y_obs', mu = mu, sigma = 1, observed = Y) # Amostrando da posteriori, porque n\u00e3o estamos calculando a forma fechada trace = sample(1000) traces.append(trace) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The acceptance probability does not match the target. It is 0.8800702431829607, but should be close to 0.8. Try to increase the number of tuning steps. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. fig, ax = plt.subplots(1,3,figsize=(20, 5)) # Definindo cores cmap = mpl.cm.autumn for index, param in enumerate(['alpha', 'beta0', 'beta1']): for update_i, trace in enumerate(traces): samples = trace[param] smin, smax = np.min(samples), np.max(samples) x = np.linspace(smin, smax, 100) y = stats.gaussian_kde(samples)(x) ax[index].plot(x, y, color=cmap(1 - update_i / len(traces)), alpha = update_i/len(traces)) ax[index].axvline({'alpha': alpha_true, 'beta0': beta0_true, 'beta1': beta1_true}[param], c='k') ax[index].set_ylabel('Frequency') ax[index].set_title(param) plt.tight_layout();","title":"Distribui\u00e7\u00f5es a Priori e a Posteriori"},{"location":"infestatistica_BSc/PrioriPosteriori/PrioriPosteriori/#distribuicoes-a-priori-e-a-posteriori","text":"","title":"Distribui\u00e7\u00f5es a Priori e a Posteriori"},{"location":"infestatistica_BSc/PrioriPosteriori/PrioriPosteriori/#priori","text":"Tratamos \\theta de um modelo como uma vari\u00e1vel aleat\u00f3ria e atribuimos uma distribui\u00e7\u00e3o para esse par\u00e2metro. O nome ser\u00e1 distribui\u00e7\u00e3o a priori. Ao fazer modelagens, ela \u00e9 em geral pr\u00e9-definida pelo modelador, que \u00e9 em geral aconselhado por um especialista.","title":"Priori"},{"location":"infestatistica_BSc/PrioriPosteriori/PrioriPosteriori/#posteriori","text":"Sejam X_1, ..., X_n v.a. observadas e um par\u00e2metro \\theta desconhecido. A distribui\u00e7\u00e3o de \\theta condicionado nas vari\u00e1veis aleat\u00f3rias \u00e9 a distribui\u00e7\u00e3o a posteriori. Observe a rela\u00e7\u00e3o com o Teorema de Bayes.","title":"Posteriori"},{"location":"infestatistica_BSc/PrioriPosteriori/PrioriPosteriori/#teorema","text":"Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o f(x|\\theta) . Suponha que o par\u00e2metro seja desconhecido e que a distribui\u00e7\u00e3o da priori seja \\xi(\\theta) . Ent\u00e3o, a distribui\u00e7\u00e3o a posteriori \u00e9: \\xi(\\theta|x) = \\frac{f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta)}{g_n(x)}, \\theta \\in \\Omega Onde g_n \u00e9 a distribui\u00e7\u00e3o marginal conjunta de X_1,...,X_n Observe que, essencialmente \\xi(\\theta|x) \\propto f(x_1|\\theta)...f(x_n|\\theta)\\xi(\\theta) , mas que sua integral seja 1 . Queremos que essa fun\u00e7\u00e3o seja integr\u00e1vel e a integral sobre o dom\u00ednio seja 1 .","title":"Teorema"},{"location":"infestatistica_BSc/PrioriPosteriori/PrioriPosteriori/#funcao-de-verossimilhanca","text":"Quando a fun\u00e7\u00e3o de densidade de probabilidade f_n(x|\\theta) das observa\u00e7\u00f5es de uma amostra aleat\u00f3ria \u00e9 vista como uma fun\u00e7\u00e3o de \\theta , chamamos ela de fun\u00e7\u00e3o de verossimilhan\u00e7a. \\theta \\mapsto f_n(x|\\theta) := L(\\theta|x)","title":"Fun\u00e7\u00e3o de Verossimilhan\u00e7a"},{"location":"infestatistica_BSc/PrioriPosteriori/PrioriPosteriori/#observacoes-sequenciais-e-predicoes","text":"Nesse caso a ordem das vari\u00e1veis X_1, ..., X_n importam (como uma s\u00e9rie temporal, por exemplo). Nesse caso, podemos, iterativamente fazer: \\xi(\\theta|x_1) \\propto f(x_1|\\theta)\\xi(\\theta) \\xi(\\theta|x_1,...,x_{n+1}) \\propto f(x_n|\\theta)\\xi(\\theta|x_1,....,x_n) Isso acontece dada a independ\u00eancia das vari\u00e1veis aleat\u00f3rias. Notebook de Refer\u00eancia","title":"Observa\u00e7\u00f5es Sequenciais e Predi\u00e7\u00f5es"},{"location":"infestatistica_BSc/PrioriPosteriori/PrioriPosteriori/#frequentistas","text":"Os dados observados s\u00e3o considerados aleat\u00f3rios, realidados de um processo aleat\u00f3rio. Os par\u00e2metros do modelo s\u00e3o fixos e desconhecidos Queremos derivas estimadores para os par\u00e2metros desconhecidos.","title":"Frequentistas"},{"location":"infestatistica_BSc/PrioriPosteriori/PrioriPosteriori/#bayesianos","text":"Os dados s\u00e3o fixos, isto \u00e9, vieram de um processo aleat\u00f3rio, mas depois eles n\u00e3o se alteram. Os par\u00e2metros s\u00e3o usualmente representados por distribui\u00e7\u00f5es, s\u00e3o vari\u00e1veis aleat\u00f3rias. F\u00f3rmula de Bayes.","title":"Bayesianos"},{"location":"infestatistica_BSc/PrioriPosteriori/PrioriPosteriori/#simples-exemplo-de-inferencia-bayesiana","text":"Hemofilia \u00e9 uma disordem gen\u00e9tica que prejudica a coagula\u00e7\u00e3o em resposta a rupturas em vasos sangu\u00edneos. \u00c9 recessiva ligada ao cromossomo X. Isso implica que homens com 1 gene s\u00e3o afetados, enquanto as mulheres n\u00e3o s\u00e3o afetadas, mas portadoras. Considere uma mulher cuja m\u00e3e \u00e9 portadora e tem um irm\u00e3o afetado. Ela se casa com um homem n\u00e3o afetado. A mulher tem dois filhos consecutivos que n\u00e3o s\u00e3o afetados. Ser\u00e1 que a m\u00e3e \u00e9 portadora? A pergunra \u00e9 simples. Vamos tentar usar um pouco do que sabemos. Seja W = 1 se a mulher \u00e9 portadora e W = 0 se ela n\u00e3o for portadora. Queremos saber P(W = 1|s_1 = 0, s_2 = 0) , isto \u00e9, os filhos n\u00e3o s\u00e3o afetados. Que informa\u00e7\u00e3o n\u00f3s temos ? A m\u00e3e dela \u00e9 portadora, portanto uma priori interessante \u00e9: P(W = 1) = 0.5 \\Rightarrow O(W = 1) = \\frac{P(W=1)}{P(W=0)} = 1 \\text{ chances (odds) a priori } Podemos calcular a fun\u00e7\u00e3o de verossimilhan\u00e7a: L(W = 1|s_1 = 0, s_2 = 0) = F(s_1 = 0, s_2 = 0 | W = 1) = (0.5)(0.5) = 0.25 L(W = 0|s_1 = 0, s_2 = 0) = (1)(1) = 1 import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm priori = 0.5 # P(W = 1) p = 0.5 # prob de um filho ser afetado Likelihood = lambda w, s: np.prod([(1 - i, p**i*(1-p)**(1 - i))[w]for i in s]) s = [0,0] posteriori = Likelihood(1,s)*priori/(Likelihood(1,s)*priori + Likelihood(0,s)*(1 - priori)) print(\"A probabilidade da m\u00e3e portar \u00e9 {} com dois filhos n\u00e3o portadores.\".format(posteriori)) A probabilidade da m\u00e3e portar \u00e9 0.2 com dois filhos n\u00e3o portadores. s = [0] # terceiro filho priori = posteriori posteriori = Likelihood(1,s)*priori/(Likelihood(1,s)*priori + Likelihood(0,s)*(1 - priori)) print(\"A probabilidade da m\u00e3e portar \u00e9 {:.3f} com tr\u00eas filhos n\u00e3o portadores.\".format(posteriori)) A probabilidade da m\u00e3e portar \u00e9 0.111 com tr\u00eas filhos n\u00e3o portadores. priori = 0.5 p = 0.5 s = [0] posteriori = [] for i in range(50): posteriori.append(Likelihood(1,s)*priori/(Likelihood(1,s)*priori + Likelihood(0,s)*(1 - priori))) priori = posteriori[-1] priori = 0.5 posteriori2 = [] for i in range(50): posteriori2.append(Likelihood(1,s)*priori/(Likelihood(1,s)*priori + Likelihood(0,s)*(1 - priori))) priori = posteriori2[-1] s = [np.random.choice([0,1], p = (0.9, 0.1))] plt.plot(range(50), posteriori, label = 'Situa\u00e7\u00e3o 1') plt.plot(range(50), posteriori2, label = 'Situa\u00e7\u00e3o 2') plt.legend() plt.title('Probabilidade dado cada filho') plt.show()","title":"Simples exemplo de infer\u00eancia Bayesiana"},{"location":"infestatistica_BSc/PrioriPosteriori/PrioriPosteriori/#principio-de-verossimilhanca","text":"Afirma que para uma infer\u00eancia sobre um par\u00e2metro \\theta , toda evid\u00eancia de qualquer observa\u00e7\u00e3o de uma vari\u00e1vel aleat\u00f3ria X = x com distribui\u00e7\u00e3o X \\sim f(x|\\theta) se encontra na fun\u00e7\u00e3o de verossimilhan\u00e7a L(\\theta|x) . A interpreta\u00e7\u00e3o \u00e9 de que qualquer observa\u00e7\u00e3o de X pode construir conclus\u00f5es sobre \\theta . Al\u00e9m disso, se pud\u00e9ssemos obter informa\u00e7\u00e3o de \\theta sobre outra vari\u00e1vel aleat\u00f3ria Y com verossimilhan\u00e7a \\tilde{L} , teremos que L(\\theta|x) = c\\cdot \\tilde{L}(\\theta|y) . Isto \u00e9, as conclus\u00f5es sobre o par\u00e2metro n\u00e3o dependem da observa\u00e7\u00e3o feita. Qual o problema? Jeffreys : \"An hypothesis that may be true is rejected because it has failed to predict observable results that have not occurred. \" import pymc3 as pm from pymc3 import Model, Normal, Slice from pymc3 import sample from pymc3 import traceplot from pymc3.distributions import Interpolated from scipy import stats import matplotlib as mpl plt.style.use('seaborn-darkgrid')","title":"Princ\u00edpio de Verossimilhan\u00e7a"},{"location":"infestatistica_BSc/PrioriPosteriori/PrioriPosteriori/#gerando-os-dados","text":"Y = \\alpha + \\beta_0\\cdot X_1 + \\beta_1\\cdot X_2 + \\text{erro} # True parameters alpha_true = 5 beta0_true = 7 beta1_true = 13 # Size of the dataset size = 100 # Random variables np.random.seed(1) X1 = np.random.randn(size) X2 = np.random.randn(size)*0.2 e = np.random.randn(size) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e","title":"Gerando os dados"},{"location":"infestatistica_BSc/PrioriPosteriori/PrioriPosteriori/#especificando-o-modelo","text":"Vamos fazer um modelo simples aqui, s\u00f3 para mostrar essa biblioteca nova. A ideia \u00e9 mostrar como funciona a ideia de priori e posteriori. Vamos dizer que Y \\sim N(\\mu, 1) , onde \\mu = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 model = Model() #cria um novo modelo, que em Python, \u00e9 um objeto with model: # Isso cria um contexto em python # Vamos dizer nossas prioris! alpha = Normal('alpha', mu = 0, sigma = 1) beta0 = Normal('beta0', mu = 12, sigma = 1) beta1 = Normal('beta1', mu = 18, sigma = 1) # Valor esperado da sa\u00edda mu = alpha + beta0*X1 + beta1*X2 # Verossimilhan\u00e7a das observa\u00e7\u00f5es Y_obs = Normal('Y_obs', mu = mu, sigma = 1, observed = Y) # Amostras da distribui\u00e7\u00e3o trace = sample(1000) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. A pr\u00f3xima fun\u00e7\u00e3o \u00e9 um plot de distribui\u00e7\u00e3o a priori amostrada (que \u00e9 basicamente um histograma) de cada par\u00e2metro. Veja que parece um chiado em torno da m\u00e9dia. traceplot(trace); Agora que n\u00f3s temos os dados gerados Y , vamos atualizar nosso conhecimento, nossa confian\u00e7a nos par\u00e2metros, atrav\u00e9s da distribui\u00e7\u00e3o a posteriori. Os dados devem ser independentes a ada intera\u00e7\u00e3o para que valha o que estudamos. Para isso, precisamos calcular a posteriori de cada par\u00e2metro. Nesse caso, vamos utilizar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o, utlizando aproxima\u00e7\u00e3o Kernel . N\u00e3o se preocupe com isso, \u00e9 s\u00f3 para mostrar que estamos calculado a posteriori def from_posterior(param, samples): smin, smax = np.min(samples), np.max(samples) width = smax - smin x = np.linspace(smin, smax, 100) y = stats.gaussian_kde(samples)(x) # what was never sampled should have a small probability but not 0, # so we'll extend the domain and use linear approximation of density on it x = np.concatenate([[x[0] - 3 * width], x, [x[-1] + 3 * width]]) y = np.concatenate([[0], y, [0]]) return Interpolated(param, x, y) Agora, vamos gerar mais dados e usar a F\u00f3rmula de Bayes e usaremos uma forma sequencial das observa\u00e7\u00f5es, isto \u00e9, a posteriori da itera\u00e7\u00e3o n-1 ser\u00e1 a priori da itera\u00e7\u00e3o n . traces = [trace] # salva os tra\u00e7os para que plotamos depois. for _ in range(10): # _ indica uma vari\u00e1vel que n\u00e3o \u00e9 usada # Gerando mais e mais dados! X1 = np.random.randn(size) X2 = np.random.randn(size) * 0.2 e = np.random.randn(size) Y = alpha_true + beta0_true * X1 + beta1_true * X2 + e model = Model() with model: # As novas prioris s\u00e3o as posterioris alpha = from_posterior('alpha', trace['alpha']) beta0 = from_posterior('beta0', trace['beta0']) beta1 = from_posterior('beta1', trace['beta1']) # EValor esperado da sa\u00edda mu = alpha + beta0 * X1 + beta1 * X2 # Calculando a verossimilhan\u00e7a dos novos dados Y_obs = Normal('Y_obs', mu = mu, sigma = 1, observed = Y) # Amostrando da posteriori, porque n\u00e3o estamos calculando a forma fechada trace = sample(1000) traces.append(trace) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The number of effective samples is smaller than 25% for some parameters. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. The acceptance probability does not match the target. It is 0.8800702431829607, but should be close to 0.8. Try to increase the number of tuning steps. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta1, beta0, alpha] \u2588Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds. fig, ax = plt.subplots(1,3,figsize=(20, 5)) # Definindo cores cmap = mpl.cm.autumn for index, param in enumerate(['alpha', 'beta0', 'beta1']): for update_i, trace in enumerate(traces): samples = trace[param] smin, smax = np.min(samples), np.max(samples) x = np.linspace(smin, smax, 100) y = stats.gaussian_kde(samples)(x) ax[index].plot(x, y, color=cmap(1 - update_i / len(traces)), alpha = update_i/len(traces)) ax[index].axvline({'alpha': alpha_true, 'beta0': beta0_true, 'beta1': beta1_true}[param], c='k') ax[index].set_ylabel('Frequency') ax[index].set_title(param) plt.tight_layout();","title":"Especificando o modelo"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/","text":"Distribui\u00e7\u00e3o Chi-Quadrado Para cada m positivo, a distribui\u00e7\u00e3o Gamma(m/2, 1/2) \u00e9 chamada de distribui\u00e7\u00e3o \\chi^2 . Ela foi primeiramente descrita por Helmert para computar a distribui\u00e7\u00e3o amostral de uma popula\u00e7\u00e3o normal. Vamos ver como a normal se relaciona mais a frente. f(x) = \\frac{1}{2^{m/2}\\Gamma(m/2)}x^{m/2 - 1}e^{-x/2} Propriedades Se X \\sim \\chi^2(m) , ent\u00e3o: E(X) = m Var(X) = 2m \\psi(t) = \\left(\\frac{1}{1-2t}\\right)^{m/2}, t < \\frac{1}{2} Soma de \\chi^2 Se X_1, ..., X_k s\u00e3o independentes e cada uma tem grau de liberdade m_i , ent\u00e3o X_1 + ... + X_n tem distribui\u00e7\u00e3o \\chi^2(m_1 + .... + m_k) Rela\u00e7\u00e3o com a Normal Se X tem distribui\u00e7\u00e3o normal padr\u00e3o, Y = X^2 \\sim \\chi^2(1) De fato, se juntarmos as \u00faltimos dois teoremas, veremos que a soma de quadrados de normais independentes e identicamente distribuidas ser\u00e1 \\chi^2(m) , onde m \u00e9 o n\u00famero de parcelas. Implementa\u00e7\u00e3o import numpy as np import matplotlib.pyplot as plt from scipy.stats import chi2 from matplotlib import animation, cm from IPython.display import HTML # Random Object ro = np.random.default_rng(1000) # Para assegurar reprodutibilidade degree_freedom = 10 mean, var, skew, kurt = chi2.stats(degree_freedom, moments = 'mvsk') print('Propriedades') print('M\u00e9dia: {}'.format(mean)) print('Var: {}'.format(var)) print('Assimetria: {}'.format(skew)) print('Curtose: {}'.format(kurt)) Propriedades M\u00e9dia: 10.0 Var: 20.0 Assimetria: 0.8944271909999159 Curtose: 1.2 fig, ax = plt.subplots(1, 1) x = np.linspace(chi2.ppf(0.01, degree_freedom), chi2.ppf(0.99, degree_freedom), 100) ax.plot(x, chi2.pdf(x, degree_freedom), 'r-', lw=5, alpha=0.6, label='chi2 pdf') r = chi2.rvs(degree_freedom, size = 10000) ax.hist(r, density = True, alpha = 0.2) ax.legend() plt.show() fig, ax = plt.subplots() line, = ax.plot(x, chi2.pdf(x, degree_freedom), 'r-', lw=5, alpha=0.6) ax.set_xlim((0,150)) ax.set_title('Chi-Square') def animate(i, degree_freedom): x = np.linspace(0, chi2.ppf(0.99, degree_freedom + i), 100) line.set_data(x, chi2.pdf(x, degree_freedom + i)) return line, anim = animation.FuncAnimation(fig, animate, frames = 100, interval = 50, fargs=(degree_freedom,), repeat = False) HTML(anim.to_html5_video()) Your browser does not support the video tag. Distribui\u00e7\u00e3o Conjunta da m\u00e9dia e vari\u00e2ncia amostrais X_1,...,X_n formam uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal e com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 desconhecidos. Estamos interessados na distribui\u00e7\u00e3o conjunta dos estimadores de m\u00e1xima verossimilhan\u00e7a para m\u00e9dia e vari\u00e2ncia da amostra. Teorema de Basu Sejam \\hat{\\mu} = \\bar{X}_n e \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 a m\u00e9dia e vari\u00e2ncia amostrais, respectivamente. Ent\u00e3o \\hat{\\mu} tem distribui\u00e7\u00e3o normal com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 /n , enquanto n\\hat{\\sigma}^2/\\sigma^2 tem a distribui\u00e7\u00e3o \\chi^2(n-1) , isto \u00e9, com n-1 grau de liberdade. Al\u00e9m disso elas s\u00e3o independentes. Esse teorema \u00e9 um pouco mais complexo e, na verdade, essa seria uma esp\u00e9cie de aplica\u00e7\u00e3o do teorema, na verdade. O teorema de Basu diz que: Se T \u00e9 uma estat\u00edstica suficiente completa (Considere, nesse teorema, g uma fun\u00e7\u00e3o integr\u00e1vel limitada) para \\theta e A uma estat\u00edstica ancillary, ent\u00e3o T \u00e9 independente de A . Nesse caso \\hat{\\mu} \u00e9 completa suficiente e \\hat{\\sigma}^2 \u00e9 ancillary, por que n\u00e3o depende de \\mu . O mais interessante \u00e9 que essa propriedade \u00e9 s\u00f3 vista com a distribui\u00e7\u00e3o normal ! Olhem a p\u00e1gina 9. Demonstra\u00e7\u00e3o O livro tem uma abordagem um pouco mais voltado \u00e0 \u00c1lgebra Linear. Aqui vou mostrar uma ideia um pouco diferente, onde voc\u00eas podem demonstrar os passos, como exerc\u00edcio. Passo 1: \\sum_{i=1}^n X_i^2 = n\\hat{\\sigma}^2 + n\\hat{\\mu}^2 Dica: Escrever \\hat{\\sigma}^2 e abrir em tr\u00eas somat\u00f3rios. Passo 2: \\sum_{i=1}^n (X_i - \\mu)^2 = n\\hat{\\sigma}^2 + n(\\hat{\\mu} - \\mu)^2 Dica: O Passo 1 \u00e9 um caso especial do Passo 2. O processo \u00e9 o mesmo. Passo 3: \\hat{\\mu} \u00e9 independente de X_i - \\hat{\\mu}, i = 1,...,n . Dica: Montar a pdf conjunta de X_1, ..., X_n (j\u00e1 fizemos isso atrave\u015b da verossimilhan\u00e7a) e fazer uma mudan\u00e7a de vari\u00e1vel Y_1 = \\hat{\\mu}, Y_2 = X_2 - \\hat{\\mu}, ..., Y_n = X_n - \\hat{\\mu} . Com essa mudan\u00e7a, \u00e9 poss\u00edvel montar a pdf como fun\u00e7\u00e3o de y_1,...,y_n . Esse processo \u00e9 um pouco mais chato, mas \u00e9 bom lembrar como fazez mudan\u00e7a de vari\u00e1vel para pdfs. Aqui voc\u00ea pode conferir como . \u00c9 importante lembrar que \u00e9 uma fun\u00e7\u00e3o de y ap\u00f3s transformada e n\u00e3o de x . Dica 2: Fatorizar a pdf conjunta. Voc\u00ea vai ver como se destaca a independ\u00eancia aqui. Passo 4: Mostrar que \\hat{\\mu} e \\hat{\\sigma}^2 s\u00e3o independentes. Refer\u00eancias 1 2 Simples visualiza\u00e7\u00e3o Eu gostaria de comparar o que acontece com a m\u00e9dia e vari\u00e2ncia amostral da distribui\u00e7\u00e3o normal e da distribui\u00e7\u00e3o gamma. Para isso, geero amostras de tamanho n , calculo as estat\u00edsticas e salvo. Fa\u00e7o esse procedimento o n\u00famero de pontos que quiser. ite = 10000 n = 10000 # Par\u00e2metros da Normal mu = 5 sigma = 2 # Par\u00e2metros da Gamma alpha = 5 beta = 4 means = np.zeros((ite,2)) variances = np.zeros((ite,2)) for i in range(ite): X = ro.normal(loc = mu, scale = sigma, size = n) Y = ro.gamma(shape = alpha, scale = 1/beta, size = n) means[i,0] = np.mean(X) means[i,1] = np.mean(Y) variances[i,0] = np.var(X, ddof = 0) variances[i,1] = np.var(Y, ddof = 0) coef_normal = np.polyfit(x = means[:,0], y = variances[:,0], deg = 1) coef_gamma = np.polyfit(x = means[:,1], y = variances[:,1], deg = 1) fig, ax = plt.subplots(1,2,figsize = (14,5)) fig.suptitle('Comparando m\u00e9dia e vari\u00e2ncia amostral') ax[0].scatter(means[:,0], variances[:,0]) ax[1].scatter(means[:,1], variances[:,1]) ax[0].plot(means[:,0], coef_normal[0]*means[:,0] + coef_normal[1], color = 'red') ax[1].plot(means[:,1], coef_gamma[0]*means[:,1] + coef_gamma[1], color = 'red') ax[0].set_xlabel(r'$\\bar{X}_n$', fontsize = 18) ax[1].set_xlabel(r'$\\bar{X}_n$', fontsize = 18) ax[0].set_ylabel(r'$\\sum (X_i - \\bar{X}_n)^2$', fontsize = 18) ax[1].set_ylabel(r'$\\sum (X_i - \\bar{X}_n)^2$', fontsize = 18) ax[0].set_title('Distribui\u00e7\u00e3o Normal') ax[1].set_title('Distribui\u00e7\u00e3o Gamma') ax[0].grid(alpha = 0.5, linestyle = '--') ax[1].grid(alpha = 0.5, linestyle = '--') plt.show() Obs: A n\u00e3o inclina\u00e7\u00e3o da reta n\u00e3o significa que existe independ\u00eancia, mas como s\u00e3o independentes, a gente espera que a inclina\u00e7\u00e3o seja pequena. Distribui\u00e7\u00f5es T Student Artigo original : Olhe a p\u00e1gina 9! Defini\u00e7\u00e3o Sejam Y \\sim \\chi^2(m) e Z \\sim N(0,1) independentes. Ent\u00e3o X = \\frac{Z}{\\left(\\frac{Y}{m}\\right)^{1/2}} \\sim t(m) onde t(m) \u00e9 a distribui\u00e7\u00e3o t-student com m graus de liberdade. Fun\u00e7\u00e3o densidade de probabilidade Para escrever essa fun\u00e7\u00e3o de probabilidade, defina X como acima e W = Y . J\u00e1 sabemos a distribui\u00e7\u00e3o conjunta de Y e Z , pois eles s\u00e3o independentes. Com essa mudan\u00e7a de vari\u00e1vel ( confira aqui se n\u00e3o lembra como \u00e9 feito ), voc\u00ea conseque escrever a distribui\u00e7\u00e3o conjunta de X e W . Depois, basta calcular a distribui\u00e7\u00e3o marginal de X , integrando em W . f(x) = \\frac{\\Gamma\\left(\\frac{m+1}{2}\\right)}{(m\\pi)^{1/2}\\Gamma\\left(\\frac{m}{2}\\right)}\\left(1 + \\frac{x^2}{m} \\right)^{-(m+1)/2}, x \\in \\mathbb{R}, onde \\Gamma \u00e9 a fun\u00e7\u00e3o Gamma , tal que, n \\in \\mathbb{N}, \\Gamma(n) = (n-1)! \\Gamma(z+1) = z\\Gamma(z) \\Gamma(1/2) = \\sqrt{\\pi} Quando m \\leq 1 , a m\u00e9dia \u00e9 divergente. Isso pode ser vizualizado pelo expoente que ser\u00e1 \\leq -1 , o que diverge (lembre de \\int 1/x ). Quando m > 1 , a m\u00e9dia existe e \u00e9 0 pela simetria da distribui\u00e7\u00e3o. Em particular, podemos mostrar que se k < m , E[|X^k|] < + \\infty e se k \\geq m , o momento diverge. Se X \\sim t(m), m > 2 , Var(X) = \\frac{m}{m-2} Teorema Seja X_1, ..., X_n \\overset{iid}{\\sim} N(\\mu,\\sigma^2) . Seja \\sigma ' = \\left[\\frac{\\sum_{i=1}^n (X_i - \\bar{X}_n)^2}{n-1}\\right]^{1/2} Ent\u00e3o n^{1/2}(\\bar{X}_n - \\mu)/\\sigma ' \\sim t(n-1) Rela\u00e7\u00e3o com a Normal e Cauchy Da mesma forma que a distribui\u00e7\u00e3o normal e a distribui\u00e7\u00e3o Cauchy, a distribui\u00e7\u00e3o t \u00e9 centrada em 0 e tem sua moda nesse valor. Entretanto a cauda a distribui\u00e7\u00e3o t (quando x \\to -\\infty ou x \\to +\\infty ), \u00e9 mais pesada, no sentido de que tende para 0 em uma velocidade menor do que a normal. Outra coisa interessante \u00e9 que a ditrivui\u00e7\u00e3o t(1) \u00e9 a distribui\u00e7\u00e3o Cauchy . Al\u00e9m disso, quando n \\to \\infty , converge para a pdf da normal padr\u00e3o ( Normal(0,1) ). Ferramentas para demonstrar a converg\u00eancia Teorema de Slutsky : Considere o corol\u00e1rio com f(x,y) = \\frac{x}{y} Lei dos Grandes N\u00fameros : Escreva a qui-quadrado como soma de normais. from scipy.stats import t, norm, cauchy Implementa\u00e7\u00e3o Primeiro vamos ver a cara da distribui\u00e7\u00e3o t m = 10 X = t(df = m) w = np.arange(-3, 3, 0.1) fig, ax = plt.subplots(1,2,figsize = (12,5)) ax[0].plot(w, X.pdf(w), lw = 5, color = 'orange') ax[1].plot(w, X.cdf(w), lw = 5, color = 'orange') ax[0].set_title('PDF t-Student') ax[1].set_title('CDF t-Student') plt.show() Vamos ver o que acontece quando m \\leq 1 ? ite = 1000 n = 10000 m1 = 10 m2 = 0.5 means = np.zeros((ite,2)) for i in range(ite): X = ro.standard_t(df = m1, size = n) Y = ro.standard_t(df = m2, size = n) means[i,0] = np.mean(X) means[i,1] = np.mean(Y) fig, ax = plt.subplots(1,2,figsize = (14,5)) ax[0].hist(means[:,0], bins = 100) ax[1].hist(np.log(means[:,1]), bins = 10) ax[0].set_xlabel('E[X]') ax[1].set_xlabel('log E[X]') ax[0].set_title('m = 10') ax[1].set_title('m = 0.5') plt.show() <ipython-input-11-2bfd1961d53b>:3: RuntimeWarning: invalid value encountered in log ax[1].hist(np.log(means[:,1]), bins = 10) No eixo x do segundo gr\u00e1fico plotei o logaritmo, dado que alguns resultados eram extremamente grandes! Isso indica visualmente que a m\u00e9dia diverge! Rela\u00e7\u00e3o com a Normal e com Cauchy C = cauchy() Z = norm(loc = 0, scale = 1) T = t(df = 1) fig, ax = plt.subplots(1,2,figsize = (14,5)) ax[0].plot(w,C.pdf(w), label = 'Cauchy') ax[0].scatter(w, T.pdf(w), c = 'red', marker = \"*\", label = 't-Student') ax[0].legend() ax[0].set_title('t-Student e Cauchy quando m = 1') ax[1].plot(w,Z.pdf(w), label = 'N(0,1)') ax[1].set_title('Converg\u00eancia da t para a normal') for i in np.logspace(np.log10(1), np.log10(20), 5): T = t(df = int(i)) ax[1].plot(w, T.pdf(w), linestyle = '--', alpha = i/40 + 0.5, color = 'grey', label = 't({})'.format(int(i))) ax[1].legend(loc = 'upper right') plt.show() Distribui\u00e7\u00e3o F Sejam Y \\sim \\chi^2_m e W \\sim \\chi^2_n independentes. Defina X = \\frac{Y/m}{W/n} = \\frac{nY}{mW} Dizemos que X tem distribui\u00e7\u00e3o F . A sua motiva\u00e7\u00e3o vem do teste de hip\u00f3teses que compara vari\u00e2ncias de duas normais. Fun\u00e7\u00e3o de densidade de probabilidade Seja X \\sim F_{m,n} . Ent\u00e3o sua pdf tem suporte em x > 0 e pe definida f(x) = \\frac{\\Gamma\\left[\\frac{1}{2}(m+n)\\right]m^{m/2}n^{n/2}}{\\Gamma\\left(\\frac{1}{2}m\\right)\\Gamma\\left(\\frac{1}{2}n\\right)}\\cdot \\frac{x^{(m/2) - 1}}{(mx + n)^{(m+n)/2)}} Observe que ela n\u00e3o \u00e9 sim\u00e9trica em m e n . Assim, se trocarmos eles de lugar, teremos um resultado diferente. Propriedades Seja X \\sim F_{m,n} . Ent\u00e3o 1/X \\sim F_{n,m} . Se Y \\sim t_n , ent\u00e3o Y^2 \\sim F_{1,n} . Existem diversas rela\u00e7\u00f5es que s\u00e3o encontradas com outras distribui\u00e7\u00f5es. Confira aqui E[X] = \\frac{n}{n-2}, n > 2 Var[X] = \\frac{2n^2(m + n - 2)}{m(n-2)^2(n-4)} import numpy as np from scipy.stats import f import matplotlib.pyplot as plt Vamos ver como \u00e9 a cara dessa distribui\u00e7\u00e3o: m, n = 20, 10 X = f(dfn = m, dfd = n) w = np.arange(0, 5, 0.1) fig, ax = plt.subplots(1,2,figsize = (12,5)) ax[0].plot(w, X.pdf(w), lw = 5, color = 'orange') ax[1].plot(w, X.cdf(w), lw = 5, color = 'orange') ax[0].set_title('PDF Distribui\u00e7\u00e3o F') ax[1].set_title('CDF Distribui\u00e7\u00e3o F') plt.show()","title":"Distribui\u00e7\u00e3o Chi-Quadrado"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#distribuicao-chi-quadrado","text":"Para cada m positivo, a distribui\u00e7\u00e3o Gamma(m/2, 1/2) \u00e9 chamada de distribui\u00e7\u00e3o \\chi^2 . Ela foi primeiramente descrita por Helmert para computar a distribui\u00e7\u00e3o amostral de uma popula\u00e7\u00e3o normal. Vamos ver como a normal se relaciona mais a frente. f(x) = \\frac{1}{2^{m/2}\\Gamma(m/2)}x^{m/2 - 1}e^{-x/2}","title":"Distribui\u00e7\u00e3o Chi-Quadrado"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#propriedades","text":"Se X \\sim \\chi^2(m) , ent\u00e3o: E(X) = m Var(X) = 2m \\psi(t) = \\left(\\frac{1}{1-2t}\\right)^{m/2}, t < \\frac{1}{2}","title":"Propriedades"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#soma-de-chi2","text":"Se X_1, ..., X_k s\u00e3o independentes e cada uma tem grau de liberdade m_i , ent\u00e3o X_1 + ... + X_n tem distribui\u00e7\u00e3o \\chi^2(m_1 + .... + m_k)","title":"Soma de \\chi^2"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#relacao-com-a-normal","text":"Se X tem distribui\u00e7\u00e3o normal padr\u00e3o, Y = X^2 \\sim \\chi^2(1) De fato, se juntarmos as \u00faltimos dois teoremas, veremos que a soma de quadrados de normais independentes e identicamente distribuidas ser\u00e1 \\chi^2(m) , onde m \u00e9 o n\u00famero de parcelas.","title":"Rela\u00e7\u00e3o com a Normal"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#implementacao","text":"import numpy as np import matplotlib.pyplot as plt from scipy.stats import chi2 from matplotlib import animation, cm from IPython.display import HTML # Random Object ro = np.random.default_rng(1000) # Para assegurar reprodutibilidade degree_freedom = 10 mean, var, skew, kurt = chi2.stats(degree_freedom, moments = 'mvsk') print('Propriedades') print('M\u00e9dia: {}'.format(mean)) print('Var: {}'.format(var)) print('Assimetria: {}'.format(skew)) print('Curtose: {}'.format(kurt)) Propriedades M\u00e9dia: 10.0 Var: 20.0 Assimetria: 0.8944271909999159 Curtose: 1.2 fig, ax = plt.subplots(1, 1) x = np.linspace(chi2.ppf(0.01, degree_freedom), chi2.ppf(0.99, degree_freedom), 100) ax.plot(x, chi2.pdf(x, degree_freedom), 'r-', lw=5, alpha=0.6, label='chi2 pdf') r = chi2.rvs(degree_freedom, size = 10000) ax.hist(r, density = True, alpha = 0.2) ax.legend() plt.show() fig, ax = plt.subplots() line, = ax.plot(x, chi2.pdf(x, degree_freedom), 'r-', lw=5, alpha=0.6) ax.set_xlim((0,150)) ax.set_title('Chi-Square') def animate(i, degree_freedom): x = np.linspace(0, chi2.ppf(0.99, degree_freedom + i), 100) line.set_data(x, chi2.pdf(x, degree_freedom + i)) return line, anim = animation.FuncAnimation(fig, animate, frames = 100, interval = 50, fargs=(degree_freedom,), repeat = False) HTML(anim.to_html5_video()) Your browser does not support the video tag.","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#distribuicao-conjunta-da-media-e-variancia-amostrais","text":"X_1,...,X_n formam uma amostra aleat\u00f3ria com distribui\u00e7\u00e3o normal e com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 desconhecidos. Estamos interessados na distribui\u00e7\u00e3o conjunta dos estimadores de m\u00e1xima verossimilhan\u00e7a para m\u00e9dia e vari\u00e2ncia da amostra.","title":"Distribui\u00e7\u00e3o Conjunta da m\u00e9dia e vari\u00e2ncia amostrais"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#teorema-de-basu","text":"Sejam \\hat{\\mu} = \\bar{X}_n e \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 a m\u00e9dia e vari\u00e2ncia amostrais, respectivamente. Ent\u00e3o \\hat{\\mu} tem distribui\u00e7\u00e3o normal com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 /n , enquanto n\\hat{\\sigma}^2/\\sigma^2 tem a distribui\u00e7\u00e3o \\chi^2(n-1) , isto \u00e9, com n-1 grau de liberdade. Al\u00e9m disso elas s\u00e3o independentes. Esse teorema \u00e9 um pouco mais complexo e, na verdade, essa seria uma esp\u00e9cie de aplica\u00e7\u00e3o do teorema, na verdade. O teorema de Basu diz que: Se T \u00e9 uma estat\u00edstica suficiente completa (Considere, nesse teorema, g uma fun\u00e7\u00e3o integr\u00e1vel limitada) para \\theta e A uma estat\u00edstica ancillary, ent\u00e3o T \u00e9 independente de A . Nesse caso \\hat{\\mu} \u00e9 completa suficiente e \\hat{\\sigma}^2 \u00e9 ancillary, por que n\u00e3o depende de \\mu . O mais interessante \u00e9 que essa propriedade \u00e9 s\u00f3 vista com a distribui\u00e7\u00e3o normal ! Olhem a p\u00e1gina 9.","title":"Teorema de Basu"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#demonstracao","text":"O livro tem uma abordagem um pouco mais voltado \u00e0 \u00c1lgebra Linear. Aqui vou mostrar uma ideia um pouco diferente, onde voc\u00eas podem demonstrar os passos, como exerc\u00edcio. Passo 1: \\sum_{i=1}^n X_i^2 = n\\hat{\\sigma}^2 + n\\hat{\\mu}^2 Dica: Escrever \\hat{\\sigma}^2 e abrir em tr\u00eas somat\u00f3rios. Passo 2: \\sum_{i=1}^n (X_i - \\mu)^2 = n\\hat{\\sigma}^2 + n(\\hat{\\mu} - \\mu)^2 Dica: O Passo 1 \u00e9 um caso especial do Passo 2. O processo \u00e9 o mesmo. Passo 3: \\hat{\\mu} \u00e9 independente de X_i - \\hat{\\mu}, i = 1,...,n . Dica: Montar a pdf conjunta de X_1, ..., X_n (j\u00e1 fizemos isso atrave\u015b da verossimilhan\u00e7a) e fazer uma mudan\u00e7a de vari\u00e1vel Y_1 = \\hat{\\mu}, Y_2 = X_2 - \\hat{\\mu}, ..., Y_n = X_n - \\hat{\\mu} . Com essa mudan\u00e7a, \u00e9 poss\u00edvel montar a pdf como fun\u00e7\u00e3o de y_1,...,y_n . Esse processo \u00e9 um pouco mais chato, mas \u00e9 bom lembrar como fazez mudan\u00e7a de vari\u00e1vel para pdfs. Aqui voc\u00ea pode conferir como . \u00c9 importante lembrar que \u00e9 uma fun\u00e7\u00e3o de y ap\u00f3s transformada e n\u00e3o de x . Dica 2: Fatorizar a pdf conjunta. Voc\u00ea vai ver como se destaca a independ\u00eancia aqui. Passo 4: Mostrar que \\hat{\\mu} e \\hat{\\sigma}^2 s\u00e3o independentes.","title":"Demonstra\u00e7\u00e3o"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#referencias","text":"1 2","title":"Refer\u00eancias"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#simples-visualizacao","text":"Eu gostaria de comparar o que acontece com a m\u00e9dia e vari\u00e2ncia amostral da distribui\u00e7\u00e3o normal e da distribui\u00e7\u00e3o gamma. Para isso, geero amostras de tamanho n , calculo as estat\u00edsticas e salvo. Fa\u00e7o esse procedimento o n\u00famero de pontos que quiser. ite = 10000 n = 10000 # Par\u00e2metros da Normal mu = 5 sigma = 2 # Par\u00e2metros da Gamma alpha = 5 beta = 4 means = np.zeros((ite,2)) variances = np.zeros((ite,2)) for i in range(ite): X = ro.normal(loc = mu, scale = sigma, size = n) Y = ro.gamma(shape = alpha, scale = 1/beta, size = n) means[i,0] = np.mean(X) means[i,1] = np.mean(Y) variances[i,0] = np.var(X, ddof = 0) variances[i,1] = np.var(Y, ddof = 0) coef_normal = np.polyfit(x = means[:,0], y = variances[:,0], deg = 1) coef_gamma = np.polyfit(x = means[:,1], y = variances[:,1], deg = 1) fig, ax = plt.subplots(1,2,figsize = (14,5)) fig.suptitle('Comparando m\u00e9dia e vari\u00e2ncia amostral') ax[0].scatter(means[:,0], variances[:,0]) ax[1].scatter(means[:,1], variances[:,1]) ax[0].plot(means[:,0], coef_normal[0]*means[:,0] + coef_normal[1], color = 'red') ax[1].plot(means[:,1], coef_gamma[0]*means[:,1] + coef_gamma[1], color = 'red') ax[0].set_xlabel(r'$\\bar{X}_n$', fontsize = 18) ax[1].set_xlabel(r'$\\bar{X}_n$', fontsize = 18) ax[0].set_ylabel(r'$\\sum (X_i - \\bar{X}_n)^2$', fontsize = 18) ax[1].set_ylabel(r'$\\sum (X_i - \\bar{X}_n)^2$', fontsize = 18) ax[0].set_title('Distribui\u00e7\u00e3o Normal') ax[1].set_title('Distribui\u00e7\u00e3o Gamma') ax[0].grid(alpha = 0.5, linestyle = '--') ax[1].grid(alpha = 0.5, linestyle = '--') plt.show() Obs: A n\u00e3o inclina\u00e7\u00e3o da reta n\u00e3o significa que existe independ\u00eancia, mas como s\u00e3o independentes, a gente espera que a inclina\u00e7\u00e3o seja pequena.","title":"Simples visualiza\u00e7\u00e3o"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#distribuicoes-t-student","text":"Artigo original : Olhe a p\u00e1gina 9!","title":"Distribui\u00e7\u00f5es T Student"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#definicao","text":"Sejam Y \\sim \\chi^2(m) e Z \\sim N(0,1) independentes. Ent\u00e3o X = \\frac{Z}{\\left(\\frac{Y}{m}\\right)^{1/2}} \\sim t(m) onde t(m) \u00e9 a distribui\u00e7\u00e3o t-student com m graus de liberdade.","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#funcao-densidade-de-probabilidade","text":"Para escrever essa fun\u00e7\u00e3o de probabilidade, defina X como acima e W = Y . J\u00e1 sabemos a distribui\u00e7\u00e3o conjunta de Y e Z , pois eles s\u00e3o independentes. Com essa mudan\u00e7a de vari\u00e1vel ( confira aqui se n\u00e3o lembra como \u00e9 feito ), voc\u00ea conseque escrever a distribui\u00e7\u00e3o conjunta de X e W . Depois, basta calcular a distribui\u00e7\u00e3o marginal de X , integrando em W . f(x) = \\frac{\\Gamma\\left(\\frac{m+1}{2}\\right)}{(m\\pi)^{1/2}\\Gamma\\left(\\frac{m}{2}\\right)}\\left(1 + \\frac{x^2}{m} \\right)^{-(m+1)/2}, x \\in \\mathbb{R}, onde \\Gamma \u00e9 a fun\u00e7\u00e3o Gamma , tal que, n \\in \\mathbb{N}, \\Gamma(n) = (n-1)! \\Gamma(z+1) = z\\Gamma(z) \\Gamma(1/2) = \\sqrt{\\pi} Quando m \\leq 1 , a m\u00e9dia \u00e9 divergente. Isso pode ser vizualizado pelo expoente que ser\u00e1 \\leq -1 , o que diverge (lembre de \\int 1/x ). Quando m > 1 , a m\u00e9dia existe e \u00e9 0 pela simetria da distribui\u00e7\u00e3o. Em particular, podemos mostrar que se k < m , E[|X^k|] < + \\infty e se k \\geq m , o momento diverge. Se X \\sim t(m), m > 2 , Var(X) = \\frac{m}{m-2}","title":"Fun\u00e7\u00e3o densidade de probabilidade"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#teorema","text":"Seja X_1, ..., X_n \\overset{iid}{\\sim} N(\\mu,\\sigma^2) . Seja \\sigma ' = \\left[\\frac{\\sum_{i=1}^n (X_i - \\bar{X}_n)^2}{n-1}\\right]^{1/2} Ent\u00e3o n^{1/2}(\\bar{X}_n - \\mu)/\\sigma ' \\sim t(n-1)","title":"Teorema"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#relacao-com-a-normal-e-cauchy","text":"Da mesma forma que a distribui\u00e7\u00e3o normal e a distribui\u00e7\u00e3o Cauchy, a distribui\u00e7\u00e3o t \u00e9 centrada em 0 e tem sua moda nesse valor. Entretanto a cauda a distribui\u00e7\u00e3o t (quando x \\to -\\infty ou x \\to +\\infty ), \u00e9 mais pesada, no sentido de que tende para 0 em uma velocidade menor do que a normal. Outra coisa interessante \u00e9 que a ditrivui\u00e7\u00e3o t(1) \u00e9 a distribui\u00e7\u00e3o Cauchy . Al\u00e9m disso, quando n \\to \\infty , converge para a pdf da normal padr\u00e3o ( Normal(0,1) ).","title":"Rela\u00e7\u00e3o com a Normal e Cauchy"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#ferramentas-para-demonstrar-a-convergencia","text":"Teorema de Slutsky : Considere o corol\u00e1rio com f(x,y) = \\frac{x}{y} Lei dos Grandes N\u00fameros : Escreva a qui-quadrado como soma de normais. from scipy.stats import t, norm, cauchy","title":"Ferramentas para demonstrar a converg\u00eancia"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#implementacao_1","text":"Primeiro vamos ver a cara da distribui\u00e7\u00e3o t m = 10 X = t(df = m) w = np.arange(-3, 3, 0.1) fig, ax = plt.subplots(1,2,figsize = (12,5)) ax[0].plot(w, X.pdf(w), lw = 5, color = 'orange') ax[1].plot(w, X.cdf(w), lw = 5, color = 'orange') ax[0].set_title('PDF t-Student') ax[1].set_title('CDF t-Student') plt.show() Vamos ver o que acontece quando m \\leq 1 ? ite = 1000 n = 10000 m1 = 10 m2 = 0.5 means = np.zeros((ite,2)) for i in range(ite): X = ro.standard_t(df = m1, size = n) Y = ro.standard_t(df = m2, size = n) means[i,0] = np.mean(X) means[i,1] = np.mean(Y) fig, ax = plt.subplots(1,2,figsize = (14,5)) ax[0].hist(means[:,0], bins = 100) ax[1].hist(np.log(means[:,1]), bins = 10) ax[0].set_xlabel('E[X]') ax[1].set_xlabel('log E[X]') ax[0].set_title('m = 10') ax[1].set_title('m = 0.5') plt.show() <ipython-input-11-2bfd1961d53b>:3: RuntimeWarning: invalid value encountered in log ax[1].hist(np.log(means[:,1]), bins = 10) No eixo x do segundo gr\u00e1fico plotei o logaritmo, dado que alguns resultados eram extremamente grandes! Isso indica visualmente que a m\u00e9dia diverge!","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#relacao-com-a-normal-e-com-cauchy","text":"C = cauchy() Z = norm(loc = 0, scale = 1) T = t(df = 1) fig, ax = plt.subplots(1,2,figsize = (14,5)) ax[0].plot(w,C.pdf(w), label = 'Cauchy') ax[0].scatter(w, T.pdf(w), c = 'red', marker = \"*\", label = 't-Student') ax[0].legend() ax[0].set_title('t-Student e Cauchy quando m = 1') ax[1].plot(w,Z.pdf(w), label = 'N(0,1)') ax[1].set_title('Converg\u00eancia da t para a normal') for i in np.logspace(np.log10(1), np.log10(20), 5): T = t(df = int(i)) ax[1].plot(w, T.pdf(w), linestyle = '--', alpha = i/40 + 0.5, color = 'grey', label = 't({})'.format(int(i))) ax[1].legend(loc = 'upper right') plt.show()","title":"Rela\u00e7\u00e3o com a Normal e com Cauchy"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#distribuicao-f","text":"Sejam Y \\sim \\chi^2_m e W \\sim \\chi^2_n independentes. Defina X = \\frac{Y/m}{W/n} = \\frac{nY}{mW} Dizemos que X tem distribui\u00e7\u00e3o F . A sua motiva\u00e7\u00e3o vem do teste de hip\u00f3teses que compara vari\u00e2ncias de duas normais.","title":"Distribui\u00e7\u00e3o F"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#funcao-de-densidade-de-probabilidade","text":"Seja X \\sim F_{m,n} . Ent\u00e3o sua pdf tem suporte em x > 0 e pe definida f(x) = \\frac{\\Gamma\\left[\\frac{1}{2}(m+n)\\right]m^{m/2}n^{n/2}}{\\Gamma\\left(\\frac{1}{2}m\\right)\\Gamma\\left(\\frac{1}{2}n\\right)}\\cdot \\frac{x^{(m/2) - 1}}{(mx + n)^{(m+n)/2)}} Observe que ela n\u00e3o \u00e9 sim\u00e9trica em m e n . Assim, se trocarmos eles de lugar, teremos um resultado diferente.","title":"Fun\u00e7\u00e3o de densidade de probabilidade"},{"location":"infestatistica_BSc/SamplingDistribution/SamplingDistribution/#propriedades_1","text":"Seja X \\sim F_{m,n} . Ent\u00e3o 1/X \\sim F_{n,m} . Se Y \\sim t_n , ent\u00e3o Y^2 \\sim F_{1,n} . Existem diversas rela\u00e7\u00f5es que s\u00e3o encontradas com outras distribui\u00e7\u00f5es. Confira aqui E[X] = \\frac{n}{n-2}, n > 2 Var[X] = \\frac{2n^2(m + n - 2)}{m(n-2)^2(n-4)} import numpy as np from scipy.stats import f import matplotlib.pyplot as plt Vamos ver como \u00e9 a cara dessa distribui\u00e7\u00e3o: m, n = 20, 10 X = f(dfn = m, dfd = n) w = np.arange(0, 5, 0.1) fig, ax = plt.subplots(1,2,figsize = (12,5)) ax[0].plot(w, X.pdf(w), lw = 5, color = 'orange') ax[1].plot(w, X.cdf(w), lw = 5, color = 'orange') ax[0].set_title('PDF Distribui\u00e7\u00e3o F') ax[1].set_title('CDF Distribui\u00e7\u00e3o F') plt.show()","title":"Propriedades"},{"location":"infestatistica_BSc/StatisticalInference/StatisticalInference/","text":"Infer\u00eancia Estat\u00edstica Procedimento que objetiva produzir uma proposi\u00e7\u00e3o probabil\u00edsitca sobre um modelo estat\u00edstico. Modelo Estat\u00edstico Identificar vari\u00e1veis aleat\u00f3rias de interesse, especificar uma distribui\u00e7\u00e3o conjunta (ou fam\u00edlia), par\u00e2metros relevantes e uma especifica\u00e7\u00e3o para uma distribui\u00e7\u00e3o para os par\u00e2metros desconhecidos (baysianos adoram essa parte, p \\sim N(0,1) ) Espa\u00e7o dos Par\u00e2metros Uma caracter\u00edstica ou uma combina\u00e7\u00e3o de caracter\u00edsticas para determinar uma distribui\u00e7\u00e3o conjunta para as vari\u00e1veis aleat\u00f3rias forma o par\u00e2metro, que pertence a um espa\u00e7o denominado \\Omega . Estat\u00edstica Fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias observ\u00e1veis Problemas estudados Predi\u00e7\u00e3o: Baseado na \u00e9poca do ano que estamos, fatores climatol\u00f3gicos dos \u00faltimos dias, entre outros fatores, qual a probabilidade de chuva amanh\u00e3? Problemas de decis\u00e3o estat\u00edstica: \u00c9 relacionado ao risco e teste de hip\u00f3teses. Resposta consider\u00e1vel Desenho de experimentos: um psic\u00f3logo quer inferir qu\u00e3o avesso ao risco \u00e9 uma determinada popula\u00e7\u00e3o. Ele pode determinar, desenhar o experimento para isso. Infer\u00eancia Estat\u00edstica com Python import numpy as np import pandas as pd from scipy.stats import poisson import matplotlib.pyplot as plt import seaborn as sns sns.set() Importando os Dados Este banco de dados inclui um registro para cada vazamento ou derramamento de oleoduto relatado \u00e0 Administra\u00e7\u00e3o de Seguran\u00e7a de Dutos e Materiais Perigosos desde 2010. Esses registros incluem a data e hora do incidente, operador e oleoduto, causa do incidente, tipo de l\u00edquido perigoso e quantidade perdida, ferimentos e fatalidades e custos associados. oil_accident_df = pd.read_csv('../data/oil_pipeline.csv') oil_accident_df.sample() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Report Number Supplemental Number Accident Year Accident Date/Time Operator ID Operator Name Pipeline/Facility Name Pipeline Location Pipeline Type Liquid Type ... Other Fatalities Public Fatalities All Fatalities Property Damage Costs Lost Commodity Costs Public/Private Property Damage Costs Emergency Response Costs Environmental Remediation Costs Other Costs All Costs 871 20120202 17135 2012 6/15/2012 3:50 PM 31476 ROSE ROCK MIDSTREAM L.P. BURKETT DISCHARGE ONSHORE UNDERGROUND CRUDE OIL ... NaN NaN NaN 6020.0 200.0 2500.0 10500.0 8500.0 16000.0 43720 1 rows \u00d7 48 columns cols_of_interest = ['Accident Date/Time','Accident State','Pipeline Location', 'Liquid Type','Net Loss (Barrels)','All Costs'] data = oil_accident_df[cols_of_interest] data['All Costs'] = data['All Costs'] / 1000000 # unidade em milh\u00e3o. data.sample() /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Accident Date/Time Accident State Pipeline Location Liquid Type Net Loss (Barrels) All Costs 263 10/11/2010 4:10 PM NJ ONSHORE REFINED AND/OR PETROLEUM PRODUCT (NON-HVL), LI... 0.0 0.0 Vamos entender um pouco como esta informa\u00e7\u00e3o esta organizada. data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Net Loss (Barrels) All Costs count 2795.000000 2795.000000 mean 132.194050 0.834033 std 1185.019252 16.578298 min 0.000000 0.000000 25% 0.000000 0.005040 50% 0.000000 0.023129 75% 2.000000 0.117232 max 30565.000000 840.526118 Vamos analisar os dados utilizando leis da probabilidade para aprender sobre a popula\u00e7\u00e3o. Veja que n\u00e3o temos a informa\u00e7\u00e3o completa, apenas a partir de 2010. fig, ax = plt.subplots(1,2,figsize = (15,4)) sns.boxplot(data['All Costs'], data=data, ax = ax[0]) ax[0].set_title('Custos dos Acidentes por Milh\u00e3o US$') sns.boxplot(data['Net Loss (Barrels)'], data=data, ax = ax[1]) ax[1].set_title('Preju\u00edzo L\u00edquido (Barris)') plt.show() Mas esse n\u00e3o era para ser um boxplot? Cade a caixa? Isso indica que valores grandes nos dois dados s\u00e3o muito maiores relativamente aos outros dados. Poder\u00edamos prever o custo de um acidente usando a mediana dos valores? \u00c9 de fato um modelo, mas nesse caso, parece ser ruim dado os valores grandes. O que s\u00e3o esses valores grandes, afinal? Em alguns casos, podem realmente apresentar erros, mas nesse caso fica dif\u00edcil de afirmar. Bom. Podemos, dados esses problemas, trabalhar com outra vari\u00e1vel dispon\u00edvel: o tempo do acidente. Conhecemos uma fam\u00edlia de distribui\u00e7\u00f5es de probabilidade que modela frequ\u00eancia de acidentes em um intervalo de tempo? Distribui\u00e7\u00e3o de Poisson: probabilidade de uma s\u00e9rie de eventos ocorrer num certo per\u00edodo de tempo se estes eventos ocorrem independentemente de quando ocorreu o \u00faltimo evento. De forma geral, podemos dizer que isso \u00e9 verdade para acidentes de \u00f3leo. Assim, temos uma vari\u00e1vel aleat\u00f3ria de interesser X , que indica o n\u00famero de acidentes, j\u00e1 temos uma distribui\u00e7\u00e3o para essa vari\u00e1vel (Poisson) e j\u00e1 temos o par\u00e2metro \\lambda desconhecido. data['Accident Date/Time'] = pd.to_datetime(data['Accident Date/Time']) totaltimespan = np.max(data['Accident Date/Time']) - np.min(data['Accident Date/Time']) totaltime_hour = (totaltimespan.days*24 + totaltimespan.seconds/(3600)) totaltime_month = (totaltimespan.days + totaltimespan.seconds/(3600*24)) *12/365 lmda_h = len(data) / totaltime_hour lmda_m = len(data) / totaltime_month print('N\u00famero estimado de acidentes por hora: {}'.format(lmda_h)) print('N\u00famero estimado de acidentes por m\u00eas {}'.format(lmda_m)) N\u00famero estimado de acidentes por hora: 0.04540255169379675 N\u00famero estimado de acidentes por m\u00eas 33.14386273647162 /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy \"\"\"Entry point for launching an IPython kernel. Poder\u00edamos ter procedimentos para estimar \\lambda , mas por hora, vamos tomar ele como a m\u00e9dia das observa\u00e7\u00f5es. Pela Lei dos Grandes N\u00fameros, sabemos que a m\u00e9dia da Poisson \u00e9 \\lambda e a m\u00e9dia amostral tende para ela. lamda = 33 X = poisson(lamda) I = np.arange(0,60,1) #intervalo(0,60), passo = 1 samples_poisson = np.sort(np.random.poisson(lamda,10000)) Y = X.cdf(samples_poisson) #fun\u00e7\u00e3o de densidade acumulada fig, ax = plt.subplots(1,2,figsize = (20,8)) ax[0].scatter(I,X.pmf(I) , color = 'purple') ax[0].set_xlabel('N\u00famero de Acidentes por m\u00eas (n)') ax[0].set_ylabel('P(X <= n)') ax[0].set_title('Fun\u00e7\u00e3o de Massa de Probabilidade') ax[1].scatter(samples_poisson, Y, color = 'purple') ax[1].hlines(0.5, xmin = min(samples_poisson), xmax = max(samples_poisson), linestyle = '--', color = 'black') ax[1].set_xlabel('N\u00famero de acidentes por m\u00eas (n)') ax[1].set_ylabel('P(X <= n)') ax[1].set_title('Fun\u00e7\u00e3o de Distribui\u00e7\u00e3o Acumulada') plt.show() A partir de nosso modelo, j\u00e1 podemos fazer acerta\u00e7\u00f5es probabil\u00edstica! real_data = np.array(data['Accident Date/Time'].apply(lambda x: (x.year, x.month))) accidents_count = {2010 + i: {m: 0 for m in range(1,13)} for i in range(8)} for info in real_data: accidents_count[info[0]][info[1]] += 1 distribution = [accidents_count[y][m] for y in accidents_count.keys() for m in accidents_count[y].keys()] distribution = distribution[:-12] #Tirando 2 observa\u00e7\u00f5es de 2017 fig, ax = plt.subplots() sns.distplot(distribution, bins = 15, ax = ax, label = 'Original data', kde = False, norm_hist = True) ax.scatter(I,X.pmf(I) , color = 'purple', label = 'Nosso modelo') ax.legend() ax.set_title('Comparando modelo com dados reais') plt.show()","title":"Infer\u00eancia Estat\u00edstica"},{"location":"infestatistica_BSc/StatisticalInference/StatisticalInference/#inferencia-estatistica","text":"Procedimento que objetiva produzir uma proposi\u00e7\u00e3o probabil\u00edsitca sobre um modelo estat\u00edstico.","title":"Infer\u00eancia Estat\u00edstica"},{"location":"infestatistica_BSc/StatisticalInference/StatisticalInference/#modelo-estatistico","text":"Identificar vari\u00e1veis aleat\u00f3rias de interesse, especificar uma distribui\u00e7\u00e3o conjunta (ou fam\u00edlia), par\u00e2metros relevantes e uma especifica\u00e7\u00e3o para uma distribui\u00e7\u00e3o para os par\u00e2metros desconhecidos (baysianos adoram essa parte, p \\sim N(0,1) )","title":"Modelo Estat\u00edstico"},{"location":"infestatistica_BSc/StatisticalInference/StatisticalInference/#espaco-dos-parametros","text":"Uma caracter\u00edstica ou uma combina\u00e7\u00e3o de caracter\u00edsticas para determinar uma distribui\u00e7\u00e3o conjunta para as vari\u00e1veis aleat\u00f3rias forma o par\u00e2metro, que pertence a um espa\u00e7o denominado \\Omega .","title":"Espa\u00e7o dos Par\u00e2metros"},{"location":"infestatistica_BSc/StatisticalInference/StatisticalInference/#estatistica","text":"Fun\u00e7\u00e3o das vari\u00e1veis aleat\u00f3rias observ\u00e1veis","title":"Estat\u00edstica"},{"location":"infestatistica_BSc/StatisticalInference/StatisticalInference/#problemas-estudados","text":"Predi\u00e7\u00e3o: Baseado na \u00e9poca do ano que estamos, fatores climatol\u00f3gicos dos \u00faltimos dias, entre outros fatores, qual a probabilidade de chuva amanh\u00e3? Problemas de decis\u00e3o estat\u00edstica: \u00c9 relacionado ao risco e teste de hip\u00f3teses. Resposta consider\u00e1vel Desenho de experimentos: um psic\u00f3logo quer inferir qu\u00e3o avesso ao risco \u00e9 uma determinada popula\u00e7\u00e3o. Ele pode determinar, desenhar o experimento para isso. Infer\u00eancia Estat\u00edstica com Python import numpy as np import pandas as pd from scipy.stats import poisson import matplotlib.pyplot as plt import seaborn as sns sns.set()","title":"Problemas estudados"},{"location":"infestatistica_BSc/StatisticalInference/StatisticalInference/#importando-os-dados","text":"Este banco de dados inclui um registro para cada vazamento ou derramamento de oleoduto relatado \u00e0 Administra\u00e7\u00e3o de Seguran\u00e7a de Dutos e Materiais Perigosos desde 2010. Esses registros incluem a data e hora do incidente, operador e oleoduto, causa do incidente, tipo de l\u00edquido perigoso e quantidade perdida, ferimentos e fatalidades e custos associados. oil_accident_df = pd.read_csv('../data/oil_pipeline.csv') oil_accident_df.sample() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Report Number Supplemental Number Accident Year Accident Date/Time Operator ID Operator Name Pipeline/Facility Name Pipeline Location Pipeline Type Liquid Type ... Other Fatalities Public Fatalities All Fatalities Property Damage Costs Lost Commodity Costs Public/Private Property Damage Costs Emergency Response Costs Environmental Remediation Costs Other Costs All Costs 871 20120202 17135 2012 6/15/2012 3:50 PM 31476 ROSE ROCK MIDSTREAM L.P. BURKETT DISCHARGE ONSHORE UNDERGROUND CRUDE OIL ... NaN NaN NaN 6020.0 200.0 2500.0 10500.0 8500.0 16000.0 43720 1 rows \u00d7 48 columns cols_of_interest = ['Accident Date/Time','Accident State','Pipeline Location', 'Liquid Type','Net Loss (Barrels)','All Costs'] data = oil_accident_df[cols_of_interest] data['All Costs'] = data['All Costs'] / 1000000 # unidade em milh\u00e3o. data.sample() /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Accident Date/Time Accident State Pipeline Location Liquid Type Net Loss (Barrels) All Costs 263 10/11/2010 4:10 PM NJ ONSHORE REFINED AND/OR PETROLEUM PRODUCT (NON-HVL), LI... 0.0 0.0 Vamos entender um pouco como esta informa\u00e7\u00e3o esta organizada. data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Net Loss (Barrels) All Costs count 2795.000000 2795.000000 mean 132.194050 0.834033 std 1185.019252 16.578298 min 0.000000 0.000000 25% 0.000000 0.005040 50% 0.000000 0.023129 75% 2.000000 0.117232 max 30565.000000 840.526118 Vamos analisar os dados utilizando leis da probabilidade para aprender sobre a popula\u00e7\u00e3o. Veja que n\u00e3o temos a informa\u00e7\u00e3o completa, apenas a partir de 2010. fig, ax = plt.subplots(1,2,figsize = (15,4)) sns.boxplot(data['All Costs'], data=data, ax = ax[0]) ax[0].set_title('Custos dos Acidentes por Milh\u00e3o US$') sns.boxplot(data['Net Loss (Barrels)'], data=data, ax = ax[1]) ax[1].set_title('Preju\u00edzo L\u00edquido (Barris)') plt.show() Mas esse n\u00e3o era para ser um boxplot? Cade a caixa? Isso indica que valores grandes nos dois dados s\u00e3o muito maiores relativamente aos outros dados. Poder\u00edamos prever o custo de um acidente usando a mediana dos valores? \u00c9 de fato um modelo, mas nesse caso, parece ser ruim dado os valores grandes. O que s\u00e3o esses valores grandes, afinal? Em alguns casos, podem realmente apresentar erros, mas nesse caso fica dif\u00edcil de afirmar. Bom. Podemos, dados esses problemas, trabalhar com outra vari\u00e1vel dispon\u00edvel: o tempo do acidente. Conhecemos uma fam\u00edlia de distribui\u00e7\u00f5es de probabilidade que modela frequ\u00eancia de acidentes em um intervalo de tempo? Distribui\u00e7\u00e3o de Poisson: probabilidade de uma s\u00e9rie de eventos ocorrer num certo per\u00edodo de tempo se estes eventos ocorrem independentemente de quando ocorreu o \u00faltimo evento. De forma geral, podemos dizer que isso \u00e9 verdade para acidentes de \u00f3leo. Assim, temos uma vari\u00e1vel aleat\u00f3ria de interesser X , que indica o n\u00famero de acidentes, j\u00e1 temos uma distribui\u00e7\u00e3o para essa vari\u00e1vel (Poisson) e j\u00e1 temos o par\u00e2metro \\lambda desconhecido. data['Accident Date/Time'] = pd.to_datetime(data['Accident Date/Time']) totaltimespan = np.max(data['Accident Date/Time']) - np.min(data['Accident Date/Time']) totaltime_hour = (totaltimespan.days*24 + totaltimespan.seconds/(3600)) totaltime_month = (totaltimespan.days + totaltimespan.seconds/(3600*24)) *12/365 lmda_h = len(data) / totaltime_hour lmda_m = len(data) / totaltime_month print('N\u00famero estimado de acidentes por hora: {}'.format(lmda_h)) print('N\u00famero estimado de acidentes por m\u00eas {}'.format(lmda_m)) N\u00famero estimado de acidentes por hora: 0.04540255169379675 N\u00famero estimado de acidentes por m\u00eas 33.14386273647162 /home/lucasmoschen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy \"\"\"Entry point for launching an IPython kernel. Poder\u00edamos ter procedimentos para estimar \\lambda , mas por hora, vamos tomar ele como a m\u00e9dia das observa\u00e7\u00f5es. Pela Lei dos Grandes N\u00fameros, sabemos que a m\u00e9dia da Poisson \u00e9 \\lambda e a m\u00e9dia amostral tende para ela. lamda = 33 X = poisson(lamda) I = np.arange(0,60,1) #intervalo(0,60), passo = 1 samples_poisson = np.sort(np.random.poisson(lamda,10000)) Y = X.cdf(samples_poisson) #fun\u00e7\u00e3o de densidade acumulada fig, ax = plt.subplots(1,2,figsize = (20,8)) ax[0].scatter(I,X.pmf(I) , color = 'purple') ax[0].set_xlabel('N\u00famero de Acidentes por m\u00eas (n)') ax[0].set_ylabel('P(X <= n)') ax[0].set_title('Fun\u00e7\u00e3o de Massa de Probabilidade') ax[1].scatter(samples_poisson, Y, color = 'purple') ax[1].hlines(0.5, xmin = min(samples_poisson), xmax = max(samples_poisson), linestyle = '--', color = 'black') ax[1].set_xlabel('N\u00famero de acidentes por m\u00eas (n)') ax[1].set_ylabel('P(X <= n)') ax[1].set_title('Fun\u00e7\u00e3o de Distribui\u00e7\u00e3o Acumulada') plt.show() A partir de nosso modelo, j\u00e1 podemos fazer acerta\u00e7\u00f5es probabil\u00edstica! real_data = np.array(data['Accident Date/Time'].apply(lambda x: (x.year, x.month))) accidents_count = {2010 + i: {m: 0 for m in range(1,13)} for i in range(8)} for info in real_data: accidents_count[info[0]][info[1]] += 1 distribution = [accidents_count[y][m] for y in accidents_count.keys() for m in accidents_count[y].keys()] distribution = distribution[:-12] #Tirando 2 observa\u00e7\u00f5es de 2017 fig, ax = plt.subplots() sns.distplot(distribution, bins = 15, ax = ax, label = 'Original data', kde = False, norm_hist = True) ax.scatter(I,X.pmf(I) , color = 'purple', label = 'Nosso modelo') ax.legend() ax.set_title('Comparando modelo com dados reais') plt.show()","title":"Importando os Dados"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/","text":"Teste de Hip\u00f3teses: Defini\u00e7\u00f5es Hip\u00f3tese Nula e Alternativa Regi\u00e3o Cr\u00edtica Estat\u00edstica de Teste Fun\u00e7\u00e3o de Poder Tipos de Erro N\u00edvel e tamanho do teste P-valor Temos um problema estat\u00edstico que envolve um par\u00e2metro \\theta tal que tenha valor desconhecido, mas reside em um espa\u00e7o \\Omega . Suponha que particionemos \\Omega = \\Omega_1 \\dot\\cup ~\\Omega_2 e o estat\u00edstico est\u00e1 interessado se \\theta est\u00e1 em \\Omega_0 ou est\u00e1 em \\Omega_1 . Hip\u00f3tese Nula e Alternativa Dizemos que H_0 \u00e9 a hip\u00f3tese de que \\theta \\in \\Omega_0 e chamamos H_0 de hip\u00f3tese nula , enquanto H_1 \u00e9 a hip\u00f3tese alternativa e representa \\theta \\in H_1 . Queremos decidir qual das hip\u00f3teses \u00e9 verdadeira (e s\u00f3 uma ser\u00e1, porque a parti\u00e7\u00e3o \u00e9 disjunta). Se decidimos que \\theta \\in \\Omega_1 , rejeitamos H_0 , e se \\theta \\in \\Omega_0 , n\u00e3o rejeitamos H_0 . Hip\u00f3tese Simples e Composta Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria com pdf f(x|\\theta) . Queremos testar a hip\u00f3tese de que H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Se \\Omega_i contem apenas um valor, ent\u00e3o H_i \u00e9 dita hip\u00f3tese simples. Se cont\u00e9m mais de um valor, dizemos que \u00e9 composta. Hip\u00f3tese Unilateral e Bilateral Seja \\theta um par\u00e2metro unidimensional. Dizemos que a hip\u00f3tese H_0 \u00e9 unilateral (ou one tailed ) quando \u00e9 da forma \\theta \\leq \\theta_0 ou \\theta \\geq \\theta_0 . Ela ser\u00e1 bilateral quando \u00e9 do tipo H_0 \\neq \\theta_0 . Regi\u00e3o Cr\u00edtica Suponha que queremos testar a hip\u00f3tese de que H_0: \\theta \\in \\Omega_0 \\text{ e } H_1: \\theta \\in \\Omega_1 Quando queremos decidir qual hip\u00f3tese escolher, observamos uma amostra dessa distribui\u00e7\u00e3o no espa\u00e7o de amostras S . O dever do estat\u00edstico \u00e9 especificar um procedimento que particione o conjunto em dois subconjuntos S_0 e S_1 , onde S_1 cont\u00e9m os valores de X que rejeitam H_0 . Regi\u00e3o cr\u00edtica \u00e9 o conjunto S_1 , isto \u00e9, o conjunto de amostras que, a partir de um procedimento, rejeita H_0 . Estat\u00edstica de Teste Seja X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Sejam T = r(X) uma estat\u00edstica e R um subconjunto da reta. Suponha que nosso procedimento de teste \u00e9 o seguinte: Rejeitamos H_0 se T\\in R . Chamamos T de estat\u00edstica de teste e R de regi\u00e3o de rejei\u00e7\u00e3o . Dessa forma a regi\u00e3o cr\u00edtica ser\u00e1: S_1 = \\{x \\in S: r(x) \\in R\\} . Na pr\u00e1tica a maioria dos testes \u00e9 do tipo Rejeitamos H_0 se T \\geq c, c \\in \\mathbb{R} . Observa\u00e7\u00e3o sobre a divis\u00e3o de conjuntos \u00c9 importante lembrar que h\u00e1 duas diferentes divis\u00f5es: \\Omega = \\Omega_0 \\dot\\cup \\Omega_1 , que \u00e9 a divis\u00e3o do espa\u00e7o dos par\u00e2metros, e S = S_1 \\dot\\cup S_1 \u00e9 a divis\u00e3o do espa\u00e7o das amostras. Mas qual a rela\u00e7\u00e3o entre eles? Se X \\in S_1 , ent\u00e3o rejeitamos a hip\u00f3tese \\theta \\in \\Omega_0 . Al\u00e9m do mais, podemos encontrar S_1 e S_2 , mas dificilmente saberemos em qual dos conjuntos \\theta pertence. Fun\u00e7\u00e3o de Poder e Tipos de Erro Fun\u00e7\u00e3o Poder Seja \\delta um procedimento de teste (como esse assinalado acima). Se S_1 \u00e9 a regi\u00e3o cr\u00edtica, \\pi(\\theta|\\delta) = P(X \\in S_1|\\theta) = P(T \\in R|\\theta) Sendo que a \u00faltima igualdade ocorre quando o proocedimento de teste \u00e9 o citado acima. O seu significado? \u00c9 a probabilidade, para cada valor de \\theta , de que \\delta rejeita H_0 . Queremos, intuitivamente que: \\theta \\in \\Omega_0 \\rightarrow \\text{ Queremos n\u00e3o rejeitar} H_0 \\rightarrow \\pi(\\theta|\\delta) = 0 \\theta \\in \\Omega_1 \\rightarrow \\text{ Queremos rejeitar} H_0 \\rightarrow \\pi(\\theta|\\delta) = 1 Entretanto isso n\u00e3o \u00e9 em geral o que acontece. Por isso definimos: Erros I e II \\theta \\in \\Omega_0 \\theta \\in \\Omega_1 \\delta rejeita H_0 Erro Tipo I Certo \\delta n\u00e3o rejeita H_0 Certo Erro Tipo II Portanto se \\theta \\in \\Omega_0, \\pi(\\theta|\\delta) \u00e9 a probabilidade de cometermos o erro do tipo I. Se \\theta \\in \\Omega_1, 1 - \\pi(\\theta|\\delta) \u00e9 a probabilidade de cometer o erro do tipo II. N\u00edvel/Tamanho Um teste que satisfaz \\pi(\\theta|\\delta) \\leq \\alpha_0, \\forall \\theta \\in \\Omega_0 \u00e9 chamado de teste n\u00edvel \\alpha_0 , ou que o teste tem n\u00edvel de signific\u00e2ncia \\alpha_0 . O tamanho de um teste \u00e9 \\alpha(\\delta) = \\sup_{\\theta \\in \\Omega_0} \\pi(\\theta, \\delta) . Um teste ter\u00e1 n\u00edvel \\alpha_0 se, e s\u00f3 se, seu tamanho for no m\u00e1ximo \\alpha_0 . P-valor \u00c9 o menor n\u00edvel \\alpha_0 tal que rejeitar\u00edamos a hip\u00f3tese nula a n\u00edvel \\alpha_0 com os dados observados. Se rejeitamos a hip\u00f3tese nula se, e somente se, o p-valor \u00e9 no m\u00e1ximo \\alpha_0 , estamos usando um teste com n\u00edvel de signific\u00e2ncia \\alpha_0 . Equival\u00eancia entre Testes e Conjuntos de Confian\u00e7a Teorema Seja \\vec{X} = (X_1,...,X_n) \\overset{iid}{\\sim} F(\\theta) . Seja g(\\theta) , e suponha que para todo valor c na imagem de g (ou seja, c = g(x) , para algum x ), exista um teste \\delta_c de n\u00edvel \\alpha_0 para a hip\u00f3tese H_{0,c}:g(\\theta) = c, ~ H_{1,c}: g(\\theta) \\neq c Defina \\omega(x) := \\{c: \\delta_c \\text{ n\u00e3o rejeita } H_{0,c} \\text{ se } \\vec{X} = \\vec{x} \\text{ \u00e9 observado } \\} . Ent\u00e3o: P[g(\\theta_0) \\in \\omega(\\vec{X})|\\theta = \\theta_0] \\geq 1 - \\alpha_0, para todo valor \\theta \\in \\Omega . Compreens\u00e3o e Implementa\u00e7\u00e3o Teste de hip\u00f3tese \u00e9 um m\u00e9todo para que fa\u00e7amos decis\u00f5es estat\u00edsticas a partir dos dados. \u00c9 uma forma de compreender (fazer infer\u00e2ncia sobre) um par\u00e2metro. Exemplo: Belgas tem, em m\u00e9dia, maior altura do que peruanos. Exemplo 2: Temperatura n\u00e3o \u00e9 um fator relevante para o processo de cultivo de uva. Estamos avaliando afirma\u00e7\u00f5es mutualmente exclusivas, ou os belgas tem maior altura do que os peruanos, ou n\u00e3o tem! Queremos saber qual dessas afirma\u00e7\u00f5es \u00e9 suportada pelos dados que obtivermos. A hip\u00f3tese nula \u00e9 a afirma\u00e7\u00e3o a ser testada e muitas vezes estabelece uma conjectura de que as caracter\u00edsticas observadas em uma popula\u00e7\u00e3o s\u00e3o por um acaso, isto \u00e9, o fator a ser estudado \"n\u00e3o existe\". Por exemplo: o n\u00famero de voos entre Rio de Janeiro e S\u00e3o Paulo n\u00e3o tem correla\u00e7\u00e3o com o n\u00edvel do mar no Jap\u00e3o. Em geral queremos anul\u00e1-la, rejeit\u00e1-la (da\u00ed o nome). Exemplo Vamos considerar um exemplo simples utilizando a distribui\u00e7\u00e3o normal. \u00c9 a distribui\u00e7\u00e3o com c\u00e1lculos simples e uma boa visualiza\u00e7\u00e3o. A ideia nesse exemplo vai ser a seguinte: O pre\u00e7o do quilo ouro varia diariamente e essa varia\u00e7\u00e3o em unidade de d\u00f3lares ser\u00e1 nosso objeto de interesse. Por exemplo: Se no dia 1 o pre\u00e7o mil e no dia 2 o pre\u00e7o era 1 050 e no dia 3 o pre\u00e7o \u00e9 1 025 temos que X_1 = 50 e X_2 = -25 . Vamos supor que as varia\u00e7\u00f5es entre dois diferentes pares de dias s\u00e3o independentes (essa j\u00e1 uma simplifica\u00e7\u00e3o da realidade!) Primeiro vamos importar os dados. import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.patches as mpatches import seaborn as sns sns.set() gold_df = pd.read_csv('../data/gold.csv', low_memory = False, header=[2,3,4]) # There has a lot of data. I will get average diary from USD price gold_df = gold_df[[('Priced In', 'Price Type', 'Summary'), ('USD', 'Ask', 'Average')]] gold_df.columns = ['Day', 'Price'] gold_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Day Price 0 1/01/68 NaN 1 2/01/68 NaN 2 3/01/68 NaN 3 4/01/68 NaN 4 5/01/68 NaN Observe que existem diversos Nan values. Na pr\u00e1tica eu teria que fazer alguma esp\u00e9cie de limpeza rigorosa. Nesse caso, para tornar tudo bem simples, vou apenas limpar. Tamb\u00e9m precisamos garantir que as informa\u00e7\u00f5es estejam em formato float . # Inplace assegura que eu n\u00e3o crie outro DataFrame gold_df.dropna(inplace = True) gold_df.Price = gold_df.Price.apply(lambda x: float(x.strip().replace(',', ''))) plt.plot(gold_df['Price']) plt.title('Pre\u00e7o do Ouro em D\u00f3lares') plt.show() variation = gold_df.Price.diff() sns.violinplot(x = variation) plt.xlim((-50,50)) plt.title('Distribui\u00e7\u00e3o da varia\u00e7\u00e3o di\u00e1ria') plt.show() De fato n\u00e3o parece uma normal (na verdade uma distribui\u00e7\u00e3o de cauda mais pesada talvez fosse interessante. Mas tudo bem!), mas vamos modelar dessa forma. A partir de agora vamos nos preocupar mais com as defini\u00e7\u00f5es para dar a devida interpreta\u00e7\u00e3o. O exemplo \u00e9 s\u00f3 motivador. Qual hip\u00f3tese queremos testar? O que queremos saber sobre a varia\u00e7\u00e3o? A pergunta que nasce \u00e9 o seguinte: ser\u00e1 que a m\u00e9dia dessa distribui\u00e7\u00e3o \u00e9 0? Isto \u00e9, ser\u00e1 que se calcularmos as m\u00e9dias das varia\u00e7\u00f5es, teremos que com infinitas observa\u00e7\u00f5es, o resultado seria 0? Isso \u00e9 importante porque vai nos ajudar a identificar se existe uma tend\u00eancia de crescimento nas varia\u00e7\u00f5es di\u00e1rias. Hip\u00f3tese Nula: \\mu = 0 , onde X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Hip\u00f3tese Alternativa: \\mu \\neq 0 . Vamos supor que \\sigma \u00e9 conhecido e que \\sigma^2 = 73 , mas a m\u00e9dia \u00e9 desconhecida. Quem \u00e9 \\Omega_0 e \\Omega_1 ? \\Omega_0 \u00e9 a regi\u00e3o dos par\u00e2metros onde a hip\u00f3tese nula \u00e9 verdadeira, isto \u00e9 \\Omega_0 = \\{0\\} , \u00e9 um conjunto unit\u00e1rio. Por outro lado \\Omega_1 = \\mathbb{R} - \\{0\\} , por que a m\u00e9dia pode assumir, em teoria, qualquer valor real. Qual \u00e9 a regi\u00e3o cr\u00edtica? Bom, ainda n\u00e3o podemos determinar essa resposta, afinal para determinar a regi\u00e3o cr\u00edtica (subconjunto do espa\u00e7o dos estados em que se rejeita a hip\u00f3tese nula), precisamos de um procedimento de teste. Mas vamos imaginar que o espa\u00e7o de estados \u00e9 S = \\mathbb{R}^2 , pois vamos considerar apenas duas amostras, inicialmente (queremos visualizar S). Qual ser\u00e1 nosso procedimento de teste? Procedimento de teste \u00e9 uma maneira de tomarmos uma decis\u00e3o. Ele tem a forma: rejeitamos H_0 se isso acontecer. Um exemplo bobo seria: Rejeitamos a hip\u00f3tese nula se 0 \\not \\in (\\min(X_1, X_2), \\max(X_1, X_2)) O problema \u00e9 que em geral esse tipo de procedimento n\u00e3o \u00e9 interessante. Para isso estabelecemos uma estat\u00edstica de teste T e uma Regi\u00e3o de Rejei\u00e7\u00e3o T , tal que nosso procedimento seja: Rejeitamos H_0 se T \\in R . Nesse caso vamos considerar T = \\frac{X_1 + X_2}{2} e vamos rejeitar a hip\u00f3tese se |T| estiver muito longe de 0 , isto \u00e9, se |T| \\geq c . Portanto definimor nossa Regi\u00e3o de Rejei\u00e7\u00e3o como (-\\infty,-c]\\cup[c, + \\infty) , o que reduz nosso problema a determinar c . Qual c seria razo\u00e1vel? 4, 5, 1? Essa pergunta n\u00e3o vai ser respondida. Antes vamos visualizar como fica a regi\u00e3o cr\u00edtica (2). Rejeitamos a hip\u00f3tese se \\left|\\frac{X_1 + X_2}{2}\\right| \\geq c \\rightarrow |X_1 + X_2| \\geq 2c \\rightarrow X_1 + X_2 \\geq 2c \\text{ ou } X_1 + X_2 \\leq -2c Assim: S_1 = \\{(x_1, x_2): x_1 + x_2 \\geq 2c \\text{ ou } x_1 + x_2 \\leq -2c\\} C = [1, 10, 30] decision = lambda x1, x2, c: 1*np.logical_or(x1 + x2 >= 2*c, x1 + x2 <= -2*c) x1,x2 = np.meshgrid(np.arange(-50,50,0.5),np.arange(-50,50,0.5)) fig, ax = plt.subplots(1,3, figsize = (21, 7)) for i, c in enumerate(C): ax[i].contourf(x1,x2,decision(x1,x2, c), levels = [0, 0.5, 1], colors = ['#fdcdac', '#cbd5e8']) ax[i].set_xlabel(r'$x_1$', fontsize = 20) ax[i].set_ylabel(r'$x_2$', fontsize = 20) ax[i].set_title('Regi\u00e3o Cr\u00edtica quando c = {}'.format(c), fontsize = 20) ax[i].legend(handles = [mpatches.Patch(color='#fdcdac', label=r'$S_0$'), mpatches.Patch(color='#cbd5e8', label=r'$S_1$')]) sample = gold_df.Price.diff().sample(n = 2) X1, X2 = sample.iloc[0], sample.iloc[1] ax[i].scatter(X1, X2, color = 'black') ax[i].text(X1 + 1, X2 + 1, s = r'$(X_1, X_2)$', fontsize = 15) Como a fun\u00e7\u00e3o poder entra nessa hist\u00f3ria? A fun\u00e7\u00e3o poder \u00e9 uma fun\u00e7\u00e3o do par\u00e2metro, no caso \\mu , e retorna a probabilidade de rejeitarmos a hip\u00f3tese, considerando esse par\u00e2metro. Isto \u00e9, \\pi(\\mu) = P_{\\mu}((X_1, X_2) \\in S_1) O que poder\u00edamos fazer, ent\u00e3o, \u00e9 obter a distribui\u00e7\u00e3o conjunta de (X_1, X_2) e integrar na regi\u00e3o S_1 . Vamos considerar dois casos separados: \\mu \\in \\Omega_0 : N\u00e3o sabemos disso, e em geral n\u00e3o \u00e9 poss\u00edvel sabermos. Nesse caso \\pi(\\mu) indica a probabilidade de rejeitarmos a hip\u00f3tese nula, mesmo ela sendo verdadeira (chamamos isso de Erro do Tipo I). \\mu \\in \\Omega_1 : Nesse caso \\pi(\\mu) indica a probabilidade de rejeitarmos a hip\u00f3tese nula, quando de ela \u00e9 falsa. Nesse caso 1 - \\pi(\\mu) \u00e9 a probabilidade de n\u00e3o rejeitarmos a hip\u00f3tese nula, quando de fato dever\u00edamos (chamamos de Erro do Tipo II). ] A fun\u00e7\u00e3o poder se trata mais do teste que estamos usando do que os dados em si. Por isso, podemos comparar testes usando essa fun\u00e7\u00e3o poder. Ent\u00e3o vamos ver nesse caso quem \u00e9 a fun\u00e7\u00e3o poder! Vou calcular usando um m\u00e9todo num\u00e9rico para que peguem a ideia. Agora \u00e9 poss\u00edvel fazer as contas, mas nem sempre \u00e9 trivial. Assim teremos apenas aproxima\u00e7\u00e3o da fun\u00e7\u00e3o poder. C = [1, 10, 30] mu_v = np.arange(-100, 100, 1) n = 10000 power = np.zeros((len(C),len(mu_v))) for i, c in enumerate(C): for j, mu in enumerate(mu_v): X1 = np.random.normal(loc = mu, scale = 73, size = n) X2 = np.random.normal(loc = mu, scale = 73, size = n) p = (sum(X1 + X2 >= 2*c) + sum(X1 + X2 <= -2*c))/n power[i,j] = p plt.plot(mu_v, power[i,:], label = 'c = {}'.format(c)) plt.title('Fun\u00e7\u00e3o poder') plt.xlabel(r'$\\mu$') plt.ylabel('Prob') plt.legend() plt.show() Vamos definir c agora? Sim, vamos. Para isso vamos usar o ponto (4) e a defini\u00e7\u00e3o de tamanho do teste. Uma forma poss\u00edvel de se fazer isso \u00e9 a seguinte: limitamos o Erro I por \\alpha_0 e miniminizamos o Erro II, isso \u00e9 minimizamos 1 - \\pi(\\mu) quando \\mu \\neq 0 , ou melhor, maximizamos \\pi(\\mu) . Para isso dizemos que o tamanho do teste \u00e9 o m\u00e1ximo da fun\u00e7\u00e3o poder, quando \\theta \\in \\Omega_0 . Nesse caso \\alpha(\\delta) = \\pi(0) . Queremos, ent\u00e3o que: \\pi(0) = P_{\\mu = 0}(T \\geq c) \\leq \\alpha_0 Precisamos ent\u00e3o encontrar c tal que \\pi(\\mu) = P_{\\mu \\neq 0}(T \\ge c) seja maximado. Observamos que, quando \\mu = 0 , T \\sim N(0, \\sigma^2/2) \\rightarrow \\sqrt{2}T/\\sigma = Z \\sim N(0,1) . Logo P(|T| \\ge c) = P(|Z| \\ge \\sqrt{2}c/\\sigma) = 2(1 - \\Phi(\\sqrt{2}c/\\sigma)) \\leq \\alpha_0 Para maximizar \\pi(\\mu) em \\Omega_1 , observamos que \\pi(\\mu) decresce com c (os gr\u00e1ficos acima representam bem isso). Como queremos maximizar, gostar\u00edamos de tomar c o m\u00ednimo poss\u00edvel, restrito a 2(1 - \\Phi(\\sqrt{2}c/\\sigma)) \\leq \\alpha_0 \\rightarrow 1 - \\alpha_0/2 \\leq \\Phi(\\sqrt{2}c/\\sigma) como vimos acima. Estamos lidando com uma fun\u00e7\u00e3o invers\u00edvel, ent\u00e3o \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) \\leq c O melhor valor de c que respeita essa condi\u00e7\u00e3o e maximiza a rela\u00e7\u00e3o \u00e9, portanto c = \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) from scipy.stats import norm Lembre que \\alpha_0 indica o m\u00e1ximo de Erro I que aceitamos. alpha0 = 0.05 c = np.sqrt(73)/np.sqrt(2)*norm.ppf(1 - alpha0/2) print(c) 11.841167465893536 \u00c9 bem pr\u00f3ximo do gr\u00e1fico acima mostrado, quando testamos para c = 10 . t = np.arange(-20,20,0.1) X = norm(loc = 0, scale = np.sqrt(73)/np.sqrt(2)) plt.plot(t, X.pdf(t)) plt.fill_between(t[(t < -c)], X.pdf(t[(t < -c)]), color = 'blue') plt.fill_between(t[(t > c)], X.pdf(t[(t > c)]), color = 'blue') plt.title('Distribui\u00e7\u00e3o Normal e Regi\u00e3o de Rejei\u00e7\u00e3o') plt.show() Por exemplo vamos tirar duas amostras de nossa distribui\u00e7\u00e3o X1, X2 = gold_df.Price.diff().sample(2) T = np.abs(X1 + X2)/2 T >= c False Mas como escolher \\alpha_0 agora? Agora entra o conceito mais complexo, o do p-valor. Ele est\u00e1 associado \u00e0 ideia de escolher o menor \\alpha_0 poss\u00edvel, para que rejeitemos a hip\u00f3tese nula. Isso significa o seguinte: Queremos minimizar o Erro do Tipo I e rejeitar a Hip\u00f3tese Nula com os dados que obtivemos. Se o p-valor for muito alto, significa que o Erro do Tipo I \u00e9 grande se rejeitarmos a hip\u00f3tese nula. Voc\u00ea apostaria que podemos rejeitar a hip\u00f3tese nula nesse caso? Agora, se o p-valor for pequeno e rejeitarmos nossa hip\u00f3tese nula, o erro do tipo I vai ser pequeno, ent\u00e3o apostar que a hip\u00f3tese nula deva ser rejeitada \u00e9 mais confort\u00e1vel. Assim n\u00e3o escolhemos \\alpha_0 , s\u00f3 observamos seu menor valor e vemos se faz sentido. Em geral se p-valor < 0.05, as pessoas rejeitam a hip\u00f3tese nula. No nosso caso calcular o p-valor \u00e9 tranquilo. Para calcular o p-valor, precisamos dos dados . Queremos rejeitar a hip\u00f3tese nula, isto \u00e9, queremos que t \\geq c que \\alpha_0 seja o menor poss\u00edvel, onde t \u00e9 o valor observado de T . Vamos diminuindo \\alpha_0 e para cada \\alpha_0 podemos calcular c e verificamos se t \\ge c . Podemos fazer isso at\u00e9 que c = t , assim: t = \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) \\rightarrow \\alpha_0 = 2\\left(1 - \\Phi\\left(\\frac{\\sqrt{2}}{\\sigma}t\\right)\\right) p_value = 2*(1 - norm.cdf(np.sqrt(2)/np.sqrt(73)*T)) print(p_value) 0.8477386286989927 Como o p-valor \u00e9 alto, n\u00e3o faz sentido rejeitar a hip\u00f3tese nula. Encerramos a atividade aqui! Testes de Raz\u00e3o Verossimilhan\u00e7a S\u00e3o testes baseados na verossimilha\u00e7a do modelo f_n(x|\\theta) . Suponha que queremos testar a hip\u00f3tese: H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Vamos lembrar que a fun\u00e7\u00e3o de verossimilhan\u00e7a tende a ser mais alta pr\u00f3ximo do valor verdadeiro do par\u00e2metro. Com isso em mente, gostar\u00edamos de saber se a verossimilhan\u00e7a \u00e9 maior em \\Omega_0 ou em \\Omega_1 . Para isso, definimos a estat\u00edstica de raz\u00e3o de verossimilhan\u00e7a : \\Lambda(x) = \\frac{\\sup_{\\theta \\in \\Omega_0}f_n(x|\\theta)}{\\sup_{\\theta \\in \\Omega}f_n(x|\\theta)} Observe que o denominador \u00e9 o valor da fun\u00e7\u00e3o de verossimilhan\u00e7a no Estimador de M\u00e1xima Verossimilhan\u00e7a. Se o par\u00e2metro verdadeiro estiver em \\Omega_0 , o n\u00famerador deve ser mais alto em \\Omega_0 , ent\u00e3o a estat\u00edstica se aproxima de 1. Baseado nisso, o teste de raz\u00e3o de verossimilhan\u00e7a \u00e9: Rejeitamos H_0 se \\Lambda(x) \\le k , para algum k . Teorema: Seja \\Omega \\in \\mathbb{R}^p aberto e suponha que H_0 seja \\theta_{i_1} = \\theta_{01}, ..., \\theta_{i_k} = \\theta_{0k} , onde \\theta = (\\theta_1, ..., \\theta_p) . Assuma que H_0 seja verdadeira e a fun\u00e7\u00e3o de verossimilhan\u00e7a satisfa\u00e7a as condi\u00e7\u00f5es para que o MLE seja assintoticamente normal e assintoticamente eficiente. Ent\u00e3o: -2\\log\\Lambda(x) \\overset{d}{\\to} \\chi^2(k) (converge em distribui\u00e7\u00e3o quando n \\to \\infty ). A demonstra\u00e7\u00e3o pode ser encontrada no StatLect Testes n\u00e3o enviesados Um teste \u00e9 dito n\u00e3o enviesado se \\forall \\theta \\in \\Omega_0 e \\theta ' \\in \\Omega_1, \\pi(\\theta|\\delta) \\le \\pi(\\theta '|\\delta) N\u00e3o \u00e9 muito utilizado dado seu dif\u00edcil c\u00e1lculo num\u00e9rico e n\u00e3o traz resultados quem valem a pena.","title":"Teste de Hip\u00f3teses: Defini\u00e7\u00f5es"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#teste-de-hipoteses-definicoes","text":"Hip\u00f3tese Nula e Alternativa Regi\u00e3o Cr\u00edtica Estat\u00edstica de Teste Fun\u00e7\u00e3o de Poder Tipos de Erro N\u00edvel e tamanho do teste P-valor Temos um problema estat\u00edstico que envolve um par\u00e2metro \\theta tal que tenha valor desconhecido, mas reside em um espa\u00e7o \\Omega . Suponha que particionemos \\Omega = \\Omega_1 \\dot\\cup ~\\Omega_2 e o estat\u00edstico est\u00e1 interessado se \\theta est\u00e1 em \\Omega_0 ou est\u00e1 em \\Omega_1 .","title":"Teste de Hip\u00f3teses: Defini\u00e7\u00f5es"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#hipotese-nula-e-alternativa","text":"Dizemos que H_0 \u00e9 a hip\u00f3tese de que \\theta \\in \\Omega_0 e chamamos H_0 de hip\u00f3tese nula , enquanto H_1 \u00e9 a hip\u00f3tese alternativa e representa \\theta \\in H_1 . Queremos decidir qual das hip\u00f3teses \u00e9 verdadeira (e s\u00f3 uma ser\u00e1, porque a parti\u00e7\u00e3o \u00e9 disjunta). Se decidimos que \\theta \\in \\Omega_1 , rejeitamos H_0 , e se \\theta \\in \\Omega_0 , n\u00e3o rejeitamos H_0 .","title":"Hip\u00f3tese Nula e Alternativa"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#hipotese-simples-e-composta","text":"Suponha que X_1, ..., X_n formam uma amostra aleat\u00f3ria com pdf f(x|\\theta) . Queremos testar a hip\u00f3tese de que H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Se \\Omega_i contem apenas um valor, ent\u00e3o H_i \u00e9 dita hip\u00f3tese simples. Se cont\u00e9m mais de um valor, dizemos que \u00e9 composta.","title":"Hip\u00f3tese Simples e Composta"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#hipotese-unilateral-e-bilateral","text":"Seja \\theta um par\u00e2metro unidimensional. Dizemos que a hip\u00f3tese H_0 \u00e9 unilateral (ou one tailed ) quando \u00e9 da forma \\theta \\leq \\theta_0 ou \\theta \\geq \\theta_0 . Ela ser\u00e1 bilateral quando \u00e9 do tipo H_0 \\neq \\theta_0 .","title":"Hip\u00f3tese Unilateral e Bilateral"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#regiao-critica","text":"Suponha que queremos testar a hip\u00f3tese de que H_0: \\theta \\in \\Omega_0 \\text{ e } H_1: \\theta \\in \\Omega_1 Quando queremos decidir qual hip\u00f3tese escolher, observamos uma amostra dessa distribui\u00e7\u00e3o no espa\u00e7o de amostras S . O dever do estat\u00edstico \u00e9 especificar um procedimento que particione o conjunto em dois subconjuntos S_0 e S_1 , onde S_1 cont\u00e9m os valores de X que rejeitam H_0 . Regi\u00e3o cr\u00edtica \u00e9 o conjunto S_1 , isto \u00e9, o conjunto de amostras que, a partir de um procedimento, rejeita H_0 .","title":"Regi\u00e3o Cr\u00edtica"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#estatistica-de-teste","text":"Seja X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Sejam T = r(X) uma estat\u00edstica e R um subconjunto da reta. Suponha que nosso procedimento de teste \u00e9 o seguinte: Rejeitamos H_0 se T\\in R . Chamamos T de estat\u00edstica de teste e R de regi\u00e3o de rejei\u00e7\u00e3o . Dessa forma a regi\u00e3o cr\u00edtica ser\u00e1: S_1 = \\{x \\in S: r(x) \\in R\\} . Na pr\u00e1tica a maioria dos testes \u00e9 do tipo Rejeitamos H_0 se T \\geq c, c \\in \\mathbb{R} .","title":"Estat\u00edstica de Teste"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#observacao-sobre-a-divisao-de-conjuntos","text":"\u00c9 importante lembrar que h\u00e1 duas diferentes divis\u00f5es: \\Omega = \\Omega_0 \\dot\\cup \\Omega_1 , que \u00e9 a divis\u00e3o do espa\u00e7o dos par\u00e2metros, e S = S_1 \\dot\\cup S_1 \u00e9 a divis\u00e3o do espa\u00e7o das amostras. Mas qual a rela\u00e7\u00e3o entre eles? Se X \\in S_1 , ent\u00e3o rejeitamos a hip\u00f3tese \\theta \\in \\Omega_0 . Al\u00e9m do mais, podemos encontrar S_1 e S_2 , mas dificilmente saberemos em qual dos conjuntos \\theta pertence.","title":"Observa\u00e7\u00e3o sobre a divis\u00e3o de conjuntos"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#funcao-de-poder-e-tipos-de-erro","text":"","title":"Fun\u00e7\u00e3o de Poder e Tipos de Erro"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#funcao-poder","text":"Seja \\delta um procedimento de teste (como esse assinalado acima). Se S_1 \u00e9 a regi\u00e3o cr\u00edtica, \\pi(\\theta|\\delta) = P(X \\in S_1|\\theta) = P(T \\in R|\\theta) Sendo que a \u00faltima igualdade ocorre quando o proocedimento de teste \u00e9 o citado acima. O seu significado? \u00c9 a probabilidade, para cada valor de \\theta , de que \\delta rejeita H_0 . Queremos, intuitivamente que: \\theta \\in \\Omega_0 \\rightarrow \\text{ Queremos n\u00e3o rejeitar} H_0 \\rightarrow \\pi(\\theta|\\delta) = 0 \\theta \\in \\Omega_1 \\rightarrow \\text{ Queremos rejeitar} H_0 \\rightarrow \\pi(\\theta|\\delta) = 1 Entretanto isso n\u00e3o \u00e9 em geral o que acontece. Por isso definimos:","title":"Fun\u00e7\u00e3o Poder"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#erros-i-e-ii","text":"\\theta \\in \\Omega_0 \\theta \\in \\Omega_1 \\delta rejeita H_0 Erro Tipo I Certo \\delta n\u00e3o rejeita H_0 Certo Erro Tipo II Portanto se \\theta \\in \\Omega_0, \\pi(\\theta|\\delta) \u00e9 a probabilidade de cometermos o erro do tipo I. Se \\theta \\in \\Omega_1, 1 - \\pi(\\theta|\\delta) \u00e9 a probabilidade de cometer o erro do tipo II.","title":"Erros I e II"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#niveltamanho","text":"Um teste que satisfaz \\pi(\\theta|\\delta) \\leq \\alpha_0, \\forall \\theta \\in \\Omega_0 \u00e9 chamado de teste n\u00edvel \\alpha_0 , ou que o teste tem n\u00edvel de signific\u00e2ncia \\alpha_0 . O tamanho de um teste \u00e9 \\alpha(\\delta) = \\sup_{\\theta \\in \\Omega_0} \\pi(\\theta, \\delta) . Um teste ter\u00e1 n\u00edvel \\alpha_0 se, e s\u00f3 se, seu tamanho for no m\u00e1ximo \\alpha_0 .","title":"N\u00edvel/Tamanho"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#p-valor","text":"\u00c9 o menor n\u00edvel \\alpha_0 tal que rejeitar\u00edamos a hip\u00f3tese nula a n\u00edvel \\alpha_0 com os dados observados. Se rejeitamos a hip\u00f3tese nula se, e somente se, o p-valor \u00e9 no m\u00e1ximo \\alpha_0 , estamos usando um teste com n\u00edvel de signific\u00e2ncia \\alpha_0 .","title":"P-valor"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#equivalencia-entre-testes-e-conjuntos-de-confianca","text":"","title":"Equival\u00eancia entre Testes e Conjuntos de Confian\u00e7a"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#teorema","text":"Seja \\vec{X} = (X_1,...,X_n) \\overset{iid}{\\sim} F(\\theta) . Seja g(\\theta) , e suponha que para todo valor c na imagem de g (ou seja, c = g(x) , para algum x ), exista um teste \\delta_c de n\u00edvel \\alpha_0 para a hip\u00f3tese H_{0,c}:g(\\theta) = c, ~ H_{1,c}: g(\\theta) \\neq c Defina \\omega(x) := \\{c: \\delta_c \\text{ n\u00e3o rejeita } H_{0,c} \\text{ se } \\vec{X} = \\vec{x} \\text{ \u00e9 observado } \\} . Ent\u00e3o: P[g(\\theta_0) \\in \\omega(\\vec{X})|\\theta = \\theta_0] \\geq 1 - \\alpha_0, para todo valor \\theta \\in \\Omega .","title":"Teorema"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#compreensao-e-implementacao","text":"Teste de hip\u00f3tese \u00e9 um m\u00e9todo para que fa\u00e7amos decis\u00f5es estat\u00edsticas a partir dos dados. \u00c9 uma forma de compreender (fazer infer\u00e2ncia sobre) um par\u00e2metro. Exemplo: Belgas tem, em m\u00e9dia, maior altura do que peruanos. Exemplo 2: Temperatura n\u00e3o \u00e9 um fator relevante para o processo de cultivo de uva. Estamos avaliando afirma\u00e7\u00f5es mutualmente exclusivas, ou os belgas tem maior altura do que os peruanos, ou n\u00e3o tem! Queremos saber qual dessas afirma\u00e7\u00f5es \u00e9 suportada pelos dados que obtivermos. A hip\u00f3tese nula \u00e9 a afirma\u00e7\u00e3o a ser testada e muitas vezes estabelece uma conjectura de que as caracter\u00edsticas observadas em uma popula\u00e7\u00e3o s\u00e3o por um acaso, isto \u00e9, o fator a ser estudado \"n\u00e3o existe\". Por exemplo: o n\u00famero de voos entre Rio de Janeiro e S\u00e3o Paulo n\u00e3o tem correla\u00e7\u00e3o com o n\u00edvel do mar no Jap\u00e3o. Em geral queremos anul\u00e1-la, rejeit\u00e1-la (da\u00ed o nome).","title":"Compreens\u00e3o e Implementa\u00e7\u00e3o"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#exemplo","text":"Vamos considerar um exemplo simples utilizando a distribui\u00e7\u00e3o normal. \u00c9 a distribui\u00e7\u00e3o com c\u00e1lculos simples e uma boa visualiza\u00e7\u00e3o. A ideia nesse exemplo vai ser a seguinte: O pre\u00e7o do quilo ouro varia diariamente e essa varia\u00e7\u00e3o em unidade de d\u00f3lares ser\u00e1 nosso objeto de interesse. Por exemplo: Se no dia 1 o pre\u00e7o mil e no dia 2 o pre\u00e7o era 1 050 e no dia 3 o pre\u00e7o \u00e9 1 025 temos que X_1 = 50 e X_2 = -25 . Vamos supor que as varia\u00e7\u00f5es entre dois diferentes pares de dias s\u00e3o independentes (essa j\u00e1 uma simplifica\u00e7\u00e3o da realidade!) Primeiro vamos importar os dados. import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.patches as mpatches import seaborn as sns sns.set() gold_df = pd.read_csv('../data/gold.csv', low_memory = False, header=[2,3,4]) # There has a lot of data. I will get average diary from USD price gold_df = gold_df[[('Priced In', 'Price Type', 'Summary'), ('USD', 'Ask', 'Average')]] gold_df.columns = ['Day', 'Price'] gold_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Day Price 0 1/01/68 NaN 1 2/01/68 NaN 2 3/01/68 NaN 3 4/01/68 NaN 4 5/01/68 NaN Observe que existem diversos Nan values. Na pr\u00e1tica eu teria que fazer alguma esp\u00e9cie de limpeza rigorosa. Nesse caso, para tornar tudo bem simples, vou apenas limpar. Tamb\u00e9m precisamos garantir que as informa\u00e7\u00f5es estejam em formato float . # Inplace assegura que eu n\u00e3o crie outro DataFrame gold_df.dropna(inplace = True) gold_df.Price = gold_df.Price.apply(lambda x: float(x.strip().replace(',', ''))) plt.plot(gold_df['Price']) plt.title('Pre\u00e7o do Ouro em D\u00f3lares') plt.show() variation = gold_df.Price.diff() sns.violinplot(x = variation) plt.xlim((-50,50)) plt.title('Distribui\u00e7\u00e3o da varia\u00e7\u00e3o di\u00e1ria') plt.show() De fato n\u00e3o parece uma normal (na verdade uma distribui\u00e7\u00e3o de cauda mais pesada talvez fosse interessante. Mas tudo bem!), mas vamos modelar dessa forma. A partir de agora vamos nos preocupar mais com as defini\u00e7\u00f5es para dar a devida interpreta\u00e7\u00e3o. O exemplo \u00e9 s\u00f3 motivador. Qual hip\u00f3tese queremos testar? O que queremos saber sobre a varia\u00e7\u00e3o? A pergunta que nasce \u00e9 o seguinte: ser\u00e1 que a m\u00e9dia dessa distribui\u00e7\u00e3o \u00e9 0? Isto \u00e9, ser\u00e1 que se calcularmos as m\u00e9dias das varia\u00e7\u00f5es, teremos que com infinitas observa\u00e7\u00f5es, o resultado seria 0? Isso \u00e9 importante porque vai nos ajudar a identificar se existe uma tend\u00eancia de crescimento nas varia\u00e7\u00f5es di\u00e1rias. Hip\u00f3tese Nula: \\mu = 0 , onde X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Hip\u00f3tese Alternativa: \\mu \\neq 0 . Vamos supor que \\sigma \u00e9 conhecido e que \\sigma^2 = 73 , mas a m\u00e9dia \u00e9 desconhecida. Quem \u00e9 \\Omega_0 e \\Omega_1 ? \\Omega_0 \u00e9 a regi\u00e3o dos par\u00e2metros onde a hip\u00f3tese nula \u00e9 verdadeira, isto \u00e9 \\Omega_0 = \\{0\\} , \u00e9 um conjunto unit\u00e1rio. Por outro lado \\Omega_1 = \\mathbb{R} - \\{0\\} , por que a m\u00e9dia pode assumir, em teoria, qualquer valor real. Qual \u00e9 a regi\u00e3o cr\u00edtica? Bom, ainda n\u00e3o podemos determinar essa resposta, afinal para determinar a regi\u00e3o cr\u00edtica (subconjunto do espa\u00e7o dos estados em que se rejeita a hip\u00f3tese nula), precisamos de um procedimento de teste. Mas vamos imaginar que o espa\u00e7o de estados \u00e9 S = \\mathbb{R}^2 , pois vamos considerar apenas duas amostras, inicialmente (queremos visualizar S). Qual ser\u00e1 nosso procedimento de teste? Procedimento de teste \u00e9 uma maneira de tomarmos uma decis\u00e3o. Ele tem a forma: rejeitamos H_0 se isso acontecer. Um exemplo bobo seria: Rejeitamos a hip\u00f3tese nula se 0 \\not \\in (\\min(X_1, X_2), \\max(X_1, X_2)) O problema \u00e9 que em geral esse tipo de procedimento n\u00e3o \u00e9 interessante. Para isso estabelecemos uma estat\u00edstica de teste T e uma Regi\u00e3o de Rejei\u00e7\u00e3o T , tal que nosso procedimento seja: Rejeitamos H_0 se T \\in R . Nesse caso vamos considerar T = \\frac{X_1 + X_2}{2} e vamos rejeitar a hip\u00f3tese se |T| estiver muito longe de 0 , isto \u00e9, se |T| \\geq c . Portanto definimor nossa Regi\u00e3o de Rejei\u00e7\u00e3o como (-\\infty,-c]\\cup[c, + \\infty) , o que reduz nosso problema a determinar c . Qual c seria razo\u00e1vel? 4, 5, 1? Essa pergunta n\u00e3o vai ser respondida. Antes vamos visualizar como fica a regi\u00e3o cr\u00edtica (2). Rejeitamos a hip\u00f3tese se \\left|\\frac{X_1 + X_2}{2}\\right| \\geq c \\rightarrow |X_1 + X_2| \\geq 2c \\rightarrow X_1 + X_2 \\geq 2c \\text{ ou } X_1 + X_2 \\leq -2c Assim: S_1 = \\{(x_1, x_2): x_1 + x_2 \\geq 2c \\text{ ou } x_1 + x_2 \\leq -2c\\} C = [1, 10, 30] decision = lambda x1, x2, c: 1*np.logical_or(x1 + x2 >= 2*c, x1 + x2 <= -2*c) x1,x2 = np.meshgrid(np.arange(-50,50,0.5),np.arange(-50,50,0.5)) fig, ax = plt.subplots(1,3, figsize = (21, 7)) for i, c in enumerate(C): ax[i].contourf(x1,x2,decision(x1,x2, c), levels = [0, 0.5, 1], colors = ['#fdcdac', '#cbd5e8']) ax[i].set_xlabel(r'$x_1$', fontsize = 20) ax[i].set_ylabel(r'$x_2$', fontsize = 20) ax[i].set_title('Regi\u00e3o Cr\u00edtica quando c = {}'.format(c), fontsize = 20) ax[i].legend(handles = [mpatches.Patch(color='#fdcdac', label=r'$S_0$'), mpatches.Patch(color='#cbd5e8', label=r'$S_1$')]) sample = gold_df.Price.diff().sample(n = 2) X1, X2 = sample.iloc[0], sample.iloc[1] ax[i].scatter(X1, X2, color = 'black') ax[i].text(X1 + 1, X2 + 1, s = r'$(X_1, X_2)$', fontsize = 15) Como a fun\u00e7\u00e3o poder entra nessa hist\u00f3ria? A fun\u00e7\u00e3o poder \u00e9 uma fun\u00e7\u00e3o do par\u00e2metro, no caso \\mu , e retorna a probabilidade de rejeitarmos a hip\u00f3tese, considerando esse par\u00e2metro. Isto \u00e9, \\pi(\\mu) = P_{\\mu}((X_1, X_2) \\in S_1) O que poder\u00edamos fazer, ent\u00e3o, \u00e9 obter a distribui\u00e7\u00e3o conjunta de (X_1, X_2) e integrar na regi\u00e3o S_1 . Vamos considerar dois casos separados: \\mu \\in \\Omega_0 : N\u00e3o sabemos disso, e em geral n\u00e3o \u00e9 poss\u00edvel sabermos. Nesse caso \\pi(\\mu) indica a probabilidade de rejeitarmos a hip\u00f3tese nula, mesmo ela sendo verdadeira (chamamos isso de Erro do Tipo I). \\mu \\in \\Omega_1 : Nesse caso \\pi(\\mu) indica a probabilidade de rejeitarmos a hip\u00f3tese nula, quando de ela \u00e9 falsa. Nesse caso 1 - \\pi(\\mu) \u00e9 a probabilidade de n\u00e3o rejeitarmos a hip\u00f3tese nula, quando de fato dever\u00edamos (chamamos de Erro do Tipo II). ] A fun\u00e7\u00e3o poder se trata mais do teste que estamos usando do que os dados em si. Por isso, podemos comparar testes usando essa fun\u00e7\u00e3o poder. Ent\u00e3o vamos ver nesse caso quem \u00e9 a fun\u00e7\u00e3o poder! Vou calcular usando um m\u00e9todo num\u00e9rico para que peguem a ideia. Agora \u00e9 poss\u00edvel fazer as contas, mas nem sempre \u00e9 trivial. Assim teremos apenas aproxima\u00e7\u00e3o da fun\u00e7\u00e3o poder. C = [1, 10, 30] mu_v = np.arange(-100, 100, 1) n = 10000 power = np.zeros((len(C),len(mu_v))) for i, c in enumerate(C): for j, mu in enumerate(mu_v): X1 = np.random.normal(loc = mu, scale = 73, size = n) X2 = np.random.normal(loc = mu, scale = 73, size = n) p = (sum(X1 + X2 >= 2*c) + sum(X1 + X2 <= -2*c))/n power[i,j] = p plt.plot(mu_v, power[i,:], label = 'c = {}'.format(c)) plt.title('Fun\u00e7\u00e3o poder') plt.xlabel(r'$\\mu$') plt.ylabel('Prob') plt.legend() plt.show() Vamos definir c agora? Sim, vamos. Para isso vamos usar o ponto (4) e a defini\u00e7\u00e3o de tamanho do teste. Uma forma poss\u00edvel de se fazer isso \u00e9 a seguinte: limitamos o Erro I por \\alpha_0 e miniminizamos o Erro II, isso \u00e9 minimizamos 1 - \\pi(\\mu) quando \\mu \\neq 0 , ou melhor, maximizamos \\pi(\\mu) . Para isso dizemos que o tamanho do teste \u00e9 o m\u00e1ximo da fun\u00e7\u00e3o poder, quando \\theta \\in \\Omega_0 . Nesse caso \\alpha(\\delta) = \\pi(0) . Queremos, ent\u00e3o que: \\pi(0) = P_{\\mu = 0}(T \\geq c) \\leq \\alpha_0 Precisamos ent\u00e3o encontrar c tal que \\pi(\\mu) = P_{\\mu \\neq 0}(T \\ge c) seja maximado. Observamos que, quando \\mu = 0 , T \\sim N(0, \\sigma^2/2) \\rightarrow \\sqrt{2}T/\\sigma = Z \\sim N(0,1) . Logo P(|T| \\ge c) = P(|Z| \\ge \\sqrt{2}c/\\sigma) = 2(1 - \\Phi(\\sqrt{2}c/\\sigma)) \\leq \\alpha_0 Para maximizar \\pi(\\mu) em \\Omega_1 , observamos que \\pi(\\mu) decresce com c (os gr\u00e1ficos acima representam bem isso). Como queremos maximizar, gostar\u00edamos de tomar c o m\u00ednimo poss\u00edvel, restrito a 2(1 - \\Phi(\\sqrt{2}c/\\sigma)) \\leq \\alpha_0 \\rightarrow 1 - \\alpha_0/2 \\leq \\Phi(\\sqrt{2}c/\\sigma) como vimos acima. Estamos lidando com uma fun\u00e7\u00e3o invers\u00edvel, ent\u00e3o \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) \\leq c O melhor valor de c que respeita essa condi\u00e7\u00e3o e maximiza a rela\u00e7\u00e3o \u00e9, portanto c = \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) from scipy.stats import norm Lembre que \\alpha_0 indica o m\u00e1ximo de Erro I que aceitamos. alpha0 = 0.05 c = np.sqrt(73)/np.sqrt(2)*norm.ppf(1 - alpha0/2) print(c) 11.841167465893536 \u00c9 bem pr\u00f3ximo do gr\u00e1fico acima mostrado, quando testamos para c = 10 . t = np.arange(-20,20,0.1) X = norm(loc = 0, scale = np.sqrt(73)/np.sqrt(2)) plt.plot(t, X.pdf(t)) plt.fill_between(t[(t < -c)], X.pdf(t[(t < -c)]), color = 'blue') plt.fill_between(t[(t > c)], X.pdf(t[(t > c)]), color = 'blue') plt.title('Distribui\u00e7\u00e3o Normal e Regi\u00e3o de Rejei\u00e7\u00e3o') plt.show() Por exemplo vamos tirar duas amostras de nossa distribui\u00e7\u00e3o X1, X2 = gold_df.Price.diff().sample(2) T = np.abs(X1 + X2)/2 T >= c False Mas como escolher \\alpha_0 agora? Agora entra o conceito mais complexo, o do p-valor. Ele est\u00e1 associado \u00e0 ideia de escolher o menor \\alpha_0 poss\u00edvel, para que rejeitemos a hip\u00f3tese nula. Isso significa o seguinte: Queremos minimizar o Erro do Tipo I e rejeitar a Hip\u00f3tese Nula com os dados que obtivemos. Se o p-valor for muito alto, significa que o Erro do Tipo I \u00e9 grande se rejeitarmos a hip\u00f3tese nula. Voc\u00ea apostaria que podemos rejeitar a hip\u00f3tese nula nesse caso? Agora, se o p-valor for pequeno e rejeitarmos nossa hip\u00f3tese nula, o erro do tipo I vai ser pequeno, ent\u00e3o apostar que a hip\u00f3tese nula deva ser rejeitada \u00e9 mais confort\u00e1vel. Assim n\u00e3o escolhemos \\alpha_0 , s\u00f3 observamos seu menor valor e vemos se faz sentido. Em geral se p-valor < 0.05, as pessoas rejeitam a hip\u00f3tese nula. No nosso caso calcular o p-valor \u00e9 tranquilo. Para calcular o p-valor, precisamos dos dados . Queremos rejeitar a hip\u00f3tese nula, isto \u00e9, queremos que t \\geq c que \\alpha_0 seja o menor poss\u00edvel, onde t \u00e9 o valor observado de T . Vamos diminuindo \\alpha_0 e para cada \\alpha_0 podemos calcular c e verificamos se t \\ge c . Podemos fazer isso at\u00e9 que c = t , assim: t = \\frac{\\sigma}{\\sqrt{2}}\\Phi^{-1}(1 - \\alpha_0/2) \\rightarrow \\alpha_0 = 2\\left(1 - \\Phi\\left(\\frac{\\sqrt{2}}{\\sigma}t\\right)\\right) p_value = 2*(1 - norm.cdf(np.sqrt(2)/np.sqrt(73)*T)) print(p_value) 0.8477386286989927 Como o p-valor \u00e9 alto, n\u00e3o faz sentido rejeitar a hip\u00f3tese nula. Encerramos a atividade aqui!","title":"Exemplo"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#testes-de-razao-verossimilhanca","text":"S\u00e3o testes baseados na verossimilha\u00e7a do modelo f_n(x|\\theta) . Suponha que queremos testar a hip\u00f3tese: H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Vamos lembrar que a fun\u00e7\u00e3o de verossimilhan\u00e7a tende a ser mais alta pr\u00f3ximo do valor verdadeiro do par\u00e2metro. Com isso em mente, gostar\u00edamos de saber se a verossimilhan\u00e7a \u00e9 maior em \\Omega_0 ou em \\Omega_1 . Para isso, definimos a estat\u00edstica de raz\u00e3o de verossimilhan\u00e7a : \\Lambda(x) = \\frac{\\sup_{\\theta \\in \\Omega_0}f_n(x|\\theta)}{\\sup_{\\theta \\in \\Omega}f_n(x|\\theta)} Observe que o denominador \u00e9 o valor da fun\u00e7\u00e3o de verossimilhan\u00e7a no Estimador de M\u00e1xima Verossimilhan\u00e7a. Se o par\u00e2metro verdadeiro estiver em \\Omega_0 , o n\u00famerador deve ser mais alto em \\Omega_0 , ent\u00e3o a estat\u00edstica se aproxima de 1. Baseado nisso, o teste de raz\u00e3o de verossimilhan\u00e7a \u00e9: Rejeitamos H_0 se \\Lambda(x) \\le k , para algum k .","title":"Testes de Raz\u00e3o Verossimilhan\u00e7a"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#teorema_1","text":"Seja \\Omega \\in \\mathbb{R}^p aberto e suponha que H_0 seja \\theta_{i_1} = \\theta_{01}, ..., \\theta_{i_k} = \\theta_{0k} , onde \\theta = (\\theta_1, ..., \\theta_p) . Assuma que H_0 seja verdadeira e a fun\u00e7\u00e3o de verossimilhan\u00e7a satisfa\u00e7a as condi\u00e7\u00f5es para que o MLE seja assintoticamente normal e assintoticamente eficiente. Ent\u00e3o: -2\\log\\Lambda(x) \\overset{d}{\\to} \\chi^2(k) (converge em distribui\u00e7\u00e3o quando n \\to \\infty ). A demonstra\u00e7\u00e3o pode ser encontrada no StatLect","title":"Teorema:"},{"location":"infestatistica_BSc/TestingHypotheses/TestingHypotheses/#testes-nao-enviesados","text":"Um teste \u00e9 dito n\u00e3o enviesado se \\forall \\theta \\in \\Omega_0 e \\theta ' \\in \\Omega_1, \\pi(\\theta|\\delta) \\le \\pi(\\theta '|\\delta) N\u00e3o \u00e9 muito utilizado dado seu dif\u00edcil c\u00e1lculo num\u00e9rico e n\u00e3o traz resultados quem valem a pena.","title":"Testes n\u00e3o enviesados"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/","text":"Teste de Hip\u00f3teses II Nesse notebook veremos: Teste de Hip\u00f3teses Simples Hip\u00f3tese Alternativa Bilateral Teste T Comparando m\u00e9dias de duas Normais Comparando vari\u00e2ncias de duas Normais Teste de Hip\u00f3tese Simples O objetivo \u00e9 considerar se um vetor de observa\u00e7\u00f5es vem de uma entre duas observa\u00e7\u00f5es. Nesse caso o espa\u00e7o \\Omega \u00e9 formado por dois pontos, e n\u00e3o \u00e9 um espa\u00e7o de par\u00e2metros, mas espa\u00e7o de distribui\u00e7\u00f5es, em particular dessas duas distribui\u00e7\u00f5es. Isto \u00e9, vamos assumir que X = (X_1, ..., X_n) vem de f_0(x) ou f_1(x) . Assim \\Omega = \\{\\theta_0, \\theta_1\\} e \\theta = \\theta_i se os dados tem distribui\u00e7\u00e3o f_i(x), i = 0,1 . Vamos denotar: \\alpha(\\delta) = P(\\text{Rejeitar} H_0|\\theta = \\theta_0) = P(\\text{Erro I}) \\beta(\\delta) = P(\\text{N\u00e3o rejeitar} H_0|\\theta = \\theta_1) = P(\\text{Erro II}) Teorema Seja \\delta^* o procedimento de teste que n\u00e3o rejeita H_0 se af_0(x) > bf_1(x) e rejeita se af_0(x) < bf_1(x) . Ent\u00e3o, para todo outro procedimento de teste \\delta , a\\alpha(\\delta^*) + b\\beta(\\delta^*) \\le a\\alpha(\\delta) + b\\beta(\\delta) Queremos escolher um teste que minimize essa combina\u00e7\u00e3o linear a\\alpha(\\delta) + b\\beta(\\delta) . Claro que seria \u00f3timo ter esse erro zerado, mas sabemos que existe uma esp\u00e9cie de trade off entre esses erros. Esse teorema d\u00e1 o teste necess\u00e1rio para que isso aconten\u00e7a. Corol\u00e1rio Considere as hip\u00f3teses do teorema anterior, a > 0 e b > 0 . Defina estat\u00edstica de teste raz\u00e3o de verossimilhan\u00e7a : \\Lambda(x) = \\begin{cases} \\frac{f_0(x)}{f_1(x)}, \\text{ se } f_0(x) \\le f_1(x) \\\\ 1, \\text{ caso contr\u00e1rio }. \\end{cases} Defina o procedimento de teste \\delta : Rejeita H_0 se \\Lambda(x) > a/b . Ent\u00e3o o valor de af_0(x) + bf_1(x) \u00e9 m\u00ednimo. Lema Nayman-Pearson Suponha que \\delta ' tem a seguinte forma, para algum k > 0 : H_0 n\u00e3o \u00e9 rejeitada se f_1(x) < kf_0(x) e o \u00e9 quando f_1(x) > kf_0(x). Se \\delta \u00e9 outro procedimento de teste tal que \\alpha(\\delta) \\le \\alpha(\\delta ') , ent\u00e3o \\beta(\\delta) \\ge \\beta(\\delta '). Implementa\u00e7\u00e3o Vamos fazer uma simples implementa\u00e7\u00e3o de uso para esse tipo de problema. import numpy as np from scipy.stats import bernoulli, binom from scipy.optimize import brute Nesse caso, vamos fazer uma simples simula\u00e7\u00e3o, onde um par\u00e2metro de uma distribui\u00e7\u00e3o de Bernoulli pode ser p = 0.4 ou p = 0.6 . Vamos gerar essa amostra, mas sem de fato conhecer p verdadeiro. ro = np.random.RandomState(1000000) #random state p = ro.choice([0.4, 0.6]) Teremos uma amostra de tamanho n . n = 20 X = ro.binomial(1, p, size = n) Vamos utilizar o Lema Nayman-Pearson. O objetivo \u00e9 testar as seguintes hip\u00f3teses: H_0: p = 0.4 H_1: p = 0.6 Vamos fixar \\alpha_0 = 0.05 o tamanho do teste. Temos que, se y = \\sum_{i=1}^n x_i \\sim Binomial(n,p) , \\frac{f_1(x)}{f_0(x)} = \\frac{0.6^y0.4^{n-y}}{0.4^y0.6^{n-y}} = \\left(\\frac{3}{2}\\right)^y\\left(\\frac{2}{3}\\right)^{n-y} = \\left(\\frac{3}{2}\\right)^{2y - n} Assim: \\begin{split} 0.05 &= P(f_1(x) > kf_0(x)|p = 0.4) = P\\left(\\left(\\frac{3}{2}\\right)^{2y - n} > k\\right) \\\\ &= P\\left(2y - n > \\frac{\\log(k)}{\\log(3/2)}\\right) \\\\ &= P\\left(y > \\frac{\\log(k)}{2\\log(3/2)} + \\frac{n}{2}\\right), y \\sim Binomial(n,0.4) \\end{split} Isto \u00e9, preciso escolher k que satisfa\u00e7a essa rela\u00e7\u00e3o. Vamos calcular k numericamente utilizando um m\u00e9todo de otimiza\u00e7\u00e3o por bruta for\u00e7a (s\u00e3o poucas as op\u00e7\u00f5es). Como n\u00e3o queremos que seja marior do que 0.05, precisamos colocar peso para que n\u00e3o seja. Veja que existem v\u00e1rios valores de k que satisfazem isso. alpha0 = 0.05 Y = binom(n = n, p = 0.4) func = lambda k, n: np.abs(0.95 - Y.cdf((1/2)*np.log(k)/np.log(3/2) + n/2)) + \\ 10*(0.95 > Y.cdf((1/2)*np.log(k)/np.log(3/2) + n/2)) k = brute(func, ranges = (slice(1,20,1),), args = (n,))[0] k 6.0 Por esse motivo, vamos tomar k=6 . Pela Lema de Neyman Pearson, esse teste \u00e9 o que minimiza o Erro do Tipo II. Vamos ver se rejeitamos ou n\u00e3o a hip\u00f3tese nula baseado nos dados obtidos. f0 = lambda x: 0.4**(sum(x))*0.6**(len(x) - sum(x)) f1 = lambda x: 0.6**(sum(x))*0.4**(len(x) - sum(x)) if f1(X) > k*f0(X): print(r'Rejeitamos H0.') else: print(r'N\u00e3o rejeitamos H0.') N\u00e3o rejeitamos H0. Vamos ver quem \u00e9 p , ent\u00e3o. print('O valor de p \u00e9 .... ') print(p) O valor de p \u00e9 .... 0.4 Fizemos bem em n\u00e3o rejeitar a hip\u00f3tese nula! Hip\u00f3tese Alternativa Bilateral Seja X = (X_1, ..., X_n) uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o normal com m\u00e9dia \\mu desconhecida e vari\u00e2ncia \\sigma^2 conhecida e queremos testar a hip\u00f3tese H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0 Como \\bar{X}_n \u00e9 um estimador consistente de \\mu , faz sentido rejeitar a hip\u00f3tese nula quando a m\u00e9dia amostral se afasta de \\mu_0 . Para isso, vamos escolher c_1, c_2 de forma que P(\\bar{X}_n \\leq c_1|\\mu = \\mu_0) + P(\\bar{X}_n \\geq c_2|\\mu = \\mu_0) = \\alpha_0 \\Rightarrow P\\left(Z \\leq \\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) + P\\left(Z \\geq \\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = \\alpha_0 \\Rightarrow \\Phi\\left(\\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) + 1 - \\Phi\\left(\\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = \\alpha_0 \\Rightarrow \\Phi\\left(\\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) = \\alpha_1 \\text{ e } \\Phi\\left(\\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = 1 - \\alpha_2, \\text{ com } \\alpha_1 + \\alpha_2 = \\alpha_0 Observa\u00e7\u00e3o: \\bar{X}_n \\sim N(\\mu, \\sigma^2/n) \\Rightarrow Z = \\sqrt{n}\\frac{\\bar{X}_n - \\mu}{\\sigma} \\sim N(0,1) Observa\u00e7\u00e3o 2: No c\u00e1lculo substituimos \\mu por \\mu_0 , porque estamos \"condicionando\" no conhecimento deles serem iguais. Isto \u00e9, queremos que o tamanho do teste seja \\alpha_0 , lembrando que o tamanho do teste \u00e9 o supremo das probabilidades de se rejeitar a hip\u00f3tese nula quando ela \u00e9 verdadeira. Teste t Suponha que (X_1,...,X_n) \u00e9 uma amostra aleat\u00f3ria da distribui\u00e7\u00e3o N(\\mu,\\sigma^2) , com par\u00e2metros desconhecidos e queremos testar a hip\u00f3tese: H_0: \\mu \\le \\mu_0 \\implies \\Omega_0 = \\{(x,y) \\in \\mathbb{R}^2 | x \\le \\mu_0 \\text{ e } y > 0\\} H_1: \\mu > \\mu_0 \\implies \\Omega_1 = \\{(x,y) \\in \\mathbb{R}^2 | x > \\mu_0 \\text{ e } y > 0\\} Sabemos que U = n^{1/2}\\frac{\\bar{X}_n - \\mu_0}{\\sigma '} \u00e9 uma boa estat\u00edstica de teste e rejeitamos H_0 se U \\ge c . Essa estat\u00edstica \u00e9 interessante porque quando \\mu = \\mu_0, U \\sim t(n-1) . Por isso chamamos de testes t quando baseados na estat\u00edstica U . Podemos tamb\u00e9m inverter os sinais de desigualdade e rejeitar H_0 quando U \\le c . from pandas import DataFrame from scipy.stats import t import matplotlib.pyplot as plt import seaborn as sns sns.set() %matplotlib notebook mu0 = 10 # Vamos escolher mu e sigma de forma aleat\u00f3ria, mas n\u00e3o significa que \u00e9 uma vari\u00e1vel aleat\u00f3ria. n = 20 Distibui\u00e7\u00e3o de U Vamos gerar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o de U para um determinado \\mu . U_values = {} for i in range(6): mu = ro.normal(mu0, 1) if i < 5 else 10 sigma = ro.exponential(mu0) key = 'mu = {}, sigma = {}'.format(np.round(mu,2), np.round(sigma,2)) U_values[key] = np.zeros(10000) for j in range(10000): X = ro.normal(mu, sigma, size = n) U = np.sqrt(n)*(np.mean(X) - mu0)/np.std(X, ddof = 1) U_values[key][j] = U U_values = DataFrame(U_values) fig, ax = plt.subplots(figsize = (10,6)) sns.kdeplot(data = U_values, ax = ax) ax.set_title('Distribui\u00e7\u00e3o aproximada de U') plt.show() Teorema Seja c o 1 - \\alpha_0 quartil da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Ent\u00e3o, segundo o teste citado acima, a fun\u00e7\u00e3o poder tem as seguintes propriedades: \\pi(\\mu, \\sigma^2|\\delta) = \\alpha_0 , quando \\mu = \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) < \\alpha_0 , quando \\mu < \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) > \\alpha_0 , quando \\mu > \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) \\to 0 , quando \\mu \\to -\\infty . \\pi(\\mu, \\sigma^2|\\delta) \\to 1 , quando \\mu \\to \\infty . O teste tamb\u00e9m \u00e9 n\u00e3o enviesado como consequ\u00eancia. P-valores para testes t Seja u a estat\u00edstica U quando observada. Seja T_{n-1}(\\cdot) a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Ent\u00e3o o p-valor para H_0: \\mu \\leq \\mu_0 \u00e9 1 - T_{n-1}(u) , enquanto o p-valor para H_0: \\mu \\ge \\mu_0 \u00e9 T_{n-1}(u) . Distribui\u00e7\u00e3o t n\u00e3o central O objetivo \u00e9 encontrar a distribui\u00e7\u00e3o de U mesmo quando \\mu \\neq \\mu_0 . Seja W e Y vari\u00e1veis alet\u00f3rias independentes com distribui\u00e7\u00e3o N(\\psi, 1) e \\chi^2(m) , respectivamente. Ent\u00e3o X = \\frac{W}{\\left(\\frac{Y}{m}\\right)^{1/2}} tem distribui\u00e7\u00e3o t n\u00e3o central com m graus de liberdade e n\u00e3o centralidade \\psi . Denotaremos T_m(x|\\psi) a cdf dessa distribui\u00e7\u00e3o. Teorema (Fun\u00e7\u00e3o Poder) Seja X_1, ..., X_n amostra aleat\u00f3ria de N(\\mu,\\sigma^2) . A distribui\u00e7\u00e3o de U \u00e9 t n\u00e3o central com n-1 graus de liberdade e par\u00e2metro de n\u00e3o centralidade \\psi = n^{1/2}(\\mu - \\mu_0)/\\sigma ( Observe que isso ocorre porque dividimos o numerador e o denominador por \\sigma . Al\u00e9m disso, note que X n\u00e3o \u00e9 uma quantidade pivotal, dado que sua distribui\u00e7\u00e3o depende de par\u00e2metros desconhecidos ) Suponha que o procedimento \\delta rejeita H_0: \\mu \\le \\mu_0 se U \\ge c . Ent\u00e3o a fun\u00e7\u00e3o poder \u00e9 \\pi(\\mu,\\sigma^2|\\delta) = 1 - T_{n-1}(c,\\psi) Se \\delta ' rejeita H_0: \\mu \\ge \\mu_0 se U \\le c . Ent\u00e3o a fun\u00e7\u00e3o poder \u00e9 \\pi(\\mu,\\sigma^2|\\delta) = T_{n-1}(c,\\psi) from scipy.stats import nct #noncentral t dsitribution from matplotlib import animation from IPython.display import HTML import warnings warnings.filterwarnings('ignore') n = 10 mu0 = 5 sigma = 2 psi = lambda mu: np.sqrt(n)*(mu - mu0)/sigma X = nct(df = n-1, nc = psi(-20)) Vamos ver o que acontece quando variamos \\mu . Nesse caso -20 \\leq \\mu \\geq 20 . fig, ax = plt.subplots() x = np.linspace(X.ppf(0.01), X.ppf(0.99), 100) line, = ax.plot(x, X.pdf(x), 'r-', lw=5, alpha=0.6) ax.set_xlim((-60,60)) ax.set_ylim((0,0.3)) ax.set_title('t n\u00e3o central') def animate(i,n): x = np.linspace(-60, 60, 100) line.set_data(x, nct.pdf(x, df = n-1, nc = psi(i-20))) return line, HTML(animation.FuncAnimation(fig, animate, frames = 40, interval = 100, fargs=(n,), repeat = False).to_html5_video()) Your browser does not support the video tag. Alternativa Bilateral Tome agora a hip\u00f3tese H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0 Podemos usar a mesma estat\u00edstica U , mas agora que temos dois lados, vamos fazer o seguinte processo (vou construir de forma intuitiva, no livro tem uma formaliza\u00e7\u00e3o): O procedimento de teste \u00e9 do tipo: Rejeitamos H_0 se U \\le c_1 ou U \\ge c_2 . Vamos considerar c_1 = -c e c_2 = c , para simplificar. Seja \\alpha_0 o tamanho do teste, isto \u00e9, a probabilidade de rejeitarmos a hip\u00f3tese nula quando \\mu = \\mu_0 . Quando \\mu = \\mu_0 , U tem distribui\u00e7\u00e3o t com n-1 graus de liberdade. Assim: P(|U| \\ge c|\\mu = \\mu_0) = \\alpha_0 = P(U \\le -c) + P(U \\ge c) \\overset{simetria}{=} 2P( U \\ge c) = 2(1 - P(U \\le c)) n = 20 alpha0 = 0.05 c = t.ppf(df = n-1, q = 1 - alpha0/2) X = t(df = n-1) x = np.arange(-5,5,0.1) plt.plot(x, X.pdf(x)) plt.fill_between(x[(x < -c)], X.pdf(x[(x < -c)]), color = 'blue') plt.fill_between(x[(x > c)], X.pdf(x[(x > c)]), color = 'blue') plt.title('Distribui\u00e7\u00e3o de U e Regi\u00e3o de Rejei\u00e7\u00e3o') plt.show() Fun\u00e7\u00e3o Poder \\pi(\\mu,\\sigma^2,|\\delta) = T_{n-1}(-x|\\psi) + 1 - T_{n-1}(c|\\psi) P-valor Seja u o valor observado da vari\u00e1vel U . Vamos lembrar que o p-valor \u00e9 o menor tamanho \\alpha_0 tal que se rejeita a hip\u00f3tese com esse valor observado. Como s\u00f3 rejeitamos se: |u| \\ge c = T_{n-1}^{-1}(1 - \\alpha_0/2) \\implies \\alpha_0 \\ge 2 - 2T_{n-1}(|u|) Logo o p-valor \u00e9 2 - 2T_{n-1}(|u|) . Comparando m\u00e9dias de duas normais Assumimos que X = (X_1,...,X_m) \u00e9 uma amostra da distribui\u00e7\u00e3o normal com m\u00e9dia \\mu_1 e vari\u00e2ncia \\sigma^2 , enquanto Y = (Y_1, ..., Y_n) \u00e9 normal com m\u00e9dia \\mu_2 e vari\u00e2ncia \\sigma^2 . Estamos interessados no teste H_0: \\mu_1 \\le \\mu_2 H_1: \\mu_1 > \\mu_2 A fun\u00e7\u00e3o poder \u00e9 dada por \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) . A discuss\u00e3o quando as normais tem diferentes normais ser\u00e1 postergada. Defina S_x = \\sum_{i=1}^m (X_i - \\bar{X}_m)^2 S_y = \\sum_{i=1}^n (Y_i - \\bar{Y}_n)^2 U = \\frac{(m + n - 2)^{1/2}(\\bar{X}_m - \\bar{Y}_n)}{\\left(\\frac{1}{n} + \\frac{1}{m}\\right)^{1/2}(S_x^2 + S_y^2)^{1/2}} A distribui\u00e7\u00e3o: U \\sim t com m + n - 2 graus de liberdade, com par\u00e2metro de n\u00e3o centralidade \\psi= \\frac{\\mu_1 - \\mu_2}{\\sigma\\left(\\frac{1}{m} + \\frac{1}{n}\\right)^{1/2}} Note que se \\mu_1 = \\mu_2 , U \u00e9 uma distribui\u00e7\u00e3o t padr\u00e3o. Fun\u00e7\u00e3o Poder Considere o procedimento de teste \\delta que rejeite H_0 se U \\ge T_{m+n-2}^{-1}(1 - \\alpha_0) . Assim: \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) = \\alpha_0 , quando \\mu_1 = \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) < \\alpha_0 , quando \\mu_1 < \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) > \\alpha_0 , quando \\mu_1 > \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) \\to 0 , quando \\mu_1 - \\mu_2 \\to -\\infty . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) \\to 1 , quando \\mu_1 - \\mu_2 \\to \\infty . Al\u00e9m do mais o teste \u00e9 n\u00e3o enviesado. P-valor Depois de termos observado as amostras, seja u a estat\u00edstica observada de U . O p-valor da hip\u00f3tese \u00e9 1 - T_{m+n-2}(u) . Equivalentemente com o teste t do item 3, podemos expressar tudo com a hip\u00f3tese bilateral e s\u00f3 altera o graude liberade quando comparado com o teste t anterior. Vari\u00e2ncias diferentes Raz\u00e3o entre as vari\u00e2ncias \u00e9 conhecida Suponha que se as vari\u00e2ncias de X e Y s\u00e3o \\sigma_1^2 e \\sigma_2^2 e que \\sigma^2_2 = k\\sigma^2_1, k > 0 . Ent\u00e3o podemos usar a estat\u00edstica U = \\frac{(m + n - 2)^{1/2}(\\bar{X}_m - \\bar{Y}_n)}{\\left(\\frac{1}{n} + \\frac{k}{m}\\right)^{1/2}(S_x^2 + \\frac{S_y^2}{k})^{1/2}} Problema de Behrens-Fisher Quando os 4 par\u00e2metros das normais s\u00e3o desconhecidos, t\u00e3o pouco a raz\u00e3o de vari\u00e2ncias, nem a estat\u00edstica de raz\u00e3o de verossimilhan\u00e7a tem distribui\u00e7\u00e3o conhecida. Algumas tentativas j\u00e1 foram feitas, como Welch e outros . Comparando vari\u00e2ncias de duas Normais Assumimos que X = (X_1,...,X_m) \u00e9 uma amostra da distribui\u00e7\u00e3o normal com m\u00e9dia \\mu_1 e vari\u00e2ncia \\sigma^2 , enquanto Y = (Y_1, ..., Y_n) \u00e9 normal com m\u00e9dia \\mu_2 e vari\u00e2ncia \\sigma^2 . Estamos interessados no teste H_0: \\sigma_1^2 \\le \\sigma_2^2 H_1: \\sigma_1^2 > \\sigma_2^2 A fun\u00e7\u00e3o poder \u00e9 dada por \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) . COnsidere S_x^2 e S_y^2 definidos anteriormente. Ent\u00e3o temos que S_x^2/(m-1) \u00e9 estimador para \\sigma_1^2 , enquanto S_y^2/(n-1) \u00e9 estimador para \\sigma_2^2 . Defina V = \\frac{S_x^2/(m-1)}{S_y^2/(n-1)} Rejeitaremos X_0 se V \\ge c , onde c ser\u00e1 escolhido para que esse teste tenha n\u00edvel de signific\u00e2ncia \\alpha_0 . Esse teste \u00e9 chamado de teste F, pois a distribui\u00e7\u00e3o de (\\sigma_1^2/\\sigma_2^2)V \u00e9 F com par\u00e2metros m-1 e n-1 . Em particular se \\sigma_1^2 = \\sigma_2^2 , V tem distribui\u00e7\u00e3o F. Onde a distribui\u00e7\u00e3o F \u00e9 descrita aqui . Fun\u00e7\u00e3o Poder Considere o procedimento de teste \\delta que rejeite H_0 se V \\ge F_{m-1,n-1}^{-1}(1 - \\alpha_0) . Assim: \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) = 1 - F_{m-1,n-1}(\\frac{\\sigma_2^2}{\\sigma_1^2}c) \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) = \\alpha_0 , quando \\sigma_1^2 = \\sigma^2_2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) < \\alpha_0 , quando \\sigma_1^2 < \\sigma_2^2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) > \\alpha_0 , quando \\sigma^2_1 > \\sigma_2^2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) \\to 0 , quando \\sigma_1^2/\\sigma_2^2 \\to 0 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) \\to 1 , quando \\sigma_1^2/\\sigma_2^2 \\to \\infty . Al\u00e9m do mais o teste \u00e9 n\u00e3o enviesado. P-valor Depois de termos observado as amostras, seja v a estat\u00edstica observada de V . O p-valor da hip\u00f3tese \u00e9 1 - F_{m-1,n-1}(v) .","title":"Teste de Hip\u00f3teses II"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#teste-de-hipoteses-ii","text":"Nesse notebook veremos: Teste de Hip\u00f3teses Simples Hip\u00f3tese Alternativa Bilateral Teste T Comparando m\u00e9dias de duas Normais Comparando vari\u00e2ncias de duas Normais","title":"Teste de Hip\u00f3teses II"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#teste-de-hipotese-simples","text":"O objetivo \u00e9 considerar se um vetor de observa\u00e7\u00f5es vem de uma entre duas observa\u00e7\u00f5es. Nesse caso o espa\u00e7o \\Omega \u00e9 formado por dois pontos, e n\u00e3o \u00e9 um espa\u00e7o de par\u00e2metros, mas espa\u00e7o de distribui\u00e7\u00f5es, em particular dessas duas distribui\u00e7\u00f5es. Isto \u00e9, vamos assumir que X = (X_1, ..., X_n) vem de f_0(x) ou f_1(x) . Assim \\Omega = \\{\\theta_0, \\theta_1\\} e \\theta = \\theta_i se os dados tem distribui\u00e7\u00e3o f_i(x), i = 0,1 . Vamos denotar: \\alpha(\\delta) = P(\\text{Rejeitar} H_0|\\theta = \\theta_0) = P(\\text{Erro I}) \\beta(\\delta) = P(\\text{N\u00e3o rejeitar} H_0|\\theta = \\theta_1) = P(\\text{Erro II})","title":"Teste de Hip\u00f3tese Simples"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#teorema","text":"Seja \\delta^* o procedimento de teste que n\u00e3o rejeita H_0 se af_0(x) > bf_1(x) e rejeita se af_0(x) < bf_1(x) . Ent\u00e3o, para todo outro procedimento de teste \\delta , a\\alpha(\\delta^*) + b\\beta(\\delta^*) \\le a\\alpha(\\delta) + b\\beta(\\delta) Queremos escolher um teste que minimize essa combina\u00e7\u00e3o linear a\\alpha(\\delta) + b\\beta(\\delta) . Claro que seria \u00f3timo ter esse erro zerado, mas sabemos que existe uma esp\u00e9cie de trade off entre esses erros. Esse teorema d\u00e1 o teste necess\u00e1rio para que isso aconten\u00e7a.","title":"Teorema"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#corolario","text":"Considere as hip\u00f3teses do teorema anterior, a > 0 e b > 0 . Defina estat\u00edstica de teste raz\u00e3o de verossimilhan\u00e7a : \\Lambda(x) = \\begin{cases} \\frac{f_0(x)}{f_1(x)}, \\text{ se } f_0(x) \\le f_1(x) \\\\ 1, \\text{ caso contr\u00e1rio }. \\end{cases} Defina o procedimento de teste \\delta : Rejeita H_0 se \\Lambda(x) > a/b . Ent\u00e3o o valor de af_0(x) + bf_1(x) \u00e9 m\u00ednimo.","title":"Corol\u00e1rio"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#lema-nayman-pearson","text":"Suponha que \\delta ' tem a seguinte forma, para algum k > 0 : H_0 n\u00e3o \u00e9 rejeitada se f_1(x) < kf_0(x) e o \u00e9 quando f_1(x) > kf_0(x). Se \\delta \u00e9 outro procedimento de teste tal que \\alpha(\\delta) \\le \\alpha(\\delta ') , ent\u00e3o \\beta(\\delta) \\ge \\beta(\\delta ').","title":"Lema Nayman-Pearson"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#implementacao","text":"Vamos fazer uma simples implementa\u00e7\u00e3o de uso para esse tipo de problema. import numpy as np from scipy.stats import bernoulli, binom from scipy.optimize import brute Nesse caso, vamos fazer uma simples simula\u00e7\u00e3o, onde um par\u00e2metro de uma distribui\u00e7\u00e3o de Bernoulli pode ser p = 0.4 ou p = 0.6 . Vamos gerar essa amostra, mas sem de fato conhecer p verdadeiro. ro = np.random.RandomState(1000000) #random state p = ro.choice([0.4, 0.6]) Teremos uma amostra de tamanho n . n = 20 X = ro.binomial(1, p, size = n) Vamos utilizar o Lema Nayman-Pearson. O objetivo \u00e9 testar as seguintes hip\u00f3teses: H_0: p = 0.4 H_1: p = 0.6 Vamos fixar \\alpha_0 = 0.05 o tamanho do teste. Temos que, se y = \\sum_{i=1}^n x_i \\sim Binomial(n,p) , \\frac{f_1(x)}{f_0(x)} = \\frac{0.6^y0.4^{n-y}}{0.4^y0.6^{n-y}} = \\left(\\frac{3}{2}\\right)^y\\left(\\frac{2}{3}\\right)^{n-y} = \\left(\\frac{3}{2}\\right)^{2y - n} Assim: \\begin{split} 0.05 &= P(f_1(x) > kf_0(x)|p = 0.4) = P\\left(\\left(\\frac{3}{2}\\right)^{2y - n} > k\\right) \\\\ &= P\\left(2y - n > \\frac{\\log(k)}{\\log(3/2)}\\right) \\\\ &= P\\left(y > \\frac{\\log(k)}{2\\log(3/2)} + \\frac{n}{2}\\right), y \\sim Binomial(n,0.4) \\end{split} Isto \u00e9, preciso escolher k que satisfa\u00e7a essa rela\u00e7\u00e3o. Vamos calcular k numericamente utilizando um m\u00e9todo de otimiza\u00e7\u00e3o por bruta for\u00e7a (s\u00e3o poucas as op\u00e7\u00f5es). Como n\u00e3o queremos que seja marior do que 0.05, precisamos colocar peso para que n\u00e3o seja. Veja que existem v\u00e1rios valores de k que satisfazem isso. alpha0 = 0.05 Y = binom(n = n, p = 0.4) func = lambda k, n: np.abs(0.95 - Y.cdf((1/2)*np.log(k)/np.log(3/2) + n/2)) + \\ 10*(0.95 > Y.cdf((1/2)*np.log(k)/np.log(3/2) + n/2)) k = brute(func, ranges = (slice(1,20,1),), args = (n,))[0] k 6.0 Por esse motivo, vamos tomar k=6 . Pela Lema de Neyman Pearson, esse teste \u00e9 o que minimiza o Erro do Tipo II. Vamos ver se rejeitamos ou n\u00e3o a hip\u00f3tese nula baseado nos dados obtidos. f0 = lambda x: 0.4**(sum(x))*0.6**(len(x) - sum(x)) f1 = lambda x: 0.6**(sum(x))*0.4**(len(x) - sum(x)) if f1(X) > k*f0(X): print(r'Rejeitamos H0.') else: print(r'N\u00e3o rejeitamos H0.') N\u00e3o rejeitamos H0. Vamos ver quem \u00e9 p , ent\u00e3o. print('O valor de p \u00e9 .... ') print(p) O valor de p \u00e9 .... 0.4 Fizemos bem em n\u00e3o rejeitar a hip\u00f3tese nula!","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#hipotese-alternativa-bilateral","text":"Seja X = (X_1, ..., X_n) uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o normal com m\u00e9dia \\mu desconhecida e vari\u00e2ncia \\sigma^2 conhecida e queremos testar a hip\u00f3tese H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0 Como \\bar{X}_n \u00e9 um estimador consistente de \\mu , faz sentido rejeitar a hip\u00f3tese nula quando a m\u00e9dia amostral se afasta de \\mu_0 . Para isso, vamos escolher c_1, c_2 de forma que P(\\bar{X}_n \\leq c_1|\\mu = \\mu_0) + P(\\bar{X}_n \\geq c_2|\\mu = \\mu_0) = \\alpha_0 \\Rightarrow P\\left(Z \\leq \\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) + P\\left(Z \\geq \\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = \\alpha_0 \\Rightarrow \\Phi\\left(\\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) + 1 - \\Phi\\left(\\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = \\alpha_0 \\Rightarrow \\Phi\\left(\\sqrt{n}\\frac{c_1 - \\mu_0}{\\sigma}\\right) = \\alpha_1 \\text{ e } \\Phi\\left(\\sqrt{n}\\frac{c_2 - \\mu_0}{\\sigma}\\right) = 1 - \\alpha_2, \\text{ com } \\alpha_1 + \\alpha_2 = \\alpha_0 Observa\u00e7\u00e3o: \\bar{X}_n \\sim N(\\mu, \\sigma^2/n) \\Rightarrow Z = \\sqrt{n}\\frac{\\bar{X}_n - \\mu}{\\sigma} \\sim N(0,1) Observa\u00e7\u00e3o 2: No c\u00e1lculo substituimos \\mu por \\mu_0 , porque estamos \"condicionando\" no conhecimento deles serem iguais. Isto \u00e9, queremos que o tamanho do teste seja \\alpha_0 , lembrando que o tamanho do teste \u00e9 o supremo das probabilidades de se rejeitar a hip\u00f3tese nula quando ela \u00e9 verdadeira.","title":"Hip\u00f3tese Alternativa Bilateral"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#teste-t","text":"Suponha que (X_1,...,X_n) \u00e9 uma amostra aleat\u00f3ria da distribui\u00e7\u00e3o N(\\mu,\\sigma^2) , com par\u00e2metros desconhecidos e queremos testar a hip\u00f3tese: H_0: \\mu \\le \\mu_0 \\implies \\Omega_0 = \\{(x,y) \\in \\mathbb{R}^2 | x \\le \\mu_0 \\text{ e } y > 0\\} H_1: \\mu > \\mu_0 \\implies \\Omega_1 = \\{(x,y) \\in \\mathbb{R}^2 | x > \\mu_0 \\text{ e } y > 0\\} Sabemos que U = n^{1/2}\\frac{\\bar{X}_n - \\mu_0}{\\sigma '} \u00e9 uma boa estat\u00edstica de teste e rejeitamos H_0 se U \\ge c . Essa estat\u00edstica \u00e9 interessante porque quando \\mu = \\mu_0, U \\sim t(n-1) . Por isso chamamos de testes t quando baseados na estat\u00edstica U . Podemos tamb\u00e9m inverter os sinais de desigualdade e rejeitar H_0 quando U \\le c . from pandas import DataFrame from scipy.stats import t import matplotlib.pyplot as plt import seaborn as sns sns.set() %matplotlib notebook mu0 = 10 # Vamos escolher mu e sigma de forma aleat\u00f3ria, mas n\u00e3o significa que \u00e9 uma vari\u00e1vel aleat\u00f3ria. n = 20","title":"Teste t"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#distibuicao-de-u","text":"Vamos gerar uma aproxima\u00e7\u00e3o para a distribui\u00e7\u00e3o de U para um determinado \\mu . U_values = {} for i in range(6): mu = ro.normal(mu0, 1) if i < 5 else 10 sigma = ro.exponential(mu0) key = 'mu = {}, sigma = {}'.format(np.round(mu,2), np.round(sigma,2)) U_values[key] = np.zeros(10000) for j in range(10000): X = ro.normal(mu, sigma, size = n) U = np.sqrt(n)*(np.mean(X) - mu0)/np.std(X, ddof = 1) U_values[key][j] = U U_values = DataFrame(U_values) fig, ax = plt.subplots(figsize = (10,6)) sns.kdeplot(data = U_values, ax = ax) ax.set_title('Distribui\u00e7\u00e3o aproximada de U') plt.show()","title":"Distibui\u00e7\u00e3o de U"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#teorema_1","text":"Seja c o 1 - \\alpha_0 quartil da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Ent\u00e3o, segundo o teste citado acima, a fun\u00e7\u00e3o poder tem as seguintes propriedades: \\pi(\\mu, \\sigma^2|\\delta) = \\alpha_0 , quando \\mu = \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) < \\alpha_0 , quando \\mu < \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) > \\alpha_0 , quando \\mu > \\mu_0 . \\pi(\\mu, \\sigma^2|\\delta) \\to 0 , quando \\mu \\to -\\infty . \\pi(\\mu, \\sigma^2|\\delta) \\to 1 , quando \\mu \\to \\infty . O teste tamb\u00e9m \u00e9 n\u00e3o enviesado como consequ\u00eancia.","title":"Teorema"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#p-valores-para-testes-t","text":"Seja u a estat\u00edstica U quando observada. Seja T_{n-1}(\\cdot) a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Ent\u00e3o o p-valor para H_0: \\mu \\leq \\mu_0 \u00e9 1 - T_{n-1}(u) , enquanto o p-valor para H_0: \\mu \\ge \\mu_0 \u00e9 T_{n-1}(u) .","title":"P-valores para testes t"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#distribuicao-t-nao-central","text":"O objetivo \u00e9 encontrar a distribui\u00e7\u00e3o de U mesmo quando \\mu \\neq \\mu_0 . Seja W e Y vari\u00e1veis alet\u00f3rias independentes com distribui\u00e7\u00e3o N(\\psi, 1) e \\chi^2(m) , respectivamente. Ent\u00e3o X = \\frac{W}{\\left(\\frac{Y}{m}\\right)^{1/2}} tem distribui\u00e7\u00e3o t n\u00e3o central com m graus de liberdade e n\u00e3o centralidade \\psi . Denotaremos T_m(x|\\psi) a cdf dessa distribui\u00e7\u00e3o.","title":"Distribui\u00e7\u00e3o t n\u00e3o central"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#teorema-funcao-poder","text":"Seja X_1, ..., X_n amostra aleat\u00f3ria de N(\\mu,\\sigma^2) . A distribui\u00e7\u00e3o de U \u00e9 t n\u00e3o central com n-1 graus de liberdade e par\u00e2metro de n\u00e3o centralidade \\psi = n^{1/2}(\\mu - \\mu_0)/\\sigma ( Observe que isso ocorre porque dividimos o numerador e o denominador por \\sigma . Al\u00e9m disso, note que X n\u00e3o \u00e9 uma quantidade pivotal, dado que sua distribui\u00e7\u00e3o depende de par\u00e2metros desconhecidos ) Suponha que o procedimento \\delta rejeita H_0: \\mu \\le \\mu_0 se U \\ge c . Ent\u00e3o a fun\u00e7\u00e3o poder \u00e9 \\pi(\\mu,\\sigma^2|\\delta) = 1 - T_{n-1}(c,\\psi) Se \\delta ' rejeita H_0: \\mu \\ge \\mu_0 se U \\le c . Ent\u00e3o a fun\u00e7\u00e3o poder \u00e9 \\pi(\\mu,\\sigma^2|\\delta) = T_{n-1}(c,\\psi) from scipy.stats import nct #noncentral t dsitribution from matplotlib import animation from IPython.display import HTML import warnings warnings.filterwarnings('ignore') n = 10 mu0 = 5 sigma = 2 psi = lambda mu: np.sqrt(n)*(mu - mu0)/sigma X = nct(df = n-1, nc = psi(-20)) Vamos ver o que acontece quando variamos \\mu . Nesse caso -20 \\leq \\mu \\geq 20 . fig, ax = plt.subplots() x = np.linspace(X.ppf(0.01), X.ppf(0.99), 100) line, = ax.plot(x, X.pdf(x), 'r-', lw=5, alpha=0.6) ax.set_xlim((-60,60)) ax.set_ylim((0,0.3)) ax.set_title('t n\u00e3o central') def animate(i,n): x = np.linspace(-60, 60, 100) line.set_data(x, nct.pdf(x, df = n-1, nc = psi(i-20))) return line, HTML(animation.FuncAnimation(fig, animate, frames = 40, interval = 100, fargs=(n,), repeat = False).to_html5_video()) Your browser does not support the video tag.","title":"Teorema  (Fun\u00e7\u00e3o Poder)"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#alternativa-bilateral","text":"Tome agora a hip\u00f3tese H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0 Podemos usar a mesma estat\u00edstica U , mas agora que temos dois lados, vamos fazer o seguinte processo (vou construir de forma intuitiva, no livro tem uma formaliza\u00e7\u00e3o): O procedimento de teste \u00e9 do tipo: Rejeitamos H_0 se U \\le c_1 ou U \\ge c_2 . Vamos considerar c_1 = -c e c_2 = c , para simplificar. Seja \\alpha_0 o tamanho do teste, isto \u00e9, a probabilidade de rejeitarmos a hip\u00f3tese nula quando \\mu = \\mu_0 . Quando \\mu = \\mu_0 , U tem distribui\u00e7\u00e3o t com n-1 graus de liberdade. Assim: P(|U| \\ge c|\\mu = \\mu_0) = \\alpha_0 = P(U \\le -c) + P(U \\ge c) \\overset{simetria}{=} 2P( U \\ge c) = 2(1 - P(U \\le c)) n = 20 alpha0 = 0.05 c = t.ppf(df = n-1, q = 1 - alpha0/2) X = t(df = n-1) x = np.arange(-5,5,0.1) plt.plot(x, X.pdf(x)) plt.fill_between(x[(x < -c)], X.pdf(x[(x < -c)]), color = 'blue') plt.fill_between(x[(x > c)], X.pdf(x[(x > c)]), color = 'blue') plt.title('Distribui\u00e7\u00e3o de U e Regi\u00e3o de Rejei\u00e7\u00e3o') plt.show()","title":"Alternativa Bilateral"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#funcao-poder","text":"\\pi(\\mu,\\sigma^2,|\\delta) = T_{n-1}(-x|\\psi) + 1 - T_{n-1}(c|\\psi)","title":"Fun\u00e7\u00e3o Poder"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#p-valor","text":"Seja u o valor observado da vari\u00e1vel U . Vamos lembrar que o p-valor \u00e9 o menor tamanho \\alpha_0 tal que se rejeita a hip\u00f3tese com esse valor observado. Como s\u00f3 rejeitamos se: |u| \\ge c = T_{n-1}^{-1}(1 - \\alpha_0/2) \\implies \\alpha_0 \\ge 2 - 2T_{n-1}(|u|) Logo o p-valor \u00e9 2 - 2T_{n-1}(|u|) .","title":"P-valor"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#comparando-medias-de-duas-normais","text":"Assumimos que X = (X_1,...,X_m) \u00e9 uma amostra da distribui\u00e7\u00e3o normal com m\u00e9dia \\mu_1 e vari\u00e2ncia \\sigma^2 , enquanto Y = (Y_1, ..., Y_n) \u00e9 normal com m\u00e9dia \\mu_2 e vari\u00e2ncia \\sigma^2 . Estamos interessados no teste H_0: \\mu_1 \\le \\mu_2 H_1: \\mu_1 > \\mu_2 A fun\u00e7\u00e3o poder \u00e9 dada por \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) . A discuss\u00e3o quando as normais tem diferentes normais ser\u00e1 postergada. Defina S_x = \\sum_{i=1}^m (X_i - \\bar{X}_m)^2 S_y = \\sum_{i=1}^n (Y_i - \\bar{Y}_n)^2 U = \\frac{(m + n - 2)^{1/2}(\\bar{X}_m - \\bar{Y}_n)}{\\left(\\frac{1}{n} + \\frac{1}{m}\\right)^{1/2}(S_x^2 + S_y^2)^{1/2}} A distribui\u00e7\u00e3o: U \\sim t com m + n - 2 graus de liberdade, com par\u00e2metro de n\u00e3o centralidade \\psi= \\frac{\\mu_1 - \\mu_2}{\\sigma\\left(\\frac{1}{m} + \\frac{1}{n}\\right)^{1/2}} Note que se \\mu_1 = \\mu_2 , U \u00e9 uma distribui\u00e7\u00e3o t padr\u00e3o.","title":"Comparando m\u00e9dias de duas normais"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#funcao-poder_1","text":"Considere o procedimento de teste \\delta que rejeite H_0 se U \\ge T_{m+n-2}^{-1}(1 - \\alpha_0) . Assim: \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) = \\alpha_0 , quando \\mu_1 = \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) < \\alpha_0 , quando \\mu_1 < \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) > \\alpha_0 , quando \\mu_1 > \\mu_2 . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) \\to 0 , quando \\mu_1 - \\mu_2 \\to -\\infty . \\pi(\\mu_1, \\mu_2, \\sigma^2|\\delta) \\to 1 , quando \\mu_1 - \\mu_2 \\to \\infty . Al\u00e9m do mais o teste \u00e9 n\u00e3o enviesado.","title":"Fun\u00e7\u00e3o Poder"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#p-valor_1","text":"Depois de termos observado as amostras, seja u a estat\u00edstica observada de U . O p-valor da hip\u00f3tese \u00e9 1 - T_{m+n-2}(u) . Equivalentemente com o teste t do item 3, podemos expressar tudo com a hip\u00f3tese bilateral e s\u00f3 altera o graude liberade quando comparado com o teste t anterior.","title":"P-valor"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#variancias-diferentes","text":"","title":"Vari\u00e2ncias diferentes"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#razao-entre-as-variancias-e-conhecida","text":"Suponha que se as vari\u00e2ncias de X e Y s\u00e3o \\sigma_1^2 e \\sigma_2^2 e que \\sigma^2_2 = k\\sigma^2_1, k > 0 . Ent\u00e3o podemos usar a estat\u00edstica U = \\frac{(m + n - 2)^{1/2}(\\bar{X}_m - \\bar{Y}_n)}{\\left(\\frac{1}{n} + \\frac{k}{m}\\right)^{1/2}(S_x^2 + \\frac{S_y^2}{k})^{1/2}}","title":"Raz\u00e3o entre as vari\u00e2ncias \u00e9 conhecida"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#problema-de-behrens-fisher","text":"Quando os 4 par\u00e2metros das normais s\u00e3o desconhecidos, t\u00e3o pouco a raz\u00e3o de vari\u00e2ncias, nem a estat\u00edstica de raz\u00e3o de verossimilhan\u00e7a tem distribui\u00e7\u00e3o conhecida. Algumas tentativas j\u00e1 foram feitas, como Welch e outros .","title":"Problema de Behrens-Fisher"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#comparando-variancias-de-duas-normais","text":"Assumimos que X = (X_1,...,X_m) \u00e9 uma amostra da distribui\u00e7\u00e3o normal com m\u00e9dia \\mu_1 e vari\u00e2ncia \\sigma^2 , enquanto Y = (Y_1, ..., Y_n) \u00e9 normal com m\u00e9dia \\mu_2 e vari\u00e2ncia \\sigma^2 . Estamos interessados no teste H_0: \\sigma_1^2 \\le \\sigma_2^2 H_1: \\sigma_1^2 > \\sigma_2^2 A fun\u00e7\u00e3o poder \u00e9 dada por \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) . COnsidere S_x^2 e S_y^2 definidos anteriormente. Ent\u00e3o temos que S_x^2/(m-1) \u00e9 estimador para \\sigma_1^2 , enquanto S_y^2/(n-1) \u00e9 estimador para \\sigma_2^2 . Defina V = \\frac{S_x^2/(m-1)}{S_y^2/(n-1)} Rejeitaremos X_0 se V \\ge c , onde c ser\u00e1 escolhido para que esse teste tenha n\u00edvel de signific\u00e2ncia \\alpha_0 . Esse teste \u00e9 chamado de teste F, pois a distribui\u00e7\u00e3o de (\\sigma_1^2/\\sigma_2^2)V \u00e9 F com par\u00e2metros m-1 e n-1 . Em particular se \\sigma_1^2 = \\sigma_2^2 , V tem distribui\u00e7\u00e3o F. Onde a distribui\u00e7\u00e3o F \u00e9 descrita aqui .","title":"Comparando vari\u00e2ncias de duas Normais"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#funcao-poder_2","text":"Considere o procedimento de teste \\delta que rejeite H_0 se V \\ge F_{m-1,n-1}^{-1}(1 - \\alpha_0) . Assim: \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) = 1 - F_{m-1,n-1}(\\frac{\\sigma_2^2}{\\sigma_1^2}c) \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) = \\alpha_0 , quando \\sigma_1^2 = \\sigma^2_2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) < \\alpha_0 , quando \\sigma_1^2 < \\sigma_2^2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) > \\alpha_0 , quando \\sigma^2_1 > \\sigma_2^2 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) \\to 0 , quando \\sigma_1^2/\\sigma_2^2 \\to 0 . \\pi(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2|\\delta) \\to 1 , quando \\sigma_1^2/\\sigma_2^2 \\to \\infty . Al\u00e9m do mais o teste \u00e9 n\u00e3o enviesado.","title":"Fun\u00e7\u00e3o Poder"},{"location":"infestatistica_BSc/TestingHypothesesII/TestingHypothesesII/#p-valor_2","text":"Depois de termos observado as amostras, seja v a estat\u00edstica observada de V . O p-valor da hip\u00f3tese \u00e9 1 - F_{m-1,n-1}(v) .","title":"P-valor"},{"location":"infestatistica_BSc/TestsUniformlyPoweful/TestsUniformlyPoweful/","text":"Testes Uniformemente mais Poderosos Estamos lidando com um teste de hip\u00f3teses com as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n de uma distribui\u00e7\u00e3o parametrizada em \\theta desconhecido. H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Assumiremos que \\Omega_1 n\u00e3o \u00e9 conjunto unit\u00e1rio. Tamb\u00e9m suponha que o n\u00edvel de signific\u00e2ncia do teste seja \\alpha_0 , isto \u00e9 \\pi(\\theta|\\delta) \\le \\alpha_0, \\forall \\theta \\in \\Omega_0 . Segundo essas condi\u00e7\u00f5es, queremos encontrar o procedimento de teste \\delta que tem a menor probabilidade de erro do tipo II. TODO (Descrever os principais resultados)","title":"Testes Uniformemente mais Poderosos"},{"location":"infestatistica_BSc/TestsUniformlyPoweful/TestsUniformlyPoweful/#testes-uniformemente-mais-poderosos","text":"Estamos lidando com um teste de hip\u00f3teses com as vari\u00e1veis aleat\u00f3rias X_1, ..., X_n de uma distribui\u00e7\u00e3o parametrizada em \\theta desconhecido. H_0: \\theta \\in \\Omega_0 H_1: \\theta \\in \\Omega_1 Assumiremos que \\Omega_1 n\u00e3o \u00e9 conjunto unit\u00e1rio. Tamb\u00e9m suponha que o n\u00edvel de signific\u00e2ncia do teste seja \\alpha_0 , isto \u00e9 \\pi(\\theta|\\delta) \\le \\alpha_0, \\forall \\theta \\in \\Omega_0 . Segundo essas condi\u00e7\u00f5es, queremos encontrar o procedimento de teste \\delta que tem a menor probabilidade de erro do tipo II. TODO (Descrever os principais resultados)","title":"Testes Uniformemente mais Poderosos"},{"location":"infestatistica_BSc/UnbiasedEstimators/UnbiasedEstimators/","text":"Estimadores n\u00e3o enviesados Defini\u00e7\u00e3o Um estimador \\delta(X) \u00e9 dito n\u00e3o enviesado para g(\\theta) se E_{\\theta}[\\delta(X)] = g(\\theta) para todo valor de \\theta . O vi\u00e9s do estimador \u00e9 definido por E_{\\theta}[\\delta(X)] - g(\\theta) . Se \\delta \u00e9 um estimador com vari\u00e2ncia finita, ent\u00e3o: R(\\theta, \\delta) = Var(\\delta) + Vi\u00e9s(\\delta)^2 Estimador n\u00e3o enviesado para vari\u00e2ncia s^2 = \\hat{\\sigma}_1^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 # Importando bibliotecas import numpy as np import pandas as pd from IPython.display import display, Math # Display latex import matplotlib.pyplot as plt %matplotlib inline np.random.seed(1000) # Garantindo reprodutibilidade Nota: Por que garantir reprodutibilidade? Reprodutibilidade \u00e9 a ideia de tornar o processo que foi feito por voc\u00ea reprodut\u00edvel por qualquer outra pessoa, para que ela possa obter os mesmos resultados seguindo os mesmos passos que voc\u00ea. Quando escolhemos um n\u00famero aleat\u00f3rio ( pseudoaleat\u00f3rio na verdade), ele muda de tempos em tempos. Mas isso vai tirar a ideia de \"garantir os mesmos resultados\". O resultado pode ser parecido, mas n\u00e3o exatamente igual. Isso \u00e9 muito importante no meio cient\u00edfico. Exemplo Vamos ver como se comporta esse estimador n\u00e3o viesado em uma popula\u00e7\u00e3o que representa o Brasil todo! Veja que eu n\u00e3o peguei dados online, porque quero TODA a popula\u00e7\u00e3o. Por isso vamos fazer uma simula\u00e7\u00e3o. A m\u00e9dia verdadeira da distribui\u00e7\u00e3o \u00e9 161,1cm e o desvio padr\u00e3o \u00e9 10cm. # Tamanho da popula\u00e7\u00e3o N = int(200e5) # Popula\u00e7\u00e3o gerada por simula\u00e7\u00e3o, usando a distribui\u00e7\u00e3o normal. population_height = pd.Series(np.random.normal(loc = 161.1, scale = 10, size = N)) Podemos ver a m\u00e9dia dessa popula\u00e7\u00e3o. population_height.mean() 161.10097546939974 O que a fun\u00e7\u00e3o var do pandas faz? Vamos comparar com o estimador trivial. ddof = 1 # Se ddof = 0, teremos a divis\u00e3o por N population_height.var(ddof = ddof) 100.02715920849081 Divindindo por N sigma_square_hat = ((population_height - population_height.mean())**2).sum()/N sigma1_square_hat = ((population_height - population_height.mean())**2).sum()/(N-1) display(Math(r'\\hat\\sigma^2 = {}'.format(sigma_square_hat))) display(Math(r'\\hat\\sigma_1^2 = {}'.format(sigma1_square_hat))) \\displaystyle \\hat\\sigma^2 = 100.02715420713488 \\displaystyle \\hat\\sigma_1^2 = 100.02715920849283 Estima\u00e7\u00e3o dos Par\u00e2metros Vamos supor que n\u00e3o conhecemos os par\u00e2metros da nossa popula\u00e7\u00e3o e podemos conhecer apenas uma amostra aleat\u00f3ria dela. Vamos fazer 500 dessas simula\u00e7\u00f5es number_simulations = 500 sample_size = 30 sample = pd.DataFrame(population_height.sample(n = number_simulations*sample_size, replace=True, random_state = 100), columns = ['height']) reshape = sample.to_numpy().reshape((-1,30)) samples = pd.DataFrame(reshape, columns = range(0,30)) Vamos estimar a m\u00e9dia com a m\u00e9dia amostral que \u00e9 n\u00e3o viesada tamb\u00e9m! Al\u00e9m disso ela \u00e9 o MLE. Estamos estimado para cada amostra a m\u00e9dia! Se fizermos uma m\u00e9dia das m\u00e9dias, veremos que ela chegar\u00e1 pr\u00f3ximo a m\u00e9dia verdadeira. estimated_mean = samples.mean(axis = 1) #Axis = 1 faz a m\u00e9dia por linha. estimated_mean_of_means = estimated_mean.expanding().mean() fig, ax = plt.subplots(figsize = (8,5)) ax.plot(estimated_mean_of_means, label = 'Valor estimado') ax.hlines(population_height.mean(), xmin = 0, xmax = number_simulations, color = 'grey', linestyle = '--', alpha = 0.8, label = 'Valor verdadeiro') ax.grid(alpha = 0.4) ax.set_title('Estimado a m\u00e9dia verdadeira') plt.show() Vamos comparar os estimadores para a vari\u00e2ncia, o viesado e o n\u00e3o viesado. df = pd.DataFrame({'enviesado (dividido por n)': samples.var(ddof=0, axis = 1).expanding().mean(), 'nao_viesado (dividido por n - 1)': samples.var(ddof=1, axis = 1).expanding().mean(), 'verdadeiro': pd.Series(population_height.var(ddof=0), index=samples.index)}) ax = df.plot() ax.set_title('Estimadores para a vari\u00e2ncia') ax.grid(alpha = 0.4) plt.show() # Comparar com Conscist\u00eancia","title":"Estimadores n\u00e3o enviesados"},{"location":"infestatistica_BSc/UnbiasedEstimators/UnbiasedEstimators/#estimadores-nao-enviesados","text":"","title":"Estimadores n\u00e3o enviesados"},{"location":"infestatistica_BSc/UnbiasedEstimators/UnbiasedEstimators/#definicao","text":"Um estimador \\delta(X) \u00e9 dito n\u00e3o enviesado para g(\\theta) se E_{\\theta}[\\delta(X)] = g(\\theta) para todo valor de \\theta . O vi\u00e9s do estimador \u00e9 definido por E_{\\theta}[\\delta(X)] - g(\\theta) . Se \\delta \u00e9 um estimador com vari\u00e2ncia finita, ent\u00e3o: R(\\theta, \\delta) = Var(\\delta) + Vi\u00e9s(\\delta)^2","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica_BSc/UnbiasedEstimators/UnbiasedEstimators/#estimador-nao-enviesado-para-variancia","text":"s^2 = \\hat{\\sigma}_1^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 # Importando bibliotecas import numpy as np import pandas as pd from IPython.display import display, Math # Display latex import matplotlib.pyplot as plt %matplotlib inline np.random.seed(1000) # Garantindo reprodutibilidade","title":"Estimador n\u00e3o enviesado para vari\u00e2ncia"},{"location":"infestatistica_BSc/UnbiasedEstimators/UnbiasedEstimators/#nota-por-que-garantir-reprodutibilidade","text":"Reprodutibilidade \u00e9 a ideia de tornar o processo que foi feito por voc\u00ea reprodut\u00edvel por qualquer outra pessoa, para que ela possa obter os mesmos resultados seguindo os mesmos passos que voc\u00ea. Quando escolhemos um n\u00famero aleat\u00f3rio ( pseudoaleat\u00f3rio na verdade), ele muda de tempos em tempos. Mas isso vai tirar a ideia de \"garantir os mesmos resultados\". O resultado pode ser parecido, mas n\u00e3o exatamente igual. Isso \u00e9 muito importante no meio cient\u00edfico.","title":"Nota: Por que garantir reprodutibilidade?"},{"location":"infestatistica_BSc/UnbiasedEstimators/UnbiasedEstimators/#exemplo","text":"Vamos ver como se comporta esse estimador n\u00e3o viesado em uma popula\u00e7\u00e3o que representa o Brasil todo! Veja que eu n\u00e3o peguei dados online, porque quero TODA a popula\u00e7\u00e3o. Por isso vamos fazer uma simula\u00e7\u00e3o. A m\u00e9dia verdadeira da distribui\u00e7\u00e3o \u00e9 161,1cm e o desvio padr\u00e3o \u00e9 10cm. # Tamanho da popula\u00e7\u00e3o N = int(200e5) # Popula\u00e7\u00e3o gerada por simula\u00e7\u00e3o, usando a distribui\u00e7\u00e3o normal. population_height = pd.Series(np.random.normal(loc = 161.1, scale = 10, size = N)) Podemos ver a m\u00e9dia dessa popula\u00e7\u00e3o. population_height.mean() 161.10097546939974 O que a fun\u00e7\u00e3o var do pandas faz? Vamos comparar com o estimador trivial. ddof = 1 # Se ddof = 0, teremos a divis\u00e3o por N population_height.var(ddof = ddof) 100.02715920849081 Divindindo por N sigma_square_hat = ((population_height - population_height.mean())**2).sum()/N sigma1_square_hat = ((population_height - population_height.mean())**2).sum()/(N-1) display(Math(r'\\hat\\sigma^2 = {}'.format(sigma_square_hat))) display(Math(r'\\hat\\sigma_1^2 = {}'.format(sigma1_square_hat))) \\displaystyle \\hat\\sigma^2 = 100.02715420713488 \\displaystyle \\hat\\sigma_1^2 = 100.02715920849283","title":"Exemplo"},{"location":"infestatistica_BSc/UnbiasedEstimators/UnbiasedEstimators/#estimacao-dos-parametros","text":"Vamos supor que n\u00e3o conhecemos os par\u00e2metros da nossa popula\u00e7\u00e3o e podemos conhecer apenas uma amostra aleat\u00f3ria dela. Vamos fazer 500 dessas simula\u00e7\u00f5es number_simulations = 500 sample_size = 30 sample = pd.DataFrame(population_height.sample(n = number_simulations*sample_size, replace=True, random_state = 100), columns = ['height']) reshape = sample.to_numpy().reshape((-1,30)) samples = pd.DataFrame(reshape, columns = range(0,30)) Vamos estimar a m\u00e9dia com a m\u00e9dia amostral que \u00e9 n\u00e3o viesada tamb\u00e9m! Al\u00e9m disso ela \u00e9 o MLE. Estamos estimado para cada amostra a m\u00e9dia! Se fizermos uma m\u00e9dia das m\u00e9dias, veremos que ela chegar\u00e1 pr\u00f3ximo a m\u00e9dia verdadeira. estimated_mean = samples.mean(axis = 1) #Axis = 1 faz a m\u00e9dia por linha. estimated_mean_of_means = estimated_mean.expanding().mean() fig, ax = plt.subplots(figsize = (8,5)) ax.plot(estimated_mean_of_means, label = 'Valor estimado') ax.hlines(population_height.mean(), xmin = 0, xmax = number_simulations, color = 'grey', linestyle = '--', alpha = 0.8, label = 'Valor verdadeiro') ax.grid(alpha = 0.4) ax.set_title('Estimado a m\u00e9dia verdadeira') plt.show() Vamos comparar os estimadores para a vari\u00e2ncia, o viesado e o n\u00e3o viesado. df = pd.DataFrame({'enviesado (dividido por n)': samples.var(ddof=0, axis = 1).expanding().mean(), 'nao_viesado (dividido por n - 1)': samples.var(ddof=1, axis = 1).expanding().mean(), 'verdadeiro': pd.Series(population_height.var(ddof=0), index=samples.index)}) ax = df.plot() ax.set_title('Estimadores para a vari\u00e2ncia') ax.grid(alpha = 0.4) plt.show() # Comparar com Conscist\u00eancia","title":"Estima\u00e7\u00e3o dos Par\u00e2metros"},{"location":"infestatistica_MSc/info/","text":"Informa\u00e7\u00f5es Gerais Monitoria de Infer\u00eancia Estat\u00edstica (Mestrado) correspondente ao per\u00edodo de 2022.4. GitHub Professor Hor\u00e1rio: Quartas-feiras, 18h. Link 18h-19h , Link 19h-20h T\u00f3picos Conceitos Introdut\u00f3rios Revis\u00e3o de probabilidade Modelo estat\u00edstico Amostras aleat\u00f3rias Fun\u00e7\u00e3o de risco Fam\u00edlia Exponencial Propriedades de estimadores Sufici\u00eancia Estima\u00e7\u00e3o pontual Informa\u00e7\u00e3o de Fisher e Cram\u00e9r-Rao Introdu\u00e7\u00e3o a grandes amostras Intervalos de confian\u00e7a Exerc\u00edcios Os exerc\u00edcios resolvidos est\u00e3o dispon\u00edveis: Prova 1 Lista T\u00f3picos Solu\u00e7\u00e3o 1 Verossimilhan\u00e7a e sufici\u00eancia 1 2 Sufici\u00eancia m\u00ednima, completude e ancilaridade 2 3 Estima\u00e7\u00e3o n\u00e3o enviesada 3 4 M\u00e9todos de estima\u00e7\u00e3o 4 5 Revis\u00e3o para a P1 5 Prova 2 Lista T\u00f3picos Solu\u00e7\u00e3o Notas Monitoria Itens discutidos Arquivo 04/07/2022 Conceitos da lista 1 ver arquivo 13/07/2022 Lista 1 ver arquivo 20/07/2022 Lista adicional Ver solu\u00e7\u00f5es 26/07/2022 Revis\u00e3o geral ver arquivo Documentos Adicionais Theory of Statistical Estimation (Ronald Fisher) : problema da estima\u00e7\u00e3o \u00e9 abordado e as t\u00e9cnicas apresentadas por Ronald Fisher. Ele se debru\u00e7a sobre estat\u00edsticas eficientes e suficientes. Mathematical Foudations Statistics (Ronald Fisher) : refer\u00eancia em estat\u00edstica com principais conceitos da mat\u00e9ria. Digital TextBook Statlect Bayesian x Frequentist Inference","title":"Infer\u00eancia Estat\u00edstica"},{"location":"infestatistica_MSc/info/#informacoes-gerais","text":"Monitoria de Infer\u00eancia Estat\u00edstica (Mestrado) correspondente ao per\u00edodo de 2022.4. GitHub Professor Hor\u00e1rio: Quartas-feiras, 18h. Link 18h-19h , Link 19h-20h","title":"Informa\u00e7\u00f5es Gerais"},{"location":"infestatistica_MSc/info/#topicos","text":"Conceitos Introdut\u00f3rios Revis\u00e3o de probabilidade Modelo estat\u00edstico Amostras aleat\u00f3rias Fun\u00e7\u00e3o de risco Fam\u00edlia Exponencial Propriedades de estimadores Sufici\u00eancia Estima\u00e7\u00e3o pontual Informa\u00e7\u00e3o de Fisher e Cram\u00e9r-Rao Introdu\u00e7\u00e3o a grandes amostras Intervalos de confian\u00e7a","title":"T\u00f3picos"},{"location":"infestatistica_MSc/info/#exercicios","text":"Os exerc\u00edcios resolvidos est\u00e3o dispon\u00edveis:","title":"Exerc\u00edcios"},{"location":"infestatistica_MSc/info/#prova-1","text":"Lista T\u00f3picos Solu\u00e7\u00e3o 1 Verossimilhan\u00e7a e sufici\u00eancia 1 2 Sufici\u00eancia m\u00ednima, completude e ancilaridade 2 3 Estima\u00e7\u00e3o n\u00e3o enviesada 3 4 M\u00e9todos de estima\u00e7\u00e3o 4 5 Revis\u00e3o para a P1 5","title":"Prova 1"},{"location":"infestatistica_MSc/info/#prova-2","text":"Lista T\u00f3picos Solu\u00e7\u00e3o","title":"Prova 2"},{"location":"infestatistica_MSc/info/#notas","text":"Monitoria Itens discutidos Arquivo 04/07/2022 Conceitos da lista 1 ver arquivo 13/07/2022 Lista 1 ver arquivo 20/07/2022 Lista adicional Ver solu\u00e7\u00f5es 26/07/2022 Revis\u00e3o geral ver arquivo","title":"Notas"},{"location":"infestatistica_MSc/info/#documentos-adicionais","text":"Theory of Statistical Estimation (Ronald Fisher) : problema da estima\u00e7\u00e3o \u00e9 abordado e as t\u00e9cnicas apresentadas por Ronald Fisher. Ele se debru\u00e7a sobre estat\u00edsticas eficientes e suficientes. Mathematical Foudations Statistics (Ronald Fisher) : refer\u00eancia em estat\u00edstica com principais conceitos da mat\u00e9ria. Digital TextBook Statlect Bayesian x Frequentist Inference","title":"Documentos Adicionais"},{"location":"infestatistica_MSc/probability/","text":"Revis\u00e3o de Probabilidade Nessa se\u00e7\u00e3o, vamos introduzir alguns conceitos chave de probabilidade e medida para a compreens\u00e3o do curso de estat\u00edstica. Medida Uma medida atribui um valor n\u00e3o negativo a subconjuntos A \\subseteq \\mathcal{X} , com certas propriedades que intuitivamente gostar\u00edamos de observar. Por exemplo, se \\mathcal{X} for um conjunto enumer\u00e1vel, como os n\u00fameros naturais, poder\u00edamos definir uma medida para A como \\mu(A) = |A| = \\text{n\u00famero de elementos de } A, conhecida como medida de contagem . No caso em que \\mathcal{X} = \\mathbb{R}^n para algum n , podemos definir a no\u00e7\u00e3o de volume de A com \\mu(A) = \\int_{\\mathbb{R}^n} I\\{\\boldsymbol{x} \\in A\\} \\, d\\boldsymbol{x}, conhecida como medida de Lebesgue . Para tornar esse conceito mais rigoroso, definimos uma \\sigma -\u00e1lgebra como Uma cole\u00e7\u00e3o de subconjuntos \\mathcal{A} de \\mathcal{X} , isto \u00e9, \\mathcal{A} \\subseteq \\mathcal{P}(\\mathcal{X}) (conjunto das partes de \\mathcal{X} ) \u00e9 uma \\sigma -\u00e1lgebra se: i) \\emptyset \\in \\mathcal{A} . ii) Se A \\in \\mathcal{A} , ent\u00e3o A^c \\in \\mathcal{A} . iii) Se A_1, A_2, \\dots \\in \\mathcal{A} , ent\u00e3o \\cup_{i=1}^{\\infty} A_i \\in \\mathcal{A} . Uma medida \u00e9 uma fun\u00e7\u00e3o \\mu : \\mathcal{A} \\to \\mathbb{R}_{+} que satisfaz a propriedade de que se a sequ\u00eancia A_1, A_2, \\dots \u00e9 disjunta dois a dois, ent\u00e3o \\mu\\left(\\cup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} \\mu(A_i), chamada de axioma da aditividade enumer\u00e1vel. Podemos definir algumas caracter\u00edsticas de interesse para medidas: \\mu \u00e9 medida finita se \\mu(\\mathcal{X}) < +\\infty . \\mu \u00e9 medida \\sigma -finita se existe sequ\u00eancia A_1, A_2, \\dots \\in \\mathcal{A} com \\mu(A_i) < +\\infty tal que \\mathcal{X} = \\cup_{i=1}^n A_i . \\mu \u00e9 medida de probabilidade se \\mu(\\mathcal{X}) = 1 . Chamamos o par (\\mathcal{X}, \\mathcal{A}) de espa\u00e7o mensur\u00e1vel e a tripla (\\mathcal{X}, \\mathcal{A}, \\mu) de espa\u00e7o medida . No caso em que \\mu \u00e9 medida de probabilidade, temos um espa\u00e7o de probabilidade . Se \\mu(N)=0 , dizemos que N tem medida nula. Se alguma propriedade \\tau vale quase sempre , queremos dizer que o conjunto em que \\tau n\u00e3o vale tem medida nula. Observa\u00e7\u00e3o: Os axiomas da probabilidade ou de Kolmogorov s\u00e3o os tr\u00eas seguintes: (I) P(A) \\ge 0 para todo evento A , (II) P(\\mathcal{X}) = 1 e (III) P(\\cup_{i=1}^n A_i) = \\sum_{i=1}^n P(A_i) para uma sequ\u00eancia disjunta. Alguns estat\u00edsticos, como deFinetti, acreditam que o axioma (III) \u00e9 muito forte e pouco auto-evidente. Para isso, sugerem a * aditividade finita** P(A \\cup B) = P(A) + P(B) quando A \\cap B = \\emptyset . Essa altera\u00e7\u00e3o, todavia, gera complica\u00e7\u00f5es que n\u00e3o, necessariamente, melhoram o entendimento dos conceitos de probabilidade.* Integra\u00e7\u00e3o Uma fun\u00e7\u00e3o f com valores reais e definida em \\mathcal{X} \u00e9 mensur\u00e1vel se o conjunto f^{-1}(B) = \\{x \\in \\mathcal{X} : f(x) \\in B\\} pertence a \\mathcal{A} para todo B conjunto de Borel . Podemos tomar B da forma (-\\infty, b) , por exemplo. Podemos mostrar que fun\u00e7\u00f5es cont\u00ednuas ou cont\u00ednuas por partes s\u00e3o mensur\u00e1veis. Podemos ent\u00e3o, definir propriedades b\u00e1sicas da integral de Lebesgue. Denote 1_A(x) = I(x \\in A) . Assim: (1) \\int 1_A \\, d\\mu = \\mu(A) . (2) Se f e g s\u00e3o fun\u00e7\u00f5es mensur\u00e1veis n\u00e3o negativas e a,b > 0 : \\int (af + bg) \\, d\\mu = a\\int f \\, d\\mu + b\\int g \\, d\\mu (3) Se 0 \\le f_1 \\le f_2 \\le \\dots e f(x) = \\lim_{n\\to\\infty} f_n(x) , ent\u00e3o \\int f \\, d\\mu = \\lim_{n\\to \\infty} \\int f_n \\, d\\mu, e essa propriedade \u00e9 o Teorema da Converg\u00eancia Mon\u00f3tona . Uma fun\u00e7\u00e3o simples \u00e9 uma fun\u00e7\u00e3o constante por partes que assume um valor finito de valores a_1, \\dots a_n . Podemos escrever uma fun\u00e7\u00e3o simples como s(x) = \\sum_{i=1}^n a_i 1_{A_i}(x), para A_1, \\dots, A_n disjuntos, e obter que \\int s \\, d\\mu = \\sum_{i=1}^n a_i \\mu(A_i). Al\u00e9m disso, um teorema bem interessante mostra que se f \u00e9 n\u00e3o negativa e mensur\u00e1vel, ent\u00e3o existe uma sequ\u00eancia de fun\u00e7\u00f5es simples f_1 \\le f_2 \\le \\dots \\le f com f = \\lim f_n . Isso mostra que qualquer fun\u00e7\u00e3o n\u00e3o negativa mensur\u00e1vel pode ser integrada. Por fim, basta ver que f(x) = f^{+}(x) - f^{-}(x) = \\max\\{0, f(x)\\} - (-\\min\\{f(x), 0\\}) \u00e9 a soma de duas fun\u00e7\u00f5es n\u00e3o negativas, e portanto, pode ser integrada atrav\u00e9s de \\int f \\, d\\mu = \\int f^+ \\, d\\mu - \\int f^{-} \\, d\\mu. Dizemos que f \u00e9 integr\u00e1vel se \\int |f| \\, d\\mu < +\\infty. Algumas consequ\u00eancias importantes: Se f = 0 quase sempre, ent\u00e3o \\int f \\, d\\mu = 0 Se f \\ge 0 e \\int f \\, d\\mu = 0 , ent\u00e3o f = 0 quase sempre. Se f = g quase sempre, ent\u00e3o \\int f \\, d\\mu = \\int g \\, d\\mu se uma das integrais existe. Se \\int 1_{(c,x)} f \\, d\\mu = 0 para todo x > c , ent\u00e3o f(x) = 0 quase sempre em x > c . Se f > g s\u00e3o integr\u00e1veis, ent\u00e3o \\int f\\, d\\mu > \\int g \\, d\\mu , a menos que \\mu seja identicamente nula. Eventos e vari\u00e1veis aleat\u00f3rias Seja (\\mathcal{X}, \\mathcal{A}, P) um espa\u00e7o de probabilidade. Dizemos que A \\in \\mathcal{A} \u00e9 um evento . Todos os poss\u00edveis desfechos de um experimento dado pelo conjunto \\mathcal{X} formam o espa\u00e7o amostral . Uma vari\u00e1vel aleat\u00f3ria \u00e9 uma fun\u00e7\u00e3o mensur\u00e1vel X : \\mathcal{X} \\to \\mathbb{R} . A distribui\u00e7\u00e3o de X \u00e9 Q , isto \u00e9, X \\sim Q quando Q(B) = P(\\{x \\in \\mathcal{X} | X(x) \\in A\\}) := P(X \\in B), isto \u00e9, Q \u00e9 definida nos conjuntos de Borel de \\mathbb{R} . A fun\u00e7\u00e3o de distribui\u00e7\u00e3o acumulada (FDA) (em ingl\u00eas, CDF) de X \u00e9 a fun\u00e7\u00e3o F_X(x) = P(X \\le x) = Q((-\\infty, x]), para x \\in \\mathbb{R} . Caracteriza\u00e7\u00e3o da FDA: A fun\u00e7\u00e3o F \u00e9 uma FDA se, e somente se, as seguintes condi\u00e7\u00f5es valem: 1) \\lim_{x \\to -\\infty} F(x) = 0 e \\lim_{x \\to \\infty} F(x) = 1 . 2) F(x) \u00e9 uma fun\u00e7\u00e3o mon\u00f3tona n\u00e3o decrescente de x . 3) F(x) \u00e9 cont\u00ednua \u00e0 direita, isto \u00e9, \\lim_{x \\to x_0^+} F(x) = F(x_0) . Al\u00e9m do mais, se P(X \\in A) = P(Y \\in A) para todo conjunto de Borel A , ent\u00e3o X e Y s\u00e3o identicamente distribu\u00eddas. Isso \u00e9 equivalente a notar que F_X(x) = F_Y(x) para todo valor de x . Densidades Uma medida \u00e9 absolutamente cont\u00ednua com respeito a outra se ela d\u00e1 volume 0 para as regi\u00f5es que a outra tamb\u00e9m d\u00e1, isto \u00e9: Sejam P e \\mu medidas definidas em (\\mathcal{X}, \\mathcal{A}) . Dizemos que P \u00e9 absolutamente cont\u00ednua com respeito a \\mu se P(A) = 0 sempre que \\mu(A) = 0 . Nesse caso, escrevemos que P \\ll \\mu . Tamb\u00e9m dizemos que \\mu domina P . Logo P \\ll \\mu \u00e9 equivalente a observar que o suporte de \\mu cont\u00e9m o suporte de P . Teorema de Radon-Nikodym: Se uma medida P finita \u00e9 absolutamente cont\u00ednua com respeito a uma medida \\mu \\sigma -finita, ent\u00e3o existe uma fun\u00e7\u00e3o mensur\u00e1vel f tal que P(A) = \\int_A f \\, d\\mu := \\int f1_A \\, d\\mu. Chamamos f de derivada Radon-Nikodym de P com respeito a \\mu , ou a densidade de P com respeito a \\mu e escrevemos f = \\frac{dP}{d\\mu}. Vejam que de fato isso generaliza a no\u00e7\u00e3o de derivada, pois pelo Teorema fundamental do C\u00e1lculo, F(x) = \\int_a^x f(t) \\, dt \\implies f = \\frac{d F}{d x}, em que nesse caso estamos tomando a medida de Lebesgue na reta como medida dominante. Conclu\u00edmos tamb\u00e9m que a densidade de P \u00e9 determinada a menos de um conjunto de medida nula. Esperan\u00e7a Se X \u00e9 uma vari\u00e1vel aleat\u00f3ria definida em (\\mathcal{X}, \\mathcal{A}, P) , o valor esperado de X \u00e9 definido como \\mathbb{E}[X] = \\int X \\, dP. Se X \\sim P_X , podemos mostrar que \\mathbb{E}[X] = \\int x dP_X(x). Al\u00e9m do mais, se Y = f(X) , ent\u00e3o \\mathbb{E}[Y] = \\int f \\, dP_X. Se P_X tem densidade p com respeito a \\mu , ent\u00e3o \\int f \\, dP_X = \\int fp\\, d\\mu. Finalmente, definimos \\operatorname{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] , \\operatorname{Cov}(X,Y) = \\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])] e \\operatorname{Cor}(X,Y) = \\operatorname{Cov}(X,Y)\\big/\\sqrt{\\operatorname{Var}(X) \\operatorname{Var}(Y)} . Vetores Aleat\u00f3rios Seja X : \\mathcal{X} \\to \\mathbb{R}^n uma fun\u00e7\u00e3o mensur\u00e1vel (no sentido dos conjuntos de Borel em \\mathbb{R}^n ). Se X \\sim P_X e P_X \u00e9 absolutamente cont\u00ednua com respeito a medida de Lebesgue em \\mathbb{R}^n com densidade p , temos que P_X(B) = \\int_{B} p(x) \\, dx. A esperan\u00e7a de X \u00e9 \\mathbb{E}[X] = \\begin{pmatrix} \\mathbb{E}[X_1] \\\\ \\vdots \\\\ \\mathbb{E}[X_n] \\end{pmatrix}. E se T : \\mathbb{R}^n \\to \\mathbb{R} \u00e9 mensur\u00e1vel, ent\u00e3o T(X) \u00e9 vari\u00e1vel aleat\u00f3ria com \\mathbb{E}[T(X)] = \\int T \\, dP_X. Medida produto Sejam (\\mathcal{X}, \\mathcal{A}, \\mu) e (\\mathcal{Y}, \\mathcal{B}, \\nu) espa\u00e7os de medida. Ent\u00e3o existe uma \u00fanica medida \\mu \\times \\nu chamada de medida produto em (\\mathcal{X} \\times \\mathcal{Y}, \\sigma(\\mathcal{A} \\times \\mathcal{B})) tal que (\\mu \\times \\nu)(A \\times B) = \\mu(A)\\nu(B), para A \\in \\mathcal{A} e B \\in \\mathcal{B} . Denotamos \\sigma(C) como a menor \\sigma -\u00e1lgebra que cont\u00e9m C . Com respeito a integra\u00e7\u00e3o, obtemos o seguinte teorema Teorema de Fubini: Se f \\ge 0 , ent\u00e3o \\int f \\, d(\\mu \\times \\nu) = \\int\\left[\\int f(x,y) \\, d\\nu(y)\\right] \\, d\\mu(x) = \\int\\left[\\int f(x,y) \\, d\\mu(x)\\right] \\, d\\nu(y). Al\u00e9m do mais, se f \\ge 0 n\u00e3o \u00e9 satisfeito, mas f \u00e9 integr\u00e1vel com respeito a \\mu \\times \\nu , a rela\u00e7\u00e3o ainda \u00e9 v\u00e1lida. Independ\u00eancia A partir da defini\u00e7\u00e3o de medida produto, podemos falar do conceito de independ\u00eancia de duas vari\u00e1veis aleat\u00f3rias. Sejam dois vetores aleat\u00f3rios X(\\cdot) \\subseteq \\mathbb{R}^n e Y(\\cdot) \\subseteq \\mathbb{R}^m . Dizemos que eles s\u00e3o independentes se P(X \\in A, Y \\in B) = P(X \\in A)P(Y \\in B), para todos os conjuntos de Borel A e B . Seja Z = (X,Y) em \\mathbb{R}^{n+m} . Temos que Z \\in A \\times B se, e s\u00f3 se, X \\in A e Y \\in B . Nesse caso, P_Z(A \\times B) = P_X(A)P_Y(B) \\implies P_Z = P_A \\times P_Y. A densidade de Z tamb\u00e9m \u00e9 dada pelo produto das densidades de X e Y pelo Teorema de Fubini. Finalmente, temos o resultado de que se X_1, \\dots, X_n s\u00e3o vetores aleat\u00f3rios independentes e se f_1, \\dots, f_n s\u00e3o fun\u00e7\u00f5es mensur\u00e1veis, ent\u00e3o f_1(X_1), \\dots, f_n(X_n) s\u00e3o independentes. Densidades conjunta e marginais A densidade conjunta de X e Y denotada por p_Z satisfaz P(Z \\in B) = \\int \\int 1_B(x,y)p_Z(x,y) \\, d\\mu(x) \\, d\\nu(y). A densidade marginal de X ( Y ) pode ser obtida integrando Y ( X ), e assim: p_X(x) = \\int p_X(x,y) \\, d\\nu(y), com respeito a \\mu . C\u00e1lculo de probabilidades A partir dos Axiomas da Probabilidade, podemos obter alguns resultados importantes: 1) P(\\emptyset) = 0 2) P(A^c) = 1 - P(A) 3) P(\\cup_{i=1}^{\\infty} A_i) \\le \\sum_{i=1}^{\\infty} P(A_i) Condicionando Considere duas vari\u00e1veis aleat\u00f3rias X e Y n\u00e3o independentes. Com isso, se observamos X=x , temos alguma informa\u00e7\u00e3o adicional sobre Y , e sua distribui\u00e7\u00e3o marginal n\u00e3o representa mais a totalidade. Considere X e Y duas vari\u00e1veis discretas definidas em \\mathcal{X} e \\mathcal{Y} , respectivamente. Definimos a probabilidade condicional de X = x_i dado Y = y_j como P(X=x_i | Y=y_j) = \\frac{P(X=x_i, Y=y_j)}{P(Y=y_j)} := p_{i|j}. \u00c9 f\u00e1cil verificar que P(\\cdot | Y=y_j) define uma medida de probabilidade em \\mathcal{X} . A m\u00e9dia condicional de uma fun\u00e7\u00e3o f de X \u00e9 dada por \\mathbb{E}[f(X) | Y=y_j] = \\sum_{i=1}^{\\infty} f(x_i) p_{i|j}. De forma geral, se P(B) > 0 , escrevemos P(A|B) = P(A\\cap B)/P(B) para eventos A e B . Mas o que fazer quando P(B) = 0 ? Em especial, isso ocorre quando falamos de vari\u00e1veis aleat\u00f3rias cont\u00ednuas, j\u00e1 que P(X=1) = 0 , por exemplo. Probabilidade condicional: Dadas vari\u00e1veis aleat\u00f3rias X e Y com \\mathbb{E}|Y| < +\\infty e um evento A , dizemos que P(A|X) \u00e9 probabilidade condicional de A dado X se \u00e9 uma vari\u00e1vel aleat\u00f3ria \\sigma(X) -mensur\u00e1vel ( \\sigma(X) \u00e9 a menor \\sigma -\u00e1lgebra em X \u00e9 mensur\u00e1vel) e, para todos os Boreis S \\subseteq \\mathbb{R} , temos \\mathbb{E}[P(A|X)1_{S}(X)] = P(A \\cap \\{X \\in S\\}). De forma similar, introduzimos o conceito de \\mathbb{E}[Y|X] , substituindo acima probabilidades por esperan\u00e7as: \\mathbb{E}[\\mathbb{E}[Y|X]1_{S}(X)] = \\mathbb{E}[Y1_S(X)]. Como consequ\u00eancia, probabilidades e esperan\u00e7as condicionais s\u00e3o definidas a menos de um conjunto de medida nula. Distribui\u00e7\u00e3o condicional: Uma fun\u00e7\u00e3o Q \u00e9 distribui\u00e7\u00e3o condicional de Y dado X=x se 1) Q_x(\\cdot) \u00e9 uma medida de probabilidade para todo x ; 2) Q_x(B) \u00e9 fun\u00e7\u00e3o mensur\u00e1vel de x para todo Borel B ; 3) P(X \\in A, Y \\in B) = \\int_A Q_x(B) \\, dP_X(x) . Escrevemos ent\u00e3o que Y|X=x \\sim Q_x . Teorema (densidade condicional): Suponha que X e Y sejam vetores aleat\u00f3rios com densidade conjunta p_Z com respeito a \\mu \\times \\nu . Seja E = \\{x : p_X(x) > 0\\} . Para x \\in E , defina p_{Y|X}(y|x) = \\frac{p_Z(x,y)}{p_X(x)}, e seja Q_x a medida de probabilidade com densidade P_{Y|X}(\\cdot|x) com respeito a \\nu . Se x \\not \\in E , ent\u00e3o defina p_{Y|X}(y|x) = p_0(y) em que p_0 \u00e9 densidade de uma distribui\u00e7\u00e3o de probabilidade P_0 arbitr\u00e1ria e seja Q_x = P_0 . Ent\u00e3o Q \u00e9 distribui\u00e7\u00e3o condicional de Y dado X=x . Uma consequ\u00eancia \u00e9 que \\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X|Y]], quando as esperan\u00e7as existem. Essa rela\u00e7\u00e3o \u00e9 conhecida como Lei da Probabilidade Total ou Lei da Esperan\u00e7a Total . Teorema: Seja (\\mathcal{X}, \\mathcal{B}) um espa\u00e7o de Borel com medida \\nu_{\\mathcal{X}} \\sigma -finita. Sejam X uma vari\u00e1vel (vetor) aleat\u00f3ria e g uma fun\u00e7\u00e3o mensur\u00e1vel. Defina Y = g(X) . Suponha que a distribui\u00e7\u00e3o de X tenha densidade f_X com respeito a \\nu_{\\mathcal{X}} . A distribui\u00e7\u00e3o conjunta \\mu_{X,Y} de X,Y \u00e9 absolutamente cont\u00ednua a \\nu , que \u00e9 definida por \\nu(C) = \\nu_{\\mathcal{X}}(\\{x \\in \\mathcal{X} : (x,g(x)) \\in C\\}), e \\mu_{X,Y} tem densidade f_{X,Y}(x,y) = f_{X}(x)I_{g(x)}(y) . Al\u00e9m do mais, f_{X|Y}(x|y) = \\frac{f_X(x)}{f_Y(y)}I_{g(x)}(y). Esse resultado \u00e9 o Corol\u00e1rio B.55 descrito no livro do Schervish. Conceitos de converg\u00eancia Veja Ap\u00eandice B.4 do livro do Schervish (p. 634) e a se\u00e7\u00e3o 5.5 do livro de Casella e Berger (p. 232). Conceitos abordados: - Converg\u00eancia em probabilidade e a Lei Fraca dos Grandes N\u00fameros; - Converg\u00eancia quase certa ou com probabilidade 1 e a Lei Forte dos Grandes N\u00fameros; - Converg\u00eancia em distribui\u00e7\u00e3o e o Teorema Central do Limite; - Converg\u00eancia fraca atrav\u00e9s de integrais, equivalente \u00e0 converg\u00eancia em distribui\u00e7\u00e3o; - Teorema de Slutsky e o Teorema da fun\u00e7\u00e3o cont\u00ednua. Material adicional Notas de Terence Tao : notas que iniciam com um pouco da ideia da constru\u00e7\u00e3o de probabilidade, probabilidade em espa\u00e7os discretos e introdu\u00e7\u00e3o de medida em probabilidade. S\u00e3o notas para um curso de probabilidade da gradua\u00e7\u00e3o. A First Look at Rigorous Probability Theory : bom livro de probabilidade para o n\u00edvel de mestrado.","title":"Revis\u00e3o de Probabilidade"},{"location":"infestatistica_MSc/probability/#revisao-de-probabilidade","text":"Nessa se\u00e7\u00e3o, vamos introduzir alguns conceitos chave de probabilidade e medida para a compreens\u00e3o do curso de estat\u00edstica.","title":"Revis\u00e3o de Probabilidade"},{"location":"infestatistica_MSc/probability/#medida","text":"Uma medida atribui um valor n\u00e3o negativo a subconjuntos A \\subseteq \\mathcal{X} , com certas propriedades que intuitivamente gostar\u00edamos de observar. Por exemplo, se \\mathcal{X} for um conjunto enumer\u00e1vel, como os n\u00fameros naturais, poder\u00edamos definir uma medida para A como \\mu(A) = |A| = \\text{n\u00famero de elementos de } A, conhecida como medida de contagem . No caso em que \\mathcal{X} = \\mathbb{R}^n para algum n , podemos definir a no\u00e7\u00e3o de volume de A com \\mu(A) = \\int_{\\mathbb{R}^n} I\\{\\boldsymbol{x} \\in A\\} \\, d\\boldsymbol{x}, conhecida como medida de Lebesgue . Para tornar esse conceito mais rigoroso, definimos uma \\sigma -\u00e1lgebra como Uma cole\u00e7\u00e3o de subconjuntos \\mathcal{A} de \\mathcal{X} , isto \u00e9, \\mathcal{A} \\subseteq \\mathcal{P}(\\mathcal{X}) (conjunto das partes de \\mathcal{X} ) \u00e9 uma \\sigma -\u00e1lgebra se: i) \\emptyset \\in \\mathcal{A} . ii) Se A \\in \\mathcal{A} , ent\u00e3o A^c \\in \\mathcal{A} . iii) Se A_1, A_2, \\dots \\in \\mathcal{A} , ent\u00e3o \\cup_{i=1}^{\\infty} A_i \\in \\mathcal{A} . Uma medida \u00e9 uma fun\u00e7\u00e3o \\mu : \\mathcal{A} \\to \\mathbb{R}_{+} que satisfaz a propriedade de que se a sequ\u00eancia A_1, A_2, \\dots \u00e9 disjunta dois a dois, ent\u00e3o \\mu\\left(\\cup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} \\mu(A_i), chamada de axioma da aditividade enumer\u00e1vel. Podemos definir algumas caracter\u00edsticas de interesse para medidas: \\mu \u00e9 medida finita se \\mu(\\mathcal{X}) < +\\infty . \\mu \u00e9 medida \\sigma -finita se existe sequ\u00eancia A_1, A_2, \\dots \\in \\mathcal{A} com \\mu(A_i) < +\\infty tal que \\mathcal{X} = \\cup_{i=1}^n A_i . \\mu \u00e9 medida de probabilidade se \\mu(\\mathcal{X}) = 1 . Chamamos o par (\\mathcal{X}, \\mathcal{A}) de espa\u00e7o mensur\u00e1vel e a tripla (\\mathcal{X}, \\mathcal{A}, \\mu) de espa\u00e7o medida . No caso em que \\mu \u00e9 medida de probabilidade, temos um espa\u00e7o de probabilidade . Se \\mu(N)=0 , dizemos que N tem medida nula. Se alguma propriedade \\tau vale quase sempre , queremos dizer que o conjunto em que \\tau n\u00e3o vale tem medida nula. Observa\u00e7\u00e3o: Os axiomas da probabilidade ou de Kolmogorov s\u00e3o os tr\u00eas seguintes: (I) P(A) \\ge 0 para todo evento A , (II) P(\\mathcal{X}) = 1 e (III) P(\\cup_{i=1}^n A_i) = \\sum_{i=1}^n P(A_i) para uma sequ\u00eancia disjunta. Alguns estat\u00edsticos, como deFinetti, acreditam que o axioma (III) \u00e9 muito forte e pouco auto-evidente. Para isso, sugerem a * aditividade finita** P(A \\cup B) = P(A) + P(B) quando A \\cap B = \\emptyset . Essa altera\u00e7\u00e3o, todavia, gera complica\u00e7\u00f5es que n\u00e3o, necessariamente, melhoram o entendimento dos conceitos de probabilidade.*","title":"Medida"},{"location":"infestatistica_MSc/probability/#integracao","text":"Uma fun\u00e7\u00e3o f com valores reais e definida em \\mathcal{X} \u00e9 mensur\u00e1vel se o conjunto f^{-1}(B) = \\{x \\in \\mathcal{X} : f(x) \\in B\\} pertence a \\mathcal{A} para todo B conjunto de Borel . Podemos tomar B da forma (-\\infty, b) , por exemplo. Podemos mostrar que fun\u00e7\u00f5es cont\u00ednuas ou cont\u00ednuas por partes s\u00e3o mensur\u00e1veis. Podemos ent\u00e3o, definir propriedades b\u00e1sicas da integral de Lebesgue. Denote 1_A(x) = I(x \\in A) . Assim: (1) \\int 1_A \\, d\\mu = \\mu(A) . (2) Se f e g s\u00e3o fun\u00e7\u00f5es mensur\u00e1veis n\u00e3o negativas e a,b > 0 : \\int (af + bg) \\, d\\mu = a\\int f \\, d\\mu + b\\int g \\, d\\mu (3) Se 0 \\le f_1 \\le f_2 \\le \\dots e f(x) = \\lim_{n\\to\\infty} f_n(x) , ent\u00e3o \\int f \\, d\\mu = \\lim_{n\\to \\infty} \\int f_n \\, d\\mu, e essa propriedade \u00e9 o Teorema da Converg\u00eancia Mon\u00f3tona . Uma fun\u00e7\u00e3o simples \u00e9 uma fun\u00e7\u00e3o constante por partes que assume um valor finito de valores a_1, \\dots a_n . Podemos escrever uma fun\u00e7\u00e3o simples como s(x) = \\sum_{i=1}^n a_i 1_{A_i}(x), para A_1, \\dots, A_n disjuntos, e obter que \\int s \\, d\\mu = \\sum_{i=1}^n a_i \\mu(A_i). Al\u00e9m disso, um teorema bem interessante mostra que se f \u00e9 n\u00e3o negativa e mensur\u00e1vel, ent\u00e3o existe uma sequ\u00eancia de fun\u00e7\u00f5es simples f_1 \\le f_2 \\le \\dots \\le f com f = \\lim f_n . Isso mostra que qualquer fun\u00e7\u00e3o n\u00e3o negativa mensur\u00e1vel pode ser integrada. Por fim, basta ver que f(x) = f^{+}(x) - f^{-}(x) = \\max\\{0, f(x)\\} - (-\\min\\{f(x), 0\\}) \u00e9 a soma de duas fun\u00e7\u00f5es n\u00e3o negativas, e portanto, pode ser integrada atrav\u00e9s de \\int f \\, d\\mu = \\int f^+ \\, d\\mu - \\int f^{-} \\, d\\mu. Dizemos que f \u00e9 integr\u00e1vel se \\int |f| \\, d\\mu < +\\infty. Algumas consequ\u00eancias importantes: Se f = 0 quase sempre, ent\u00e3o \\int f \\, d\\mu = 0 Se f \\ge 0 e \\int f \\, d\\mu = 0 , ent\u00e3o f = 0 quase sempre. Se f = g quase sempre, ent\u00e3o \\int f \\, d\\mu = \\int g \\, d\\mu se uma das integrais existe. Se \\int 1_{(c,x)} f \\, d\\mu = 0 para todo x > c , ent\u00e3o f(x) = 0 quase sempre em x > c . Se f > g s\u00e3o integr\u00e1veis, ent\u00e3o \\int f\\, d\\mu > \\int g \\, d\\mu , a menos que \\mu seja identicamente nula.","title":"Integra\u00e7\u00e3o"},{"location":"infestatistica_MSc/probability/#eventos-e-variaveis-aleatorias","text":"Seja (\\mathcal{X}, \\mathcal{A}, P) um espa\u00e7o de probabilidade. Dizemos que A \\in \\mathcal{A} \u00e9 um evento . Todos os poss\u00edveis desfechos de um experimento dado pelo conjunto \\mathcal{X} formam o espa\u00e7o amostral . Uma vari\u00e1vel aleat\u00f3ria \u00e9 uma fun\u00e7\u00e3o mensur\u00e1vel X : \\mathcal{X} \\to \\mathbb{R} . A distribui\u00e7\u00e3o de X \u00e9 Q , isto \u00e9, X \\sim Q quando Q(B) = P(\\{x \\in \\mathcal{X} | X(x) \\in A\\}) := P(X \\in B), isto \u00e9, Q \u00e9 definida nos conjuntos de Borel de \\mathbb{R} . A fun\u00e7\u00e3o de distribui\u00e7\u00e3o acumulada (FDA) (em ingl\u00eas, CDF) de X \u00e9 a fun\u00e7\u00e3o F_X(x) = P(X \\le x) = Q((-\\infty, x]), para x \\in \\mathbb{R} . Caracteriza\u00e7\u00e3o da FDA: A fun\u00e7\u00e3o F \u00e9 uma FDA se, e somente se, as seguintes condi\u00e7\u00f5es valem: 1) \\lim_{x \\to -\\infty} F(x) = 0 e \\lim_{x \\to \\infty} F(x) = 1 . 2) F(x) \u00e9 uma fun\u00e7\u00e3o mon\u00f3tona n\u00e3o decrescente de x . 3) F(x) \u00e9 cont\u00ednua \u00e0 direita, isto \u00e9, \\lim_{x \\to x_0^+} F(x) = F(x_0) . Al\u00e9m do mais, se P(X \\in A) = P(Y \\in A) para todo conjunto de Borel A , ent\u00e3o X e Y s\u00e3o identicamente distribu\u00eddas. Isso \u00e9 equivalente a notar que F_X(x) = F_Y(x) para todo valor de x .","title":"Eventos e vari\u00e1veis aleat\u00f3rias"},{"location":"infestatistica_MSc/probability/#densidades","text":"Uma medida \u00e9 absolutamente cont\u00ednua com respeito a outra se ela d\u00e1 volume 0 para as regi\u00f5es que a outra tamb\u00e9m d\u00e1, isto \u00e9: Sejam P e \\mu medidas definidas em (\\mathcal{X}, \\mathcal{A}) . Dizemos que P \u00e9 absolutamente cont\u00ednua com respeito a \\mu se P(A) = 0 sempre que \\mu(A) = 0 . Nesse caso, escrevemos que P \\ll \\mu . Tamb\u00e9m dizemos que \\mu domina P . Logo P \\ll \\mu \u00e9 equivalente a observar que o suporte de \\mu cont\u00e9m o suporte de P . Teorema de Radon-Nikodym: Se uma medida P finita \u00e9 absolutamente cont\u00ednua com respeito a uma medida \\mu \\sigma -finita, ent\u00e3o existe uma fun\u00e7\u00e3o mensur\u00e1vel f tal que P(A) = \\int_A f \\, d\\mu := \\int f1_A \\, d\\mu. Chamamos f de derivada Radon-Nikodym de P com respeito a \\mu , ou a densidade de P com respeito a \\mu e escrevemos f = \\frac{dP}{d\\mu}. Vejam que de fato isso generaliza a no\u00e7\u00e3o de derivada, pois pelo Teorema fundamental do C\u00e1lculo, F(x) = \\int_a^x f(t) \\, dt \\implies f = \\frac{d F}{d x}, em que nesse caso estamos tomando a medida de Lebesgue na reta como medida dominante. Conclu\u00edmos tamb\u00e9m que a densidade de P \u00e9 determinada a menos de um conjunto de medida nula.","title":"Densidades"},{"location":"infestatistica_MSc/probability/#esperanca","text":"Se X \u00e9 uma vari\u00e1vel aleat\u00f3ria definida em (\\mathcal{X}, \\mathcal{A}, P) , o valor esperado de X \u00e9 definido como \\mathbb{E}[X] = \\int X \\, dP. Se X \\sim P_X , podemos mostrar que \\mathbb{E}[X] = \\int x dP_X(x). Al\u00e9m do mais, se Y = f(X) , ent\u00e3o \\mathbb{E}[Y] = \\int f \\, dP_X. Se P_X tem densidade p com respeito a \\mu , ent\u00e3o \\int f \\, dP_X = \\int fp\\, d\\mu. Finalmente, definimos \\operatorname{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] , \\operatorname{Cov}(X,Y) = \\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])] e \\operatorname{Cor}(X,Y) = \\operatorname{Cov}(X,Y)\\big/\\sqrt{\\operatorname{Var}(X) \\operatorname{Var}(Y)} .","title":"Esperan\u00e7a"},{"location":"infestatistica_MSc/probability/#vetores-aleatorios","text":"Seja X : \\mathcal{X} \\to \\mathbb{R}^n uma fun\u00e7\u00e3o mensur\u00e1vel (no sentido dos conjuntos de Borel em \\mathbb{R}^n ). Se X \\sim P_X e P_X \u00e9 absolutamente cont\u00ednua com respeito a medida de Lebesgue em \\mathbb{R}^n com densidade p , temos que P_X(B) = \\int_{B} p(x) \\, dx. A esperan\u00e7a de X \u00e9 \\mathbb{E}[X] = \\begin{pmatrix} \\mathbb{E}[X_1] \\\\ \\vdots \\\\ \\mathbb{E}[X_n] \\end{pmatrix}. E se T : \\mathbb{R}^n \\to \\mathbb{R} \u00e9 mensur\u00e1vel, ent\u00e3o T(X) \u00e9 vari\u00e1vel aleat\u00f3ria com \\mathbb{E}[T(X)] = \\int T \\, dP_X.","title":"Vetores Aleat\u00f3rios"},{"location":"infestatistica_MSc/probability/#medida-produto","text":"Sejam (\\mathcal{X}, \\mathcal{A}, \\mu) e (\\mathcal{Y}, \\mathcal{B}, \\nu) espa\u00e7os de medida. Ent\u00e3o existe uma \u00fanica medida \\mu \\times \\nu chamada de medida produto em (\\mathcal{X} \\times \\mathcal{Y}, \\sigma(\\mathcal{A} \\times \\mathcal{B})) tal que (\\mu \\times \\nu)(A \\times B) = \\mu(A)\\nu(B), para A \\in \\mathcal{A} e B \\in \\mathcal{B} . Denotamos \\sigma(C) como a menor \\sigma -\u00e1lgebra que cont\u00e9m C . Com respeito a integra\u00e7\u00e3o, obtemos o seguinte teorema Teorema de Fubini: Se f \\ge 0 , ent\u00e3o \\int f \\, d(\\mu \\times \\nu) = \\int\\left[\\int f(x,y) \\, d\\nu(y)\\right] \\, d\\mu(x) = \\int\\left[\\int f(x,y) \\, d\\mu(x)\\right] \\, d\\nu(y). Al\u00e9m do mais, se f \\ge 0 n\u00e3o \u00e9 satisfeito, mas f \u00e9 integr\u00e1vel com respeito a \\mu \\times \\nu , a rela\u00e7\u00e3o ainda \u00e9 v\u00e1lida.","title":"Medida produto"},{"location":"infestatistica_MSc/probability/#independencia","text":"A partir da defini\u00e7\u00e3o de medida produto, podemos falar do conceito de independ\u00eancia de duas vari\u00e1veis aleat\u00f3rias. Sejam dois vetores aleat\u00f3rios X(\\cdot) \\subseteq \\mathbb{R}^n e Y(\\cdot) \\subseteq \\mathbb{R}^m . Dizemos que eles s\u00e3o independentes se P(X \\in A, Y \\in B) = P(X \\in A)P(Y \\in B), para todos os conjuntos de Borel A e B . Seja Z = (X,Y) em \\mathbb{R}^{n+m} . Temos que Z \\in A \\times B se, e s\u00f3 se, X \\in A e Y \\in B . Nesse caso, P_Z(A \\times B) = P_X(A)P_Y(B) \\implies P_Z = P_A \\times P_Y. A densidade de Z tamb\u00e9m \u00e9 dada pelo produto das densidades de X e Y pelo Teorema de Fubini. Finalmente, temos o resultado de que se X_1, \\dots, X_n s\u00e3o vetores aleat\u00f3rios independentes e se f_1, \\dots, f_n s\u00e3o fun\u00e7\u00f5es mensur\u00e1veis, ent\u00e3o f_1(X_1), \\dots, f_n(X_n) s\u00e3o independentes.","title":"Independ\u00eancia"},{"location":"infestatistica_MSc/probability/#densidades-conjunta-e-marginais","text":"A densidade conjunta de X e Y denotada por p_Z satisfaz P(Z \\in B) = \\int \\int 1_B(x,y)p_Z(x,y) \\, d\\mu(x) \\, d\\nu(y). A densidade marginal de X ( Y ) pode ser obtida integrando Y ( X ), e assim: p_X(x) = \\int p_X(x,y) \\, d\\nu(y), com respeito a \\mu .","title":"Densidades conjunta e marginais"},{"location":"infestatistica_MSc/probability/#calculo-de-probabilidades","text":"A partir dos Axiomas da Probabilidade, podemos obter alguns resultados importantes: 1) P(\\emptyset) = 0 2) P(A^c) = 1 - P(A) 3) P(\\cup_{i=1}^{\\infty} A_i) \\le \\sum_{i=1}^{\\infty} P(A_i)","title":"C\u00e1lculo de probabilidades"},{"location":"infestatistica_MSc/probability/#condicionando","text":"Considere duas vari\u00e1veis aleat\u00f3rias X e Y n\u00e3o independentes. Com isso, se observamos X=x , temos alguma informa\u00e7\u00e3o adicional sobre Y , e sua distribui\u00e7\u00e3o marginal n\u00e3o representa mais a totalidade. Considere X e Y duas vari\u00e1veis discretas definidas em \\mathcal{X} e \\mathcal{Y} , respectivamente. Definimos a probabilidade condicional de X = x_i dado Y = y_j como P(X=x_i | Y=y_j) = \\frac{P(X=x_i, Y=y_j)}{P(Y=y_j)} := p_{i|j}. \u00c9 f\u00e1cil verificar que P(\\cdot | Y=y_j) define uma medida de probabilidade em \\mathcal{X} . A m\u00e9dia condicional de uma fun\u00e7\u00e3o f de X \u00e9 dada por \\mathbb{E}[f(X) | Y=y_j] = \\sum_{i=1}^{\\infty} f(x_i) p_{i|j}. De forma geral, se P(B) > 0 , escrevemos P(A|B) = P(A\\cap B)/P(B) para eventos A e B . Mas o que fazer quando P(B) = 0 ? Em especial, isso ocorre quando falamos de vari\u00e1veis aleat\u00f3rias cont\u00ednuas, j\u00e1 que P(X=1) = 0 , por exemplo. Probabilidade condicional: Dadas vari\u00e1veis aleat\u00f3rias X e Y com \\mathbb{E}|Y| < +\\infty e um evento A , dizemos que P(A|X) \u00e9 probabilidade condicional de A dado X se \u00e9 uma vari\u00e1vel aleat\u00f3ria \\sigma(X) -mensur\u00e1vel ( \\sigma(X) \u00e9 a menor \\sigma -\u00e1lgebra em X \u00e9 mensur\u00e1vel) e, para todos os Boreis S \\subseteq \\mathbb{R} , temos \\mathbb{E}[P(A|X)1_{S}(X)] = P(A \\cap \\{X \\in S\\}). De forma similar, introduzimos o conceito de \\mathbb{E}[Y|X] , substituindo acima probabilidades por esperan\u00e7as: \\mathbb{E}[\\mathbb{E}[Y|X]1_{S}(X)] = \\mathbb{E}[Y1_S(X)]. Como consequ\u00eancia, probabilidades e esperan\u00e7as condicionais s\u00e3o definidas a menos de um conjunto de medida nula. Distribui\u00e7\u00e3o condicional: Uma fun\u00e7\u00e3o Q \u00e9 distribui\u00e7\u00e3o condicional de Y dado X=x se 1) Q_x(\\cdot) \u00e9 uma medida de probabilidade para todo x ; 2) Q_x(B) \u00e9 fun\u00e7\u00e3o mensur\u00e1vel de x para todo Borel B ; 3) P(X \\in A, Y \\in B) = \\int_A Q_x(B) \\, dP_X(x) . Escrevemos ent\u00e3o que Y|X=x \\sim Q_x . Teorema (densidade condicional): Suponha que X e Y sejam vetores aleat\u00f3rios com densidade conjunta p_Z com respeito a \\mu \\times \\nu . Seja E = \\{x : p_X(x) > 0\\} . Para x \\in E , defina p_{Y|X}(y|x) = \\frac{p_Z(x,y)}{p_X(x)}, e seja Q_x a medida de probabilidade com densidade P_{Y|X}(\\cdot|x) com respeito a \\nu . Se x \\not \\in E , ent\u00e3o defina p_{Y|X}(y|x) = p_0(y) em que p_0 \u00e9 densidade de uma distribui\u00e7\u00e3o de probabilidade P_0 arbitr\u00e1ria e seja Q_x = P_0 . Ent\u00e3o Q \u00e9 distribui\u00e7\u00e3o condicional de Y dado X=x . Uma consequ\u00eancia \u00e9 que \\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X|Y]], quando as esperan\u00e7as existem. Essa rela\u00e7\u00e3o \u00e9 conhecida como Lei da Probabilidade Total ou Lei da Esperan\u00e7a Total . Teorema: Seja (\\mathcal{X}, \\mathcal{B}) um espa\u00e7o de Borel com medida \\nu_{\\mathcal{X}} \\sigma -finita. Sejam X uma vari\u00e1vel (vetor) aleat\u00f3ria e g uma fun\u00e7\u00e3o mensur\u00e1vel. Defina Y = g(X) . Suponha que a distribui\u00e7\u00e3o de X tenha densidade f_X com respeito a \\nu_{\\mathcal{X}} . A distribui\u00e7\u00e3o conjunta \\mu_{X,Y} de X,Y \u00e9 absolutamente cont\u00ednua a \\nu , que \u00e9 definida por \\nu(C) = \\nu_{\\mathcal{X}}(\\{x \\in \\mathcal{X} : (x,g(x)) \\in C\\}), e \\mu_{X,Y} tem densidade f_{X,Y}(x,y) = f_{X}(x)I_{g(x)}(y) . Al\u00e9m do mais, f_{X|Y}(x|y) = \\frac{f_X(x)}{f_Y(y)}I_{g(x)}(y). Esse resultado \u00e9 o Corol\u00e1rio B.55 descrito no livro do Schervish.","title":"Condicionando"},{"location":"infestatistica_MSc/probability/#conceitos-de-convergencia","text":"Veja Ap\u00eandice B.4 do livro do Schervish (p. 634) e a se\u00e7\u00e3o 5.5 do livro de Casella e Berger (p. 232). Conceitos abordados: - Converg\u00eancia em probabilidade e a Lei Fraca dos Grandes N\u00fameros; - Converg\u00eancia quase certa ou com probabilidade 1 e a Lei Forte dos Grandes N\u00fameros; - Converg\u00eancia em distribui\u00e7\u00e3o e o Teorema Central do Limite; - Converg\u00eancia fraca atrav\u00e9s de integrais, equivalente \u00e0 converg\u00eancia em distribui\u00e7\u00e3o; - Teorema de Slutsky e o Teorema da fun\u00e7\u00e3o cont\u00ednua.","title":"Conceitos de converg\u00eancia"},{"location":"infestatistica_MSc/probability/#material-adicional","text":"Notas de Terence Tao : notas que iniciam com um pouco da ideia da constru\u00e7\u00e3o de probabilidade, probabilidade em espa\u00e7os discretos e introdu\u00e7\u00e3o de medida em probabilidade. S\u00e3o notas para um curso de probabilidade da gradua\u00e7\u00e3o. A First Look at Rigorous Probability Theory : bom livro de probabilidade para o n\u00edvel de mestrado.","title":"Material adicional"},{"location":"infestatistica_MSc/random_samples/","text":"Amostras aleat\u00f3rias Defini\u00e7\u00e3o: As vari\u00e1veis aleat\u00f3rias X_1, \\dots, X_n formam uma amostra aleat\u00f3ria de tamanho n se elas s\u00e3o mutualmente independentes e a densidade marginal de cada uma \u00e9 a mesma f(x) , isto \u00e9, elas s\u00e3o independentes e identicamente distribu\u00eddas (i.i.d.). O conceito de amostra aleat\u00f3ria corresponde \u00e0 realiza\u00e7\u00e3o de experimentos repetidos que se originam do mesmo processo gerador. Al\u00e9m disso, uma hip\u00f3tese comum \u00e9 de que os efeitos de um experimento n\u00e3o influenciam os seguintes. Em certas situa\u00e7\u00f5es, isso n\u00e3o \u00e9 verdadeiro, mas pode ser uma simplifica\u00e7\u00e3o razo\u00e1vel, pois, nesse caso, a densidade conjunta das vari\u00e1veis X_1, \\dots, X_n \u00e9 dada por f(x_1, \\dots, x_n) = \\prod_{i=1}^n f_i(x_i) = \\prod_{i=1}^n f(x_i), dada a independ\u00eancia na primeira igualdade, e a mesma distribui\u00e7\u00e3o na segunda. Estat\u00edstica: Seja X_1, \\dots, X_n uma amostra aleat\u00f3ria e considere a fun\u00e7\u00e3o T : \\mathcal{X}^n \\to \\mathbb{R}^m . A vari\u00e1vel aleat\u00f3ria Y = T(X_1, \\dots, X_n) \u00e9 chamada de estat\u00edstica e a distribui\u00e7\u00e3o de probabilidade de Y \u00e9 chamada de distribui\u00e7\u00e3o amostral de Y . Alguns exemplos s\u00e3o a m\u00e9dia amostral , a vari\u00e2ncia amostral e o desvio padr\u00e3o amostral , denotados por, respectivamente, \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i, S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2 \\text{ e } S = \\sqrt{S^2}. Estat\u00edstica de Ordem A estat\u00edstica de ordem de uma amostra aleat\u00f3ria X_1, \\dots, X_n s\u00e3o os valores aleat\u00f3rios da amostra ordenados de forma ascendente e s\u00e3o denotados por X_{(1)}, \\dots, X_{(n)} . Em particular, X_{(1)} = \\min_{1 \\le i \\le n} X_i e X_{(n)} = \\max_{1 \\le i \\le n} X_i . A mediana amostral \u00e9 defina por M = \\begin{cases} X_{(n+1)/2} &\\text{se } n \\text{ \u00e9 \u00edmpar} \\\\ (X_{n/2}+X_{n/2+1})/2 &\\text{se } n \\text{ \u00e9 par} \\end{cases} Teorema: Seja X_{(1)}, \\dots, X_{(n)} uma estat\u00edstica de ordem de uma amostra aleat\u00f3ria X_1, \\dots, X_n de uma popula\u00e7\u00e3o cont\u00ednua com CDF (ou FDA) F_X(x) e densidade f_X(x) . Ent\u00e3o a densidade marginal de X_{(j)} \u00e9 dada por f_{X(j)}(x) = \\frac{n!}{(j-1)!(n-j)!}f_X(x)[F_X(x)]^{j-1}[1-F_X(x)]^{n-j}. Al\u00e9m do mais, f_{X_{(1)}, \\dots, X_{(n)}}(x_1, \\dots, x_n) = n!f(x_1)f(x_2)\\cdots f(x_n), quando x_1 < x_2 < \\dots < x_n e 0 caso contr\u00e1rio.","title":"Amostras aleat\u00f3rias"},{"location":"infestatistica_MSc/random_samples/#amostras-aleatorias","text":"Defini\u00e7\u00e3o: As vari\u00e1veis aleat\u00f3rias X_1, \\dots, X_n formam uma amostra aleat\u00f3ria de tamanho n se elas s\u00e3o mutualmente independentes e a densidade marginal de cada uma \u00e9 a mesma f(x) , isto \u00e9, elas s\u00e3o independentes e identicamente distribu\u00eddas (i.i.d.). O conceito de amostra aleat\u00f3ria corresponde \u00e0 realiza\u00e7\u00e3o de experimentos repetidos que se originam do mesmo processo gerador. Al\u00e9m disso, uma hip\u00f3tese comum \u00e9 de que os efeitos de um experimento n\u00e3o influenciam os seguintes. Em certas situa\u00e7\u00f5es, isso n\u00e3o \u00e9 verdadeiro, mas pode ser uma simplifica\u00e7\u00e3o razo\u00e1vel, pois, nesse caso, a densidade conjunta das vari\u00e1veis X_1, \\dots, X_n \u00e9 dada por f(x_1, \\dots, x_n) = \\prod_{i=1}^n f_i(x_i) = \\prod_{i=1}^n f(x_i), dada a independ\u00eancia na primeira igualdade, e a mesma distribui\u00e7\u00e3o na segunda. Estat\u00edstica: Seja X_1, \\dots, X_n uma amostra aleat\u00f3ria e considere a fun\u00e7\u00e3o T : \\mathcal{X}^n \\to \\mathbb{R}^m . A vari\u00e1vel aleat\u00f3ria Y = T(X_1, \\dots, X_n) \u00e9 chamada de estat\u00edstica e a distribui\u00e7\u00e3o de probabilidade de Y \u00e9 chamada de distribui\u00e7\u00e3o amostral de Y . Alguns exemplos s\u00e3o a m\u00e9dia amostral , a vari\u00e2ncia amostral e o desvio padr\u00e3o amostral , denotados por, respectivamente, \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i, S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2 \\text{ e } S = \\sqrt{S^2}.","title":"Amostras aleat\u00f3rias"},{"location":"infestatistica_MSc/random_samples/#estatistica-de-ordem","text":"A estat\u00edstica de ordem de uma amostra aleat\u00f3ria X_1, \\dots, X_n s\u00e3o os valores aleat\u00f3rios da amostra ordenados de forma ascendente e s\u00e3o denotados por X_{(1)}, \\dots, X_{(n)} . Em particular, X_{(1)} = \\min_{1 \\le i \\le n} X_i e X_{(n)} = \\max_{1 \\le i \\le n} X_i . A mediana amostral \u00e9 defina por M = \\begin{cases} X_{(n+1)/2} &\\text{se } n \\text{ \u00e9 \u00edmpar} \\\\ (X_{n/2}+X_{n/2+1})/2 &\\text{se } n \\text{ \u00e9 par} \\end{cases} Teorema: Seja X_{(1)}, \\dots, X_{(n)} uma estat\u00edstica de ordem de uma amostra aleat\u00f3ria X_1, \\dots, X_n de uma popula\u00e7\u00e3o cont\u00ednua com CDF (ou FDA) F_X(x) e densidade f_X(x) . Ent\u00e3o a densidade marginal de X_{(j)} \u00e9 dada por f_{X(j)}(x) = \\frac{n!}{(j-1)!(n-j)!}f_X(x)[F_X(x)]^{j-1}[1-F_X(x)]^{n-j}. Al\u00e9m do mais, f_{X_{(1)}, \\dots, X_{(n)}}(x_1, \\dots, x_n) = n!f(x_1)f(x_2)\\cdots f(x_n), quando x_1 < x_2 < \\dots < x_n e 0 caso contr\u00e1rio.","title":"Estat\u00edstica de Ordem"},{"location":"infestatistica_MSc/risk_function/","text":"Fun\u00e7\u00f5es de risco Podemos ver a infer\u00eancia estat\u00edstica como a arte de aprender sobre uma quantidade desconhecida \\theta a partir de dados X , atrav\u00e9s de uma conex\u00e3o definida a partir de um modelo probabil\u00edstico X \\sim P_{\\theta} . Para estudar a performance de estimadores de g(\\theta) , conceito introduzido em Estima\u00e7\u00e3o pontual , uma abordagem formalizada \u00e9 a Teoria da Decis\u00e3o . Aqui introduziremos esses conceitos, mas uma vis\u00e3o mais geral sob a \u00f3tica de Infer\u00eancia Bayesiana pode ser encontra aqui . Seja \\mathcal{D} o espa\u00e7o das decis\u00f5es (por exemplo, uma estimativa \u00e9 uma decis\u00e3o) e \\Omega o espa\u00e7o dos par\u00e2metros. Uma fun\u00e7\u00e3o de perda \u00e9 uma fun\u00e7\u00e3o L : \\Omega \\times \\mathcal{D} \\to [0, +\\infty] e avalia uma penalidade L(\\theta, d) em tomar a decis\u00e3o d com respeito a \\theta . Quando \\mathcal{D} = h(\\Omega) , temos que L(\\theta, d) mede o erro em obter h(\\theta) por d . Sendo \\phi(X) um estimador para \\theta (uma estat\u00edstica que busca aproxim\u00e1-lo), L(\\theta, \\phi(X)) \u00e9 uma vari\u00e1vel aleat\u00f3ria que pode ser grande, mesmo que \\delta seja um bom estimador. Com isso, definimos a fun\u00e7\u00e3o de risco R como a perda m\u00e9dia em usar \\phi(X) para estimar \\theta R(\\theta, \\phi) = \\mathbb{E}_{\\theta}[L(\\theta, \\phi(X))], isto \u00e9, tomando a esperan\u00e7a quando X \\sim P_{\\theta} . \ud83d\udcdd Exemplo (caso binomial) Seja X \\sim Binomial(n, p) com n conhecido. Um estimador para p \u00e9 \\phi(X) = X/n . Seja L(\\theta, d) = (\\theta - d)^2 a perda quadr\u00e1tica. Assim, a fun\u00e7\u00e3o de risco para \\phi \u00e9 R(p, \\phi) = \\mathbb{E}_{p}[(p - X/n)^2] = Var_p(X/n) = \\frac{np(1-p)}{n^2} = \\frac{p(1-p)}{n}, p \\in [0,1]. A fun\u00e7\u00e3o de risco tem um problema cl\u00e1ssico: \u00e9 dif\u00edcil comparar dois estimadores quaisquer, pois um pode ter risco menor em certas regi\u00f5es e maior em outras. \ud83d\udcdd Exemplos de riscos Para cada perda L , definimos um risco R . A seguir, temos alguns mais famosos. Perda quadr\u00e1tica ( L(\\theta, d) = (\\theta - d)^2 ): mean squared error (erro m\u00e9dio quadrado) ou MSE. Perda absoluta ( L(\\theta, d) = |\\theta - d| ): mean absolute error (erro m\u00e9dio absoluto) ou MAE. Perda absoluta percentual ( L(\\theta, d) = |\\theta - d|/|\\theta| ): mean absolute percentage error (erro m\u00e9dio absoluto percentual) ou MAPE. etc.","title":"Fun\u00e7\u00f5es de risco"},{"location":"infestatistica_MSc/risk_function/#funcoes-de-risco","text":"Podemos ver a infer\u00eancia estat\u00edstica como a arte de aprender sobre uma quantidade desconhecida \\theta a partir de dados X , atrav\u00e9s de uma conex\u00e3o definida a partir de um modelo probabil\u00edstico X \\sim P_{\\theta} . Para estudar a performance de estimadores de g(\\theta) , conceito introduzido em Estima\u00e7\u00e3o pontual , uma abordagem formalizada \u00e9 a Teoria da Decis\u00e3o . Aqui introduziremos esses conceitos, mas uma vis\u00e3o mais geral sob a \u00f3tica de Infer\u00eancia Bayesiana pode ser encontra aqui . Seja \\mathcal{D} o espa\u00e7o das decis\u00f5es (por exemplo, uma estimativa \u00e9 uma decis\u00e3o) e \\Omega o espa\u00e7o dos par\u00e2metros. Uma fun\u00e7\u00e3o de perda \u00e9 uma fun\u00e7\u00e3o L : \\Omega \\times \\mathcal{D} \\to [0, +\\infty] e avalia uma penalidade L(\\theta, d) em tomar a decis\u00e3o d com respeito a \\theta . Quando \\mathcal{D} = h(\\Omega) , temos que L(\\theta, d) mede o erro em obter h(\\theta) por d . Sendo \\phi(X) um estimador para \\theta (uma estat\u00edstica que busca aproxim\u00e1-lo), L(\\theta, \\phi(X)) \u00e9 uma vari\u00e1vel aleat\u00f3ria que pode ser grande, mesmo que \\delta seja um bom estimador. Com isso, definimos a fun\u00e7\u00e3o de risco R como a perda m\u00e9dia em usar \\phi(X) para estimar \\theta R(\\theta, \\phi) = \\mathbb{E}_{\\theta}[L(\\theta, \\phi(X))], isto \u00e9, tomando a esperan\u00e7a quando X \\sim P_{\\theta} . \ud83d\udcdd Exemplo (caso binomial) Seja X \\sim Binomial(n, p) com n conhecido. Um estimador para p \u00e9 \\phi(X) = X/n . Seja L(\\theta, d) = (\\theta - d)^2 a perda quadr\u00e1tica. Assim, a fun\u00e7\u00e3o de risco para \\phi \u00e9 R(p, \\phi) = \\mathbb{E}_{p}[(p - X/n)^2] = Var_p(X/n) = \\frac{np(1-p)}{n^2} = \\frac{p(1-p)}{n}, p \\in [0,1]. A fun\u00e7\u00e3o de risco tem um problema cl\u00e1ssico: \u00e9 dif\u00edcil comparar dois estimadores quaisquer, pois um pode ter risco menor em certas regi\u00f5es e maior em outras. \ud83d\udcdd Exemplos de riscos Para cada perda L , definimos um risco R . A seguir, temos alguns mais famosos. Perda quadr\u00e1tica ( L(\\theta, d) = (\\theta - d)^2 ): mean squared error (erro m\u00e9dio quadrado) ou MSE. Perda absoluta ( L(\\theta, d) = |\\theta - d| ): mean absolute error (erro m\u00e9dio absoluto) ou MAE. Perda absoluta percentual ( L(\\theta, d) = |\\theta - d|/|\\theta| ): mean absolute percentage error (erro m\u00e9dio absoluto percentual) ou MAPE. etc.","title":"Fun\u00e7\u00f5es de risco"},{"location":"infestatistica_MSc/statistical_model/","text":"Modelo estat\u00edstico Uma descri\u00e7\u00e3o baseada no livro de Schervish de modelo param\u00e9trico \u00e9 dada nesse link . Nesse link, tamb\u00e9m \u00e9 poss\u00edvel obter uma introdu\u00e7\u00e3o bem geral \u00e0 Estat\u00edstica do ponto de vista do livro The Bayesian Choice de Christian P. Robert. Uma obra de arte em forma de artigo sobre o tema \u00e9 discorrido no artigo What is a statistical model? de Peter McCullagh. De forma geral, um modelo estat\u00edstico \u00e9 um conjunto de distribui\u00e7\u00f5es de probabilidade em um espa\u00e7o amostral \\mathcal{X} . Um modelo estat\u00edstico param\u00e9trico \u00e9 um conjunto de par\u00e2metros \\Theta junto de uma fun\u00e7\u00e3o P : \\Theta \\to \\mathcal{F}(\\mathcal{X}) que para cada par\u00e2metro \\theta \\in \\Theta , define uma distribui\u00e7\u00e3o de probabilidade P(\\theta) = P_\\theta definida em \\mathcal{X} . No caso de um modelo bayesiano, exigimos uma distribui\u00e7\u00e3o de probabilidade em \\Theta , a distribui\u00e7\u00e3o a priori . Estamos interessados, em particular, em modelos identific\u00e1veis , isto \u00e9, quando uma fam\u00edlia de distribui\u00e7\u00f5es de probabilidade satisfaz: P_{\\theta} = P_{\\theta '} \\implies \\theta = \\theta ' . Em outras palavras, a fun\u00e7\u00e3o P \u00e9 injetiva. Fun\u00e7\u00e3o de verossimilhan\u00e7a Denote f(\\boldsymbol{x}|\\theta) a densidade da distribui\u00e7\u00e3o de \\boldsymbol{X} = (X_1, \\dots, X_n) com respeito \u00e0 medida de Lebesgue em \\mathbb{R}^n , pertence a uma fam\u00edlia de distribui\u00e7\u00f5es parametrizada por \\theta . Dada a observa\u00e7\u00e3o \\boldsymbol{X} = \\boldsymbol{x} , a fun\u00e7\u00e3o de verossimilhan\u00e7a \u00e9 a fun\u00e7\u00e3o L : \\Theta \\to \\mathbb{R}_+ definida como L(\\theta | \\boldsymbol{x}) = f(\\boldsymbol{x}|\\theta) . Note que se L(\\theta_1 | \\boldsymbol{x}) > L(\\theta_2 | \\boldsymbol{x}) , queremos dizer que \u00e9 mais prov\u00e1vel que \\theta = \\theta_1 do que \\theta = \\theta_2 quando observamos \\boldsymbol{x} .","title":"Modelo estat\u00edstico"},{"location":"infestatistica_MSc/statistical_model/#modelo-estatistico","text":"Uma descri\u00e7\u00e3o baseada no livro de Schervish de modelo param\u00e9trico \u00e9 dada nesse link . Nesse link, tamb\u00e9m \u00e9 poss\u00edvel obter uma introdu\u00e7\u00e3o bem geral \u00e0 Estat\u00edstica do ponto de vista do livro The Bayesian Choice de Christian P. Robert. Uma obra de arte em forma de artigo sobre o tema \u00e9 discorrido no artigo What is a statistical model? de Peter McCullagh. De forma geral, um modelo estat\u00edstico \u00e9 um conjunto de distribui\u00e7\u00f5es de probabilidade em um espa\u00e7o amostral \\mathcal{X} . Um modelo estat\u00edstico param\u00e9trico \u00e9 um conjunto de par\u00e2metros \\Theta junto de uma fun\u00e7\u00e3o P : \\Theta \\to \\mathcal{F}(\\mathcal{X}) que para cada par\u00e2metro \\theta \\in \\Theta , define uma distribui\u00e7\u00e3o de probabilidade P(\\theta) = P_\\theta definida em \\mathcal{X} . No caso de um modelo bayesiano, exigimos uma distribui\u00e7\u00e3o de probabilidade em \\Theta , a distribui\u00e7\u00e3o a priori . Estamos interessados, em particular, em modelos identific\u00e1veis , isto \u00e9, quando uma fam\u00edlia de distribui\u00e7\u00f5es de probabilidade satisfaz: P_{\\theta} = P_{\\theta '} \\implies \\theta = \\theta ' . Em outras palavras, a fun\u00e7\u00e3o P \u00e9 injetiva.","title":"Modelo estat\u00edstico"},{"location":"infestatistica_MSc/statistical_model/#funcao-de-verossimilhanca","text":"Denote f(\\boldsymbol{x}|\\theta) a densidade da distribui\u00e7\u00e3o de \\boldsymbol{X} = (X_1, \\dots, X_n) com respeito \u00e0 medida de Lebesgue em \\mathbb{R}^n , pertence a uma fam\u00edlia de distribui\u00e7\u00f5es parametrizada por \\theta . Dada a observa\u00e7\u00e3o \\boldsymbol{X} = \\boldsymbol{x} , a fun\u00e7\u00e3o de verossimilhan\u00e7a \u00e9 a fun\u00e7\u00e3o L : \\Theta \\to \\mathbb{R}_+ definida como L(\\theta | \\boldsymbol{x}) = f(\\boldsymbol{x}|\\theta) . Note que se L(\\theta_1 | \\boldsymbol{x}) > L(\\theta_2 | \\boldsymbol{x}) , queremos dizer que \u00e9 mais prov\u00e1vel que \\theta = \\theta_1 do que \\theta = \\theta_2 quando observamos \\boldsymbol{x} .","title":"Fun\u00e7\u00e3o de verossimilhan\u00e7a"},{"location":"infestatistica_MSc/confidence_interval/confidence_interval/","text":"Intervalos de Confian\u00e7a Esse tema procura responder qu\u00e3o confiantes estamos de um estimador. \u00c9 claro que essa pergunta tem que ser melhor descrita matematicamente. De forma geral, estamos procurando por uma estat\u00edstica C(X) em que para cada X=x observado, C(x) \u00e9 um conjunto. Mas na pr\u00e1tica, procuramos por estat\u00edsticas (A(X), B(X)) que nos deem confian\u00e7a de que contenham o par\u00e2metro verdadeiro, isto \u00e9. O intervalo [a,b] , uma realiza\u00e7\u00e3o de [A(X),B(X)] , tem 95% de confian\u00e7a se em 95% do tempo, o par\u00e2metro procurado est\u00e1 entre a e b . Veja que a ideia \u00e9 frequentista, dado que a interpreta\u00e7\u00e3o est\u00e1 ligada \u00e0 frequ\u00eancia de pertencimento quando o n\u00famero de experimentos tende para infinito. (Cuidado: N\u00e3o vamos falar da probabilidade do par\u00e2metro estar em [a,b] , isso n\u00e3o faz sentido, pois \\theta n\u00e3o \u00e9 uma vari\u00e1vel aleat\u00f3ria, e sim um valor fixo). Defini\u00e7\u00e3o Sejam X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Uma estimador intervalar de \\theta \u00e9 um par de estat\u00edsticas (A(X), B(X)) , tal que \\mathbb{P}(A(X) \\le B(X)) = 1 Se X=x \u00e9 observado, ent\u00e3o (A(x), B(x)) \u00e9 uma estimativa intervalar. Para um estimador intervalar, a probabilidade de cobertura \u00e9 \\mathbb{P}_{\\theta}(\\theta \\in [A(X), B(X)]) = \\mathbb{P}_{\\theta}(A(X) \\le \\theta, B(X) \\ge \\theta). Um intervalo de confian\u00e7a com coeficiente de confian\u00e7a \\gamma \u00e9 um estimador intervalar que satisfaz \\inf_{\\theta} \\mathbb{P}_{\\theta}(\\theta \\in [A(X), B(X)]) = \\gamma, ou tamb\u00e9m, \\mathbb{P}(A(X) < \\theta < B(X)) \\ge \\gamma , isto \u00e9, a probabilidade de cobertura \u00e9 no m\u00ednimo \\gamma . Ap\u00f3s observarmos os valores de X_1, ..., X_n e computarmos A = a e B = b , o intervalo (a,b) \u00e9 chamado de valor observado do intervalo de confian\u00e7a. Intervalo de Confian\u00e7a para a m\u00e9dia de N(\\mu, \\sigma^2) Seja X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Para cada 0 < \\gamma < 1 , o intervalo (A,B) \u00e9 intervalo de confian\u00e7a exato para \\mu com coeficiente \\gamma , em que: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} onde T_{n-1} denota a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Esse resultado implica do fato de que a distribui\u00e7\u00e3o de U = \\frac{n^{1/2}(\\bar{X}_n - \\mu)}{\\sigma '} \u00e9 conhecida por distribui\u00e7\u00e3o t com n-1 graus de liberdade e fazemos \\gamma = P(-c < U < c) = P(A < \\mu < B) coo c sendo escolhido de acordo com \\gamma . Implementa\u00e7\u00e3o Considere dados sobre pesos de beb\u00eas logo ao nascer. bwt: peso do beb\u00ea ao nascer. gestation: dura\u00e7\u00e3o em dias da gesta\u00e7\u00e3o. parity: primeiro filho ou n\u00e3o. age: idade da m\u00e3e. height: altura da m\u00e3e em polegadas. weight: peso da m\u00e3e em pounds. smoke: se a m\u00e3e \u00e9 fumante ou n\u00e3o. # Importando bibliotecas import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import t birth_df = pd.read_csv(\"http://people.reed.edu/~jones/141/Bwt.dat\") birth_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bwt gestation parity age height weight smoke 0 120 284 0 27 62 100 0 1 113 282 0 33 64 135 0 2 128 279 0 28 64 115 1 3 108 282 0 23 67 125 1 4 136 286 0 25 62 93 0 sns.histplot(data = birth_df.bwt, kde = True) plt.title('Histograma dos pesos dos beb\u00eas') plt.show() birth_df[birth_df.smoke == 0].bwt.hist(density = True, label = 'N\u00e3o fumante') birth_df[birth_df.smoke == 1].bwt.hist(density = True, label = 'Fumante', alpha = 0.6) plt.xlabel('Peso') plt.legend() plt.show() Sabemos que essa \u00e9 uma extra\u00e7\u00e3o de uma popula\u00e7\u00e3o maior. Para conseguirmos mais amostras, vamos usar um procedimento chamado bootstrap . A ideia desse procedimento \u00e9 criar um novas amostras a partir de uma amostra inicial, usando replace = True como diferencial. Vou fazer esse procedimento diversas vezes e ir calculando a m\u00e9dia amostral. Como a m\u00e9dia amostral \u00e9 uma vari\u00e1vel aleat\u00f3ria, vamos obter um histograma das realiza\u00e7\u00f5es. Vamos supor que o peso W_i da crian\u00e7a i vem de uma distribui\u00e7\u00e3o com par\u00e2metros \\mu e \\sigma^2 desconhecidos. Nesse caso, \\bar{W}_i vir\u00e1 de uma distribui\u00e7\u00e3o normal com par\u00e2metros \\mu e \\sigma^2/n . ite = 10000 n = 200 bootstrap_means = np.zeros(ite) for i in range(ite): bootstrap_sample = birth_df.sample(n = n, replace = True, random_state=i) bootstrap_means[i] = bootstrap_sample.bwt.mean() sns.histplot(bootstrap_means, kde = True) plt.title(\"M\u00e9dias das amostras\") plt.xlabel('Peso') plt.show() Vamos calcular o nosso intervalo de confian\u00e7a com \\gamma = 0.95 . Temos que: gamma = 0.95 A = lambda x: np.mean(x) - t.ppf(q = (1 + gamma)/2, df = len(x) - 1)*np.std(x, ddof = 1)/len(x)**(1/2) B = lambda x: np.mean(x) + t.ppf(q = (1 + gamma)/2, df = len(x) - 1)*np.std(x, ddof = 1)/len(x)**(1/2) ite = 100 n = 500 bootstrap_intervals = np.zeros((ite,2)) for i in range(ite): bootstrap_sample = birth_df.sample(n = n, replace = True, random_state=i) bootstrap_intervals[i,0] = A(bootstrap_sample.bwt) bootstrap_intervals[i,1] = B(bootstrap_sample.bwt) out_values = np.where((bootstrap_intervals[:,0] > 119.5) | (bootstrap_intervals[:,1] < 119.5)) plt.figure(figsize = (6,10)) plt.scatter(bootstrap_intervals[:,0], np.arange(0,ite), color = 'red', label = 'a') plt.scatter(bootstrap_intervals[:,1], np.arange(0,ite), color = 'green', label = 'b') plt.scatter(bootstrap_intervals[out_values[0],0], out_values[0], color = 'black', label = 'Fora') plt.scatter(bootstrap_intervals[out_values[0],1], out_values[0], color = 'black') plt.vlines(119.5, ymin = 0, ymax = ite, linestyle = '--', alpha = 0.6, label = 'M\u00e9dia real') plt.legend() plt.show() Interpreta\u00e7\u00e3o Estamos fazendo uma afirma\u00e7\u00e3o probabil\u00edstica sobre o intervalo (A,B) antes de observar os dados. Ap\u00f3s observarmos os dados, n\u00e3o podemos interpretar (a,b) como um intervalo em que temos 95% de confian\u00e7a de g(\\theta) estar no intervalo. Antes de observarmos as amostras, temos a confian\u00e7a de que 95% dos intervalos conter\u00e3o \\mu . Sabemos que a realiza\u00e7\u00e3o infinita desse experimento faz com que 95% dos intervalos realizados contenham o valor verdadeiro. Sem simetria Constru\u00edmos anteriormente um intervalo sim\u00e9trico, onde a estat\u00edstica U acima mencionada estaria entre -c e c com probabilidade \\gamma . Mas podemos desenvolver intervalos n\u00e3o sim\u00e9tricos tamb\u00e9m. Uma forma que podemos fazer isso \u00e9 escolhendo \\gamma_1 e \\gamma_2 , tal que \\gamma_2 - \\gamma_1 = \\gamma . Assim: P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) = \\gamma Talvez vc esteja se perguntando: porque escolher \\gamma_1, \\gamma_2 dessa forma? Bom: \\begin{split} \\gamma &= P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) \\\\ &= P\\left(U < T_{n-1}^{-1}(\\gamma_2)\\right) - P\\left(U \\leq T_{n-1}^{-1}(\\gamma_1)\\right) \\\\ &= \\gamma_2 - \\gamma_1 \\end{split} Intervalo unilateral para a m\u00e9dia de N(\\mu,\\sigma^2) Nas mesma condi\u00e7\u00f5es do teorema anterior, mas as estat\u00edsticas para baixo e para cima com coeficiente \\gamma para \\mu s\u00e3o: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}} Quantidades Pivotais Uma vari\u00e1vel aleat\u00f3ria V(\\vec{X}, \\theta) \u00e9 uma quantidade pivotal se sua distribui\u00e7\u00e3o independe de \\theta . Assim, se X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) , ent\u00e3o V(\\vec{X}, \\theta) tem a mesma distribui\u00e7\u00e3o para todo \\theta . \ud83d\udcdd Exemplo da Gamma Sejam X_1, \\dots, X_n \\overset{iid}{\\sim} Gamma(a, \\lambda) , com a conhecido. Assim, T = \\sum_{i=1}^n X_i \\sim Gamma(na, \\lambda) . Temos que V(X, \\lambda) = \\lambda T \\sim \\gamma(na, 1), que n\u00e3o depende de \\lambda . Com isso, V \u00e9 uma quantidade pivotal. Uma forma de procurar por quantidades pivotais \u00e9 olhando a pdf da distribui\u00e7\u00e3o. No caso da Gamma, por exemplo, temos que f_T(t) = \\frac{\\lambda^{na}}{\\Gamma(na)} t^{an-1}e^{-t/\\lambda}, portanto, v = t/\\lambda , faz com que a densidade n\u00e3o depende de \\lambda . Portanto, multiplicar t por \\lambda \u00e9 suficiente. De forma geral, se a densidade de T \u00e9 da forma f(t|\\theta) = g(V(t,\\theta)) \\bigg|\\frac{\\partial}{\\partial t} V(t,\\theta)\\bigg|, para alguma fun\u00e7\u00e3o g e V uma fun\u00e7\u00e3o mon\u00f3tona em t . Veja que isso est\u00e1 relacionado com o Teorema da Mudan\u00e7a de Vari\u00e1vel. Dada uma quantidade pivotal V , ent\u00e3o C(X) = \\{\\theta : a \\le V(x,\\theta) \\le b\\} \u00e9 intervalo de confian\u00e7a com coeficiente \\gamma se \\mathbb{P}_{\\theta}(a \\le V(x,\\theta) \\le b) \\ge \\gamma . Teorema: Sejam X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Suponha que Exista V pivotal. A cdf G de V \u00e9 cont\u00ednua. Exista fun\u00e7\u00e3o r tal r(V(X,\\theta), X) = g(\\theta) , ou seja, \u00e9 uma esp\u00e9cie de \"inversa\". r(v,x) (3) \u00e9 uma fun\u00e7\u00e3o estritamente crescente em v para todo x . Ent\u00e3o A = r(G^{-1}(\\gamma_1), X) B = r(G^{-1}(\\gamma_2), X) s\u00e3o os pontos extremos do intervalo de confian\u00e7a exato para g(\\theta) de coeficiente \\gamma = \\gamma_2 - \\gamma_1 . Se r \u00e9 estritamente decrescente, invertemos A e B . Exemplo com Regress\u00e3o Linear O dataset que utilizei anteriormente n\u00e3o \u00e9 muito bom para esse exemplo, mas eu vou usar, de qualquer forma, para entendermos o processo e como pode nos ajudar o intervalo de confian\u00e7a. Em uma Regress\u00e3o Linear, queremos dizer aferir uma rela\u00e7\u00e3o linear entre duas vari\u00e1veis, isto \u00e9, queremos dizer que uma vari\u00e1vel pode ser obtida pela outra atrav\u00e9s de uma reta, mais um erro aleat\u00f3rio. Suponha que queremos estimar Y o peso da crian\u00e7a ao nascer, sabendo a informa\u00e7\u00e3o do tempo de gesta\u00e7\u00e3o X e que Y = aX + b + E, onde E \\sim N(0,\\sigma^2) . Nesse caso, estamos dizendo que Y|X \\sim N(aX + b, \\sigma^2) . Queremos estimar a e b de forma que tenhamos o melhor ajuste poss\u00edvel. Esse tema em espec\u00edfico n\u00e3o me interessa. Entretanto, podemos dizer que queremos estimar aX + b , a m\u00e9dia de uma normal, mas que muda para cada X = x observado. sns.lmplot(x = 'gestation', y = 'bwt', data = birth_df, height = 5, ci = 95) O resultado n\u00e3o foi muito bom (na verdade eu j\u00e1 imaginava isso). Mas o interessante \u00e9 tentar refletir o que essas bandas significam? Por que os pontos n\u00e3o est\u00e3o nela? Esper\u00e1vamos que estivesse? E por que ela diminui a vari\u00e2ncia com o n\u00famero de pontos? Essas perguntas v\u00e3o ser devidamente respondidas no pr\u00f3ximo curso de Modelagem Estat\u00edstica! Mas eu j\u00e1 vou adiantando que esse intervalo de confian\u00e7a \u00e9 para a m\u00e9dia estimada. Intervalos de confian\u00e7a assint\u00f3ticos Sejam X_1, \\dots, X_n \\overset{iid}{\\sim} F(\\theta) e \\hat{\\theta}_n o MLE correspondente. Assim, supondo as condi\u00e7\u00f5es de regularidade de Fisher, temos que \\sqrt{n}(\\hat{\\theta}_n - \\theta) \\to N(0, 1/I(\\theta)). Com isso, \\sqrt{n I(\\theta)}(\\hat{\\theta}_n - \\theta) \u00e9 uma quantidade aproximadamente pivotal . Com isso, defina S = \\{\\theta :\\sqrt{n I(\\theta)}|\\hat{\\theta}_n - \\theta| < z_{\\alpha/2}\\}, em que z_{\\alpha} = \\Phi^{-1}(1-\\alpha) e o quantile 1-\\alpha da normal padr\u00e3o. Portanto, \\mathbb{P}_{\\theta}(\\theta \\in S) \\to 1-\\alpha, quando n \\to \\infty . O conjunto S n\u00e3o \u00e9, todavia, um intervalo necessariamente. Al\u00e9m do mais, I(\\theta) pode ser dif\u00edcil de se obter. Na pratica, calcula-se I(\\hat{\\theta}) e se usa que I(\\hat{\\theta}_n)/I(\\theta) \\to 1 . Com isso, o intervalo \\left(\\hat{\\theta}_n - \\frac{z_{\\alpha/2}}{\\sqrt{n I(\\hat{\\theta}_n)}}, \\hat{\\theta}_n + \\frac{z_{\\alpha/2}}{\\sqrt{n I(\\hat{\\theta}_n)}}\\right) \u00e9 assintoticamente um intervalo de confian\u00e7a 1-\\alpha .","title":"Intervalos de Confian\u00e7a"},{"location":"infestatistica_MSc/confidence_interval/confidence_interval/#intervalos-de-confianca","text":"Esse tema procura responder qu\u00e3o confiantes estamos de um estimador. \u00c9 claro que essa pergunta tem que ser melhor descrita matematicamente. De forma geral, estamos procurando por uma estat\u00edstica C(X) em que para cada X=x observado, C(x) \u00e9 um conjunto. Mas na pr\u00e1tica, procuramos por estat\u00edsticas (A(X), B(X)) que nos deem confian\u00e7a de que contenham o par\u00e2metro verdadeiro, isto \u00e9. O intervalo [a,b] , uma realiza\u00e7\u00e3o de [A(X),B(X)] , tem 95% de confian\u00e7a se em 95% do tempo, o par\u00e2metro procurado est\u00e1 entre a e b . Veja que a ideia \u00e9 frequentista, dado que a interpreta\u00e7\u00e3o est\u00e1 ligada \u00e0 frequ\u00eancia de pertencimento quando o n\u00famero de experimentos tende para infinito. (Cuidado: N\u00e3o vamos falar da probabilidade do par\u00e2metro estar em [a,b] , isso n\u00e3o faz sentido, pois \\theta n\u00e3o \u00e9 uma vari\u00e1vel aleat\u00f3ria, e sim um valor fixo).","title":"Intervalos de Confian\u00e7a"},{"location":"infestatistica_MSc/confidence_interval/confidence_interval/#definicao","text":"Sejam X_1, ..., X_n \\overset{iid}{\\sim} F(\\theta) . Uma estimador intervalar de \\theta \u00e9 um par de estat\u00edsticas (A(X), B(X)) , tal que \\mathbb{P}(A(X) \\le B(X)) = 1 Se X=x \u00e9 observado, ent\u00e3o (A(x), B(x)) \u00e9 uma estimativa intervalar. Para um estimador intervalar, a probabilidade de cobertura \u00e9 \\mathbb{P}_{\\theta}(\\theta \\in [A(X), B(X)]) = \\mathbb{P}_{\\theta}(A(X) \\le \\theta, B(X) \\ge \\theta). Um intervalo de confian\u00e7a com coeficiente de confian\u00e7a \\gamma \u00e9 um estimador intervalar que satisfaz \\inf_{\\theta} \\mathbb{P}_{\\theta}(\\theta \\in [A(X), B(X)]) = \\gamma, ou tamb\u00e9m, \\mathbb{P}(A(X) < \\theta < B(X)) \\ge \\gamma , isto \u00e9, a probabilidade de cobertura \u00e9 no m\u00ednimo \\gamma . Ap\u00f3s observarmos os valores de X_1, ..., X_n e computarmos A = a e B = b , o intervalo (a,b) \u00e9 chamado de valor observado do intervalo de confian\u00e7a.","title":"Defini\u00e7\u00e3o"},{"location":"infestatistica_MSc/confidence_interval/confidence_interval/#intervalo-de-confianca-para-a-media-de-nmu-sigma2","text":"Seja X_1, ..., X_n \\sim N(\\mu, \\sigma^2) . Para cada 0 < \\gamma < 1 , o intervalo (A,B) \u00e9 intervalo de confian\u00e7a exato para \\mu com coeficiente \\gamma , em que: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\frac{1 + \\gamma}{2}\\right)\\frac{\\sigma '}{n^{1/2}} onde T_{n-1} denota a cdf da distribui\u00e7\u00e3o t com n-1 graus de liberdade. Esse resultado implica do fato de que a distribui\u00e7\u00e3o de U = \\frac{n^{1/2}(\\bar{X}_n - \\mu)}{\\sigma '} \u00e9 conhecida por distribui\u00e7\u00e3o t com n-1 graus de liberdade e fazemos \\gamma = P(-c < U < c) = P(A < \\mu < B) coo c sendo escolhido de acordo com \\gamma .","title":"Intervalo de Confian\u00e7a para a m\u00e9dia de N(\\mu, \\sigma^2)"},{"location":"infestatistica_MSc/confidence_interval/confidence_interval/#implementacao","text":"Considere dados sobre pesos de beb\u00eas logo ao nascer. bwt: peso do beb\u00ea ao nascer. gestation: dura\u00e7\u00e3o em dias da gesta\u00e7\u00e3o. parity: primeiro filho ou n\u00e3o. age: idade da m\u00e3e. height: altura da m\u00e3e em polegadas. weight: peso da m\u00e3e em pounds. smoke: se a m\u00e3e \u00e9 fumante ou n\u00e3o. # Importando bibliotecas import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import t birth_df = pd.read_csv(\"http://people.reed.edu/~jones/141/Bwt.dat\") birth_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bwt gestation parity age height weight smoke 0 120 284 0 27 62 100 0 1 113 282 0 33 64 135 0 2 128 279 0 28 64 115 1 3 108 282 0 23 67 125 1 4 136 286 0 25 62 93 0 sns.histplot(data = birth_df.bwt, kde = True) plt.title('Histograma dos pesos dos beb\u00eas') plt.show() birth_df[birth_df.smoke == 0].bwt.hist(density = True, label = 'N\u00e3o fumante') birth_df[birth_df.smoke == 1].bwt.hist(density = True, label = 'Fumante', alpha = 0.6) plt.xlabel('Peso') plt.legend() plt.show() Sabemos que essa \u00e9 uma extra\u00e7\u00e3o de uma popula\u00e7\u00e3o maior. Para conseguirmos mais amostras, vamos usar um procedimento chamado bootstrap . A ideia desse procedimento \u00e9 criar um novas amostras a partir de uma amostra inicial, usando replace = True como diferencial. Vou fazer esse procedimento diversas vezes e ir calculando a m\u00e9dia amostral. Como a m\u00e9dia amostral \u00e9 uma vari\u00e1vel aleat\u00f3ria, vamos obter um histograma das realiza\u00e7\u00f5es. Vamos supor que o peso W_i da crian\u00e7a i vem de uma distribui\u00e7\u00e3o com par\u00e2metros \\mu e \\sigma^2 desconhecidos. Nesse caso, \\bar{W}_i vir\u00e1 de uma distribui\u00e7\u00e3o normal com par\u00e2metros \\mu e \\sigma^2/n . ite = 10000 n = 200 bootstrap_means = np.zeros(ite) for i in range(ite): bootstrap_sample = birth_df.sample(n = n, replace = True, random_state=i) bootstrap_means[i] = bootstrap_sample.bwt.mean() sns.histplot(bootstrap_means, kde = True) plt.title(\"M\u00e9dias das amostras\") plt.xlabel('Peso') plt.show() Vamos calcular o nosso intervalo de confian\u00e7a com \\gamma = 0.95 . Temos que: gamma = 0.95 A = lambda x: np.mean(x) - t.ppf(q = (1 + gamma)/2, df = len(x) - 1)*np.std(x, ddof = 1)/len(x)**(1/2) B = lambda x: np.mean(x) + t.ppf(q = (1 + gamma)/2, df = len(x) - 1)*np.std(x, ddof = 1)/len(x)**(1/2) ite = 100 n = 500 bootstrap_intervals = np.zeros((ite,2)) for i in range(ite): bootstrap_sample = birth_df.sample(n = n, replace = True, random_state=i) bootstrap_intervals[i,0] = A(bootstrap_sample.bwt) bootstrap_intervals[i,1] = B(bootstrap_sample.bwt) out_values = np.where((bootstrap_intervals[:,0] > 119.5) | (bootstrap_intervals[:,1] < 119.5)) plt.figure(figsize = (6,10)) plt.scatter(bootstrap_intervals[:,0], np.arange(0,ite), color = 'red', label = 'a') plt.scatter(bootstrap_intervals[:,1], np.arange(0,ite), color = 'green', label = 'b') plt.scatter(bootstrap_intervals[out_values[0],0], out_values[0], color = 'black', label = 'Fora') plt.scatter(bootstrap_intervals[out_values[0],1], out_values[0], color = 'black') plt.vlines(119.5, ymin = 0, ymax = ite, linestyle = '--', alpha = 0.6, label = 'M\u00e9dia real') plt.legend() plt.show()","title":"Implementa\u00e7\u00e3o"},{"location":"infestatistica_MSc/confidence_interval/confidence_interval/#interpretacao","text":"Estamos fazendo uma afirma\u00e7\u00e3o probabil\u00edstica sobre o intervalo (A,B) antes de observar os dados. Ap\u00f3s observarmos os dados, n\u00e3o podemos interpretar (a,b) como um intervalo em que temos 95% de confian\u00e7a de g(\\theta) estar no intervalo. Antes de observarmos as amostras, temos a confian\u00e7a de que 95% dos intervalos conter\u00e3o \\mu . Sabemos que a realiza\u00e7\u00e3o infinita desse experimento faz com que 95% dos intervalos realizados contenham o valor verdadeiro.","title":"Interpreta\u00e7\u00e3o"},{"location":"infestatistica_MSc/confidence_interval/confidence_interval/#sem-simetria","text":"Constru\u00edmos anteriormente um intervalo sim\u00e9trico, onde a estat\u00edstica U acima mencionada estaria entre -c e c com probabilidade \\gamma . Mas podemos desenvolver intervalos n\u00e3o sim\u00e9tricos tamb\u00e9m. Uma forma que podemos fazer isso \u00e9 escolhendo \\gamma_1 e \\gamma_2 , tal que \\gamma_2 - \\gamma_1 = \\gamma . Assim: P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) = \\gamma Talvez vc esteja se perguntando: porque escolher \\gamma_1, \\gamma_2 dessa forma? Bom: \\begin{split} \\gamma &= P\\left(T_{n-1}^{-1}(\\gamma_1) < U < T_{n-1}^{-1}(\\gamma_2)\\right) \\\\ &= P\\left(U < T_{n-1}^{-1}(\\gamma_2)\\right) - P\\left(U \\leq T_{n-1}^{-1}(\\gamma_1)\\right) \\\\ &= \\gamma_2 - \\gamma_1 \\end{split}","title":"Sem simetria"},{"location":"infestatistica_MSc/confidence_interval/confidence_interval/#intervalo-unilateral-para-a-media-de-nmusigma2","text":"Nas mesma condi\u00e7\u00f5es do teorema anterior, mas as estat\u00edsticas para baixo e para cima com coeficiente \\gamma para \\mu s\u00e3o: A = \\bar{X}_n - T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}} B = \\bar{X}_n + T_{n-1}^{-1}\\left(\\gamma\\right)\\frac{\\sigma '}{n^{1/2}}","title":"Intervalo unilateral para a m\u00e9dia de N(\\mu,\\sigma^2)"},{"location":"infestatistica_MSc/confidence_interval/confidence_interval/#quantidades-pivotais","text":"Uma vari\u00e1vel aleat\u00f3ria V(\\vec{X}, \\theta) \u00e9 uma quantidade pivotal se sua distribui\u00e7\u00e3o independe de \\theta . Assim, se X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) , ent\u00e3o V(\\vec{X}, \\theta) tem a mesma distribui\u00e7\u00e3o para todo \\theta . \ud83d\udcdd Exemplo da Gamma Sejam X_1, \\dots, X_n \\overset{iid}{\\sim} Gamma(a, \\lambda) , com a conhecido. Assim, T = \\sum_{i=1}^n X_i \\sim Gamma(na, \\lambda) . Temos que V(X, \\lambda) = \\lambda T \\sim \\gamma(na, 1), que n\u00e3o depende de \\lambda . Com isso, V \u00e9 uma quantidade pivotal. Uma forma de procurar por quantidades pivotais \u00e9 olhando a pdf da distribui\u00e7\u00e3o. No caso da Gamma, por exemplo, temos que f_T(t) = \\frac{\\lambda^{na}}{\\Gamma(na)} t^{an-1}e^{-t/\\lambda}, portanto, v = t/\\lambda , faz com que a densidade n\u00e3o depende de \\lambda . Portanto, multiplicar t por \\lambda \u00e9 suficiente. De forma geral, se a densidade de T \u00e9 da forma f(t|\\theta) = g(V(t,\\theta)) \\bigg|\\frac{\\partial}{\\partial t} V(t,\\theta)\\bigg|, para alguma fun\u00e7\u00e3o g e V uma fun\u00e7\u00e3o mon\u00f3tona em t . Veja que isso est\u00e1 relacionado com o Teorema da Mudan\u00e7a de Vari\u00e1vel. Dada uma quantidade pivotal V , ent\u00e3o C(X) = \\{\\theta : a \\le V(x,\\theta) \\le b\\} \u00e9 intervalo de confian\u00e7a com coeficiente \\gamma se \\mathbb{P}_{\\theta}(a \\le V(x,\\theta) \\le b) \\ge \\gamma . Teorema: Sejam X_1, ..., X_n \\overset{idd}{\\sim} F(\\theta) . Suponha que Exista V pivotal. A cdf G de V \u00e9 cont\u00ednua. Exista fun\u00e7\u00e3o r tal r(V(X,\\theta), X) = g(\\theta) , ou seja, \u00e9 uma esp\u00e9cie de \"inversa\". r(v,x) (3) \u00e9 uma fun\u00e7\u00e3o estritamente crescente em v para todo x . Ent\u00e3o A = r(G^{-1}(\\gamma_1), X) B = r(G^{-1}(\\gamma_2), X) s\u00e3o os pontos extremos do intervalo de confian\u00e7a exato para g(\\theta) de coeficiente \\gamma = \\gamma_2 - \\gamma_1 . Se r \u00e9 estritamente decrescente, invertemos A e B .","title":"Quantidades Pivotais"},{"location":"infestatistica_MSc/confidence_interval/confidence_interval/#exemplo-com-regressao-linear","text":"O dataset que utilizei anteriormente n\u00e3o \u00e9 muito bom para esse exemplo, mas eu vou usar, de qualquer forma, para entendermos o processo e como pode nos ajudar o intervalo de confian\u00e7a. Em uma Regress\u00e3o Linear, queremos dizer aferir uma rela\u00e7\u00e3o linear entre duas vari\u00e1veis, isto \u00e9, queremos dizer que uma vari\u00e1vel pode ser obtida pela outra atrav\u00e9s de uma reta, mais um erro aleat\u00f3rio. Suponha que queremos estimar Y o peso da crian\u00e7a ao nascer, sabendo a informa\u00e7\u00e3o do tempo de gesta\u00e7\u00e3o X e que Y = aX + b + E, onde E \\sim N(0,\\sigma^2) . Nesse caso, estamos dizendo que Y|X \\sim N(aX + b, \\sigma^2) . Queremos estimar a e b de forma que tenhamos o melhor ajuste poss\u00edvel. Esse tema em espec\u00edfico n\u00e3o me interessa. Entretanto, podemos dizer que queremos estimar aX + b , a m\u00e9dia de uma normal, mas que muda para cada X = x observado. sns.lmplot(x = 'gestation', y = 'bwt', data = birth_df, height = 5, ci = 95) O resultado n\u00e3o foi muito bom (na verdade eu j\u00e1 imaginava isso). Mas o interessante \u00e9 tentar refletir o que essas bandas significam? Por que os pontos n\u00e3o est\u00e3o nela? Esper\u00e1vamos que estivesse? E por que ela diminui a vari\u00e2ncia com o n\u00famero de pontos? Essas perguntas v\u00e3o ser devidamente respondidas no pr\u00f3ximo curso de Modelagem Estat\u00edstica! Mas eu j\u00e1 vou adiantando que esse intervalo de confian\u00e7a \u00e9 para a m\u00e9dia estimada.","title":"Exemplo com Regress\u00e3o Linear"},{"location":"infestatistica_MSc/confidence_interval/confidence_interval/#intervalos-de-confianca-assintoticos","text":"Sejam X_1, \\dots, X_n \\overset{iid}{\\sim} F(\\theta) e \\hat{\\theta}_n o MLE correspondente. Assim, supondo as condi\u00e7\u00f5es de regularidade de Fisher, temos que \\sqrt{n}(\\hat{\\theta}_n - \\theta) \\to N(0, 1/I(\\theta)). Com isso, \\sqrt{n I(\\theta)}(\\hat{\\theta}_n - \\theta) \u00e9 uma quantidade aproximadamente pivotal . Com isso, defina S = \\{\\theta :\\sqrt{n I(\\theta)}|\\hat{\\theta}_n - \\theta| < z_{\\alpha/2}\\}, em que z_{\\alpha} = \\Phi^{-1}(1-\\alpha) e o quantile 1-\\alpha da normal padr\u00e3o. Portanto, \\mathbb{P}_{\\theta}(\\theta \\in S) \\to 1-\\alpha, quando n \\to \\infty . O conjunto S n\u00e3o \u00e9, todavia, um intervalo necessariamente. Al\u00e9m do mais, I(\\theta) pode ser dif\u00edcil de se obter. Na pratica, calcula-se I(\\hat{\\theta}) e se usa que I(\\hat{\\theta}_n)/I(\\theta) \\to 1 . Com isso, o intervalo \\left(\\hat{\\theta}_n - \\frac{z_{\\alpha/2}}{\\sqrt{n I(\\hat{\\theta}_n)}}, \\hat{\\theta}_n + \\frac{z_{\\alpha/2}}{\\sqrt{n I(\\hat{\\theta}_n)}}\\right) \u00e9 assintoticamente um intervalo de confian\u00e7a 1-\\alpha .","title":"Intervalos de confian\u00e7a assint\u00f3ticos"},{"location":"infestatistica_MSc/estimation/large_sample/","text":"Introdu\u00e7\u00e3o a grandes amostras Agora, vamos verificar algumas propriedades assint\u00f3ticas, isto \u00e9, quando o n\u00famero de amostras \u00e9 muito grande, com n \\to \\infty . Conceitos de converg\u00eancia Uma lista de conceitos de converg\u00eancia importantes. Em particular, o conceito de consist\u00eancia \u00e9 importante para estimadores, dado que gostar\u00edamos que, com amostras suficientes, tiv\u00e9ssemos valores razo\u00e1veis para o par\u00e2metro. Determin\u00edstica: Seja \\{x_n\\}_{n \\in \\mathbb{N}} uma sequ\u00eancia em um espa\u00e7o normado e \\{r_n\\}_{n \\in \\mathbb{N}} uma sequ\u00eancia de reais. Se para cada c > 0 , existe N suficientemente grande tal que ||x_n|| \\le c|r_n| para n \\ge N , dizemos que x_n = o(x_n) . Se existe c > 0 tal que a desigualdade anterior valha para n grande, ent\u00e3o dizemos que x_n = O(r_n) . Estoc\u00e1stica: Seja \\{X_n\\}_{n \\in \\mathbb{N}} uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias definidas em espa\u00e7os normados e \\{r_n\\}_{n \\in \\mathbb{N}} uma sequ\u00eancia de reais. Se para cada c > 0 e \\epsilon > 0 , existe N tal que \\mathbb{P}(||X_n|| \\le c|r_n|) \\ge 1 - \\epsilon para n \\ge N , dizemos que X_n = o_P(x_n) . Se para cada \\epsilon >0 , existe c > 0 tal que a desigualdade anterior valha para n grande, ent\u00e3o dizemos que X_n = O_P(r_n) . Converg\u00eancia em probabilidade: Se \\{X_n\\}_{n \\in \\mathbb{N}} e X s\u00e3o quantidades aleat\u00f3rias e, para todo \\epsilon >0 , vale que \\lim_{n \\to \\infty} Pr(||X_n - X|| > \\epsilon) = 0, ent\u00e3o dizemos que X_n \\overset{P}{\\to} X , isto \u00e9, X_n converge em probabilidade para X . Se Y_n = f_n(X_n) para uma sequ\u00eancia de fun\u00e7\u00f5es mensur\u00e1veis f_n e Y \u00e9 uma outra quantidade aleat\u00f3ria, temos que ||Y_n - Y|| = o_P(1) se, e s\u00f3 se, Y_n \\overset{P}{\\to} Y . Em particular, se f \u00e9 cont\u00ednua em c e X_n \\overset{P}{\\to} c , ent\u00e3o f(X_n) \\overset{P}{\\to} f(c) . Consist\u00eancia: Sejam g uma fun\u00e7\u00e3o mensur\u00e1vel e P_{\\theta} uma distribui\u00e7\u00e3o param\u00e9trica definida em \\mathcal{X} . A sequ\u00eancia de vari\u00e1veis aleat\u00f3rias Y_n : \\mathcal{X}^n \\to G \u00e9 consistente para g(\\theta) se Y_n \\overset{P}{\\to} g(\\theta) para todo \\theta \\in \\Omega . A lei dos grandes n\u00fameros \u00e9 um forte aliado, pois afirma que a m\u00e9dia amostral converge em probabilidade para a m\u00e9dia verdadeira. Converg\u00eancia em distribui\u00e7\u00e3o: Seja \\{X_n\\} uma sequ\u00eancia de quantidades aleat\u00f3rias e X uma quantidade aleat\u00f3ria. Se \\lim_{n \\to \\infty} \\mathbb{E}[f(X_n)] = \\mathbb{E}[f(X)], para toda fun\u00e7\u00e3o cont\u00ednua limitada f , dizemos que X_n converge em distribui\u00e7\u00e3o para X , isto \u00e9, X_n \\overset{D}{\\to} X ou X_n \\Rightarrow X . Dizemos que a distribui\u00e7\u00e3o de X \u00e9 a distribui\u00e7\u00e3o assint\u00f3tica de X_n . Al\u00e9m do mais, dizemos que a distribui\u00e7\u00e3o de X_n converge fracamente para a distribui\u00e7\u00e3o de X . Se H_n(x) \u00e9 a fun\u00e7\u00e3o de distribui\u00e7\u00e3o acumulada de X_n e H \u00e9 a CDF de X , ent\u00e3o H_n(x) \\to H(x) sempre que H \u00e9 cont\u00ednua em x se, e somente se, X_n \\overset{D}{\\to} X Teorema: Se Y_n \\Rightarrow Y , A_n \\overset{P}{\\to} a e B_n \\overset{P}{\\to} b , ent\u00e3o A_n + B_nY_n \\Rightarrow a + bY. Converg\u00eancia quase certa: Uma sequ\u00eancia \\{X_n\\} converge quase certamente para X se \\mathbb{P}(Y_n \\to Y) = 1 . Consist\u00eancia O exemplo cl\u00e1ssico de consist\u00eancia \u00e9 o seguinte: \ud83d\udcdd Exemplo (Lei fraca dos grandes n\u00fameros) Seja X_1, X_2, \\dots uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias independentes cuja distribui\u00e7\u00e3o tem densidade f(x|\\theta) . Defina g(\\theta) = \\mathbb{E}_{\\theta}[X] e \\bar{X}_n = n^{-1}(X_1 + \\dots + X_n) . Pela lei fraca dos grandes n\u00fameros, temos que \\{\\bar{X}_n\\} \u00e9 uma sequ\u00eancia de estimadores consistente para g(\\theta) . Teorema: Seja \\{W_n\\} uma sequ\u00eancia de estimadores para \\theta tal que, para todo \\theta \\in \\Omega , (i) \\lim_{n \\to \\infty} \\operatorname{Var}_{\\theta}(W_n) = 0 , (ii) \\lim_{n \\to \\infty} \\operatorname{Bias}_{\\theta}(W_n) = 0 . Ent\u00e3o W_n \u00e9 sequ\u00eancia de estimadores consistente de \\theta . Esse resultado \u00e9 consequ\u00eancia direto do fato de que \\mathbb{E}_{\\theta}[(W_n - \\theta)^2] = \\operatorname{Var}_{\\theta}(W_n) + \\operatorname{Bias}_{\\theta}(W_n)^2 e da desigualdade de Chebyshev, P_{\\theta}(|W_n - \\theta| \\ge \\epsilon) \\le \\frac{\\mathbb{E}_{\\theta}[(W_n - \\theta)^2]}{\\epsilon^2}. Al\u00e9m do mais, se fizemos U_n = a_n W_n + b_n , com a_n \\to 1 e b_n \\to 0 , temos que U_n tamb\u00e9m \u00e9 consistente. Teorema Central do Limite Seja X_1, \\dots, X_n vari\u00e1veis aleat\u00f3rias iid com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 . Ent\u00e3o \\sqrt{n}(\\bar{X}_n - \\mu) \\Rightarrow N(0, \\sigma^2). Propriedades assint\u00f3ticas MLE O seguinte Teorema afirma que sob o modelo f(x|\\theta_0) , a probabilidade de que a fun\u00e7\u00e3o de verossimilhan\u00e7a seja estritamente maior em \\theta_0 tende a 1 quando n \\to \\infty . Nesse sentido, se n \u00e9 suficientemente grande, nossa probabilidade de que sob aquela amostra a verossimilhan\u00e7a seja maior do que qualquer outro ponto \u00e9 pr\u00f3ximo a 1. Teorema: Seja f_n(x|\\theta) a densidade de uma amostra aleat\u00f3ria (X_1, \\dots, X_n) . Ent\u00e3o, para cada \\theta_0, \\theta \\in \\Omega com \\theta \\neq \\theta_0 , vale que \\lim_{n \\to \\infty} P_{\\theta_0}\\left[f_n(x|\\theta_0) > f_n(x|\\theta)\\right] = 1. Agora, vamos verificar que o MLE \u00e9 um estimador consistente. Teorema: Seja f_n(x|\\theta) a densidade de uma amostra aleat\u00f3ria (X_1, \\dots, X_n) e fixe \\theta_0 \\in \\Omega . Para cada M\\subseteq \\Omega e x \\in \\mathcal{X} , defina Z(M,x) = \\inf_{\\psi \\in M} \\log \\frac{f(x|\\theta_0)}{f(x|\\psi)} = \\log f(x|\\theta_0) - \\sup_{\\psi \\in \\Omega} \\log f(x|\\psi). Assuma que para cada \\theta \\neq \\theta_0 , exista uma vizinhan\u00e7a N_{\\theta} tal que \\mathbb{E}_{\\theta_0}[Z(N_{\\theta}, X_i)] > 0 . Se \\Omega n\u00e3o \u00e9 compacto, assuma que exista um compacto C \\subseteq \\Omega tal que \\theta_0 \\in C e \\mathbb{E}_{\\theta_0} Z(\\omega / C, X_i) > 0 . Ent\u00e3o \\lim_{n \\to \\infty} \\hat{\\theta}_n = \\theta_0, quase certamente. A fun\u00e7\u00e3o Z(M,x) \u00e9 a diferen\u00e7a da log-verossimilhan\u00e7a em \\theta_0 e o m\u00e1ximo que ela atinge em M . Estamos assumindo que em uma vizinhan\u00e7a de cada ponto \\theta \\in \\Omega , a m\u00e1xima verossimilhan\u00e7a nessa regi\u00e3o \u00e9, em m\u00e9dia, menor do que a verossimilhan\u00e7a em \\theta_0 . Al\u00e9m disso, para conjuntos n\u00e3o compactos, fora desse compacto, queremos que em m\u00e9dia a verossimilhan\u00e7a em \\theta_0 seja maior. A dificuldade de utilizar esse teorema \u00e9 verificar todas essas condi\u00e7\u00f5es. Assumindo a continuidade da fun\u00e7\u00e3o de verossimilhan\u00e7a para todo x e que \\mathbb{E}_{\\theta_0}[Z(N_{\\theta}, X_i)] > -\\infty , tamb\u00e9m temos que MLE \u00e9 um estimador consistente. MLE para a fam\u00edlia exponencial Seja \\hat{\\theta}_n MLE calculado a partir de X_1, \\dots, X_n cuja distribui\u00e7\u00e3o tem densidade f(x|\\theta) = h(x)\\exp\\{\\theta\\cdot x - A(\\theta)\\}, isto \u00e9, pertence \u00e0 fam\u00edlia exponencial na forma can\u00f4nica. Se \\Omega \u00e9 um subconjunto aberto de \\mathbb{R}^k , ent\u00e3o \\lim_{n\\to\\infty} \\mathbb{P}_{\\theta}(\\hat{\\theta}_n \\text{ existir}) = 1 \\sqrt{n}(\\hat{\\theta}_n - \\theta) \\overset{D}{\\to} N_k(0, I_{\\mathcal{X}}(\\theta))^{-1} , em que I_{\\mathcal{X}}(\\theta) \u00e9 a matriz informa\u00e7\u00e3o de Fisher. \\{\\hat{\\theta}_n\\} \u00e9 consistente para \\theta . Normalidade assint\u00f3tica para MLEs Sob algumas hip\u00f3teses de regularidade, podemos concluir que \\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\overset{D}{\\to} N(0, I_{\\mathcal{X}}(\\theta_0)^{-1}), em que I_{\\mathcal{X}}(\\theta_0) \u00e9 a informa\u00e7\u00e3o de Fisher. Hip\u00f3teses: \\{\\hat{\\theta}_n\\} \u00e9 consistente para \\theta . f(x|\\theta) tem derivadas de segunda ordem cont\u00ednuas com respeito a \\theta e vale o sinal de integra\u00e7\u00e3o pode ser trocado com o de diferencia\u00e7\u00e3o. Existe uma fun\u00e7\u00e3o H_r(x,\\theta) que para todo \\theta_0 tem valor esperando nulo quanto r \\to 0 e seja limite superior da diferen\u00e7a na segunda derivada em uma bola de raio r de \\theta_0 . Na pr\u00e1tica, podemos usar I_{\\mathcal{X}}(\\hat{\\theta}_n) , visto que \\theta_0 \u00e9 desconhecido.","title":"Introdu\u00e7\u00e3o a grandes amostras"},{"location":"infestatistica_MSc/estimation/large_sample/#introducao-a-grandes-amostras","text":"Agora, vamos verificar algumas propriedades assint\u00f3ticas, isto \u00e9, quando o n\u00famero de amostras \u00e9 muito grande, com n \\to \\infty .","title":"Introdu\u00e7\u00e3o a grandes amostras"},{"location":"infestatistica_MSc/estimation/large_sample/#conceitos-de-convergencia","text":"Uma lista de conceitos de converg\u00eancia importantes. Em particular, o conceito de consist\u00eancia \u00e9 importante para estimadores, dado que gostar\u00edamos que, com amostras suficientes, tiv\u00e9ssemos valores razo\u00e1veis para o par\u00e2metro. Determin\u00edstica: Seja \\{x_n\\}_{n \\in \\mathbb{N}} uma sequ\u00eancia em um espa\u00e7o normado e \\{r_n\\}_{n \\in \\mathbb{N}} uma sequ\u00eancia de reais. Se para cada c > 0 , existe N suficientemente grande tal que ||x_n|| \\le c|r_n| para n \\ge N , dizemos que x_n = o(x_n) . Se existe c > 0 tal que a desigualdade anterior valha para n grande, ent\u00e3o dizemos que x_n = O(r_n) . Estoc\u00e1stica: Seja \\{X_n\\}_{n \\in \\mathbb{N}} uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias definidas em espa\u00e7os normados e \\{r_n\\}_{n \\in \\mathbb{N}} uma sequ\u00eancia de reais. Se para cada c > 0 e \\epsilon > 0 , existe N tal que \\mathbb{P}(||X_n|| \\le c|r_n|) \\ge 1 - \\epsilon para n \\ge N , dizemos que X_n = o_P(x_n) . Se para cada \\epsilon >0 , existe c > 0 tal que a desigualdade anterior valha para n grande, ent\u00e3o dizemos que X_n = O_P(r_n) . Converg\u00eancia em probabilidade: Se \\{X_n\\}_{n \\in \\mathbb{N}} e X s\u00e3o quantidades aleat\u00f3rias e, para todo \\epsilon >0 , vale que \\lim_{n \\to \\infty} Pr(||X_n - X|| > \\epsilon) = 0, ent\u00e3o dizemos que X_n \\overset{P}{\\to} X , isto \u00e9, X_n converge em probabilidade para X . Se Y_n = f_n(X_n) para uma sequ\u00eancia de fun\u00e7\u00f5es mensur\u00e1veis f_n e Y \u00e9 uma outra quantidade aleat\u00f3ria, temos que ||Y_n - Y|| = o_P(1) se, e s\u00f3 se, Y_n \\overset{P}{\\to} Y . Em particular, se f \u00e9 cont\u00ednua em c e X_n \\overset{P}{\\to} c , ent\u00e3o f(X_n) \\overset{P}{\\to} f(c) . Consist\u00eancia: Sejam g uma fun\u00e7\u00e3o mensur\u00e1vel e P_{\\theta} uma distribui\u00e7\u00e3o param\u00e9trica definida em \\mathcal{X} . A sequ\u00eancia de vari\u00e1veis aleat\u00f3rias Y_n : \\mathcal{X}^n \\to G \u00e9 consistente para g(\\theta) se Y_n \\overset{P}{\\to} g(\\theta) para todo \\theta \\in \\Omega . A lei dos grandes n\u00fameros \u00e9 um forte aliado, pois afirma que a m\u00e9dia amostral converge em probabilidade para a m\u00e9dia verdadeira. Converg\u00eancia em distribui\u00e7\u00e3o: Seja \\{X_n\\} uma sequ\u00eancia de quantidades aleat\u00f3rias e X uma quantidade aleat\u00f3ria. Se \\lim_{n \\to \\infty} \\mathbb{E}[f(X_n)] = \\mathbb{E}[f(X)], para toda fun\u00e7\u00e3o cont\u00ednua limitada f , dizemos que X_n converge em distribui\u00e7\u00e3o para X , isto \u00e9, X_n \\overset{D}{\\to} X ou X_n \\Rightarrow X . Dizemos que a distribui\u00e7\u00e3o de X \u00e9 a distribui\u00e7\u00e3o assint\u00f3tica de X_n . Al\u00e9m do mais, dizemos que a distribui\u00e7\u00e3o de X_n converge fracamente para a distribui\u00e7\u00e3o de X . Se H_n(x) \u00e9 a fun\u00e7\u00e3o de distribui\u00e7\u00e3o acumulada de X_n e H \u00e9 a CDF de X , ent\u00e3o H_n(x) \\to H(x) sempre que H \u00e9 cont\u00ednua em x se, e somente se, X_n \\overset{D}{\\to} X Teorema: Se Y_n \\Rightarrow Y , A_n \\overset{P}{\\to} a e B_n \\overset{P}{\\to} b , ent\u00e3o A_n + B_nY_n \\Rightarrow a + bY. Converg\u00eancia quase certa: Uma sequ\u00eancia \\{X_n\\} converge quase certamente para X se \\mathbb{P}(Y_n \\to Y) = 1 .","title":"Conceitos de converg\u00eancia"},{"location":"infestatistica_MSc/estimation/large_sample/#consistencia","text":"O exemplo cl\u00e1ssico de consist\u00eancia \u00e9 o seguinte: \ud83d\udcdd Exemplo (Lei fraca dos grandes n\u00fameros) Seja X_1, X_2, \\dots uma sequ\u00eancia de vari\u00e1veis aleat\u00f3rias independentes cuja distribui\u00e7\u00e3o tem densidade f(x|\\theta) . Defina g(\\theta) = \\mathbb{E}_{\\theta}[X] e \\bar{X}_n = n^{-1}(X_1 + \\dots + X_n) . Pela lei fraca dos grandes n\u00fameros, temos que \\{\\bar{X}_n\\} \u00e9 uma sequ\u00eancia de estimadores consistente para g(\\theta) . Teorema: Seja \\{W_n\\} uma sequ\u00eancia de estimadores para \\theta tal que, para todo \\theta \\in \\Omega , (i) \\lim_{n \\to \\infty} \\operatorname{Var}_{\\theta}(W_n) = 0 , (ii) \\lim_{n \\to \\infty} \\operatorname{Bias}_{\\theta}(W_n) = 0 . Ent\u00e3o W_n \u00e9 sequ\u00eancia de estimadores consistente de \\theta . Esse resultado \u00e9 consequ\u00eancia direto do fato de que \\mathbb{E}_{\\theta}[(W_n - \\theta)^2] = \\operatorname{Var}_{\\theta}(W_n) + \\operatorname{Bias}_{\\theta}(W_n)^2 e da desigualdade de Chebyshev, P_{\\theta}(|W_n - \\theta| \\ge \\epsilon) \\le \\frac{\\mathbb{E}_{\\theta}[(W_n - \\theta)^2]}{\\epsilon^2}. Al\u00e9m do mais, se fizemos U_n = a_n W_n + b_n , com a_n \\to 1 e b_n \\to 0 , temos que U_n tamb\u00e9m \u00e9 consistente.","title":"Consist\u00eancia"},{"location":"infestatistica_MSc/estimation/large_sample/#teorema-central-do-limite","text":"Seja X_1, \\dots, X_n vari\u00e1veis aleat\u00f3rias iid com m\u00e9dia \\mu e vari\u00e2ncia \\sigma^2 . Ent\u00e3o \\sqrt{n}(\\bar{X}_n - \\mu) \\Rightarrow N(0, \\sigma^2).","title":"Teorema Central do Limite"},{"location":"infestatistica_MSc/estimation/large_sample/#propriedades-assintoticas-mle","text":"O seguinte Teorema afirma que sob o modelo f(x|\\theta_0) , a probabilidade de que a fun\u00e7\u00e3o de verossimilhan\u00e7a seja estritamente maior em \\theta_0 tende a 1 quando n \\to \\infty . Nesse sentido, se n \u00e9 suficientemente grande, nossa probabilidade de que sob aquela amostra a verossimilhan\u00e7a seja maior do que qualquer outro ponto \u00e9 pr\u00f3ximo a 1. Teorema: Seja f_n(x|\\theta) a densidade de uma amostra aleat\u00f3ria (X_1, \\dots, X_n) . Ent\u00e3o, para cada \\theta_0, \\theta \\in \\Omega com \\theta \\neq \\theta_0 , vale que \\lim_{n \\to \\infty} P_{\\theta_0}\\left[f_n(x|\\theta_0) > f_n(x|\\theta)\\right] = 1. Agora, vamos verificar que o MLE \u00e9 um estimador consistente. Teorema: Seja f_n(x|\\theta) a densidade de uma amostra aleat\u00f3ria (X_1, \\dots, X_n) e fixe \\theta_0 \\in \\Omega . Para cada M\\subseteq \\Omega e x \\in \\mathcal{X} , defina Z(M,x) = \\inf_{\\psi \\in M} \\log \\frac{f(x|\\theta_0)}{f(x|\\psi)} = \\log f(x|\\theta_0) - \\sup_{\\psi \\in \\Omega} \\log f(x|\\psi). Assuma que para cada \\theta \\neq \\theta_0 , exista uma vizinhan\u00e7a N_{\\theta} tal que \\mathbb{E}_{\\theta_0}[Z(N_{\\theta}, X_i)] > 0 . Se \\Omega n\u00e3o \u00e9 compacto, assuma que exista um compacto C \\subseteq \\Omega tal que \\theta_0 \\in C e \\mathbb{E}_{\\theta_0} Z(\\omega / C, X_i) > 0 . Ent\u00e3o \\lim_{n \\to \\infty} \\hat{\\theta}_n = \\theta_0, quase certamente. A fun\u00e7\u00e3o Z(M,x) \u00e9 a diferen\u00e7a da log-verossimilhan\u00e7a em \\theta_0 e o m\u00e1ximo que ela atinge em M . Estamos assumindo que em uma vizinhan\u00e7a de cada ponto \\theta \\in \\Omega , a m\u00e1xima verossimilhan\u00e7a nessa regi\u00e3o \u00e9, em m\u00e9dia, menor do que a verossimilhan\u00e7a em \\theta_0 . Al\u00e9m disso, para conjuntos n\u00e3o compactos, fora desse compacto, queremos que em m\u00e9dia a verossimilhan\u00e7a em \\theta_0 seja maior. A dificuldade de utilizar esse teorema \u00e9 verificar todas essas condi\u00e7\u00f5es. Assumindo a continuidade da fun\u00e7\u00e3o de verossimilhan\u00e7a para todo x e que \\mathbb{E}_{\\theta_0}[Z(N_{\\theta}, X_i)] > -\\infty , tamb\u00e9m temos que MLE \u00e9 um estimador consistente.","title":"Propriedades assint\u00f3ticas MLE"},{"location":"infestatistica_MSc/estimation/large_sample/#mle-para-a-familia-exponencial","text":"Seja \\hat{\\theta}_n MLE calculado a partir de X_1, \\dots, X_n cuja distribui\u00e7\u00e3o tem densidade f(x|\\theta) = h(x)\\exp\\{\\theta\\cdot x - A(\\theta)\\}, isto \u00e9, pertence \u00e0 fam\u00edlia exponencial na forma can\u00f4nica. Se \\Omega \u00e9 um subconjunto aberto de \\mathbb{R}^k , ent\u00e3o \\lim_{n\\to\\infty} \\mathbb{P}_{\\theta}(\\hat{\\theta}_n \\text{ existir}) = 1 \\sqrt{n}(\\hat{\\theta}_n - \\theta) \\overset{D}{\\to} N_k(0, I_{\\mathcal{X}}(\\theta))^{-1} , em que I_{\\mathcal{X}}(\\theta) \u00e9 a matriz informa\u00e7\u00e3o de Fisher. \\{\\hat{\\theta}_n\\} \u00e9 consistente para \\theta .","title":"MLE para a fam\u00edlia exponencial"},{"location":"infestatistica_MSc/estimation/large_sample/#normalidade-assintotica-para-mles","text":"Sob algumas hip\u00f3teses de regularidade, podemos concluir que \\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\overset{D}{\\to} N(0, I_{\\mathcal{X}}(\\theta_0)^{-1}), em que I_{\\mathcal{X}}(\\theta_0) \u00e9 a informa\u00e7\u00e3o de Fisher. Hip\u00f3teses: \\{\\hat{\\theta}_n\\} \u00e9 consistente para \\theta . f(x|\\theta) tem derivadas de segunda ordem cont\u00ednuas com respeito a \\theta e vale o sinal de integra\u00e7\u00e3o pode ser trocado com o de diferencia\u00e7\u00e3o. Existe uma fun\u00e7\u00e3o H_r(x,\\theta) que para todo \\theta_0 tem valor esperando nulo quanto r \\to 0 e seja limite superior da diferen\u00e7a na segunda derivada em uma bola de raio r de \\theta_0 . Na pr\u00e1tica, podemos usar I_{\\mathcal{X}}(\\hat{\\theta}_n) , visto que \\theta_0 \u00e9 desconhecido.","title":"Normalidade assint\u00f3tica para MLEs"},{"location":"infestatistica_MSc/estimation/point_estimation/","text":"Estima\u00e7\u00e3o pontual Um estimator pontual de uma g(\\theta) \u00e9 uma estat\u00edstica que toma valores em \\operatorname{Imagem}(g) . Mais formalmente: Sejam \\Omega espa\u00e7o de par\u00e2metros de uma fam\u00edlia param\u00e9trica de distribui\u00e7\u00f5es P_{\\theta} e g : \\Omega \\to G uma fun\u00e7\u00e3o mensur\u00e1vel (cont\u00ednua, por exemplo). Uma fun\u00e7\u00e3o \\phi : \\mathcal{X} \\to G' \\supseteq G \u00e9 estimador de g(\\theta) , em que \\mathcal{X} \u00e9 o espa\u00e7o amostral. Vamos explorar formas de comparar estimadores pontuais. Vi\u00e9s de um estimador Dizemos que \\phi \u00e9 n\u00e3o enviesado se \\mathbb{E}_{\\theta}[\\phi(X)] = g(\\theta), \\forall \\theta \\in \\Omega. Al\u00e9m do mais, o vi\u00e9s de um estimador \u00e9 dado por b_{\\phi}(\\theta) = \\mathbb{E}_{\\theta}[\\phi(X)] - g(\\theta) . Se existe um estimador n\u00e3o enviesado para g(\\theta) , dizemos que g \u00e9 U-estim\u00e1vel. Um estimador \u00e9 uma fun\u00e7\u00e3o das amostras, enquanto uma estimativa \u00e9 uma avalia\u00e7\u00e3o dessa fun\u00e7\u00e3o em observa\u00e7\u00f5es. \ud83d\udcdd Exemplo (Normal) Seja X_1 \\dots, X_n \\overset{iid}{\\sim} Normal(\\mu, \\sigma^2) , em que \\theta = (\\mu, \\sigma^2) \\in \\mathbb{R} \\times \\mathbb{R}_+ \u00e9 o par\u00e2metro. Suponha que estamos interessados em g(\\theta) = \\mu . Um estimador para \\mu \u00e9 \\phi(X_1, \\dots, X_n) = X_1 ou \\phi(X_1, \\dots, X_n) = \\min\\{X_i\\} . Todavia, um estimador com melhores propriedades, que veremos ao decorrer do curso, \u00e9 \\phi(X_1, \\dots, X_n) = \\bar{X} . Note que \\mathbb{E}_{\\mu}[\\bar{X}] = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_{\\mu}[X_i] = \\mu, o que mostra que \\bar{X} \u00e9 n\u00e3o enviesado. Aparentemente, um estimador ser n\u00e3o enviesado parece muito bom, n\u00e3o \u00e9? Mas nem sempre isso \u00e9 suficiente ou poss\u00edvel. No exemplo a seguir, mostramos que n\u00e3o existe estimador n\u00e3o enviesado para a taxa da distribui\u00e7\u00e3o exponencial. \ud83d\udcdd Exemplo (Exponencial) Seja X \\sim Exponencial(\\lambda) , em que \\lambda \\in \\mathbb{R}_+ \u00e9 o par\u00e2metro. Seja \\phi(X) um estimador n\u00e3o enviesado para \\lambda . Ent\u00e3o \\mathbb{E}_{\\lambda}[\\phi(X)] = \\lambda \\int_0^{+\\infty} \\phi(x) \\exp(-\\lambda x) \\, dx = \\lambda, \\forall \\lambda \\in \\mathbb{R}_+. Dividindo ambos os lados por \\lambda e diferenciando com respeito a \\lambda , pela Regra de Leibniz : - \\int_0^{+\\infty} x\\phi(x) \\exp(-\\lambda x) \\, dx = 0. Mas x\\exp(-\\lambda x) > 0, \\forall x > 0 . Se \\phi(x) > 0 em um conjunto de medida positiva, pelo que estudamos em Integra\u00e7\u00e3o , ter\u00edamos que a integral deveria ser positiva, o que \u00e9 um absurdo. Com isso \\phi(x) = 0 quase certamente e, portanto, n\u00e3o \u00e9 um estimador n\u00e3o enviesado. Conclu\u00edmos que n\u00e3o pode haver um estimador n\u00e3o enviesado para \\lambda . Um estimador ser n\u00e3o enviesado implique que, na pr\u00e1tica, se fossem feitos infinitos experimentos e calcul\u00e1ssemos o valor do estimador, o valor m\u00e9dio convergiria para o valor verdadeiro do par\u00e2metro. Isso ocorre pela Lei dos Grandes N\u00fameros. Mas na pr\u00e1tica, fazemos uma quantidade finita de experimentos e isso pode impactar nossos resultados. No pr\u00f3ximo exemplo, vamos mostras que um estimador n\u00e3o enviesado e vamos visualizar sua distribui\u00e7\u00e3o. \ud83d\udcdd Exemplo (Correla\u00e7\u00e3o normal multivariada) Seja (X_1, Y_1), \\dots, (X_n,Y_n) \\sim Normal(\\boldsymbol{\\mu}, \\Sigma) , com m\u00e9dia \\mu = (0,0) e matriz de correla\u00e7\u00e3o \\Sigma = [[1, \\rho], [\\rho, 1]] . Assim o par\u00e2metro de interesse \u00e9 a correla\u00e7\u00e3o entre X e Y \\rho \\in [-1,1] . Considere o estimador para \\rho para n amostras sendo \\phi(X_1, \\dots, X_n) = \\dfrac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\sqrt{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}}, conhecido como correla\u00e7\u00e3o emp\u00edrica ou correla\u00e7\u00e3o de Pearson . Apesar de famoso, esse estimador tem vi\u00e9s n\u00e3o nulo. Todavia, pode-se corrigi-lo para obtermos um estimador n\u00e3o enviesado (Olkin and Pratt, 1958) , da forma \\psi = G(\\phi) = \\phi F\\left(\\frac{1}{2}, \\frac{1}{2}; (n-1)/2; 1 - \\phi^2\\right), que tem a propriedade de n\u00e3o ser enviesado, em que F \u00e9 a fun\u00e7\u00e3o hipergeom\u00e9trica de Gauss. Como \\phi e \\psi s\u00e3o fun\u00e7\u00f5es mensur\u00e1veis das amostras, eles tamb\u00e9m t\u00eam uma distribui\u00e7\u00e3o amostral. Uma maneira de visualizar essas distribui\u00e7\u00f5es \u00e9 atrav\u00e9s de simula\u00e7\u00f5es de Monte Carlo. 1) Geramos n amostras (X_i, Y_i) da normal multivariada. 2) Calculamos \\phi e \\psi . 3) Repetimos esse processo M vezes e temos amostras da distribui\u00e7\u00e3o de \\phi e \\psi . # Par\u00e2metros rho = 0.1 n = 30 M = 100000 phi_samples = np.zeros(M) psi_samples = np.zeros(M) # Gerando os dados de acordo com a distribui\u00e7\u00e3o especificada rng = np.random.RandomState(1001) for i in range(M): Z = rng.multivariate_normal(mean=[0,0], cov=[[1,rho], [rho,1]], size=n) X = Z[:,0] Y = Z[:,1] # Calculando ~ correla\u00e7\u00e3o de Pearson phi_samples[i] = np.corrcoef(X,Y)[0,1] psi_samples[i] = phi_samples[i] * hyp2f1(1/2, 1/2, (n-1)/2, 1 - phi_samples[i]**2) # Desenhando as distribui\u00e7\u00f5es plt.hist(phi_samples, label=r'$\\phi$', color='pink', bins=50) plt.hist(psi_samples, label=r'$\\psi$', color='blue', bins=50) plt.legend() plt.show() Nesse caso, temos que o Erro Absoluto Percentual M\u00e9dio do estimador \\psi \u00e9, aproximadamente, \\mathbb{E}_{\\rho}\\left[\\bigg|\\frac{\\psi - \\rho}{\\rho}\\bigg|\\right] \\approx 150\\%. Ou seja, apesar de \\psi ser n\u00e3o enviesado e \\phi ter vi\u00e9s baixo, a vari\u00e2ncia dos estimadores \u00e9 bem grande. Nesse caso, temos uma probabilidade alta de obter uma estima\u00e7\u00e3o 100% maior ou menor do que o valor verdadeiro de \\rho . Note que a diferen\u00e7a entre os estimadores \u00e9 pequena, pois n \u00e9 razoavelmente grande e \\psi tem vi\u00e9s baixo. Erro quadr\u00e1tico Seja L(\\theta, d) = (\\theta - d)^2 a perda quadr\u00e1tica. A fun\u00e7\u00e3o de risco R(\\theta, \\phi) para o estimador \\phi(X) \u00e9 dada por \\begin{split} R(\\theta, \\phi) &= \\mathbb{E}_{\\theta}[(\\theta-\\varphi(X))^2] \\\\ &= \\mathbb{E}_{\\theta}[(\\theta-\\mathbb{E}[\\varphi(X)] + \\mathbb{E}[\\varphi(X)] - \\varphi(X))^2] \\\\ &= (\\theta-\\mathbb{E}[\\varphi(X)])^2 + 2(\\theta-\\mathbb{E}[\\varphi(X)])(\\mathbb{E}_{\\theta}[\\mathbb{E}[\\varphi(X)] - \\varphi(X))] + \\mathbb{E}_{\\theta}[(\\mathbb{E}[\\varphi(X)] - \\varphi(X))^2] \\\\ &= b_{\\phi}(\\theta)^2 + 2(\\theta-\\mathbb{E}[\\varphi(X)])(\\mathbb{E}[\\varphi(X)]-\\mathbb{E}[\\varphi(X)]) + \\operatorname{Var}(\\phi(X)) \\\\ &= b_{\\phi}(\\theta)^2 + \\operatorname{Var}(\\phi(X)). \\end{split} Portanto, para estimadores n\u00e3o enviesados, o risco \u00e9 dado somente pela vari\u00e2ncia do estimador. Estima\u00e7\u00e3o n\u00e3o enviesada de menor vari\u00e2ncia Note que quando queremos avaliar o melhor estimador, estamos lidando com uma classe bem grande para comparar. Por exemplo, o estimador \\hat{\\theta} = 0 para todo \\theta tem o menor MSE em \\theta = 0 , mas \u00e9 um estimador horr\u00edvel quando \\theta se afasta de zero. Nesse, sentido uma forma de avaliar estimadores \u00e9 restringindo a classe de interesse. Uma classe considerada razo\u00e1vel \u00e9 a dos estimadores n\u00e3o enviesados. Nesse caso, os MSEs ser\u00e3o iguais \u00e0s vari\u00e2ncias dos estimadores. Um estimador n\u00e3o enviesado \\phi \u00e9 uniformly minimum variance unbiased estimator (UMVUE) (estimador n\u00e3o enviesado de vari\u00e2ncia uniformemente m\u00ednima) se \\phi tem vari\u00e2ncia finita e, para todo estimador n\u00e3o enviesado, Var_{\\theta} \\phi(X) \\le \\operatorname{Var}_{\\theta} \\psi(X), \\forall \\theta \\in \\Omega . Essa defini\u00e7\u00e3o leva ao seguinte importante teorema: Teorema (Lehmann-Scheff\u00e9): Se T \u00e9 uma estat\u00edstica completa, ent\u00e3o todos os estimadores n\u00e3o enviesados de g(\\theta) , que s\u00e3o fun\u00e7\u00f5es de T , mas n\u00e3o de X , s\u00e3o iguais quase certamente para todo \\theta . Al\u00e9m disso, se existe um estimador n\u00e3o enviesado que \u00e9 fun\u00e7\u00e3o de uma estat\u00edstica suficiente completa, ent\u00e3o ele \u00e9 UMVUE. Ideia da prova: Sejam \\phi_1(T) e \\phi_2(T) estimadores n\u00e3o enviesados de g(\\theta) . Ent\u00e3o \\mathbb{E}_{\\theta}[\\phi_1(T) - \\phi_2(T)] = 0 para todo \\theta . Como T \u00e9 uma estat\u00edstica completa, vale que \\phi_1(T) = \\phi_2(T) quase certamente. Agora suponha que T \u00e9 estat\u00edstica suficiente completa e seja \\phi(T) um estimador n\u00e3o enviesado. Tome um estimador n\u00e3o enviesado \\psi(X) e defina \\phi'(T) = \\mathbb{E}[\\psi(X)|T] . Usando a perda quadr\u00e1tica, pelo Teorema de Rao-Blackwell , \\operatorname{Var}_{\\theta}(\\phi ') = R(\\theta, \\phi ') \\le R(\\theta, \\psi) = \\operatorname{Var}_{\\theta}(\\psi) para todo \\theta . Portanto \\phi ' \u00e9 UMVUE. Al\u00e9m disso, como acabamos de provar, \\phi = \\phi ' quase certamente e, portanto, tamb\u00e9m \u00e9 UMVUE. Note que basta a exist\u00eancia de um estimador n\u00e3o enviesado \\delta e de uma estat\u00edstica completa T para que encontremos um estimador UMVUE usando Rao-Blackwell: \\phi(T) = \\mathbb{E}_{\\theta}[\\delta(X) | T]. Considere o exemplo para o caso normal. \ud83d\udcdd Exemplo (Distribui\u00e7\u00e3o normal - ela de novo) Seja X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2) e \\theta = (\\mu, \\sigma^2) . Ent\u00e3o \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i, S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2 s\u00e3o estat\u00edsticas suficientes e completas, al\u00e9m de serem estimadores n\u00e3o enviesados para \\mu e \\sigma^2 respectivamente. Pelo Teorema acima, qualquer fun\u00e7\u00e3o deles \u00e9 um UMVUE pelo resultado acima, para \\mu e \\sigma^2 , respectivamente. Todavia, \u00e9 poss\u00edvel verificar que S^2 n\u00e3o minimiza o erro quadrado. Obs.: para mostrar a sufici\u00eancia, o Teorema da Fatoriza\u00e7\u00e3o \u00e9 suficiente. J\u00e1 a completude vem do fato de que o espa\u00e7o de par\u00e2metros natural cont\u00e9m um conjunto aberto em \\mathbb{R}^2 . Quando n\u00e3o existem estat\u00edsticas completas, o seguinte resultado pode ser \u00fatil: Teorema: Uma condi\u00e7\u00e3o suficiente e necess\u00e1ria para que um estimador \\delta de \\mathbb{E}_{\\theta} \\delta(X) seja UMVUE \u00e9 que para todo U que satisfa\u00e7a \\forall \\theta, \\mathbb{E}_{\\theta}[U] = 0 , valha que \\operatorname{Cov}_{\\theta}(\\delta(X), U(X)) = 0 \ud83d\udcdd Exemplo (Bernoulli) Seja X_1, \\dots, X_n \\sim \\overset{iid}{\\sim} Bernoulli(\\theta) . Temos que T(X) = \\sum_{i=1}^n X_i \u00e9 uma estat\u00edstica suficiente completa para \\theta . Al\u00e9m disso, sabemos que T(X) \\sim Binomial(n,\\theta) . Em particular, T \u00e9 estat\u00edstica completa, pois o espa\u00e7o de par\u00e2metros natural da distribui\u00e7\u00e3o binomial tem interior n\u00e3o vazio. Considere g(\\theta) = \\theta^2 . Um estimador n\u00e3o enviesado para g \u00e9 \\delta(X) = X_1 X_2 . Nesse caso, o estimador UMVUE de g \u00e9 \\mathbb{E}[\\delta(X)|T] , isto \u00e9, \\phi(T) = \\mathbb{E}_{\\theta}[X_1X_2|T] = \\mathbb{P}_{\\theta}(X_1 = 1, X_2 = 1 | T = t). Podemos calcular essa probabilidade pela defini\u00e7\u00e3o de probabilidade condicional, mas vamos fazer isso intuitivamente. Temos n espa\u00e7os e t bolinhas para preencher t desses espa\u00e7os. Quantas formas eu tenho de posicionar as bolinhas? n \\choose t . Quantas dessas formas eu tenho interesse? Posiciono duas bolinhas nas primeiras posi\u00e7\u00f5es (veja que preciso de t \\ge 2 ) e tenho n-2 \\choose t-2 formas de posicionar as outras bolinhas. Portanto: \\mathbb{P}_{\\theta}(X_1 = 1, X_2 = 1 | T = t) = \\frac{(n-2)(n-3)\\cdots(n-t+1)\\cdot t(t-1)\\cdots 1}{(t-2)(t-1)\\cdots 1 \\cdot n(n-1)\\cdots(n-t+1)} = \\frac{t(t-1)}{n(n-1)}. Portanto, o estimador \\phi(T) = T(T-1)/(n^2 - n) se T \\ge 2 e \\phi(T) = 0 se T \\le 1 \u00e9 UMVUE para g(\\theta) = \\theta^2 . M\u00e9todo dos Momentos O m\u00e9todo de momentos \u00e9 uma abordagem para derivar estimadores para os par\u00e2metros do modelo a partir dos dados. Ele \u00e9 simples para ser definido, mas pode apresentar v\u00e1rios problemas como estimador. Considere uma amostra aleat\u00f3ria X = (X_1, \\dots, X_n) com densidade f_n(x|\\theta) , com \\theta \\in \\Omega \\subseteq \\mathbb{R}^k . Ent\u00e3o, o estimador do m\u00e9todo de momentos para \\theta \u00e9 a solu\u00e7\u00e3o do sistema m_i = \\mu_i(\\theta), i=1,\\dots,k, em que m_i = \\sum_{j=1}^n X_j^i \u00e9 o i -\u00e9simo momento emp\u00edrico da amostra e \\mu_i(\\theta) = \\mathbb{E}_{\\theta}[X^i] , isto \u00e9, dada uma observa\u00e7\u00e3o, estamos igualando momentos emp\u00edricos aos momentos da distribui\u00e7\u00e3o. Um problema com o m\u00e9todo de momentos \u00e9 que ele nem sempre prov\u00ea estimativas no espa\u00e7o dos par\u00e2metros. Isso pode acontecer se a variabilidade nos dados \u00e9 muito grande. Estimador de M\u00e1xima Verossimilhan\u00e7a Outro m\u00e9todo para derivar estimadores \u00e9 o de maximar a fun\u00e7\u00e3o de verossimilhan\u00e7a. Seja X uma vari\u00e1vel aleat\u00f3ria cuja distribui\u00e7\u00e3o tem densidade f(x|\\theta) . Se X=x \u00e9 observado, a fun\u00e7\u00e3o do par\u00e2metro L(\\theta|x) = f(x|\\theta) \u00e9 a fun\u00e7\u00e3o de verossimilhan\u00e7a . O estimador de m\u00e1xima verossimilhan\u00e7a (MLE) de \\theta \u00e9 o valor \\hat{\\theta} = \\operatorname{arg}\\max_{\\theta \\in \\Omega} f(x|\\theta), isto \u00e9, \u00e9 o valor do par\u00e2metro em \\Omega que maximiza a fun\u00e7\u00e3o de verossimilhan\u00e7a. \ud83d\udcdd Exemplo (Uniforme) Considere X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{Unif}(0,\\theta) , cuja densidade \u00e9 f_n(x|\\theta) = \\frac{1}{\\theta^n}I_{[0,\\theta]}(\\max_{i} x_i). Supondo observada a amostra, queremos encontrar \\theta \\ge \\max_i x_i (caso contr\u00e1rio, a densidade se anularia) que maximize \\theta^{-n} . Isso acontece quando \\theta \u00e9 m\u00ednimo. Nesse caso, \\hat{\\theta} = \\max_i x_i . Observe que se tiv\u00e9ssemos tomado intervalo aberto, ao inv\u00e9s de fechado, ter\u00edamos que \\theta > \\max_i x_i , mas n\u00e3o poderia ser igual de fato. Nessa situa\u00e7\u00e3o, n\u00e3o existe MLE, visto que para qualquer \\theta > \\max_i x_i , existe \\theta ' tal que \\theta > \\theta ' > \\max_i x_i . Mesmo quando o MLE existe, n\u00e3o h\u00e1 garantias de que ele \u00e9 \u00fanico. Por\u00e9m, se ele existe, ele satisfaz a seguinte propriedade chamada de invari\u00e2ncia Teorema: Seja g : \\Omega \\to G uma fun\u00e7\u00e3o mensur\u00e1vel. Suponha que exista um espa\u00e7o U e uma fun\u00e7\u00e3o h : \\Omega \\to G \\times G' bijetiva tal que h(\\theta) = (g(\\theta), g'(\\theta)) para alguma fun\u00e7\u00e3o G . Se \\hat{\\theta} \u00e9 MLE de \\theta , ent\u00e3o g(\\hat{\\theta}) \u00e9 MLE de g(\\theta) . Note que uma consequ\u00eancia direta \u00e9 que transforma\u00e7\u00f5es bijetivas levam MLEs em MLEs. Note que \\hat{\\theta} maximiza a verossimilhan\u00e7a se, e somente se, maximiza o logaritmo da verossimilhan\u00e7a. Com isso, temos uma abordagem mais simples para encontrar o MLE de distribui\u00e7\u00f5es da fam\u00edlia exponencial, pois \\log L(\\theta|x) = \\log h(x) - A(\\theta) + x\\cdot \\theta. Se o MLE \\hat{\\theta} ocorre no interior do espa\u00e7o de par\u00e2metros natural, as derivadas parciais de \\log L(\\theta) se anulam em \\theta = \\hat{\\theta} . Em particular, x = \\nabla A(\\theta) = \\mathbb{E}_{\\theta}[X], como visto aqui , o que implica que o MLE de \\theta \u00e9 o valor tal que x = \\mathbb{E}_{\\hat{\\theta}}[X] . Portanto o procedimento padr\u00e3o para encontrar o MLE \u00e9 o seguinte: Verificar se a verossimilhan\u00e7a tem alguma estrutura que facilite maximizar, como no caso da distribui\u00e7\u00e3o uniforme. Tomar o logaritmo da verossimilhan\u00e7a e encontrar o ponto cr\u00edtico derivando e igualando a zero para pontos no interior do espa\u00e7o de par\u00e2metros. Verificar condi\u00e7\u00f5es de sufici\u00eancia para os pontos cr\u00edticos. Por exemplo: segunda derivada negativa (ou Hessiana no caso de multivariada negativa definida). Verificar a fronteira se necess\u00e1rio. Utilizar recursos num\u00e9ricos.","title":"Estima\u00e7\u00e3o pontual"},{"location":"infestatistica_MSc/estimation/point_estimation/#estimacao-pontual","text":"Um estimator pontual de uma g(\\theta) \u00e9 uma estat\u00edstica que toma valores em \\operatorname{Imagem}(g) . Mais formalmente: Sejam \\Omega espa\u00e7o de par\u00e2metros de uma fam\u00edlia param\u00e9trica de distribui\u00e7\u00f5es P_{\\theta} e g : \\Omega \\to G uma fun\u00e7\u00e3o mensur\u00e1vel (cont\u00ednua, por exemplo). Uma fun\u00e7\u00e3o \\phi : \\mathcal{X} \\to G' \\supseteq G \u00e9 estimador de g(\\theta) , em que \\mathcal{X} \u00e9 o espa\u00e7o amostral. Vamos explorar formas de comparar estimadores pontuais.","title":"Estima\u00e7\u00e3o pontual"},{"location":"infestatistica_MSc/estimation/point_estimation/#vies-de-um-estimador","text":"Dizemos que \\phi \u00e9 n\u00e3o enviesado se \\mathbb{E}_{\\theta}[\\phi(X)] = g(\\theta), \\forall \\theta \\in \\Omega. Al\u00e9m do mais, o vi\u00e9s de um estimador \u00e9 dado por b_{\\phi}(\\theta) = \\mathbb{E}_{\\theta}[\\phi(X)] - g(\\theta) . Se existe um estimador n\u00e3o enviesado para g(\\theta) , dizemos que g \u00e9 U-estim\u00e1vel. Um estimador \u00e9 uma fun\u00e7\u00e3o das amostras, enquanto uma estimativa \u00e9 uma avalia\u00e7\u00e3o dessa fun\u00e7\u00e3o em observa\u00e7\u00f5es. \ud83d\udcdd Exemplo (Normal) Seja X_1 \\dots, X_n \\overset{iid}{\\sim} Normal(\\mu, \\sigma^2) , em que \\theta = (\\mu, \\sigma^2) \\in \\mathbb{R} \\times \\mathbb{R}_+ \u00e9 o par\u00e2metro. Suponha que estamos interessados em g(\\theta) = \\mu . Um estimador para \\mu \u00e9 \\phi(X_1, \\dots, X_n) = X_1 ou \\phi(X_1, \\dots, X_n) = \\min\\{X_i\\} . Todavia, um estimador com melhores propriedades, que veremos ao decorrer do curso, \u00e9 \\phi(X_1, \\dots, X_n) = \\bar{X} . Note que \\mathbb{E}_{\\mu}[\\bar{X}] = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_{\\mu}[X_i] = \\mu, o que mostra que \\bar{X} \u00e9 n\u00e3o enviesado. Aparentemente, um estimador ser n\u00e3o enviesado parece muito bom, n\u00e3o \u00e9? Mas nem sempre isso \u00e9 suficiente ou poss\u00edvel. No exemplo a seguir, mostramos que n\u00e3o existe estimador n\u00e3o enviesado para a taxa da distribui\u00e7\u00e3o exponencial. \ud83d\udcdd Exemplo (Exponencial) Seja X \\sim Exponencial(\\lambda) , em que \\lambda \\in \\mathbb{R}_+ \u00e9 o par\u00e2metro. Seja \\phi(X) um estimador n\u00e3o enviesado para \\lambda . Ent\u00e3o \\mathbb{E}_{\\lambda}[\\phi(X)] = \\lambda \\int_0^{+\\infty} \\phi(x) \\exp(-\\lambda x) \\, dx = \\lambda, \\forall \\lambda \\in \\mathbb{R}_+. Dividindo ambos os lados por \\lambda e diferenciando com respeito a \\lambda , pela Regra de Leibniz : - \\int_0^{+\\infty} x\\phi(x) \\exp(-\\lambda x) \\, dx = 0. Mas x\\exp(-\\lambda x) > 0, \\forall x > 0 . Se \\phi(x) > 0 em um conjunto de medida positiva, pelo que estudamos em Integra\u00e7\u00e3o , ter\u00edamos que a integral deveria ser positiva, o que \u00e9 um absurdo. Com isso \\phi(x) = 0 quase certamente e, portanto, n\u00e3o \u00e9 um estimador n\u00e3o enviesado. Conclu\u00edmos que n\u00e3o pode haver um estimador n\u00e3o enviesado para \\lambda . Um estimador ser n\u00e3o enviesado implique que, na pr\u00e1tica, se fossem feitos infinitos experimentos e calcul\u00e1ssemos o valor do estimador, o valor m\u00e9dio convergiria para o valor verdadeiro do par\u00e2metro. Isso ocorre pela Lei dos Grandes N\u00fameros. Mas na pr\u00e1tica, fazemos uma quantidade finita de experimentos e isso pode impactar nossos resultados. No pr\u00f3ximo exemplo, vamos mostras que um estimador n\u00e3o enviesado e vamos visualizar sua distribui\u00e7\u00e3o. \ud83d\udcdd Exemplo (Correla\u00e7\u00e3o normal multivariada) Seja (X_1, Y_1), \\dots, (X_n,Y_n) \\sim Normal(\\boldsymbol{\\mu}, \\Sigma) , com m\u00e9dia \\mu = (0,0) e matriz de correla\u00e7\u00e3o \\Sigma = [[1, \\rho], [\\rho, 1]] . Assim o par\u00e2metro de interesse \u00e9 a correla\u00e7\u00e3o entre X e Y \\rho \\in [-1,1] . Considere o estimador para \\rho para n amostras sendo \\phi(X_1, \\dots, X_n) = \\dfrac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\sqrt{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}}, conhecido como correla\u00e7\u00e3o emp\u00edrica ou correla\u00e7\u00e3o de Pearson . Apesar de famoso, esse estimador tem vi\u00e9s n\u00e3o nulo. Todavia, pode-se corrigi-lo para obtermos um estimador n\u00e3o enviesado (Olkin and Pratt, 1958) , da forma \\psi = G(\\phi) = \\phi F\\left(\\frac{1}{2}, \\frac{1}{2}; (n-1)/2; 1 - \\phi^2\\right), que tem a propriedade de n\u00e3o ser enviesado, em que F \u00e9 a fun\u00e7\u00e3o hipergeom\u00e9trica de Gauss. Como \\phi e \\psi s\u00e3o fun\u00e7\u00f5es mensur\u00e1veis das amostras, eles tamb\u00e9m t\u00eam uma distribui\u00e7\u00e3o amostral. Uma maneira de visualizar essas distribui\u00e7\u00f5es \u00e9 atrav\u00e9s de simula\u00e7\u00f5es de Monte Carlo. 1) Geramos n amostras (X_i, Y_i) da normal multivariada. 2) Calculamos \\phi e \\psi . 3) Repetimos esse processo M vezes e temos amostras da distribui\u00e7\u00e3o de \\phi e \\psi . # Par\u00e2metros rho = 0.1 n = 30 M = 100000 phi_samples = np.zeros(M) psi_samples = np.zeros(M) # Gerando os dados de acordo com a distribui\u00e7\u00e3o especificada rng = np.random.RandomState(1001) for i in range(M): Z = rng.multivariate_normal(mean=[0,0], cov=[[1,rho], [rho,1]], size=n) X = Z[:,0] Y = Z[:,1] # Calculando ~ correla\u00e7\u00e3o de Pearson phi_samples[i] = np.corrcoef(X,Y)[0,1] psi_samples[i] = phi_samples[i] * hyp2f1(1/2, 1/2, (n-1)/2, 1 - phi_samples[i]**2) # Desenhando as distribui\u00e7\u00f5es plt.hist(phi_samples, label=r'$\\phi$', color='pink', bins=50) plt.hist(psi_samples, label=r'$\\psi$', color='blue', bins=50) plt.legend() plt.show() Nesse caso, temos que o Erro Absoluto Percentual M\u00e9dio do estimador \\psi \u00e9, aproximadamente, \\mathbb{E}_{\\rho}\\left[\\bigg|\\frac{\\psi - \\rho}{\\rho}\\bigg|\\right] \\approx 150\\%. Ou seja, apesar de \\psi ser n\u00e3o enviesado e \\phi ter vi\u00e9s baixo, a vari\u00e2ncia dos estimadores \u00e9 bem grande. Nesse caso, temos uma probabilidade alta de obter uma estima\u00e7\u00e3o 100% maior ou menor do que o valor verdadeiro de \\rho . Note que a diferen\u00e7a entre os estimadores \u00e9 pequena, pois n \u00e9 razoavelmente grande e \\psi tem vi\u00e9s baixo.","title":"Vi\u00e9s de um estimador"},{"location":"infestatistica_MSc/estimation/point_estimation/#erro-quadratico","text":"Seja L(\\theta, d) = (\\theta - d)^2 a perda quadr\u00e1tica. A fun\u00e7\u00e3o de risco R(\\theta, \\phi) para o estimador \\phi(X) \u00e9 dada por \\begin{split} R(\\theta, \\phi) &= \\mathbb{E}_{\\theta}[(\\theta-\\varphi(X))^2] \\\\ &= \\mathbb{E}_{\\theta}[(\\theta-\\mathbb{E}[\\varphi(X)] + \\mathbb{E}[\\varphi(X)] - \\varphi(X))^2] \\\\ &= (\\theta-\\mathbb{E}[\\varphi(X)])^2 + 2(\\theta-\\mathbb{E}[\\varphi(X)])(\\mathbb{E}_{\\theta}[\\mathbb{E}[\\varphi(X)] - \\varphi(X))] + \\mathbb{E}_{\\theta}[(\\mathbb{E}[\\varphi(X)] - \\varphi(X))^2] \\\\ &= b_{\\phi}(\\theta)^2 + 2(\\theta-\\mathbb{E}[\\varphi(X)])(\\mathbb{E}[\\varphi(X)]-\\mathbb{E}[\\varphi(X)]) + \\operatorname{Var}(\\phi(X)) \\\\ &= b_{\\phi}(\\theta)^2 + \\operatorname{Var}(\\phi(X)). \\end{split} Portanto, para estimadores n\u00e3o enviesados, o risco \u00e9 dado somente pela vari\u00e2ncia do estimador.","title":"Erro quadr\u00e1tico"},{"location":"infestatistica_MSc/estimation/point_estimation/#estimacao-nao-enviesada-de-menor-variancia","text":"Note que quando queremos avaliar o melhor estimador, estamos lidando com uma classe bem grande para comparar. Por exemplo, o estimador \\hat{\\theta} = 0 para todo \\theta tem o menor MSE em \\theta = 0 , mas \u00e9 um estimador horr\u00edvel quando \\theta se afasta de zero. Nesse, sentido uma forma de avaliar estimadores \u00e9 restringindo a classe de interesse. Uma classe considerada razo\u00e1vel \u00e9 a dos estimadores n\u00e3o enviesados. Nesse caso, os MSEs ser\u00e3o iguais \u00e0s vari\u00e2ncias dos estimadores. Um estimador n\u00e3o enviesado \\phi \u00e9 uniformly minimum variance unbiased estimator (UMVUE) (estimador n\u00e3o enviesado de vari\u00e2ncia uniformemente m\u00ednima) se \\phi tem vari\u00e2ncia finita e, para todo estimador n\u00e3o enviesado, Var_{\\theta} \\phi(X) \\le \\operatorname{Var}_{\\theta} \\psi(X), \\forall \\theta \\in \\Omega . Essa defini\u00e7\u00e3o leva ao seguinte importante teorema: Teorema (Lehmann-Scheff\u00e9): Se T \u00e9 uma estat\u00edstica completa, ent\u00e3o todos os estimadores n\u00e3o enviesados de g(\\theta) , que s\u00e3o fun\u00e7\u00f5es de T , mas n\u00e3o de X , s\u00e3o iguais quase certamente para todo \\theta . Al\u00e9m disso, se existe um estimador n\u00e3o enviesado que \u00e9 fun\u00e7\u00e3o de uma estat\u00edstica suficiente completa, ent\u00e3o ele \u00e9 UMVUE. Ideia da prova: Sejam \\phi_1(T) e \\phi_2(T) estimadores n\u00e3o enviesados de g(\\theta) . Ent\u00e3o \\mathbb{E}_{\\theta}[\\phi_1(T) - \\phi_2(T)] = 0 para todo \\theta . Como T \u00e9 uma estat\u00edstica completa, vale que \\phi_1(T) = \\phi_2(T) quase certamente. Agora suponha que T \u00e9 estat\u00edstica suficiente completa e seja \\phi(T) um estimador n\u00e3o enviesado. Tome um estimador n\u00e3o enviesado \\psi(X) e defina \\phi'(T) = \\mathbb{E}[\\psi(X)|T] . Usando a perda quadr\u00e1tica, pelo Teorema de Rao-Blackwell , \\operatorname{Var}_{\\theta}(\\phi ') = R(\\theta, \\phi ') \\le R(\\theta, \\psi) = \\operatorname{Var}_{\\theta}(\\psi) para todo \\theta . Portanto \\phi ' \u00e9 UMVUE. Al\u00e9m disso, como acabamos de provar, \\phi = \\phi ' quase certamente e, portanto, tamb\u00e9m \u00e9 UMVUE. Note que basta a exist\u00eancia de um estimador n\u00e3o enviesado \\delta e de uma estat\u00edstica completa T para que encontremos um estimador UMVUE usando Rao-Blackwell: \\phi(T) = \\mathbb{E}_{\\theta}[\\delta(X) | T]. Considere o exemplo para o caso normal. \ud83d\udcdd Exemplo (Distribui\u00e7\u00e3o normal - ela de novo) Seja X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2) e \\theta = (\\mu, \\sigma^2) . Ent\u00e3o \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i, S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2 s\u00e3o estat\u00edsticas suficientes e completas, al\u00e9m de serem estimadores n\u00e3o enviesados para \\mu e \\sigma^2 respectivamente. Pelo Teorema acima, qualquer fun\u00e7\u00e3o deles \u00e9 um UMVUE pelo resultado acima, para \\mu e \\sigma^2 , respectivamente. Todavia, \u00e9 poss\u00edvel verificar que S^2 n\u00e3o minimiza o erro quadrado. Obs.: para mostrar a sufici\u00eancia, o Teorema da Fatoriza\u00e7\u00e3o \u00e9 suficiente. J\u00e1 a completude vem do fato de que o espa\u00e7o de par\u00e2metros natural cont\u00e9m um conjunto aberto em \\mathbb{R}^2 . Quando n\u00e3o existem estat\u00edsticas completas, o seguinte resultado pode ser \u00fatil: Teorema: Uma condi\u00e7\u00e3o suficiente e necess\u00e1ria para que um estimador \\delta de \\mathbb{E}_{\\theta} \\delta(X) seja UMVUE \u00e9 que para todo U que satisfa\u00e7a \\forall \\theta, \\mathbb{E}_{\\theta}[U] = 0 , valha que \\operatorname{Cov}_{\\theta}(\\delta(X), U(X)) = 0 \ud83d\udcdd Exemplo (Bernoulli) Seja X_1, \\dots, X_n \\sim \\overset{iid}{\\sim} Bernoulli(\\theta) . Temos que T(X) = \\sum_{i=1}^n X_i \u00e9 uma estat\u00edstica suficiente completa para \\theta . Al\u00e9m disso, sabemos que T(X) \\sim Binomial(n,\\theta) . Em particular, T \u00e9 estat\u00edstica completa, pois o espa\u00e7o de par\u00e2metros natural da distribui\u00e7\u00e3o binomial tem interior n\u00e3o vazio. Considere g(\\theta) = \\theta^2 . Um estimador n\u00e3o enviesado para g \u00e9 \\delta(X) = X_1 X_2 . Nesse caso, o estimador UMVUE de g \u00e9 \\mathbb{E}[\\delta(X)|T] , isto \u00e9, \\phi(T) = \\mathbb{E}_{\\theta}[X_1X_2|T] = \\mathbb{P}_{\\theta}(X_1 = 1, X_2 = 1 | T = t). Podemos calcular essa probabilidade pela defini\u00e7\u00e3o de probabilidade condicional, mas vamos fazer isso intuitivamente. Temos n espa\u00e7os e t bolinhas para preencher t desses espa\u00e7os. Quantas formas eu tenho de posicionar as bolinhas? n \\choose t . Quantas dessas formas eu tenho interesse? Posiciono duas bolinhas nas primeiras posi\u00e7\u00f5es (veja que preciso de t \\ge 2 ) e tenho n-2 \\choose t-2 formas de posicionar as outras bolinhas. Portanto: \\mathbb{P}_{\\theta}(X_1 = 1, X_2 = 1 | T = t) = \\frac{(n-2)(n-3)\\cdots(n-t+1)\\cdot t(t-1)\\cdots 1}{(t-2)(t-1)\\cdots 1 \\cdot n(n-1)\\cdots(n-t+1)} = \\frac{t(t-1)}{n(n-1)}. Portanto, o estimador \\phi(T) = T(T-1)/(n^2 - n) se T \\ge 2 e \\phi(T) = 0 se T \\le 1 \u00e9 UMVUE para g(\\theta) = \\theta^2 .","title":"Estima\u00e7\u00e3o n\u00e3o enviesada de menor vari\u00e2ncia"},{"location":"infestatistica_MSc/estimation/point_estimation/#metodo-dos-momentos","text":"O m\u00e9todo de momentos \u00e9 uma abordagem para derivar estimadores para os par\u00e2metros do modelo a partir dos dados. Ele \u00e9 simples para ser definido, mas pode apresentar v\u00e1rios problemas como estimador. Considere uma amostra aleat\u00f3ria X = (X_1, \\dots, X_n) com densidade f_n(x|\\theta) , com \\theta \\in \\Omega \\subseteq \\mathbb{R}^k . Ent\u00e3o, o estimador do m\u00e9todo de momentos para \\theta \u00e9 a solu\u00e7\u00e3o do sistema m_i = \\mu_i(\\theta), i=1,\\dots,k, em que m_i = \\sum_{j=1}^n X_j^i \u00e9 o i -\u00e9simo momento emp\u00edrico da amostra e \\mu_i(\\theta) = \\mathbb{E}_{\\theta}[X^i] , isto \u00e9, dada uma observa\u00e7\u00e3o, estamos igualando momentos emp\u00edricos aos momentos da distribui\u00e7\u00e3o. Um problema com o m\u00e9todo de momentos \u00e9 que ele nem sempre prov\u00ea estimativas no espa\u00e7o dos par\u00e2metros. Isso pode acontecer se a variabilidade nos dados \u00e9 muito grande.","title":"M\u00e9todo dos Momentos"},{"location":"infestatistica_MSc/estimation/point_estimation/#estimador-de-maxima-verossimilhanca","text":"Outro m\u00e9todo para derivar estimadores \u00e9 o de maximar a fun\u00e7\u00e3o de verossimilhan\u00e7a. Seja X uma vari\u00e1vel aleat\u00f3ria cuja distribui\u00e7\u00e3o tem densidade f(x|\\theta) . Se X=x \u00e9 observado, a fun\u00e7\u00e3o do par\u00e2metro L(\\theta|x) = f(x|\\theta) \u00e9 a fun\u00e7\u00e3o de verossimilhan\u00e7a . O estimador de m\u00e1xima verossimilhan\u00e7a (MLE) de \\theta \u00e9 o valor \\hat{\\theta} = \\operatorname{arg}\\max_{\\theta \\in \\Omega} f(x|\\theta), isto \u00e9, \u00e9 o valor do par\u00e2metro em \\Omega que maximiza a fun\u00e7\u00e3o de verossimilhan\u00e7a. \ud83d\udcdd Exemplo (Uniforme) Considere X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{Unif}(0,\\theta) , cuja densidade \u00e9 f_n(x|\\theta) = \\frac{1}{\\theta^n}I_{[0,\\theta]}(\\max_{i} x_i). Supondo observada a amostra, queremos encontrar \\theta \\ge \\max_i x_i (caso contr\u00e1rio, a densidade se anularia) que maximize \\theta^{-n} . Isso acontece quando \\theta \u00e9 m\u00ednimo. Nesse caso, \\hat{\\theta} = \\max_i x_i . Observe que se tiv\u00e9ssemos tomado intervalo aberto, ao inv\u00e9s de fechado, ter\u00edamos que \\theta > \\max_i x_i , mas n\u00e3o poderia ser igual de fato. Nessa situa\u00e7\u00e3o, n\u00e3o existe MLE, visto que para qualquer \\theta > \\max_i x_i , existe \\theta ' tal que \\theta > \\theta ' > \\max_i x_i . Mesmo quando o MLE existe, n\u00e3o h\u00e1 garantias de que ele \u00e9 \u00fanico. Por\u00e9m, se ele existe, ele satisfaz a seguinte propriedade chamada de invari\u00e2ncia Teorema: Seja g : \\Omega \\to G uma fun\u00e7\u00e3o mensur\u00e1vel. Suponha que exista um espa\u00e7o U e uma fun\u00e7\u00e3o h : \\Omega \\to G \\times G' bijetiva tal que h(\\theta) = (g(\\theta), g'(\\theta)) para alguma fun\u00e7\u00e3o G . Se \\hat{\\theta} \u00e9 MLE de \\theta , ent\u00e3o g(\\hat{\\theta}) \u00e9 MLE de g(\\theta) . Note que uma consequ\u00eancia direta \u00e9 que transforma\u00e7\u00f5es bijetivas levam MLEs em MLEs. Note que \\hat{\\theta} maximiza a verossimilhan\u00e7a se, e somente se, maximiza o logaritmo da verossimilhan\u00e7a. Com isso, temos uma abordagem mais simples para encontrar o MLE de distribui\u00e7\u00f5es da fam\u00edlia exponencial, pois \\log L(\\theta|x) = \\log h(x) - A(\\theta) + x\\cdot \\theta. Se o MLE \\hat{\\theta} ocorre no interior do espa\u00e7o de par\u00e2metros natural, as derivadas parciais de \\log L(\\theta) se anulam em \\theta = \\hat{\\theta} . Em particular, x = \\nabla A(\\theta) = \\mathbb{E}_{\\theta}[X], como visto aqui , o que implica que o MLE de \\theta \u00e9 o valor tal que x = \\mathbb{E}_{\\hat{\\theta}}[X] . Portanto o procedimento padr\u00e3o para encontrar o MLE \u00e9 o seguinte: Verificar se a verossimilhan\u00e7a tem alguma estrutura que facilite maximizar, como no caso da distribui\u00e7\u00e3o uniforme. Tomar o logaritmo da verossimilhan\u00e7a e encontrar o ponto cr\u00edtico derivando e igualando a zero para pontos no interior do espa\u00e7o de par\u00e2metros. Verificar condi\u00e7\u00f5es de sufici\u00eancia para os pontos cr\u00edticos. Por exemplo: segunda derivada negativa (ou Hessiana no caso de multivariada negativa definida). Verificar a fronteira se necess\u00e1rio. Utilizar recursos num\u00e9ricos.","title":"Estimador de M\u00e1xima Verossimilhan\u00e7a"},{"location":"infestatistica_MSc/exponential_family/exponential_family/","text":"Fam\u00edlia exponencial Em aplica\u00e7\u00f5es, a modelagem de fen\u00f4menos naturais por vari\u00e1veis aleat\u00f3rias gera a necessidade de obter a distribui\u00e7\u00e3o da vari\u00e1vel aleat\u00f3ria, o que faz parte do processo de modelagem. Como uma forma de simplifica\u00e7\u00e3o, usamos fam\u00edlias de distribui\u00e7\u00f5es com propriedades conhecidas. A mais conhecida de todas \u00e9 a fam\u00edlia exponencial , que inclui as distribui\u00e7\u00f5es mais conhecidas de nossos cora\u00e7\u00f5es: normal, binomial, Poisson, gamma, entre outras. Densidade Seja h uma fun\u00e7\u00e3o n\u00e3o negativa e T_1, \\dots, T_k : \\mathbb{R}^n \\to \\mathbb{R} fun\u00e7\u00f5es mensur\u00e1veis. Para \\theta \\in \\mathbb{R}^k , defina A(\\theta) = \\log \\int_{\\mathbb{R}^n} \\exp\\left[\\sum_{i=1}^k \\theta_i T_i(x)\\right] h(x) \\, d\\mu(x). Se A(\\theta) < +\\infty , vale que p_{\\theta}(x) = h(x)\\exp\\left[\\sum_{i=1}^k \\theta_i T_i(x) - A(\\theta)\\right], x \\in \\mathbb{R}^n integra 1. A fam\u00edlia de distribui\u00e7\u00f5es \\{p_{\\theta} : \\theta \\in \\Theta\\} , em que \\Theta = \\{\\theta : A(\\theta) < +\\infty\\} , \u00e9 chamada de fam\u00edlia exponencial com k par\u00e2metros na forma can\u00f4nica . O conjunto \\Theta \u00e9 chamado de espa\u00e7o de par\u00e2metros natural . \ud83d\udcdd Exemplo de constru\u00e7\u00e3o Sejam h(x) = 1 se x > 0 e 0 caso contr\u00e1rio e T_1(x) = x . Ent\u00e3o A(\\theta) = \\log \\int_0^{\\infty} e^{\\theta x} \\, dx = \\log(-1/\\theta) que \u00e9 bem definido se \\theta < 0 . Logo p_{\\theta}(x) = \\exp(\\theta x - \\log(-1/\\theta)) 1(x>0) \u00e9 a densidade com \\theta < 0 . Para situa\u00e7\u00f5es mais gerais, seja uma fun\u00e7\u00e3o \\eta : \\Omega \\to \\Theta . A fam\u00edlia exponencial \u00e9, ent\u00e3o, p_{\\theta}(x) = h(x)\\exp\\left[\\sum_{i=1}^k \\eta_i(\\theta) T_i(x) - B(\\theta)\\right], em que B(\\theta) = A(\\eta(\\theta)) . Identidade para os momentos Seja p_{\\theta} uma densidade da fam\u00edlia exponencial com formula\u00e7\u00e3o can\u00f4nica. Seja \\Theta_f \\subseteq \\Theta tal que \\mathbb{E}_{\\theta}[|f(X)|] < +\\infty . Ent\u00e3o, a fun\u00e7\u00e3o g(\\theta) = \\mathbb{E}_{\\theta}[f(X)] \u00e9 cont\u00ednua e com derivadas parciais de todas as ordens para \\theta no interior de \\Theta_f (mais do que isso, ela \u00e9 anal\u00edtica ). A partir dessa propriedade com f=1 , podemos concluir que \\mathbb{E}_{\\theta}[T_j(X)] = \\frac{\\partial A(\\theta)}{\\partial \\theta_j}. Tamb\u00e9m podemos obter que \\operatorname{Cov}_{\\theta}(T_i, T_j) = \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} A(\\theta). Propriedades Proposi\u00e7\u00e3o: Se X tem uma distribui\u00e7\u00e3o da fam\u00edlia exponencial, ent\u00e3o a estat\u00edstica suficiente natural T(X) tamb\u00e9m \u00e9. Em particular, a densidade \u00e9 p_{T,\\theta}(t) = \\exp\\{\\theta \\cdot t - A(\\theta)\\}. Para ver que T(X) \u00e9 estat\u00edstica suficiente, basta aplicar a fatoriza\u00e7\u00e3o de Fisher-Neyman. Essa fatoriza\u00e7\u00e3o tamb\u00e9m permite a deriva\u00e7\u00e3o da densidade da distribui\u00e7\u00e3o de T , que a define como pertencente \u00e0 fam\u00edlia exponencial. Teorema: O espa\u00e7o de par\u00e2metros natural \\Theta \u00e9 convexo e \\exp\\{A(\\theta)\\} \u00e9 uma fun\u00e7\u00e3o convexa. Agora um Teorema importante! Teorema: Se o espa\u00e7o dos par\u00e2metros natural \\Theta de uma fam\u00edlia exponencial cont\u00e9m um conjunto aberto em \\mathbb{R}^k , ent\u00e3o T(X) \u00e9 uma estat\u00edstica suficiente completa. Teorema de caracteriza\u00e7\u00e3o Seja X=(X_1, \\dots, X_n) uma amostra aleat\u00f3ria com densidade f(x|\\theta) . Seja T uma estat\u00edstica suficiente de dimens\u00e3o 1. Seja f(x|\\theta) = \\prod_{i=1}^n f(x_i|\\theta) = m_1(x)m_2(t,\\theta) e defina K_{\\theta}(t) = \\frac{\\partial}{\\partial \\theta} \\log m_2(t, \\theta). Assuma que 1) C = \\{x : f(x|\\theta) > 0\\} n\u00e3o depende de \\theta . 2) A verossimilhan\u00e7a \u00e9 diferenci\u00e1vel com respeito a \\theta . 3) A densidade \u00e9 diferenci\u00e1vel com respeito a x . 4) Existe \\theta_0 tal que K_{\\theta_0}(t) \u00e9 invert\u00edvel. Ent\u00e3o X tem uma distribui\u00e7\u00e3o da fam\u00edlia exponencial com um par\u00e2metro natural de dimens\u00e3o 1 .","title":"Fam\u00edlia exponencial"},{"location":"infestatistica_MSc/exponential_family/exponential_family/#familia-exponencial","text":"Em aplica\u00e7\u00f5es, a modelagem de fen\u00f4menos naturais por vari\u00e1veis aleat\u00f3rias gera a necessidade de obter a distribui\u00e7\u00e3o da vari\u00e1vel aleat\u00f3ria, o que faz parte do processo de modelagem. Como uma forma de simplifica\u00e7\u00e3o, usamos fam\u00edlias de distribui\u00e7\u00f5es com propriedades conhecidas. A mais conhecida de todas \u00e9 a fam\u00edlia exponencial , que inclui as distribui\u00e7\u00f5es mais conhecidas de nossos cora\u00e7\u00f5es: normal, binomial, Poisson, gamma, entre outras.","title":"Fam\u00edlia exponencial"},{"location":"infestatistica_MSc/exponential_family/exponential_family/#densidade","text":"Seja h uma fun\u00e7\u00e3o n\u00e3o negativa e T_1, \\dots, T_k : \\mathbb{R}^n \\to \\mathbb{R} fun\u00e7\u00f5es mensur\u00e1veis. Para \\theta \\in \\mathbb{R}^k , defina A(\\theta) = \\log \\int_{\\mathbb{R}^n} \\exp\\left[\\sum_{i=1}^k \\theta_i T_i(x)\\right] h(x) \\, d\\mu(x). Se A(\\theta) < +\\infty , vale que p_{\\theta}(x) = h(x)\\exp\\left[\\sum_{i=1}^k \\theta_i T_i(x) - A(\\theta)\\right], x \\in \\mathbb{R}^n integra 1. A fam\u00edlia de distribui\u00e7\u00f5es \\{p_{\\theta} : \\theta \\in \\Theta\\} , em que \\Theta = \\{\\theta : A(\\theta) < +\\infty\\} , \u00e9 chamada de fam\u00edlia exponencial com k par\u00e2metros na forma can\u00f4nica . O conjunto \\Theta \u00e9 chamado de espa\u00e7o de par\u00e2metros natural . \ud83d\udcdd Exemplo de constru\u00e7\u00e3o Sejam h(x) = 1 se x > 0 e 0 caso contr\u00e1rio e T_1(x) = x . Ent\u00e3o A(\\theta) = \\log \\int_0^{\\infty} e^{\\theta x} \\, dx = \\log(-1/\\theta) que \u00e9 bem definido se \\theta < 0 . Logo p_{\\theta}(x) = \\exp(\\theta x - \\log(-1/\\theta)) 1(x>0) \u00e9 a densidade com \\theta < 0 . Para situa\u00e7\u00f5es mais gerais, seja uma fun\u00e7\u00e3o \\eta : \\Omega \\to \\Theta . A fam\u00edlia exponencial \u00e9, ent\u00e3o, p_{\\theta}(x) = h(x)\\exp\\left[\\sum_{i=1}^k \\eta_i(\\theta) T_i(x) - B(\\theta)\\right], em que B(\\theta) = A(\\eta(\\theta)) .","title":"Densidade"},{"location":"infestatistica_MSc/exponential_family/exponential_family/#identidade-para-os-momentos","text":"Seja p_{\\theta} uma densidade da fam\u00edlia exponencial com formula\u00e7\u00e3o can\u00f4nica. Seja \\Theta_f \\subseteq \\Theta tal que \\mathbb{E}_{\\theta}[|f(X)|] < +\\infty . Ent\u00e3o, a fun\u00e7\u00e3o g(\\theta) = \\mathbb{E}_{\\theta}[f(X)] \u00e9 cont\u00ednua e com derivadas parciais de todas as ordens para \\theta no interior de \\Theta_f (mais do que isso, ela \u00e9 anal\u00edtica ). A partir dessa propriedade com f=1 , podemos concluir que \\mathbb{E}_{\\theta}[T_j(X)] = \\frac{\\partial A(\\theta)}{\\partial \\theta_j}. Tamb\u00e9m podemos obter que \\operatorname{Cov}_{\\theta}(T_i, T_j) = \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} A(\\theta).","title":"Identidade para os momentos"},{"location":"infestatistica_MSc/exponential_family/exponential_family/#propriedades","text":"Proposi\u00e7\u00e3o: Se X tem uma distribui\u00e7\u00e3o da fam\u00edlia exponencial, ent\u00e3o a estat\u00edstica suficiente natural T(X) tamb\u00e9m \u00e9. Em particular, a densidade \u00e9 p_{T,\\theta}(t) = \\exp\\{\\theta \\cdot t - A(\\theta)\\}. Para ver que T(X) \u00e9 estat\u00edstica suficiente, basta aplicar a fatoriza\u00e7\u00e3o de Fisher-Neyman. Essa fatoriza\u00e7\u00e3o tamb\u00e9m permite a deriva\u00e7\u00e3o da densidade da distribui\u00e7\u00e3o de T , que a define como pertencente \u00e0 fam\u00edlia exponencial. Teorema: O espa\u00e7o de par\u00e2metros natural \\Theta \u00e9 convexo e \\exp\\{A(\\theta)\\} \u00e9 uma fun\u00e7\u00e3o convexa. Agora um Teorema importante! Teorema: Se o espa\u00e7o dos par\u00e2metros natural \\Theta de uma fam\u00edlia exponencial cont\u00e9m um conjunto aberto em \\mathbb{R}^k , ent\u00e3o T(X) \u00e9 uma estat\u00edstica suficiente completa.","title":"Propriedades"},{"location":"infestatistica_MSc/exponential_family/exponential_family/#teorema-de-caracterizacao","text":"Seja X=(X_1, \\dots, X_n) uma amostra aleat\u00f3ria com densidade f(x|\\theta) . Seja T uma estat\u00edstica suficiente de dimens\u00e3o 1. Seja f(x|\\theta) = \\prod_{i=1}^n f(x_i|\\theta) = m_1(x)m_2(t,\\theta) e defina K_{\\theta}(t) = \\frac{\\partial}{\\partial \\theta} \\log m_2(t, \\theta). Assuma que 1) C = \\{x : f(x|\\theta) > 0\\} n\u00e3o depende de \\theta . 2) A verossimilhan\u00e7a \u00e9 diferenci\u00e1vel com respeito a \\theta . 3) A densidade \u00e9 diferenci\u00e1vel com respeito a x . 4) Existe \\theta_0 tal que K_{\\theta_0}(t) \u00e9 invert\u00edvel. Ent\u00e3o X tem uma distribui\u00e7\u00e3o da fam\u00edlia exponencial com um par\u00e2metro natural de dimens\u00e3o 1 .","title":"Teorema de caracteriza\u00e7\u00e3o"},{"location":"infestatistica_MSc/fisher/fisher/","text":"Informa\u00e7\u00e3o de Fisher e Cram\u00e9r-Rao Seja X uma vari\u00e1vel aleat\u00f3ria cuja distribui\u00e7\u00e3o depende de \\theta com densidade f(x|\\theta) . As condi\u00e7\u00f5es de regularidade FI s\u00e3o A derivada de f(x|\\theta) com respeito a \\theta existe com probabilidade 1. Podemos diferenciar \\int f(x|\\theta) d\\mu(x) sob o sinal da integra\u00e7\u00e3o. (Veja aqui) . O conjunto C = \\{x: f(x|\\theta) > 0\\} n\u00e3o depende de \\theta . Assuma as condi\u00e7\u00f5es FI. A informa\u00e7\u00e3o de Fisher \u00e9 definida como I_{\\mathcal{X}}(\\theta) = \\mathbb{E}_{\\theta}\\left[\\left(\\frac{d}{d\\theta} \\log f(x|\\theta)\\right)^2\\right]. A fun\u00e7\u00e3o \\partial \\log f(X|\\theta)/d\\theta \u00e9 chamada de fun\u00e7\u00e3o score . Se \\theta \\in \\mathbb{R}^k , definimos as matrix informa\u00e7\u00e3o de Fisher como I_{\\mathcal{X}, i,j }(\\theta) = \\operatorname{Cov}_{\\theta}\\left(\\frac{\\partial}{\\partial \\theta_i} \\log f(x|\\theta), \\frac{\\partial}{\\partial \\theta_j} \\log f(x|\\theta) \\right). Agora seja X = (X_1, \\dots, X_n) uma amostra aleat\u00f3ria e f_n(x|\\theta) a densidade conjunta de X . Denote \\lambda_n(x|\\theta) = \\log f_n(x|\\theta) . Como definimos, a informa\u00e7\u00e3o de Fisher \u00e9 I_n(\\theta) = \\mathbb{E}_{\\theta}\\{[\\lambda_n'(X|\\theta)]^2\\} . Como \\log f_n(x|\\theta) = \\sum_{i=1}^n \\log f(x_i|\\theta) , temos que I_n(\\theta) = nI_{\\mathcal{X}}(\\theta). Portanto, para amostras aleat\u00f3rias, basta calcular a informa\u00e7\u00e3o considerando a densidade de uma vari\u00e1vel aleat\u00f3ria. Teorema: Se valem as condi\u00e7\u00f5es de regularidade FI, a m\u00e9dia da fun\u00e7\u00e3o score \u00e9 0, isto \u00e9, \\mathbb{E}_{\\theta}\\left[\\frac{d}{d\\theta} \\log f(X|\\theta)\\right] = 0, pois podemos tirar a derivada do valor esperado e, ent\u00e3o, basta ver que a integral da densidade \u00e9 constante igual a 1 . Logo, vale que a derivada \u00e9 nula. Al\u00e9m do mais, I_{\\mathcal{X}}(\\theta) = -\\mathbb{E}_{\\theta}\\left[\\frac{d^2}{d\\theta^2} \\log f(X|\\theta)\\right] Esse resultado se estende para mais dimens\u00f5es, com I_{\\mathcal{X}, i,j}(\\theta) = -\\mathbb{E}_{\\theta}\\left[\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log f(X|\\theta)\\right]. import numpy as np from scipy.stats import norm from scipy.misc import derivative from scipy.optimize import curve_fit import matplotlib.pyplot as plt from seaborn import violinplot import inspect Exemplo Construtivo Vamos pensar num caso bem simples: amostra aleat\u00f3ria X_1, ..., X_n \\sim \\text{Normal}(\\mu, \\sigma^2) , onde o par\u00e2metro \\sigma^2 \u00e9 conhecido e \\mu n\u00e3o. De forma direta, poder\u00edamos perguntar qual a Informa\u00e7\u00e3o de Fisher (ou Informa\u00e7\u00e3o Diferencial) da amostra aleat\u00f3ria sobre o par\u00e2metro desconhecido \\mu . Vamos encontrar a distribui\u00e7\u00e3o conjunta: f(x|\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2}\\right] \\begin{split} f_n(x|\\mu) &= \\prod_{i=1}^n f(x_i|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i^2 - 2x_i\\mu + \\mu^2)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^n x_i^2 - 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\end{split} Vamos encontrar a verossimilhan\u00e7a: \u00e9 a distribui\u00e7\u00e3o conjunta como fun\u00e7\u00e3o do par\u00e2metro! f_n(x|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] Vamos comparar para \\sigma = 1 e \\sigma = 5 loglikelihood = lambda mu, sigma, x: np.sum(np.log([norm(loc = mu, scale = sigma).pdf(xi) for xi in x]), axis = 0) sigmas = [1,3,5,10] mu_true = 5 mu_range = np.linspace(0,10,1000) fig,ax = plt.subplots(2,2,figsize = (16, 10)) fig.suptitle('Comparando Log-verossimilhan\u00e7as da Distribui\u00e7\u00e3o Normal') def generate_curves(sigma, ax, n = 20, n_times = 50): for i in range(n_times): x = np.random.normal(loc = mu_true, scale = sigma, size = n) logvalues = loglikelihood(mu_range, sigma, x) ax.plot(mu_range, logvalues, color = 'blue', alpha = 0.2) ax.vlines(mu_true, ymin = ax.get_ylim()[0], ymax = ax.get_ylim()[1], linestyle = '--') ax.set_title(r'$\\sigma =$ {}'.format(sigma)) ax.set_xlabel(r'$\\mu$') generate_curves(sigmas[0], ax[0][0]) generate_curves(sigmas[1], ax[0][1]) generate_curves(sigmas[2], ax[1][0]) generate_curves(sigmas[3], ax[1][1]) Vamos ver como se comporta derivada. Esse \u00e9 o score: \\lambda '_n(y|\\mu) = \\frac{1}{\\sigma^2}\\left(n\\bar{x}_n - \\mu\\right) score = lambda mu, sigma, x: derivative(loglikelihood, mu, dx = 1e-5, args = (sigma, x)) fig,ax = plt.subplots(2,2,figsize = (16, 10)) fig.suptitle('Comparando Scores da Distribui\u00e7\u00e3o Normal') def generate_curves(sigma, ax, n = 20, n_times = 50): for i in range(n_times): x = np.random.normal(loc = mu_true, scale = sigma, size = n) scorevalues = score(mu_range, sigma, x) ax.plot(mu_range, scorevalues, color = 'blue', alpha = 0.2) ax.vlines(mu_true, ymin = ax.get_ylim()[0], ymax = ax.get_ylim()[1], linestyle = '--') ax.set_title(r'$\\sigma =$ {}'.format(sigma)) ax.set_xlabel(r'$\\mu$') ax.set_ylim((-10,10)) generate_curves(sigmas[0], ax[0][0]) generate_curves(sigmas[1], ax[0][1]) generate_curves(sigmas[2], ax[1][0]) generate_curves(sigmas[3], ax[1][1]) fig,ax = plt.subplots(2,2,figsize = (16, 10)) fig.suptitle('Comparando Histogramas dos Scores para mu') def generate_histograms(mu, sigma, ax, n = 15, n_times = 100): scorevalues = [] for i in range(n_times): x = np.random.normal(loc = mu_true, scale = sigma, size = n) scorevalues.append(score(mu, sigma, x)) violinplot(scorevalues, ax = ax) ax.set_title(r'$\\sigma =$ {}'.format(sigma)) ax.set_xlabel('score') generate_histograms(5, sigmas[0], ax[0][0]) generate_histograms(5, sigmas[1], ax[0][1]) generate_histograms(5, sigmas[2], ax[1][0]) generate_histograms(5, sigmas[3], ax[1][1]) A informa\u00e7\u00e3o de Fisher \u00e9 a Vari\u00e2ncia da fun\u00e7\u00e3o score em X , isto \u00e9: \\begin{split} I_n(\\mu) &= Var(\\lambda '_n(x|p)) = E[(\\lambda '_n(x|p))^2] - E[\\lambda '_n(x|p)]^2\\\\ &= \\frac{1}{\\sigma^4}Var\\left[n\\bar{x}_n - \\mu\\right] \\\\ &= \\frac{n^2}{\\sigma^4}Var(\\bar{x}_n) \\\\ &= \\frac{n^2\\sigma^2}{n\\sigma^4} \\\\ &= \\frac{n}{\\sigma^2} \\end{split} Limites inferiores para a vari\u00e2ncia Para estimadores n\u00e3o enviesados, o erro quadr\u00e1tico se iguala \u00e0 vari\u00e2ncia. Como nem sempre \u00e9 poss\u00edvel obter valores exatos para vari\u00e2ncia, \u00e9 de interesse procurar por limites inferiores desses valores. Mais do que isso, se encontramos um estimador cuja vari\u00e2ncia seja um limite inferior, estaremos encontrando um UMVUE. Desigualdade da vari\u00e2ncia A desigualdade de Cauchy-Schwarz para vari\u00e1veis aleat\u00f3rias, tratando a covari\u00e2ncia como um produto interno nesse espa\u00e7o, \u00e9 escrita da seguinte forma: |\\operatorname{Cov}(X,Y)| \\le \\sqrt{\\operatorname{Var}(X)}\\sqrt{\\operatorname{Var}(Y)} Limite inferior de Cram\u00e9r-Rao Sejam I_{\\mathcal{X}}(\\theta) a informa\u00e7\u00e3o de Fisher e \\phi(X) uma estat\u00edstica com esperan\u00e7a finita. Suponha que valha algumas condi\u00e7\u00f5es de regularidade(XXX: Quais?) e que \\forall \\theta, I_{\\mathcal{X}}(\\theta) > 0 . Ent\u00e3o \\operatorname{Var}_{\\theta}(\\phi(X)) \\ge \\frac{\\left(\\frac{d}{d\\theta} \\mathbb{E}_{\\theta} \\phi(X) \\right)^2}{I_{\\mathcal{X}}(\\theta)}. Observe que se \\phi(X) \u00e9 n\u00e3o enviesado, ent\u00e3o o limite inferior da vari\u00e2ncia \u00e9 o inverso da informa\u00e7\u00e3o de Fisher. \ud83d\udcdd Exemplo (Normal) Seja X \\sim N(\\theta, b) com b fixo. Um estimador para \\theta \u00e9 \\phi(X) = X , que \u00e9 n\u00e3o enviesado, em particular. A informa\u00e7\u00e3o de Fisher \u00e9 I_{\\mathcal{X}}(\\theta) = - \\mathbb{E}_{\\theta}\\left[\\frac{d^2}{d\\theta^2} \\log f(X|\\theta) \\right] = 1/b, o que implica que \\operatorname{Var}(\\phi(X)) \u00e9 limitado inferiormente por b . S\u00f3 que sabemos que ele atinge esse valor. Portanto, esse estimador \u00e9 UMVUE, pois tem vari\u00e2ncia m\u00ednima. A desigualdade de Cram\u00e9r-Rao \u00e9 uma consequ\u00eancia da desigualdade de Cauchy-Schwarz. Esta \u00faltima diz que vale a igualdade se, e somente se, os fatores s\u00e3o linearmente independentes. No nosso, caso, isso significa que a desigualdade de Cram\u00e9r-Rao se torna uma igualdade se, e somente se, \\phi(X) e a fun\u00e7\u00e3o \\frac{d}{d\\theta}\\log f(X|\\theta) s\u00e3o linearmente relacionadas, isto \u00e9, \\frac{d}{d\\theta} \\log f(X|\\theta) = a(\\theta)\\phi(X) + b(\\theta). Resolvendo essa equa\u00e7\u00e3o, obtemos que f(X|\\theta) = c(\\theta)h(x)\\exp\\{\\pi(\\theta)\\phi(x)\\}, que pertence \u00e0 fam\u00edlia exponencial. Logo, se \\phi(X) \u00e9 uma estat\u00edstica suficiente para um par\u00e2metro de uma distribui\u00e7\u00e3o vinda da fam\u00edlia exponencial, a desigualdade de Cram\u00e9r-Rao vira uma igualdade. Al\u00e9m disso, essa \u00e9 \u00fanica situa\u00e7\u00e3o. Fora da fam\u00edlia exponencial, a desigualdade de Cram\u00e9r-Rao n\u00e3o pode ser alcan\u00e7ada! Se uma estat\u00edstica tem vari\u00e2ncia igual ao limite inferior de Cram\u00e9r-Rao, ele \u00e9 dito estimador eficiente . Desigualdade de Chapman-Robbins Seja m(\\theta) = \\mathbb{E}_{\\theta}(\\phi(X)) e supp(\\theta) o suporte da distribui\u00e7\u00e3o de X . Assuma que para cada \\theta \\in \\Omega , exista \\theta'\\neq\\theta tal que supp(\\theta') \\subseteq supp(\\theta) (isso acontece quando supp \\theta \u00e9 o mesmo para todo \\theta ). Ent\u00e3o, \\operatorname{Var}_{\\theta}(\\phi(X)) \\ge \\sup_{\\theta ' : supp(\\theta') \\subseteq supp(\\theta)} \\left\\{\\frac{[m(\\theta) - m(\\theta ')]^2}{\\mathbb{E}_{\\theta}\\left[\\frac{f_{X|\\theta}(X|\\theta ')}{f_{X|\\theta}(X|\\theta)} - 1\\right]^2}\\right\\} Note que essa desigualdade vale para casos mais gerais. Por\u00e9m, ela tamb\u00e9m \u00e9 consequ\u00eancia da desigualdade de Cauchy-Schwarz. Sistema de Bhattacharyya Assuma as condi\u00e7\u00f5es do Teorema de Cram\u00e9r-Rao e que a derivada sob \\theta pode passar sob o sinal de integra\u00e7\u00e3o Ent\u00e3o \\operatorname{Var} \\phi(X) \\ge \\gamma^T (\\theta) J^{-1}(\\theta)\\gamma(\\theta) , em que \\gamma_i(\\theta) = \\frac{d^i}{d\\theta^i}\\mathbb{E}_{\\theta}[\\phi(X)], J_{ij}(\\theta) = \\operatorname{Cor}(\\psi_i(X,\\theta), \\psi_j(X,\\theta)), \\psi_i(x,\\theta) = \\frac{1}{f(x|\\theta)}\\frac{d^i}{d\\theta^i} f(x|\\theta). Esse resultado \u00e9 uma consequ\u00eancia direta de uma desigualdade envolvendo vari\u00e2ncia de um estimador e covari\u00e2ncias dele com outras fun\u00e7\u00f5es dos dados e do par\u00e2metro. Desigualdade de Cram\u00e9r-Rao multidimensional Assumindo as condi\u00e7\u00f5es do caso unidimensional mais que a matriz informa\u00e7\u00e3o de Fisher seja positiva definida, ent\u00e3o \\operatorname{Var}_{\\theta}(\\phi(X)) \\ge \\gamma^T(\\theta) I_X^{-1}(\\theta)\\gamma(\\theta), em que \\gamma_i(\\theta) = \\frac{d}{d\\theta_i} \\mathbb{E}_{\\theta} \\phi(X) . Exemplo Num\u00e9rico do limite de Cram\u00e9r-Rao Refer\u00eancia Considere um sinal (como uma m\u00fasica) com tr\u00eas par\u00e2metros, amplitude, frequ\u00eancia e fase inicia. Saberemos o n\u00famero de amostras que sera 100Hz com n\u00edvel de ru\u00eddo de 0.1 s = lambda t,a,f,ph: a*np.sin(2*np.pi*f*t + ph) # fun\u00e7\u00e3o que representa o sinal p0 = [2,8,0] # Amplitude, frequ\u00eancia e fase inicial para testar noise = 0.1 T = np.linspace(0,1,100) #100 valores entre 0 e 1 igualmente espa\u00e7ados plt.plot(T, s(T, *p0), '.-k') plt.xlabel('Tempo (s)') plt.title('Sinal') plt.show() Vamos usar inspect para nos ajudar a pegar labels das fun\u00e7\u00f5es, isto \u00e9, os par\u00e2metros necess\u00e1rios das fun\u00e7\u00f5es. Essa biblioteca fornece v\u00e1rias fun\u00e7\u00f5es de ajuda desse tipo. D\u00ea uma olhada. parameters = str(inspect.signature(s)).strip('()').replace(' ', '').split(',')[1:] p0dict = dict(zip(parameters, p0)) p0dict {'a': 2, 'f': 8, 'ph': 0} No caso geral, calcular a Matriz de Informa\u00e7\u00e3o de Fisher n\u00e3o \u00e9 trivial. Por isso, vamos calcular para o caso em que as medi\u00e7\u00f5es s\u00e3o de uma amostra com distribui\u00e7\u00e3o multivariada normal, isto \u00e9, \u00e9 uma distribui\u00e7\u00e3o normal, s\u00f3 que em mais dimens\u00f5es, em particular, 441 dimens\u00f5es (n\u00famero de pontos no tempo) Se calcularmos a informa\u00e7\u00e3o de Fisher, podemos ver que: \\mathcal{I}_{mn} = \\frac{1}{\\sigma^2} \\frac{\\partial \\mu^\\mathrm{T}}{\\partial \\theta_m} \\frac{\\partial \\mu}{\\partial \\theta_n} = \\frac{1}{\\sigma^2} \\sum_k \\frac{\\partial \\mu_k}{\\partial \\theta_m} \\frac{\\partial \\mu_k}{\\partial \\theta_n} onde \\theta = [a,f,ph]^T , \\mu = \\mu(\\theta) \u00e9 o vetor m\u00e9dia da normal multivariada e \\sigma^2 \u00e9 a vari\u00e2ncia de cada marginal da normal. N\u00e3o se assuste. Na multivariada, temos uma matriz para indicar as vari\u00e2ncias (ela se chama Matriz de Covari\u00e2ncias, na verdade). O que estou dizendo \u00e9 que ela \u00e9 \\sigma^2 vezes a identidade. \u00c9 bom conhecer essa distribui\u00e7\u00e3o! Por enquando acredite em mim! Ou no Wikipedia . Vou chamar D_{ik} = \\frac{\\partial \\mu_k}{\\partial \\theta_i} # Usamos ** para desempacotar elementos de um dicion\u00e1rio. string = \"a: {a} f: {f} ph: {ph}\".format(**p0dict) print(string) a: 2 f: 8 ph: 0 D = np.zeros((len(p0), len(T))) # para cada par\u00e2metro for i, parameter in enumerate(parameters): # para cada ponto no tempo for k, t in enumerate(T): func = lambda x: s(t, **dict(p0dict, **{parameter: x})) # Calculamos a derivada com respeito a x, que nesse caso \u00e9 o valor do parametro D[i,k] = derivative(func, p0dict[parameter], dx = 1e-4) Veja que o tamanho de D \u00e9 o seguinte: D.shape (3, 100) plt.plot(T, s(T, *p0), '--k', lw=2, label='Sinal') for Di, parameter in zip(D, parameters): # Estamos acessando Di = linha_i(D) plt.plot(T, Di, '.-', label=parameter) plt.legend() plt.xlabel('Tempo (s)') plt.show() O que D_{ik} indica? \u00c9 a derivada da k-\u00e9sima m\u00e9dia com respeito ao i-\u00e9simo par\u00e2metro. Logo indica o quanto o quando a amostra k afeta o par\u00e2metro i . Veja que quando temos picos no seno, teremos pico na amplitude,. Tamb\u00e9m vemos que a fase inicial n\u00e3o tem essa relev\u00e2ncia. Vemos tamb\u00e9m que o sinal se torna mais e mais sens\u00edvel \u00e0 frequ\u00eancia. Assim, podemos calular a informa\u00e7\u00e3o de fisher, usando einsum I = 1/noise**2*np.einsum('mk,nk', D, D) print(I) [[ 4.95000000e+03 -5.64643569e+02 -3.43706036e-09] [-5.64643569e+02 2.68635205e+05 6.34601694e+04] [-3.43706036e-09 6.34601694e+04 2.01999999e+04]] Podemos calcular o limite de Cram\u00e9r-Rao para qualquer estimador n\u00e3o enviesado. Nesse caso, veja aqui para mais detalhes. Mas n\u00e3o se incomode com os detalhes, se preferir. iI = np.linalg.inv(I) print('Cram\u00e9r-Rao Limite Inferior') for parameter, variance in zip(parameters, iI.diagonal()): print('{}: {:.2g}'.format(parameter, np.sqrt(variance))) Cram\u00e9r-Rao Limite Inferior a: 0.014 f: 0.0038 ph: 0.014","title":"Informa\u00e7\u00e3o de Fisher e Cram\u00e9r-Rao"},{"location":"infestatistica_MSc/fisher/fisher/#informacao-de-fisher-e-cramer-rao","text":"Seja X uma vari\u00e1vel aleat\u00f3ria cuja distribui\u00e7\u00e3o depende de \\theta com densidade f(x|\\theta) . As condi\u00e7\u00f5es de regularidade FI s\u00e3o A derivada de f(x|\\theta) com respeito a \\theta existe com probabilidade 1. Podemos diferenciar \\int f(x|\\theta) d\\mu(x) sob o sinal da integra\u00e7\u00e3o. (Veja aqui) . O conjunto C = \\{x: f(x|\\theta) > 0\\} n\u00e3o depende de \\theta . Assuma as condi\u00e7\u00f5es FI. A informa\u00e7\u00e3o de Fisher \u00e9 definida como I_{\\mathcal{X}}(\\theta) = \\mathbb{E}_{\\theta}\\left[\\left(\\frac{d}{d\\theta} \\log f(x|\\theta)\\right)^2\\right]. A fun\u00e7\u00e3o \\partial \\log f(X|\\theta)/d\\theta \u00e9 chamada de fun\u00e7\u00e3o score . Se \\theta \\in \\mathbb{R}^k , definimos as matrix informa\u00e7\u00e3o de Fisher como I_{\\mathcal{X}, i,j }(\\theta) = \\operatorname{Cov}_{\\theta}\\left(\\frac{\\partial}{\\partial \\theta_i} \\log f(x|\\theta), \\frac{\\partial}{\\partial \\theta_j} \\log f(x|\\theta) \\right). Agora seja X = (X_1, \\dots, X_n) uma amostra aleat\u00f3ria e f_n(x|\\theta) a densidade conjunta de X . Denote \\lambda_n(x|\\theta) = \\log f_n(x|\\theta) . Como definimos, a informa\u00e7\u00e3o de Fisher \u00e9 I_n(\\theta) = \\mathbb{E}_{\\theta}\\{[\\lambda_n'(X|\\theta)]^2\\} . Como \\log f_n(x|\\theta) = \\sum_{i=1}^n \\log f(x_i|\\theta) , temos que I_n(\\theta) = nI_{\\mathcal{X}}(\\theta). Portanto, para amostras aleat\u00f3rias, basta calcular a informa\u00e7\u00e3o considerando a densidade de uma vari\u00e1vel aleat\u00f3ria. Teorema: Se valem as condi\u00e7\u00f5es de regularidade FI, a m\u00e9dia da fun\u00e7\u00e3o score \u00e9 0, isto \u00e9, \\mathbb{E}_{\\theta}\\left[\\frac{d}{d\\theta} \\log f(X|\\theta)\\right] = 0, pois podemos tirar a derivada do valor esperado e, ent\u00e3o, basta ver que a integral da densidade \u00e9 constante igual a 1 . Logo, vale que a derivada \u00e9 nula. Al\u00e9m do mais, I_{\\mathcal{X}}(\\theta) = -\\mathbb{E}_{\\theta}\\left[\\frac{d^2}{d\\theta^2} \\log f(X|\\theta)\\right] Esse resultado se estende para mais dimens\u00f5es, com I_{\\mathcal{X}, i,j}(\\theta) = -\\mathbb{E}_{\\theta}\\left[\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log f(X|\\theta)\\right]. import numpy as np from scipy.stats import norm from scipy.misc import derivative from scipy.optimize import curve_fit import matplotlib.pyplot as plt from seaborn import violinplot import inspect","title":"Informa\u00e7\u00e3o de Fisher e Cram\u00e9r-Rao"},{"location":"infestatistica_MSc/fisher/fisher/#exemplo-construtivo","text":"Vamos pensar num caso bem simples: amostra aleat\u00f3ria X_1, ..., X_n \\sim \\text{Normal}(\\mu, \\sigma^2) , onde o par\u00e2metro \\sigma^2 \u00e9 conhecido e \\mu n\u00e3o. De forma direta, poder\u00edamos perguntar qual a Informa\u00e7\u00e3o de Fisher (ou Informa\u00e7\u00e3o Diferencial) da amostra aleat\u00f3ria sobre o par\u00e2metro desconhecido \\mu . Vamos encontrar a distribui\u00e7\u00e3o conjunta: f(x|\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2}\\right] \\begin{split} f_n(x|\\mu) &= \\prod_{i=1}^n f(x_i|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i^2 - 2x_i\\mu + \\mu^2)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^n x_i^2 - 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\\\ &= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] \\end{split} Vamos encontrar a verossimilhan\u00e7a: \u00e9 a distribui\u00e7\u00e3o conjunta como fun\u00e7\u00e3o do par\u00e2metro! f_n(x|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right]\\exp\\left[-\\frac{1}{2\\sigma^2}\\left(- 2n\\bar{x}_n\\mu + n\\mu^2\\right)\\right] Vamos comparar para \\sigma = 1 e \\sigma = 5 loglikelihood = lambda mu, sigma, x: np.sum(np.log([norm(loc = mu, scale = sigma).pdf(xi) for xi in x]), axis = 0) sigmas = [1,3,5,10] mu_true = 5 mu_range = np.linspace(0,10,1000) fig,ax = plt.subplots(2,2,figsize = (16, 10)) fig.suptitle('Comparando Log-verossimilhan\u00e7as da Distribui\u00e7\u00e3o Normal') def generate_curves(sigma, ax, n = 20, n_times = 50): for i in range(n_times): x = np.random.normal(loc = mu_true, scale = sigma, size = n) logvalues = loglikelihood(mu_range, sigma, x) ax.plot(mu_range, logvalues, color = 'blue', alpha = 0.2) ax.vlines(mu_true, ymin = ax.get_ylim()[0], ymax = ax.get_ylim()[1], linestyle = '--') ax.set_title(r'$\\sigma =$ {}'.format(sigma)) ax.set_xlabel(r'$\\mu$') generate_curves(sigmas[0], ax[0][0]) generate_curves(sigmas[1], ax[0][1]) generate_curves(sigmas[2], ax[1][0]) generate_curves(sigmas[3], ax[1][1]) Vamos ver como se comporta derivada. Esse \u00e9 o score: \\lambda '_n(y|\\mu) = \\frac{1}{\\sigma^2}\\left(n\\bar{x}_n - \\mu\\right) score = lambda mu, sigma, x: derivative(loglikelihood, mu, dx = 1e-5, args = (sigma, x)) fig,ax = plt.subplots(2,2,figsize = (16, 10)) fig.suptitle('Comparando Scores da Distribui\u00e7\u00e3o Normal') def generate_curves(sigma, ax, n = 20, n_times = 50): for i in range(n_times): x = np.random.normal(loc = mu_true, scale = sigma, size = n) scorevalues = score(mu_range, sigma, x) ax.plot(mu_range, scorevalues, color = 'blue', alpha = 0.2) ax.vlines(mu_true, ymin = ax.get_ylim()[0], ymax = ax.get_ylim()[1], linestyle = '--') ax.set_title(r'$\\sigma =$ {}'.format(sigma)) ax.set_xlabel(r'$\\mu$') ax.set_ylim((-10,10)) generate_curves(sigmas[0], ax[0][0]) generate_curves(sigmas[1], ax[0][1]) generate_curves(sigmas[2], ax[1][0]) generate_curves(sigmas[3], ax[1][1]) fig,ax = plt.subplots(2,2,figsize = (16, 10)) fig.suptitle('Comparando Histogramas dos Scores para mu') def generate_histograms(mu, sigma, ax, n = 15, n_times = 100): scorevalues = [] for i in range(n_times): x = np.random.normal(loc = mu_true, scale = sigma, size = n) scorevalues.append(score(mu, sigma, x)) violinplot(scorevalues, ax = ax) ax.set_title(r'$\\sigma =$ {}'.format(sigma)) ax.set_xlabel('score') generate_histograms(5, sigmas[0], ax[0][0]) generate_histograms(5, sigmas[1], ax[0][1]) generate_histograms(5, sigmas[2], ax[1][0]) generate_histograms(5, sigmas[3], ax[1][1]) A informa\u00e7\u00e3o de Fisher \u00e9 a Vari\u00e2ncia da fun\u00e7\u00e3o score em X , isto \u00e9: \\begin{split} I_n(\\mu) &= Var(\\lambda '_n(x|p)) = E[(\\lambda '_n(x|p))^2] - E[\\lambda '_n(x|p)]^2\\\\ &= \\frac{1}{\\sigma^4}Var\\left[n\\bar{x}_n - \\mu\\right] \\\\ &= \\frac{n^2}{\\sigma^4}Var(\\bar{x}_n) \\\\ &= \\frac{n^2\\sigma^2}{n\\sigma^4} \\\\ &= \\frac{n}{\\sigma^2} \\end{split}","title":"Exemplo Construtivo"},{"location":"infestatistica_MSc/fisher/fisher/#limites-inferiores-para-a-variancia","text":"Para estimadores n\u00e3o enviesados, o erro quadr\u00e1tico se iguala \u00e0 vari\u00e2ncia. Como nem sempre \u00e9 poss\u00edvel obter valores exatos para vari\u00e2ncia, \u00e9 de interesse procurar por limites inferiores desses valores. Mais do que isso, se encontramos um estimador cuja vari\u00e2ncia seja um limite inferior, estaremos encontrando um UMVUE.","title":"Limites inferiores para a vari\u00e2ncia"},{"location":"infestatistica_MSc/fisher/fisher/#desigualdade-da-variancia","text":"A desigualdade de Cauchy-Schwarz para vari\u00e1veis aleat\u00f3rias, tratando a covari\u00e2ncia como um produto interno nesse espa\u00e7o, \u00e9 escrita da seguinte forma: |\\operatorname{Cov}(X,Y)| \\le \\sqrt{\\operatorname{Var}(X)}\\sqrt{\\operatorname{Var}(Y)}","title":"Desigualdade da vari\u00e2ncia"},{"location":"infestatistica_MSc/fisher/fisher/#limite-inferior-de-cramer-rao","text":"Sejam I_{\\mathcal{X}}(\\theta) a informa\u00e7\u00e3o de Fisher e \\phi(X) uma estat\u00edstica com esperan\u00e7a finita. Suponha que valha algumas condi\u00e7\u00f5es de regularidade(XXX: Quais?) e que \\forall \\theta, I_{\\mathcal{X}}(\\theta) > 0 . Ent\u00e3o \\operatorname{Var}_{\\theta}(\\phi(X)) \\ge \\frac{\\left(\\frac{d}{d\\theta} \\mathbb{E}_{\\theta} \\phi(X) \\right)^2}{I_{\\mathcal{X}}(\\theta)}. Observe que se \\phi(X) \u00e9 n\u00e3o enviesado, ent\u00e3o o limite inferior da vari\u00e2ncia \u00e9 o inverso da informa\u00e7\u00e3o de Fisher. \ud83d\udcdd Exemplo (Normal) Seja X \\sim N(\\theta, b) com b fixo. Um estimador para \\theta \u00e9 \\phi(X) = X , que \u00e9 n\u00e3o enviesado, em particular. A informa\u00e7\u00e3o de Fisher \u00e9 I_{\\mathcal{X}}(\\theta) = - \\mathbb{E}_{\\theta}\\left[\\frac{d^2}{d\\theta^2} \\log f(X|\\theta) \\right] = 1/b, o que implica que \\operatorname{Var}(\\phi(X)) \u00e9 limitado inferiormente por b . S\u00f3 que sabemos que ele atinge esse valor. Portanto, esse estimador \u00e9 UMVUE, pois tem vari\u00e2ncia m\u00ednima. A desigualdade de Cram\u00e9r-Rao \u00e9 uma consequ\u00eancia da desigualdade de Cauchy-Schwarz. Esta \u00faltima diz que vale a igualdade se, e somente se, os fatores s\u00e3o linearmente independentes. No nosso, caso, isso significa que a desigualdade de Cram\u00e9r-Rao se torna uma igualdade se, e somente se, \\phi(X) e a fun\u00e7\u00e3o \\frac{d}{d\\theta}\\log f(X|\\theta) s\u00e3o linearmente relacionadas, isto \u00e9, \\frac{d}{d\\theta} \\log f(X|\\theta) = a(\\theta)\\phi(X) + b(\\theta). Resolvendo essa equa\u00e7\u00e3o, obtemos que f(X|\\theta) = c(\\theta)h(x)\\exp\\{\\pi(\\theta)\\phi(x)\\}, que pertence \u00e0 fam\u00edlia exponencial. Logo, se \\phi(X) \u00e9 uma estat\u00edstica suficiente para um par\u00e2metro de uma distribui\u00e7\u00e3o vinda da fam\u00edlia exponencial, a desigualdade de Cram\u00e9r-Rao vira uma igualdade. Al\u00e9m disso, essa \u00e9 \u00fanica situa\u00e7\u00e3o. Fora da fam\u00edlia exponencial, a desigualdade de Cram\u00e9r-Rao n\u00e3o pode ser alcan\u00e7ada! Se uma estat\u00edstica tem vari\u00e2ncia igual ao limite inferior de Cram\u00e9r-Rao, ele \u00e9 dito estimador eficiente .","title":"Limite inferior de Cram\u00e9r-Rao"},{"location":"infestatistica_MSc/fisher/fisher/#desigualdade-de-chapman-robbins","text":"Seja m(\\theta) = \\mathbb{E}_{\\theta}(\\phi(X)) e supp(\\theta) o suporte da distribui\u00e7\u00e3o de X . Assuma que para cada \\theta \\in \\Omega , exista \\theta'\\neq\\theta tal que supp(\\theta') \\subseteq supp(\\theta) (isso acontece quando supp \\theta \u00e9 o mesmo para todo \\theta ). Ent\u00e3o, \\operatorname{Var}_{\\theta}(\\phi(X)) \\ge \\sup_{\\theta ' : supp(\\theta') \\subseteq supp(\\theta)} \\left\\{\\frac{[m(\\theta) - m(\\theta ')]^2}{\\mathbb{E}_{\\theta}\\left[\\frac{f_{X|\\theta}(X|\\theta ')}{f_{X|\\theta}(X|\\theta)} - 1\\right]^2}\\right\\} Note que essa desigualdade vale para casos mais gerais. Por\u00e9m, ela tamb\u00e9m \u00e9 consequ\u00eancia da desigualdade de Cauchy-Schwarz.","title":"Desigualdade de Chapman-Robbins"},{"location":"infestatistica_MSc/fisher/fisher/#sistema-de-bhattacharyya","text":"Assuma as condi\u00e7\u00f5es do Teorema de Cram\u00e9r-Rao e que a derivada sob \\theta pode passar sob o sinal de integra\u00e7\u00e3o Ent\u00e3o \\operatorname{Var} \\phi(X) \\ge \\gamma^T (\\theta) J^{-1}(\\theta)\\gamma(\\theta) , em que \\gamma_i(\\theta) = \\frac{d^i}{d\\theta^i}\\mathbb{E}_{\\theta}[\\phi(X)], J_{ij}(\\theta) = \\operatorname{Cor}(\\psi_i(X,\\theta), \\psi_j(X,\\theta)), \\psi_i(x,\\theta) = \\frac{1}{f(x|\\theta)}\\frac{d^i}{d\\theta^i} f(x|\\theta). Esse resultado \u00e9 uma consequ\u00eancia direta de uma desigualdade envolvendo vari\u00e2ncia de um estimador e covari\u00e2ncias dele com outras fun\u00e7\u00f5es dos dados e do par\u00e2metro.","title":"Sistema de Bhattacharyya"},{"location":"infestatistica_MSc/fisher/fisher/#desigualdade-de-cramer-rao-multidimensional","text":"Assumindo as condi\u00e7\u00f5es do caso unidimensional mais que a matriz informa\u00e7\u00e3o de Fisher seja positiva definida, ent\u00e3o \\operatorname{Var}_{\\theta}(\\phi(X)) \\ge \\gamma^T(\\theta) I_X^{-1}(\\theta)\\gamma(\\theta), em que \\gamma_i(\\theta) = \\frac{d}{d\\theta_i} \\mathbb{E}_{\\theta} \\phi(X) .","title":"Desigualdade de Cram\u00e9r-Rao multidimensional"},{"location":"infestatistica_MSc/fisher/fisher/#exemplo-numerico-do-limite-de-cramer-rao","text":"Refer\u00eancia Considere um sinal (como uma m\u00fasica) com tr\u00eas par\u00e2metros, amplitude, frequ\u00eancia e fase inicia. Saberemos o n\u00famero de amostras que sera 100Hz com n\u00edvel de ru\u00eddo de 0.1 s = lambda t,a,f,ph: a*np.sin(2*np.pi*f*t + ph) # fun\u00e7\u00e3o que representa o sinal p0 = [2,8,0] # Amplitude, frequ\u00eancia e fase inicial para testar noise = 0.1 T = np.linspace(0,1,100) #100 valores entre 0 e 1 igualmente espa\u00e7ados plt.plot(T, s(T, *p0), '.-k') plt.xlabel('Tempo (s)') plt.title('Sinal') plt.show() Vamos usar inspect para nos ajudar a pegar labels das fun\u00e7\u00f5es, isto \u00e9, os par\u00e2metros necess\u00e1rios das fun\u00e7\u00f5es. Essa biblioteca fornece v\u00e1rias fun\u00e7\u00f5es de ajuda desse tipo. D\u00ea uma olhada. parameters = str(inspect.signature(s)).strip('()').replace(' ', '').split(',')[1:] p0dict = dict(zip(parameters, p0)) p0dict {'a': 2, 'f': 8, 'ph': 0} No caso geral, calcular a Matriz de Informa\u00e7\u00e3o de Fisher n\u00e3o \u00e9 trivial. Por isso, vamos calcular para o caso em que as medi\u00e7\u00f5es s\u00e3o de uma amostra com distribui\u00e7\u00e3o multivariada normal, isto \u00e9, \u00e9 uma distribui\u00e7\u00e3o normal, s\u00f3 que em mais dimens\u00f5es, em particular, 441 dimens\u00f5es (n\u00famero de pontos no tempo) Se calcularmos a informa\u00e7\u00e3o de Fisher, podemos ver que: \\mathcal{I}_{mn} = \\frac{1}{\\sigma^2} \\frac{\\partial \\mu^\\mathrm{T}}{\\partial \\theta_m} \\frac{\\partial \\mu}{\\partial \\theta_n} = \\frac{1}{\\sigma^2} \\sum_k \\frac{\\partial \\mu_k}{\\partial \\theta_m} \\frac{\\partial \\mu_k}{\\partial \\theta_n} onde \\theta = [a,f,ph]^T , \\mu = \\mu(\\theta) \u00e9 o vetor m\u00e9dia da normal multivariada e \\sigma^2 \u00e9 a vari\u00e2ncia de cada marginal da normal. N\u00e3o se assuste. Na multivariada, temos uma matriz para indicar as vari\u00e2ncias (ela se chama Matriz de Covari\u00e2ncias, na verdade). O que estou dizendo \u00e9 que ela \u00e9 \\sigma^2 vezes a identidade. \u00c9 bom conhecer essa distribui\u00e7\u00e3o! Por enquando acredite em mim! Ou no Wikipedia . Vou chamar D_{ik} = \\frac{\\partial \\mu_k}{\\partial \\theta_i} # Usamos ** para desempacotar elementos de um dicion\u00e1rio. string = \"a: {a} f: {f} ph: {ph}\".format(**p0dict) print(string) a: 2 f: 8 ph: 0 D = np.zeros((len(p0), len(T))) # para cada par\u00e2metro for i, parameter in enumerate(parameters): # para cada ponto no tempo for k, t in enumerate(T): func = lambda x: s(t, **dict(p0dict, **{parameter: x})) # Calculamos a derivada com respeito a x, que nesse caso \u00e9 o valor do parametro D[i,k] = derivative(func, p0dict[parameter], dx = 1e-4) Veja que o tamanho de D \u00e9 o seguinte: D.shape (3, 100) plt.plot(T, s(T, *p0), '--k', lw=2, label='Sinal') for Di, parameter in zip(D, parameters): # Estamos acessando Di = linha_i(D) plt.plot(T, Di, '.-', label=parameter) plt.legend() plt.xlabel('Tempo (s)') plt.show() O que D_{ik} indica? \u00c9 a derivada da k-\u00e9sima m\u00e9dia com respeito ao i-\u00e9simo par\u00e2metro. Logo indica o quanto o quando a amostra k afeta o par\u00e2metro i . Veja que quando temos picos no seno, teremos pico na amplitude,. Tamb\u00e9m vemos que a fase inicial n\u00e3o tem essa relev\u00e2ncia. Vemos tamb\u00e9m que o sinal se torna mais e mais sens\u00edvel \u00e0 frequ\u00eancia. Assim, podemos calular a informa\u00e7\u00e3o de fisher, usando einsum I = 1/noise**2*np.einsum('mk,nk', D, D) print(I) [[ 4.95000000e+03 -5.64643569e+02 -3.43706036e-09] [-5.64643569e+02 2.68635205e+05 6.34601694e+04] [-3.43706036e-09 6.34601694e+04 2.01999999e+04]] Podemos calcular o limite de Cram\u00e9r-Rao para qualquer estimador n\u00e3o enviesado. Nesse caso, veja aqui para mais detalhes. Mas n\u00e3o se incomode com os detalhes, se preferir. iI = np.linalg.inv(I) print('Cram\u00e9r-Rao Limite Inferior') for parameter, variance in zip(parameters, iI.diagonal()): print('{}: {:.2g}'.format(parameter, np.sqrt(variance))) Cram\u00e9r-Rao Limite Inferior a: 0.014 f: 0.0038 ph: 0.014","title":"Exemplo Num\u00e9rico do limite de Cram\u00e9r-Rao"},{"location":"infestatistica_MSc/sufficiency/sufficiency/","text":"Estat\u00edstica suficiente Um estat\u00edstico usa a informa\u00e7\u00e3o de uma amostra X_1, \\dots, X_n para fazer infer\u00eancia sobre uma quantidade de interesse \\theta . De acordo com Fisher, em \"On the Mathematical Foundations of Theoretical Statistics\", \"o objeto dos m\u00e9todos estat\u00edsticos \u00e9 a redu\u00e7\u00e3o dos dados\". Como a quantidade de dados \u00e9 incapaz de ser compreendida diretamente pelo c\u00e9rebro, ela \u00e9 reduzida por poucas quantidades que representam o todo, pelo menos a informa\u00e7\u00e3o relevante para o problema. Nesse sentido, uma estat\u00edstica \u00e9 uma forma de representar esses dados, em geral diminuir sua dimens\u00e3o (os pr\u00f3prios dados formam uma estat\u00edstica, ent\u00e3o isso n\u00e3o \u00e9 uma condi\u00e7\u00e3o necess\u00e1ria). De forma precisa, a defini\u00e7\u00e3o de estat\u00edstica \u00e9: Seja (\\mathcal{T}, \\mathcal{C}) um espa\u00e7o mensur\u00e1vel em que \\mathcal{C} contenha todos os conjuntos unit\u00e1rios formados pelo elementos de \\mathcal{T} . Se T : \\mathcal{X} \\to \\mathcal{T} \u00e9 mensur\u00e1vel, ent\u00e3o T \u00e9 uma estat\u00edstica. Com isso, podemos definir uma estat\u00edstica suficiente no sentido cl\u00e1ssico (lembrando que temos uma contrapartida bayesiana baseada na posteriori). Seja T: \\mathcal{X} \\to \\mathcal{T} uma estat\u00edstica e \\mathcal{P} uma fam\u00edlia de distribui\u00e7\u00f5es parametrizada por \\theta e definida em (\\mathcal{X}, \\mathcal{B}) (espa\u00e7o amostral com \\sigma -\u00e1lgebra \\mathcal{B} ). Suponha que existem P_{\\theta}(\\cdot|T) e uma fun\u00e7\u00e3o r : \\mathcal{B} \\times \\mathcal{T} \\to [0,1] tal que r(\\cdot, t) \u00e9 uma medida de probabilidade em (\\mathcal{X}, \\mathcal{B}) para todo t \\in \\mathcal{T} , r(A, \\cdot) \u00e9 mensur\u00e1vel para todo A \\in \\mathcal{B} e, para todo \\theta \\in \\Theta e B \\in \\mathcal{B} , P_{\\theta}(B|T=t) = r(B,t) quase certamente. Ent\u00e3o dizemos que T \u00e9 estat\u00edstica suficiente para \\theta . Apesar dessa defini\u00e7\u00e3o ser bastante complexa, a ideia \u00e9 que a distribui\u00e7\u00e3o condicional de X dado T=t n\u00e3o depende de \\theta , isto \u00e9, a informa\u00e7\u00e3o trazida por T=t sobre \\theta compreende toda a informa\u00e7\u00e3o dispon\u00edvel de X . Ap\u00f3s observar T(x) = t , podemos amostrar de r(\\cdot, t) e teremos dados falsos que imitam os iniciais, pois a distribui\u00e7\u00e3o ser\u00e1 a mesma. Assim, se considerarmos dois experimentadores, um tendo a amostra inteira, e o outro tendo apenas uma estat\u00edstica suficiente, ambos ter\u00e3o a mesma quantidade de informa\u00e7\u00e3o sobre o par\u00e2metro de interesse \\theta . Note que, pela lei da esperan\u00e7a total, P_{\\theta}(X \\in B) = \\mathbb{E}_{\\theta}[P_{\\theta}(X \\in B|T)] = \\mathbb{E}_{\\theta}[r(B, T)]. Se tomarmos Y \\sim r(\\cdot, t) quando T=t , teremos P_{\\theta}(Y \\in B) = \\mathbb{E}_{\\theta}[P_{\\theta}(Y \\in B|T)] = \\mathbb{E}_{\\theta}[r(B, T)]. Proposi\u00e7\u00e3o: A estat\u00edstica de ordem (X_{(1)}, \\dots, X_{(n)}) \u00e9 estat\u00edstica suficiente. Fatoriza\u00e7\u00e3o de Fisher-Neyman Seja f(\\boldsymbol{x}|\\theta) a densidade de uma distribui\u00e7\u00e3o de \\boldsymbol{X} = (X_1, \\dots, X_n) (com respeito a uma medida \\nu \\sigma -finita, como a medida de Lebesgue em \\mathbb{R}^n ). Ent\u00e3o T(\\boldsymbol{X}) \u00e9 estat\u00edstica suficiente para \\theta se, e somente se, existem fun\u00e7\u00f5es m_1 e m_2 tal que f(\\boldsymbol{x}|\\theta) = m_1(\\boldsymbol{x})m_2(T(\\boldsymbol{x}) , \\theta), \\forall \\theta \\in \\Theta. O lema 2.24 demonstrado no livro de Schervish determina que sob as hip\u00f3teses do teorema de Fisher-Neyman e assumindo que T \u00e9 suficiente, obtemos que existe uma medida em (\\mathcal{T}, \\mathcal{C}) que domina a distribui\u00e7\u00e3o de probabilidade de T e define a densidade de T=t sob o par\u00e2metro \\theta como m_2(t,\\theta) . Estat\u00edstica suficiente m\u00ednima e completa A estat\u00edstica de ordem \u00e9 uma estat\u00edstica que pouco reduz a informa\u00e7\u00e3o do dado. De fato, s\u00f3 retira a quest\u00e3o da ordena\u00e7\u00e3o. Todavia, algumas vezes uma estat\u00edstica mais simples tamb\u00e9m \u00e9 suficiente e, por isso, faz sentido definir quando ela \u00e9 m\u00ednima. Uma estat\u00edstica suficiente T \u00e9 dita suficiente m\u00ednima se para toda estat\u00edstica suficiente U , existe uma fun\u00e7\u00e3o mensur\u00e1vel g tal que T = g(U) para todo \\theta . Teorema (Lehmann-Scheff\u00e9): Seja f(x|\\theta) a densidade e T uma fun\u00e7\u00e3o mensur\u00e1vel tal que T(x) = T(y) \\iff y \\in D(x) , com D(x) = \\{y \\in \\mathcal{X} : f(y|\\theta) = f(x|\\theta)h(x,y), \\forall \\theta \\text{ e alguma fun\u00e7\u00e3o } h(x,y) \\}, ent\u00e3o T(X) \u00e9 estat\u00edstica suficiente m\u00ednima. \ud83d\udcdd Exemplo } Seja X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{Bernoulli}(\\theta) . Temos que f(x_1,\\dots,x_n|\\theta) = \\theta^{S_x}(1-\\theta)^{n-S_x}, em que S_x = \\sum_{i=1}^n x_i . Assim \\frac{f(x_1,\\dots,x_n|\\theta)}{f(y_1,\\dots,y_n|\\theta)} = \\left(\\frac{\\theta}{1-\\theta}\\right)^{S_x-S_y}, que independe de \\theta se, e somente se, S_x = S_y . Isso mostra que T(x) = S_x \u00e9 estat\u00edstica suficiente m\u00ednima. \ud83d\udcdd Exemplo - Fam\u00edlia Exponencial Considere uma distribui\u00e7\u00e3o com densidade p_{\\theta}(x) = \\exp\\{\\eta(\\theta)\\cdot T(x) - B(\\theta)\\}h(x) com respeito a medida de Lebesgue. Pelo Teorema da Fatoriza\u00e7\u00e3o T(x) \u00e9 estat\u00edstica suficiente. Assim \\log\\left(\\frac{p_{\\theta}(y)}{p_{\\theta}(x)}\\right) = \\eta(\\theta)\\cdot (T(y) - T(x)) + \\log(h(y)) - \\log(h(x)). Logo T \u00e9 estat\u00edstica suficiente m\u00ednima se, e somente se, o complemento ortogonal de \\eta(\\theta) possui apenas o 0 . Uma estat\u00edstica T \u00e9 dita completa se para toda fun\u00e7\u00e3o g mensur\u00e1vel e \\theta \\in \\Theta , \\mathbb{E}_{\\theta}[g(T)] = 0 implique g(T) = 0 . Ela ser\u00e1 completa limitada se adicionamos a condi\u00e7\u00e3o de que g \u00e9 limitada. Teorema de Bahadur: Se U \u00e9 uma estat\u00edstica suficiente completa limitada e de dimens\u00e3o finita, ent\u00e3o ela \u00e9 suficiente m\u00ednima. Ideias da demonstra\u00e7\u00e3o: Tome uma outra estat\u00edstica T e defina V_i(U) = (1 + \\exp\\{U_i\\})^{-1} para cada componente de U . Note que V \u00e9 limitada. Defina H_i(t) como o valor esperado de V_i dado que T=t e L_i(u) como o valor esperado de H_i(T) quando U=u , que n\u00e3o dependem de \\theta , pois as estat\u00edsticas s\u00e3o suficientes. Veja que \\mathbb{E}_{\\theta}[V_i(U) - L_i(U)] = 0 , o que implica que \\mathbb{P}(V_i = L_i) = 1 pois U \u00e9 completa. Conclua usando a Lei da Vari\u00e2ncia Total que U_i = V_i^{-1}(H_i(T)) . Note que se U \u00e9 completa, ent\u00e3o U \u00e9 completa limitada, o que nos d\u00e1 um resultado mais claro: Uma estat\u00edstica suficiente e completa \u00e9 suficiente m\u00ednima. Ancilaridade Na outra ponta, temos estat\u00edsticas que s\u00e3o independentes do par\u00e2metro. Uma estat\u00edstica U \u00e9 dita anciliar se a sua distribui\u00e7\u00e3o independe de \\theta . \ud83d\udcdd Exemplo - Caso normal Sejam X_1, X_2 \\overset{iid}{\\sim} \\operatorname{Normal}(\\mu, 1) e defina U = X_2 - X_1 . Soma de normais independentes faz com que U \\sim \\operatorname{Normal}(0, 2) e U \u00e9 anciliar. Se T=(T_1, T_2) \u00e9 estat\u00edstica suficiente m\u00ednima e T_2 \u00e9 anciliar, dizemos que T_1 \u00e9 suficiente condicionalmente dado T_2 . Quando uma estat\u00edstica \u00e9 anciliar, n\u00e3o significa que ela n\u00e3o tem import\u00e2ncia e, portanto, deveria ser ignorada. Significa que se ela fosse a \u00fanica observa\u00e7\u00e3o poss\u00edvel, n\u00e3o mudar\u00edamos a informa\u00e7\u00e3o sobre \\theta . Podemos, todavia, alterar a informa\u00e7\u00e3o sobre outras quantidades, todavia. Uma estat\u00edstica ancilar U \u00e9 maximal se toda outra estat\u00edstica ancilar \u00e9 fun\u00e7\u00e3o de U . Teorema de Basu: se T \u00e9 uma estat\u00edstica suficiente completa limitada e U \u00e9 anciliar, ent\u00e3o U e T s\u00e3o independentes sob P_{\\theta} para qualquer \\theta . Computacional Nesse notebook, que est\u00e1 dispon\u00edvel neste link , faremos alguns poucos experimentos para ilustrar fatos verificados no cap\u00edtulo de Estat\u00edstica Suficiente. import numpy as np import matplotlib.pyplot as plt from scipy import stats from tqdm import tqdm Amostrando! Seja X_1, \\dots, X_n uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o P_{\\theta} . Seja T(X) uma estat\u00edstica suficiente. Sabemos que \\mathbb{P}(X_1, \\dots, X_n | T(X) = t) independe de \\theta . Suponha ent\u00e3o que temos dois experimentadores. O primeiro captura todas as amostras e passa para o segundo experimentador apenas o valor de T(X) para economizar! Vamos verificar que o segundo experimentador consegue, agora, obter amostras fake da amostra original. Vamos come\u00e7ar com um exemplo simples: X_1, \\dots, X_n \\sim \\operatorname{Normal}(\\mu, 1) . Uma estat\u00edstica suficiente (m\u00ednima, inclusive) nesse caso \u00e9 \\bar{X} = n^{-1}(X_1 + \\dots + X_n) . Sabemos que \\bar{X} \\sim \\operatorname{Normal}(\\mu, n^{-1}) . Assim, f(X_1, \\dots, X_n | \\bar{X} = t) = \\frac{(2\\pi)^{-n/2}\\exp\\{-1/2 \\sum_{i=1}^n(x_i-\\mu)^2\\}}{(2\\pi n^{-1})^{-1/2}\\exp\\{-n/2(t-\\mu)^2\\}} \\propto \\exp\\left\\{-\\frac{1}{2}(S_x^2 - 2\\mu n t + n\\mu^2 - nt^2 + 2\\mu n t - n\\mu^2).\\right\\} = \\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^n (x_i^2 - t^2)\\right\\}, em que S_x^2 = \\sum_{i=1}^n x_i^2 e X_n = nt - \\sum_{i=1}^{n-1} X_i . Note que podemos escrever f(X_1, \\dots, X_{n-1} | \\bar{X} = t) \\propto \\exp\\left\\{nt^2/2\\right\\}\\exp\\left\\{-\\frac{1}{2}\\left[\\sum_{i=1}^{n-1} x_i^2 + \\left(nt - \\sum_{i=1}^{n-1} x_i\\right)^2 \\right]\\right\\} = \\exp\\left\\{nt^2/2\\right\\}\\exp\\left\\{-\\frac{1}{2}\\left[\\sum_{i=1}^{n-1} x_i^2 + n^2t^2 - 2nt\\sum_{i=1}^{n-1} x_i + \\left(\\sum_{i=1}^{n-1} x_i\\right)^2\\right]\\right\\} Assim, \\begin{split} f(X_1, \\dots, X_{n-1} | \\bar{X} = t) &\\propto \\exp\\left\\{nt^2/2\\right\\}\\exp\\left\\{-\\frac{1}{2}\\left[\\sum_{i=1}^{n-1} (x_i^2 - 2nt x_i + nt^2) +nt^2 + \\left(\\sum_{i=1}^{n-1} x_i\\right)^2\\right]\\right\\} \\\\ &= \\exp\\left\\{-\\frac{1}{2}\\left[\\sum_{i=1}^{n-1} (x_i^2 - 2t x_i + t^2) - 2(n-1)t\\sum_{i=1}^{n-1} x_i + (n-1)^2t^2 + \\left(\\sum_{i=1}^{n-1} x_i\\right)^2\\right]\\right\\} \\\\ &= \\exp\\left\\{-\\frac{1}{2}\\left[\\sum_{i=1}^{n-1} (x_i - t)^2 + \\left(\\sum_{i=1}^{n-1} (x_i - t)\\right)^2 \\right]\\right\\} \\\\ &= \\exp\\left\\{-\\frac{1}{2}\\left[\\sum_{i=1}^{n-1} (x_i - t)^2 + \\sum_{j=1}^{n-1} \\sum_{i=1}^{n-1} (x_j-t)(x_i - t) \\right]\\right\\} \\end{split} Com isso, observamos que X_1, \\dots, X_n | \\bar{X} = t tem distribui\u00e7\u00e3o Normal Multivariada com \\mathbb{E}[X_i] = t para i=1,\\dots,n-1 e matriz de covari\u00e2ncia (dada pela inversa) \\Sigma^{-1} = I_{n-1} + \\begin{bmatrix} 1 & \\cdots & 1 \\\\ \\vdots & \\ddots & \\vdots \\\\ 1 & \\cdots & 1 \\end{bmatrix} := I + A Veja que \\Sigma = I - A/n (mostre!). Assim, X_1, \\dots, X_{n-1} | \\bar{X} = t \\sim \\operatorname{Normal}((t,\\dots,t), I - A/n) . mu = 2 n = 10 Sigma = np.eye(n-1) - np.ones((n-1,n-1))/n Fa\u00e7amos um exemplo em que \\mu = 2 verdadeiro (e desconhecido para os dois estat\u00edsticos). Vamos fazer o seguinte experimento M vezes, com M = 100000 . Para o primeiro estat\u00edstico, oferecemos n amostras. Para o segundo estat\u00edstico, s\u00f3 oferecemos a m\u00e9dia dessas amostras e ele vai utilizar as contas acima para gerar outras n amostras, isto, ele gerar\u00e1 amostras novas a partir de X_1, \\dots, X_{n-1} | \\bar{X} = t \\sim \\operatorname{Normal}((t,\\dots,t), I - A/n) , pois ele conhecer\u00e1 \\bar{X} . Como faremos esses experimentos diversas vezes, vamos obter M amostras de X_1 para o estat\u00edstico 1, e o estat\u00edstico 2 vai produzir outras M a partir da distribui\u00e7\u00e3o calculada. Provamos que a distribui\u00e7\u00e3o delas ser\u00e1 a mesma. Vamos verificar graficamente atrav\u00e9s do histograma. mu = 2 n = 10 M = 100000 estatistico1 = np.zeros((M, n)) estatistico2 = np.zeros_like(estatistico1) for k in tqdm(range(M)): x = np.random.normal(loc=mu, scale=1, size=n) t = x.mean() x_fake = np.random.multivariate_normal(mean=t*np.ones(n-1), cov=np.eye(n-1) - np.ones((n-1, n-1))/n) x_n = n * t - x_fake.sum() x_fake = np.hstack([x_fake, x_n]) estatistico1[k] = x estatistico2[k] = x_fake 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100000/100000 [00:19<00:00, 5034.15it/s] Veja que de fato os histogramas s\u00e3o muito similares, como esper\u00e1vamos! Assim, apenas tendo o valor da m\u00e9dia, o estat\u00edstico 2 foi capaz de produzir novas amostras a partir da mesma distribui\u00e7\u00e3o do estat\u00edstico 1! plt.hist(estatistico1[:,0], bins=50, label='Estat\u00edstico 1') plt.hist(estatistico2[:,0], alpha=0.5, bins=50, label='Estat\u00edstico 2') plt.title('Comparando a distribui\u00e7\u00e3o do dado contra o dado falso') plt.legend() plt.show() Ancilaridade Uma estat\u00edstica anciliar n\u00e3o depende do par\u00e2metro, ent\u00e3o n\u00e3o pode atualizar a informa\u00e7\u00e3o sobre ele. Ser\u00e1 que significa que ela pode ser ignorada? Vamos verificar isso com um exemplo num\u00e9rico. Seja X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{Unif}[\\theta-1/2, \\theta+1/2] . Assim f(x_1, \\dots, x_n | \\theta) = \\prod_{i=1}^n 1\\{\\theta -1/2 < x_i < \\theta+1/2\\} = 1\\{\\theta - 1/2 < \\min\\{x_i\\}\\}1\\{\\theta + 1/2 > \\max\\{x_i\\}\\} Portanto T(X) = (\\min(X_i), \\max(X_i)) \u00e9 estat\u00edstica suficiente m\u00ednima. Seja U(X) = \\max(X_i) - \\min(X_i) . Vamos verificar que U \u00e9 estat\u00edstica anciliar Defina Y_i = X_i - \\theta + 1/2 \\sim \\operatorname{Uniform}(0,1) . Assim, U = \\max(X_i) - \\min(X_i) = \\max(X_i-\\theta+1/2) - \\min(X_i-\\theta+1/2) = \\max(Y_i) - \\min(Y_i). Como a distribui\u00e7\u00e3o de Y independe de \\theta , temos que a distribui\u00e7\u00e3o de U tamb\u00e9m independe, o que mostra que U \u00e9 estat\u00edstica anciliar. Vamos visualizar a distribui\u00e7\u00e3o de U de duas formas: partindo de X e partido de Y . theta = 5 n = 10 X = np.random.uniform(theta-1/2, theta+1/2, size=(100000, n)) Y = np.random.uniform(size=(100000, n)) # Estamos tomando o m\u00e1ximo de cada linha de X e obtendo 100000 amostras para X. U1 = X.max(axis=1) - X.min(axis=1) U2 = Y.max(axis=1) - Y.min(axis=1) Note que de fato as distribui\u00e7\u00f5es s\u00e3o muito similares. plt.hist(U1, bins=50, label='U = max(X_i) - min(X_i)') plt.hist(U2, alpha=0.5, bins=50, label='U = max(Y_i) - min(Y_i)') plt.title('Comparando a distribui\u00e7\u00e3o de U') plt.legend() plt.show() Se observamos que U(X) = u , podemos calcular que \\max(x_i) | U = u \\sim \\operatorname{Unif}(\\theta-1/2+u, \\theta+1/2) . Assim, a distribui\u00e7\u00e3o de uma estat\u00edstica muda com outra anciliar. Estimadores aleat\u00f3rios e convexidade O seguinte resultado \u00e9 a Desigualdade de Jensen: Seja I um intervalo aberto tal que \\mathbb{P}(X \\in I) = 1 e f uma fun\u00e7\u00e3o convexa . Se X \u00e9 integr\u00e1vel, ent\u00e3o f\\left(\\mathbb{E}[X]\\right) \\le \\mathbb{E}[f(X)]. Se f \u00e9 estritamente convexa, a desigualdade \u00e9 estrita a menos que X seja constante com probabilidade 1. Um estimador randomizado \u00e9 um estimador que pode ser constru\u00eddo a partir de uma estat\u00edstica suficiente T com a gera\u00e7\u00e3o auxiliar de n\u00fameros aleat\u00f3rios, isto \u00e9, \u00e9 uma fun\u00e7\u00e3o que mapeia x \\in \\mathcal{X} a uma vari\u00e1vel aleat\u00f3ria Y(x) com distribui\u00e7\u00e3o P_x . Teorema: Seja X \\sim P_{\\theta} \\in \\mathcal{P} e T = T(X) uma estat\u00edstica suficiente. Ent\u00e3o, para qualquer estimador \\phi(X) para g(\\theta) , existe um estimador randomizado baseado em T que tem a mesma fun\u00e7\u00e3o de risco de \\phi(X) . Ideia da prova: a distribui\u00e7\u00e3o de X condicionada em T n\u00e3o depende de \\theta , pois T \u00e9 suficiente. Seja r(\\cdot, T=t) essa distribui\u00e7\u00e3o. Assim, uma vari\u00e1vel aleat\u00f3ria Y com essa distribui\u00e7\u00e3o tem a mesma distribui\u00e7\u00e3o marginal de X , como verificamos aqui . Em particular \\phi(Y) tem a mesma distribui\u00e7\u00e3o de \\phi(X) . Teorema de Rao-Blackwell Sejam T uma estat\u00edstica suficiente para \\theta de uma fam\u00edlia de distribui\u00e7\u00f5es P_{\\theta} e \\delta um estimador de g(\\theta) . Defina \\eta(T) = \\mathbb{E}[\\delta(X)|T] . Se R(\\theta, \\delta) < +\\infty , em que R \u00e9 a fun\u00e7\u00e3o de risco para uma perda L , e L(\\theta, \\cdot) \u00e9 convexa, ent\u00e3o R(\\theta, \\eta) \\le R(\\theta, \\delta). Al\u00e9m do mais, se a perda \u00e9 estritamente convexa, a desigualdade \u00e9 estrita a menos que \\delta(X) = \\eta(T) com probabilidade 1. Esse resultado \u00e9 uma consequ\u00eancia direta da Desigualdade de Jensen. Esse resultado mostra que para perdas convexas, sob a \u00f3tica do risco apenas, os \u00fanicos estimadores interessantes s\u00e3o aqueles que s\u00e3o fun\u00e7\u00e3o de T e n\u00e3o de X (visto que estamos integrando em X ). Al\u00e9m do mais, qualquer estimador randomizado \u00e9 pior do que o n\u00e3o randomizado tomando a esperan\u00e7a dele condicionada em um estat\u00edstica suficiente. Observa\u00e7\u00e3o: \\eta(T) \u00e9 uma estat\u00edstica, pois n\u00e3o depende de \\theta , dado que T \u00e9 suficiente.","title":"Estat\u00edstica suficiente"},{"location":"infestatistica_MSc/sufficiency/sufficiency/#estatistica-suficiente","text":"Um estat\u00edstico usa a informa\u00e7\u00e3o de uma amostra X_1, \\dots, X_n para fazer infer\u00eancia sobre uma quantidade de interesse \\theta . De acordo com Fisher, em \"On the Mathematical Foundations of Theoretical Statistics\", \"o objeto dos m\u00e9todos estat\u00edsticos \u00e9 a redu\u00e7\u00e3o dos dados\". Como a quantidade de dados \u00e9 incapaz de ser compreendida diretamente pelo c\u00e9rebro, ela \u00e9 reduzida por poucas quantidades que representam o todo, pelo menos a informa\u00e7\u00e3o relevante para o problema. Nesse sentido, uma estat\u00edstica \u00e9 uma forma de representar esses dados, em geral diminuir sua dimens\u00e3o (os pr\u00f3prios dados formam uma estat\u00edstica, ent\u00e3o isso n\u00e3o \u00e9 uma condi\u00e7\u00e3o necess\u00e1ria). De forma precisa, a defini\u00e7\u00e3o de estat\u00edstica \u00e9: Seja (\\mathcal{T}, \\mathcal{C}) um espa\u00e7o mensur\u00e1vel em que \\mathcal{C} contenha todos os conjuntos unit\u00e1rios formados pelo elementos de \\mathcal{T} . Se T : \\mathcal{X} \\to \\mathcal{T} \u00e9 mensur\u00e1vel, ent\u00e3o T \u00e9 uma estat\u00edstica. Com isso, podemos definir uma estat\u00edstica suficiente no sentido cl\u00e1ssico (lembrando que temos uma contrapartida bayesiana baseada na posteriori). Seja T: \\mathcal{X} \\to \\mathcal{T} uma estat\u00edstica e \\mathcal{P} uma fam\u00edlia de distribui\u00e7\u00f5es parametrizada por \\theta e definida em (\\mathcal{X}, \\mathcal{B}) (espa\u00e7o amostral com \\sigma -\u00e1lgebra \\mathcal{B} ). Suponha que existem P_{\\theta}(\\cdot|T) e uma fun\u00e7\u00e3o r : \\mathcal{B} \\times \\mathcal{T} \\to [0,1] tal que r(\\cdot, t) \u00e9 uma medida de probabilidade em (\\mathcal{X}, \\mathcal{B}) para todo t \\in \\mathcal{T} , r(A, \\cdot) \u00e9 mensur\u00e1vel para todo A \\in \\mathcal{B} e, para todo \\theta \\in \\Theta e B \\in \\mathcal{B} , P_{\\theta}(B|T=t) = r(B,t) quase certamente. Ent\u00e3o dizemos que T \u00e9 estat\u00edstica suficiente para \\theta . Apesar dessa defini\u00e7\u00e3o ser bastante complexa, a ideia \u00e9 que a distribui\u00e7\u00e3o condicional de X dado T=t n\u00e3o depende de \\theta , isto \u00e9, a informa\u00e7\u00e3o trazida por T=t sobre \\theta compreende toda a informa\u00e7\u00e3o dispon\u00edvel de X . Ap\u00f3s observar T(x) = t , podemos amostrar de r(\\cdot, t) e teremos dados falsos que imitam os iniciais, pois a distribui\u00e7\u00e3o ser\u00e1 a mesma. Assim, se considerarmos dois experimentadores, um tendo a amostra inteira, e o outro tendo apenas uma estat\u00edstica suficiente, ambos ter\u00e3o a mesma quantidade de informa\u00e7\u00e3o sobre o par\u00e2metro de interesse \\theta . Note que, pela lei da esperan\u00e7a total, P_{\\theta}(X \\in B) = \\mathbb{E}_{\\theta}[P_{\\theta}(X \\in B|T)] = \\mathbb{E}_{\\theta}[r(B, T)]. Se tomarmos Y \\sim r(\\cdot, t) quando T=t , teremos P_{\\theta}(Y \\in B) = \\mathbb{E}_{\\theta}[P_{\\theta}(Y \\in B|T)] = \\mathbb{E}_{\\theta}[r(B, T)]. Proposi\u00e7\u00e3o: A estat\u00edstica de ordem (X_{(1)}, \\dots, X_{(n)}) \u00e9 estat\u00edstica suficiente.","title":"Estat\u00edstica suficiente"},{"location":"infestatistica_MSc/sufficiency/sufficiency/#fatorizacao-de-fisher-neyman","text":"Seja f(\\boldsymbol{x}|\\theta) a densidade de uma distribui\u00e7\u00e3o de \\boldsymbol{X} = (X_1, \\dots, X_n) (com respeito a uma medida \\nu \\sigma -finita, como a medida de Lebesgue em \\mathbb{R}^n ). Ent\u00e3o T(\\boldsymbol{X}) \u00e9 estat\u00edstica suficiente para \\theta se, e somente se, existem fun\u00e7\u00f5es m_1 e m_2 tal que f(\\boldsymbol{x}|\\theta) = m_1(\\boldsymbol{x})m_2(T(\\boldsymbol{x}) , \\theta), \\forall \\theta \\in \\Theta. O lema 2.24 demonstrado no livro de Schervish determina que sob as hip\u00f3teses do teorema de Fisher-Neyman e assumindo que T \u00e9 suficiente, obtemos que existe uma medida em (\\mathcal{T}, \\mathcal{C}) que domina a distribui\u00e7\u00e3o de probabilidade de T e define a densidade de T=t sob o par\u00e2metro \\theta como m_2(t,\\theta) .","title":"Fatoriza\u00e7\u00e3o de Fisher-Neyman"},{"location":"infestatistica_MSc/sufficiency/sufficiency/#estatistica-suficiente-minima-e-completa","text":"A estat\u00edstica de ordem \u00e9 uma estat\u00edstica que pouco reduz a informa\u00e7\u00e3o do dado. De fato, s\u00f3 retira a quest\u00e3o da ordena\u00e7\u00e3o. Todavia, algumas vezes uma estat\u00edstica mais simples tamb\u00e9m \u00e9 suficiente e, por isso, faz sentido definir quando ela \u00e9 m\u00ednima. Uma estat\u00edstica suficiente T \u00e9 dita suficiente m\u00ednima se para toda estat\u00edstica suficiente U , existe uma fun\u00e7\u00e3o mensur\u00e1vel g tal que T = g(U) para todo \\theta . Teorema (Lehmann-Scheff\u00e9): Seja f(x|\\theta) a densidade e T uma fun\u00e7\u00e3o mensur\u00e1vel tal que T(x) = T(y) \\iff y \\in D(x) , com D(x) = \\{y \\in \\mathcal{X} : f(y|\\theta) = f(x|\\theta)h(x,y), \\forall \\theta \\text{ e alguma fun\u00e7\u00e3o } h(x,y) \\}, ent\u00e3o T(X) \u00e9 estat\u00edstica suficiente m\u00ednima. \ud83d\udcdd Exemplo } Seja X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{Bernoulli}(\\theta) . Temos que f(x_1,\\dots,x_n|\\theta) = \\theta^{S_x}(1-\\theta)^{n-S_x}, em que S_x = \\sum_{i=1}^n x_i . Assim \\frac{f(x_1,\\dots,x_n|\\theta)}{f(y_1,\\dots,y_n|\\theta)} = \\left(\\frac{\\theta}{1-\\theta}\\right)^{S_x-S_y}, que independe de \\theta se, e somente se, S_x = S_y . Isso mostra que T(x) = S_x \u00e9 estat\u00edstica suficiente m\u00ednima. \ud83d\udcdd Exemplo - Fam\u00edlia Exponencial Considere uma distribui\u00e7\u00e3o com densidade p_{\\theta}(x) = \\exp\\{\\eta(\\theta)\\cdot T(x) - B(\\theta)\\}h(x) com respeito a medida de Lebesgue. Pelo Teorema da Fatoriza\u00e7\u00e3o T(x) \u00e9 estat\u00edstica suficiente. Assim \\log\\left(\\frac{p_{\\theta}(y)}{p_{\\theta}(x)}\\right) = \\eta(\\theta)\\cdot (T(y) - T(x)) + \\log(h(y)) - \\log(h(x)). Logo T \u00e9 estat\u00edstica suficiente m\u00ednima se, e somente se, o complemento ortogonal de \\eta(\\theta) possui apenas o 0 . Uma estat\u00edstica T \u00e9 dita completa se para toda fun\u00e7\u00e3o g mensur\u00e1vel e \\theta \\in \\Theta , \\mathbb{E}_{\\theta}[g(T)] = 0 implique g(T) = 0 . Ela ser\u00e1 completa limitada se adicionamos a condi\u00e7\u00e3o de que g \u00e9 limitada. Teorema de Bahadur: Se U \u00e9 uma estat\u00edstica suficiente completa limitada e de dimens\u00e3o finita, ent\u00e3o ela \u00e9 suficiente m\u00ednima. Ideias da demonstra\u00e7\u00e3o: Tome uma outra estat\u00edstica T e defina V_i(U) = (1 + \\exp\\{U_i\\})^{-1} para cada componente de U . Note que V \u00e9 limitada. Defina H_i(t) como o valor esperado de V_i dado que T=t e L_i(u) como o valor esperado de H_i(T) quando U=u , que n\u00e3o dependem de \\theta , pois as estat\u00edsticas s\u00e3o suficientes. Veja que \\mathbb{E}_{\\theta}[V_i(U) - L_i(U)] = 0 , o que implica que \\mathbb{P}(V_i = L_i) = 1 pois U \u00e9 completa. Conclua usando a Lei da Vari\u00e2ncia Total que U_i = V_i^{-1}(H_i(T)) . Note que se U \u00e9 completa, ent\u00e3o U \u00e9 completa limitada, o que nos d\u00e1 um resultado mais claro: Uma estat\u00edstica suficiente e completa \u00e9 suficiente m\u00ednima.","title":"Estat\u00edstica suficiente m\u00ednima e completa"},{"location":"infestatistica_MSc/sufficiency/sufficiency/#ancilaridade","text":"Na outra ponta, temos estat\u00edsticas que s\u00e3o independentes do par\u00e2metro. Uma estat\u00edstica U \u00e9 dita anciliar se a sua distribui\u00e7\u00e3o independe de \\theta . \ud83d\udcdd Exemplo - Caso normal Sejam X_1, X_2 \\overset{iid}{\\sim} \\operatorname{Normal}(\\mu, 1) e defina U = X_2 - X_1 . Soma de normais independentes faz com que U \\sim \\operatorname{Normal}(0, 2) e U \u00e9 anciliar. Se T=(T_1, T_2) \u00e9 estat\u00edstica suficiente m\u00ednima e T_2 \u00e9 anciliar, dizemos que T_1 \u00e9 suficiente condicionalmente dado T_2 . Quando uma estat\u00edstica \u00e9 anciliar, n\u00e3o significa que ela n\u00e3o tem import\u00e2ncia e, portanto, deveria ser ignorada. Significa que se ela fosse a \u00fanica observa\u00e7\u00e3o poss\u00edvel, n\u00e3o mudar\u00edamos a informa\u00e7\u00e3o sobre \\theta . Podemos, todavia, alterar a informa\u00e7\u00e3o sobre outras quantidades, todavia. Uma estat\u00edstica ancilar U \u00e9 maximal se toda outra estat\u00edstica ancilar \u00e9 fun\u00e7\u00e3o de U . Teorema de Basu: se T \u00e9 uma estat\u00edstica suficiente completa limitada e U \u00e9 anciliar, ent\u00e3o U e T s\u00e3o independentes sob P_{\\theta} para qualquer \\theta .","title":"Ancilaridade"},{"location":"infestatistica_MSc/sufficiency/sufficiency/#computacional","text":"Nesse notebook, que est\u00e1 dispon\u00edvel neste link , faremos alguns poucos experimentos para ilustrar fatos verificados no cap\u00edtulo de Estat\u00edstica Suficiente. import numpy as np import matplotlib.pyplot as plt from scipy import stats from tqdm import tqdm","title":"Computacional"},{"location":"infestatistica_MSc/sufficiency/sufficiency/#amostrando","text":"Seja X_1, \\dots, X_n uma amostra aleat\u00f3ria de uma distribui\u00e7\u00e3o P_{\\theta} . Seja T(X) uma estat\u00edstica suficiente. Sabemos que \\mathbb{P}(X_1, \\dots, X_n | T(X) = t) independe de \\theta . Suponha ent\u00e3o que temos dois experimentadores. O primeiro captura todas as amostras e passa para o segundo experimentador apenas o valor de T(X) para economizar! Vamos verificar que o segundo experimentador consegue, agora, obter amostras fake da amostra original. Vamos come\u00e7ar com um exemplo simples: X_1, \\dots, X_n \\sim \\operatorname{Normal}(\\mu, 1) . Uma estat\u00edstica suficiente (m\u00ednima, inclusive) nesse caso \u00e9 \\bar{X} = n^{-1}(X_1 + \\dots + X_n) . Sabemos que \\bar{X} \\sim \\operatorname{Normal}(\\mu, n^{-1}) . Assim, f(X_1, \\dots, X_n | \\bar{X} = t) = \\frac{(2\\pi)^{-n/2}\\exp\\{-1/2 \\sum_{i=1}^n(x_i-\\mu)^2\\}}{(2\\pi n^{-1})^{-1/2}\\exp\\{-n/2(t-\\mu)^2\\}} \\propto \\exp\\left\\{-\\frac{1}{2}(S_x^2 - 2\\mu n t + n\\mu^2 - nt^2 + 2\\mu n t - n\\mu^2).\\right\\} = \\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^n (x_i^2 - t^2)\\right\\}, em que S_x^2 = \\sum_{i=1}^n x_i^2 e X_n = nt - \\sum_{i=1}^{n-1} X_i . Note que podemos escrever f(X_1, \\dots, X_{n-1} | \\bar{X} = t) \\propto \\exp\\left\\{nt^2/2\\right\\}\\exp\\left\\{-\\frac{1}{2}\\left[\\sum_{i=1}^{n-1} x_i^2 + \\left(nt - \\sum_{i=1}^{n-1} x_i\\right)^2 \\right]\\right\\} = \\exp\\left\\{nt^2/2\\right\\}\\exp\\left\\{-\\frac{1}{2}\\left[\\sum_{i=1}^{n-1} x_i^2 + n^2t^2 - 2nt\\sum_{i=1}^{n-1} x_i + \\left(\\sum_{i=1}^{n-1} x_i\\right)^2\\right]\\right\\} Assim, \\begin{split} f(X_1, \\dots, X_{n-1} | \\bar{X} = t) &\\propto \\exp\\left\\{nt^2/2\\right\\}\\exp\\left\\{-\\frac{1}{2}\\left[\\sum_{i=1}^{n-1} (x_i^2 - 2nt x_i + nt^2) +nt^2 + \\left(\\sum_{i=1}^{n-1} x_i\\right)^2\\right]\\right\\} \\\\ &= \\exp\\left\\{-\\frac{1}{2}\\left[\\sum_{i=1}^{n-1} (x_i^2 - 2t x_i + t^2) - 2(n-1)t\\sum_{i=1}^{n-1} x_i + (n-1)^2t^2 + \\left(\\sum_{i=1}^{n-1} x_i\\right)^2\\right]\\right\\} \\\\ &= \\exp\\left\\{-\\frac{1}{2}\\left[\\sum_{i=1}^{n-1} (x_i - t)^2 + \\left(\\sum_{i=1}^{n-1} (x_i - t)\\right)^2 \\right]\\right\\} \\\\ &= \\exp\\left\\{-\\frac{1}{2}\\left[\\sum_{i=1}^{n-1} (x_i - t)^2 + \\sum_{j=1}^{n-1} \\sum_{i=1}^{n-1} (x_j-t)(x_i - t) \\right]\\right\\} \\end{split} Com isso, observamos que X_1, \\dots, X_n | \\bar{X} = t tem distribui\u00e7\u00e3o Normal Multivariada com \\mathbb{E}[X_i] = t para i=1,\\dots,n-1 e matriz de covari\u00e2ncia (dada pela inversa) \\Sigma^{-1} = I_{n-1} + \\begin{bmatrix} 1 & \\cdots & 1 \\\\ \\vdots & \\ddots & \\vdots \\\\ 1 & \\cdots & 1 \\end{bmatrix} := I + A Veja que \\Sigma = I - A/n (mostre!). Assim, X_1, \\dots, X_{n-1} | \\bar{X} = t \\sim \\operatorname{Normal}((t,\\dots,t), I - A/n) . mu = 2 n = 10 Sigma = np.eye(n-1) - np.ones((n-1,n-1))/n Fa\u00e7amos um exemplo em que \\mu = 2 verdadeiro (e desconhecido para os dois estat\u00edsticos). Vamos fazer o seguinte experimento M vezes, com M = 100000 . Para o primeiro estat\u00edstico, oferecemos n amostras. Para o segundo estat\u00edstico, s\u00f3 oferecemos a m\u00e9dia dessas amostras e ele vai utilizar as contas acima para gerar outras n amostras, isto, ele gerar\u00e1 amostras novas a partir de X_1, \\dots, X_{n-1} | \\bar{X} = t \\sim \\operatorname{Normal}((t,\\dots,t), I - A/n) , pois ele conhecer\u00e1 \\bar{X} . Como faremos esses experimentos diversas vezes, vamos obter M amostras de X_1 para o estat\u00edstico 1, e o estat\u00edstico 2 vai produzir outras M a partir da distribui\u00e7\u00e3o calculada. Provamos que a distribui\u00e7\u00e3o delas ser\u00e1 a mesma. Vamos verificar graficamente atrav\u00e9s do histograma. mu = 2 n = 10 M = 100000 estatistico1 = np.zeros((M, n)) estatistico2 = np.zeros_like(estatistico1) for k in tqdm(range(M)): x = np.random.normal(loc=mu, scale=1, size=n) t = x.mean() x_fake = np.random.multivariate_normal(mean=t*np.ones(n-1), cov=np.eye(n-1) - np.ones((n-1, n-1))/n) x_n = n * t - x_fake.sum() x_fake = np.hstack([x_fake, x_n]) estatistico1[k] = x estatistico2[k] = x_fake 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100000/100000 [00:19<00:00, 5034.15it/s] Veja que de fato os histogramas s\u00e3o muito similares, como esper\u00e1vamos! Assim, apenas tendo o valor da m\u00e9dia, o estat\u00edstico 2 foi capaz de produzir novas amostras a partir da mesma distribui\u00e7\u00e3o do estat\u00edstico 1! plt.hist(estatistico1[:,0], bins=50, label='Estat\u00edstico 1') plt.hist(estatistico2[:,0], alpha=0.5, bins=50, label='Estat\u00edstico 2') plt.title('Comparando a distribui\u00e7\u00e3o do dado contra o dado falso') plt.legend() plt.show()","title":"Amostrando!"},{"location":"infestatistica_MSc/sufficiency/sufficiency/#ancilaridade_1","text":"Uma estat\u00edstica anciliar n\u00e3o depende do par\u00e2metro, ent\u00e3o n\u00e3o pode atualizar a informa\u00e7\u00e3o sobre ele. Ser\u00e1 que significa que ela pode ser ignorada? Vamos verificar isso com um exemplo num\u00e9rico. Seja X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{Unif}[\\theta-1/2, \\theta+1/2] . Assim f(x_1, \\dots, x_n | \\theta) = \\prod_{i=1}^n 1\\{\\theta -1/2 < x_i < \\theta+1/2\\} = 1\\{\\theta - 1/2 < \\min\\{x_i\\}\\}1\\{\\theta + 1/2 > \\max\\{x_i\\}\\} Portanto T(X) = (\\min(X_i), \\max(X_i)) \u00e9 estat\u00edstica suficiente m\u00ednima. Seja U(X) = \\max(X_i) - \\min(X_i) . Vamos verificar que U \u00e9 estat\u00edstica anciliar Defina Y_i = X_i - \\theta + 1/2 \\sim \\operatorname{Uniform}(0,1) . Assim, U = \\max(X_i) - \\min(X_i) = \\max(X_i-\\theta+1/2) - \\min(X_i-\\theta+1/2) = \\max(Y_i) - \\min(Y_i). Como a distribui\u00e7\u00e3o de Y independe de \\theta , temos que a distribui\u00e7\u00e3o de U tamb\u00e9m independe, o que mostra que U \u00e9 estat\u00edstica anciliar. Vamos visualizar a distribui\u00e7\u00e3o de U de duas formas: partindo de X e partido de Y . theta = 5 n = 10 X = np.random.uniform(theta-1/2, theta+1/2, size=(100000, n)) Y = np.random.uniform(size=(100000, n)) # Estamos tomando o m\u00e1ximo de cada linha de X e obtendo 100000 amostras para X. U1 = X.max(axis=1) - X.min(axis=1) U2 = Y.max(axis=1) - Y.min(axis=1) Note que de fato as distribui\u00e7\u00f5es s\u00e3o muito similares. plt.hist(U1, bins=50, label='U = max(X_i) - min(X_i)') plt.hist(U2, alpha=0.5, bins=50, label='U = max(Y_i) - min(Y_i)') plt.title('Comparando a distribui\u00e7\u00e3o de U') plt.legend() plt.show() Se observamos que U(X) = u , podemos calcular que \\max(x_i) | U = u \\sim \\operatorname{Unif}(\\theta-1/2+u, \\theta+1/2) . Assim, a distribui\u00e7\u00e3o de uma estat\u00edstica muda com outra anciliar.","title":"Ancilaridade"},{"location":"infestatistica_MSc/sufficiency/sufficiency/#estimadores-aleatorios-e-convexidade","text":"O seguinte resultado \u00e9 a Desigualdade de Jensen: Seja I um intervalo aberto tal que \\mathbb{P}(X \\in I) = 1 e f uma fun\u00e7\u00e3o convexa . Se X \u00e9 integr\u00e1vel, ent\u00e3o f\\left(\\mathbb{E}[X]\\right) \\le \\mathbb{E}[f(X)]. Se f \u00e9 estritamente convexa, a desigualdade \u00e9 estrita a menos que X seja constante com probabilidade 1. Um estimador randomizado \u00e9 um estimador que pode ser constru\u00eddo a partir de uma estat\u00edstica suficiente T com a gera\u00e7\u00e3o auxiliar de n\u00fameros aleat\u00f3rios, isto \u00e9, \u00e9 uma fun\u00e7\u00e3o que mapeia x \\in \\mathcal{X} a uma vari\u00e1vel aleat\u00f3ria Y(x) com distribui\u00e7\u00e3o P_x . Teorema: Seja X \\sim P_{\\theta} \\in \\mathcal{P} e T = T(X) uma estat\u00edstica suficiente. Ent\u00e3o, para qualquer estimador \\phi(X) para g(\\theta) , existe um estimador randomizado baseado em T que tem a mesma fun\u00e7\u00e3o de risco de \\phi(X) . Ideia da prova: a distribui\u00e7\u00e3o de X condicionada em T n\u00e3o depende de \\theta , pois T \u00e9 suficiente. Seja r(\\cdot, T=t) essa distribui\u00e7\u00e3o. Assim, uma vari\u00e1vel aleat\u00f3ria Y com essa distribui\u00e7\u00e3o tem a mesma distribui\u00e7\u00e3o marginal de X , como verificamos aqui . Em particular \\phi(Y) tem a mesma distribui\u00e7\u00e3o de \\phi(X) .","title":"Estimadores aleat\u00f3rios e convexidade"},{"location":"infestatistica_MSc/sufficiency/sufficiency/#teorema-de-rao-blackwell","text":"Sejam T uma estat\u00edstica suficiente para \\theta de uma fam\u00edlia de distribui\u00e7\u00f5es P_{\\theta} e \\delta um estimador de g(\\theta) . Defina \\eta(T) = \\mathbb{E}[\\delta(X)|T] . Se R(\\theta, \\delta) < +\\infty , em que R \u00e9 a fun\u00e7\u00e3o de risco para uma perda L , e L(\\theta, \\cdot) \u00e9 convexa, ent\u00e3o R(\\theta, \\eta) \\le R(\\theta, \\delta). Al\u00e9m do mais, se a perda \u00e9 estritamente convexa, a desigualdade \u00e9 estrita a menos que \\delta(X) = \\eta(T) com probabilidade 1. Esse resultado \u00e9 uma consequ\u00eancia direta da Desigualdade de Jensen. Esse resultado mostra que para perdas convexas, sob a \u00f3tica do risco apenas, os \u00fanicos estimadores interessantes s\u00e3o aqueles que s\u00e3o fun\u00e7\u00e3o de T e n\u00e3o de X (visto que estamos integrando em X ). Al\u00e9m do mais, qualquer estimador randomizado \u00e9 pior do que o n\u00e3o randomizado tomando a esperan\u00e7a dele condicionada em um estat\u00edstica suficiente. Observa\u00e7\u00e3o: \\eta(T) \u00e9 uma estat\u00edstica, pois n\u00e3o depende de \\theta , dado que T \u00e9 suficiente.","title":"Teorema de Rao-Blackwell"}]}