<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../../img/favicon.ico">

    
    <title>Informação de Fisher e Cramér-Rao - Teaching Assistance</title>
    

    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/v4-shims.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3.3.0/build/web/hack.min.css">
    <link href='//rsms.me/inter/inter.css' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="../../../css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="../../../css/base.min.css" rel="stylesheet">
    <link href="../../../css/cinder.min.css" rel="stylesheet">

    
        
        <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/styles/github.min.css">
        
    

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
        <![endif]-->

    

     
</head>

<body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->

            
              <a class="navbar-brand" href="../../..">Teaching Assistance</a>
            
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                
                
                    <li >
                        <a href="../../..">Home</a>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Graduação <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../../alglin/info/">Álgebra Linear</a>
</li>

                        
                            
<li >
    <a href="../../../analisenum/info/">Análise Numérica</a>
</li>

                        
                            
<li >
    <a href="../../../curvas/info/">Curvas</a>
</li>

                        
                            
<li >
    <a href="../../../edo/info/">Equações Diferenciais Ordinárias</a>
</li>

                        
                            
<li >
    <a href="../../../edp/info/">Equações Diferenciais Parciais</a>
</li>

                        
                            
<li >
    <a href="../../../infestatistica_BSc/info/">Inferência Estatística</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Pós-graduação <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../../functional_analysis/info/">Análise Funcional</a>
</li>

                        
                            
<li >
    <a href="../../../bayesian/info/">Estatística Bayesiana</a>
</li>

                        
                            
<li >
    <a href="../../info/">Inferência Estatística</a>
</li>

                        
                        </ul>
                    </li>
                
                
                </ul>

            <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                            <i class="fas fa-search"></i> Search
                        </a>
                    </li>
                    <li>
                        <a href="https://lucasmoschen.github.io">Página Inicial</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

    <div class="container">
        
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="first-level active"><a href="#informacao-de-fisher-e-cramer-rao">Informação de Fisher e Cramér-Rao</a></li>
            <li class="second-level"><a href="#exemplo-construtivo">Exemplo Construtivo</a></li>
                
            <li class="second-level"><a href="#limites-inferiores-para-a-variancia">Limites inferiores para a variância</a></li>
                
                <li class="third-level"><a href="#desigualdade-da-variancia">Desigualdade da variância</a></li>
                <li class="third-level"><a href="#limite-inferior-de-cramer-rao">Limite inferior de Cramér-Rao</a></li>
                <li class="third-level"><a href="#desigualdade-de-chapman-robbins">Desigualdade de Chapman-Robbins</a></li>
                <li class="third-level"><a href="#sistema-de-bhattacharyya">Sistema de Bhattacharyya</a></li>
                <li class="third-level"><a href="#desigualdade-de-cramer-rao-multidimensional">Desigualdade de Cramér-Rao multidimensional</a></li>
            <li class="second-level"><a href="#exemplo-numerico-do-limite-de-cramer-rao">Exemplo Numérico do limite de Cramér-Rao</a></li>
                
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<h1 id="informacao-de-fisher-e-cramer-rao">Informação de Fisher e Cramér-Rao</h1>
<p>Seja <script type="math/tex">X</script> uma variável aleatória cuja distribuição depende de <script type="math/tex">\theta</script> com densidade <script type="math/tex">f(x|\theta)</script>. As condições de regularidade FI são </p>
<ol>
<li>
<p>A derivada de <script type="math/tex">f(x|\theta)</script> com respeito a <script type="math/tex">\theta</script> existe com probabilidade 1.</p>
</li>
<li>
<p>Podemos diferenciar <script type="math/tex">\int f(x|\theta) d\mu(x)</script> sob o sinal da integração. <a href="https://en.wikipedia.org/wiki/Leibniz_integral_rule#:~:text=General%20form%3A%20Differentiation%20under%20the%20integral%20sign,-Theorem.&amp;text=That%20is%2C%20it%20is%20related,as%20the%20Leibniz%20integral%20rule.&amp;text=the%20change%20of%20order%20of%20integration%20(integration%20under%20the%20integral,%3B%20i.e.%2C%20Fubini's%20theorem).">(Veja aqui)</a>.</p>
</li>
<li>
<p>O conjunto <script type="math/tex">C = \{x: f(x|\theta) > 0\}</script> não depende de <script type="math/tex">\theta</script>.</p>
</li>
</ol>
<p>Assuma as condições FI. 
A <strong>informação de Fisher</strong> é definida como 
<script type="math/tex; mode=display">
I_{\mathcal{X}}(\theta) = \mathbb{E}_{\theta}\left[\left(\frac{d}{d\theta} \log f(x|\theta)\right)^2\right].
</script>
A função <script type="math/tex">\partial \log f(X|\theta)/d\theta</script> é chamada de <strong>função score</strong>. </p>
<p>Se <script type="math/tex">\theta \in \mathbb{R}^k</script>, definimos as <strong>matrix informação de Fisher</strong> como 
<script type="math/tex; mode=display">
I_{\mathcal{X}, i,j }(\theta) = \operatorname{Cov}_{\theta}\left(\frac{\partial}{\partial \theta_i} \log f(x|\theta), \frac{\partial}{\partial \theta_j} \log f(x|\theta) \right).
</script>
</p>
<p>Agora seja <script type="math/tex">X = (X_1, \dots, X_n)</script> uma amostra aleatória e <script type="math/tex">f_n(x|\theta)</script> a densidade conjunta de <script type="math/tex">X</script>.
Denote <script type="math/tex">\lambda_n(x|\theta) = \log f_n(x|\theta)</script>. 
Como definimos, a informação de Fisher é <script type="math/tex">I_n(\theta) = \mathbb{E}_{\theta}\{[\lambda_n'(X|\theta)]^2\}</script>. 
Como <script type="math/tex">\log f_n(x|\theta) = \sum_{i=1}^n \log f(x_i|\theta)</script>, temos que 
<script type="math/tex; mode=display">
I_n(\theta) = nI_{\mathcal{X}}(\theta).
</script>
Portanto, para amostras aleatórias, basta calcular a informação considerando a densidade de uma variável aleatória. </p>
<p><strong>Teorema:</strong> Se valem as condições de regularidade FI, a média da função score é 0, isto é, 
<script type="math/tex; mode=display">
\mathbb{E}_{\theta}\left[\frac{d}{d\theta} \log f(X|\theta)\right] = 0,
</script>
pois podemos tirar a derivada do valor esperado e, então, basta ver que a integral da densidade é constante igual a <script type="math/tex">1</script>. 
Logo, vale que a derivada é nula.
Além do mais,
<script type="math/tex; mode=display">
I_{\mathcal{X}}(\theta) = -\mathbb{E}_{\theta}\left[\frac{d^2}{d\theta^2} \log f(X|\theta)\right]
</script>
</p>
<p>Esse resultado se estende para mais dimensões, com 
<script type="math/tex; mode=display">
I_{\mathcal{X}, i,j}(\theta) = -\mathbb{E}_{\theta}\left[\frac{\partial^2}{\partial \theta_i \partial \theta_j} \log f(X|\theta)\right].
</script>
</p>
<pre class="highlight"><code class="language-python">import numpy as np
from scipy.stats import norm
from scipy.misc import derivative
from scipy.optimize import curve_fit 

import matplotlib.pyplot as plt 
from seaborn import violinplot
import inspect</code></pre>
<h2 id="exemplo-construtivo">Exemplo Construtivo</h2>
<p>Vamos pensar num caso bem simples: amostra aleatória <script type="math/tex">X_1, ..., X_n \sim \text{Normal}(\mu, \sigma^2)</script>, onde o parâmetro <script type="math/tex">\sigma^2</script> é conhecido e <script type="math/tex">\mu</script> não.  </p>
<p>De forma direta, poderíamos perguntar qual a Informação de Fisher (ou Informação Diferencial) da amostra aleatória sobre o parâmetro desconhecido <script type="math/tex">\mu</script>. </p>
<ol>
<li>Vamos encontrar a distribuição conjunta:</li>
</ol>
<p>
<script type="math/tex; mode=display">f(x|\mu) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{1}{2}\frac{(x - \mu)^2}{\sigma^2}\right]</script>
</p>
<p>
<script type="math/tex; mode=display">
\begin{split}
f_n(x|\mu) &= \prod_{i=1}^n f(x_i|\mu) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2\right] \\ 
&= \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i^2 - 2x_i\mu + \mu^2)\right] \\
&= \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left[-\frac{1}{2\sigma^2}\left(\sum_{i=1}^n x_i^2 - 2n\bar{x}_n\mu + n\mu^2\right)\right] \\
&= \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2\right]\exp\left[-\frac{1}{2\sigma^2}\left(- 2n\bar{x}_n\mu + n\mu^2\right)\right]
\end{split}
</script>
</p>
<ol>
<li>Vamos encontrar a verossimilhança: é a distribuição conjunta como função do parâmetro! </li>
</ol>
<p>
<script type="math/tex; mode=display">f_n(x|\mu) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2\right]\exp\left[-\frac{1}{2\sigma^2}\left(- 2n\bar{x}_n\mu + n\mu^2\right)\right]</script>
</p>
<p>Vamos comparar para <script type="math/tex">\sigma = 1</script> e <script type="math/tex">\sigma = 5</script>
</p>
<pre class="highlight"><code class="language-python">loglikelihood = lambda mu, sigma, x: np.sum(np.log([norm(loc = mu, scale = sigma).pdf(xi) for xi in x]), axis = 0)</code></pre>
<pre class="highlight"><code class="language-python">sigmas = [1,3,5,10]
mu_true = 5

mu_range = np.linspace(0,10,1000)</code></pre>
<pre class="highlight"><code class="language-python">fig,ax = plt.subplots(2,2,figsize = (16, 10))

fig.suptitle('Comparando Log-verossimilhanças da Distribuição Normal')

def generate_curves(sigma, ax, n = 20, n_times = 50): 

    for i in range(n_times):

        x = np.random.normal(loc = mu_true, scale = sigma, size = n)
        logvalues = loglikelihood(mu_range, sigma, x)

        ax.plot(mu_range, logvalues, color = 'blue', alpha = 0.2)

    ax.vlines(mu_true, ymin = ax.get_ylim()[0], ymax = ax.get_ylim()[1], linestyle = '--')

    ax.set_title(r'$\sigma =$ {}'.format(sigma))
    ax.set_xlabel(r'$\mu$')

generate_curves(sigmas[0], ax[0][0])
generate_curves(sigmas[1], ax[0][1])
generate_curves(sigmas[2], ax[1][0])
generate_curves(sigmas[3], ax[1][1])    </code></pre>
<p><img alt="png" src="../output_5_0.png" /></p>
<ol>
<li>Vamos ver como se comporta derivada. Esse é o score: </li>
</ol>
<p>
<script type="math/tex; mode=display">\lambda '_n(y|\mu) = \frac{1}{\sigma^2}\left(n\bar{x}_n - \mu\right)</script>
</p>
<pre class="highlight"><code class="language-python">score = lambda mu, sigma, x: derivative(loglikelihood, mu, dx = 1e-5, args = (sigma, x))  

fig,ax = plt.subplots(2,2,figsize = (16, 10))

fig.suptitle('Comparando Scores da Distribuição Normal')

def generate_curves(sigma, ax, n = 20, n_times = 50): 

    for i in range(n_times):

        x = np.random.normal(loc = mu_true, scale = sigma, size = n)
        scorevalues = score(mu_range, sigma, x)

        ax.plot(mu_range, scorevalues, color = 'blue', alpha = 0.2)

    ax.vlines(mu_true, ymin = ax.get_ylim()[0], ymax = ax.get_ylim()[1], linestyle = '--')

    ax.set_title(r'$\sigma =$ {}'.format(sigma))
    ax.set_xlabel(r'$\mu$')

    ax.set_ylim((-10,10))

generate_curves(sigmas[0], ax[0][0])
generate_curves(sigmas[1], ax[0][1])
generate_curves(sigmas[2], ax[1][0])
generate_curves(sigmas[3], ax[1][1])    </code></pre>
<p><img alt="png" src="../output_7_0.png" /></p>
<pre class="highlight"><code class="language-python">fig,ax = plt.subplots(2,2,figsize = (16, 10))

fig.suptitle('Comparando Histogramas dos Scores para mu')

def generate_histograms(mu, sigma, ax, n = 15, n_times = 100): 

    scorevalues = []
    for i in range(n_times):

        x = np.random.normal(loc = mu_true, scale = sigma, size = n)
        scorevalues.append(score(mu, sigma, x))

    violinplot(scorevalues, ax = ax)

    ax.set_title(r'$\sigma =$ {}'.format(sigma))
    ax.set_xlabel('score')

generate_histograms(5, sigmas[0], ax[0][0])
generate_histograms(5, sigmas[1], ax[0][1])
generate_histograms(5, sigmas[2], ax[1][0])
generate_histograms(5, sigmas[3], ax[1][1])    </code></pre>
<p><img alt="png" src="../output_8_0.png" /></p>
<ol>
<li>A informação de Fisher é a Variância da função score em <script type="math/tex">X</script>, isto é: </li>
</ol>
<p>
<script type="math/tex; mode=display">
\begin{split}
I_n(\mu) &= Var(\lambda '_n(x|p)) = E[(\lambda '_n(x|p))^2] - E[\lambda '_n(x|p)]^2\\ 
&= \frac{1}{\sigma^4}Var\left[n\bar{x}_n - \mu\right] \\
&= \frac{n^2}{\sigma^4}Var(\bar{x}_n) \\
&= \frac{n^2\sigma^2}{n\sigma^4} \\
&= \frac{n}{\sigma^2}
\end{split}
</script>
</p>
<h2 id="limites-inferiores-para-a-variancia">Limites inferiores para a variância</h2>
<p>Para estimadores não enviesados, o erro quadrático se iguala à variância. 
Como nem sempre é possível obter valores exatos para variância, é de interesse procurar por limites inferiores desses valores. 
Mais do que isso, se encontramos um estimador cuja variância seja um limite inferior, estaremos encontrando um UMVUE.</p>
<h3 id="desigualdade-da-variancia">Desigualdade da variância</h3>
<p>A desigualdade de Cauchy-Schwarz para variáveis aleatórias, tratando a covariância como um produto interno nesse espaço, é escrita da seguinte forma:
<script type="math/tex; mode=display">
|\operatorname{Cov}(X,Y)| \le \sqrt{\operatorname{Var}(X)}\sqrt{\operatorname{Var}(Y)}
</script>
</p>
<h3 id="limite-inferior-de-cramer-rao">Limite inferior de Cramér-Rao</h3>
<p>Sejam <script type="math/tex">I_{\mathcal{X}}(\theta)</script> a <a href="https://lucasmoschen.github.io/ta-sessions/infestatistica_MSc/fisher/fisher">informação de Fisher</a> e <script type="math/tex">\phi(X)</script> uma estatística com esperança finita. Suponha que valha algumas condições de regularidade(XXX: Quais?) e que <script type="math/tex">\forall \theta, I_{\mathcal{X}}(\theta) > 0</script>. 
Então 
<script type="math/tex; mode=display">
\operatorname{Var}_{\theta}(\phi(X)) \ge \frac{\left(\frac{d}{d\theta} \mathbb{E}_{\theta} \phi(X) \right)^2}{I_{\mathcal{X}}(\theta)}.
</script>
</p>
<p>Observe que se <script type="math/tex">\phi(X)</script> é não enviesado, então o limite inferior da variância é o inverso da informação de Fisher.</p>
<hr />
<p><code>📝</code> <strong>Exemplo (Normal)</strong></p>
<p>Seja <script type="math/tex">X \sim N(\theta, b)</script> com <script type="math/tex">b</script> fixo. 
Um estimador para <script type="math/tex">\theta</script> é <script type="math/tex">\phi(X) = X</script>, que é não enviesado, em particular. 
A informação de Fisher é 
<script type="math/tex; mode=display">
I_{\mathcal{X}}(\theta) = - \mathbb{E}_{\theta}\left[\frac{d^2}{d\theta^2} \log f(X|\theta) \right] = 1/b,
</script>
o que implica que <script type="math/tex">\operatorname{Var}(\phi(X))</script> é limitado inferiormente por <script type="math/tex">b</script>. Só que sabemos que ele atinge esse valor.
Portanto, esse estimador é UMVUE, pois tem variância mínima.</p>
<hr />
<p>A desigualdade de Cramér-Rao é uma consequência da desigualdade de Cauchy-Schwarz. 
Esta última diz que vale a igualdade se, e somente se, os fatores são linearmente independentes. 
No nosso, caso, isso significa que a desigualdade de Cramér-Rao se torna uma igualdade se, e somente se, <script type="math/tex">\phi(X)</script> e a função <script type="math/tex">\frac{d}{d\theta}\log f(X|\theta)</script> são linearmente relacionadas, isto é,
<script type="math/tex; mode=display">
\frac{d}{d\theta} \log f(X|\theta) = a(\theta)\phi(X) + b(\theta).
</script>
Resolvendo essa equação, obtemos que 
<script type="math/tex; mode=display">
f(X|\theta) = c(\theta)h(x)\exp\{\pi(\theta)\phi(x)\},
</script>
que pertence à família exponencial.
Logo, se <script type="math/tex">\phi(X)</script> é uma estatística suficiente para um parâmetro de uma distribuição vinda da família exponencial, a desigualdade de Cramér-Rao vira uma igualdade. Além disso, essa é única situação.</p>
<blockquote>
<p>Fora da família exponencial, a desigualdade de Cramér-Rao não pode ser alcançada! Se uma estatística tem variância igual ao limite inferior de Cramér-Rao, ele é dito <strong>estimador eficiente</strong>.</p>
</blockquote>
<h3 id="desigualdade-de-chapman-robbins">Desigualdade de Chapman-Robbins</h3>
<p>Seja <script type="math/tex">m(\theta) = \mathbb{E}_{\theta}(\phi(X))</script> e <script type="math/tex">supp(\theta)</script> o suporte da distribuição de <script type="math/tex">X</script>.
Assuma que para cada <script type="math/tex">\theta \in \Omega</script>, exista <script type="math/tex">\theta'\neq\theta</script> tal que <script type="math/tex">supp(\theta') \subseteq supp(\theta)</script> (isso acontece quando <script type="math/tex">supp \theta</script> é o mesmo para todo <script type="math/tex">\theta</script>).
Então,
<script type="math/tex; mode=display">
\operatorname{Var}_{\theta}(\phi(X)) \ge \sup_{\theta ' : supp(\theta') \subseteq supp(\theta)} \left\{\frac{[m(\theta) - m(\theta ')]^2}{\mathbb{E}_{\theta}\left[\frac{f_{X|\theta}(X|\theta ')}{f_{X|\theta}(X|\theta)} - 1\right]^2}\right\}
</script>
</p>
<p>Note que essa desigualdade vale para casos mais gerais. 
Porém, ela também é consequência da desigualdade de Cauchy-Schwarz.</p>
<h3 id="sistema-de-bhattacharyya">Sistema de Bhattacharyya</h3>
<p>Assuma as condições do Teorema de Cramér-Rao e que a derivada sob <script type="math/tex">\theta</script> pode passar sob o sinal de integração
Então <script type="math/tex">\operatorname{Var} \phi(X) \ge \gamma^T (\theta) J^{-1}(\theta)\gamma(\theta)</script>, 
em que 
<script type="math/tex; mode=display">
\gamma_i(\theta) = \frac{d^i}{d\theta^i}\mathbb{E}_{\theta}[\phi(X)], J_{ij}(\theta) = \operatorname{Cor}(\psi_i(X,\theta), \psi_j(X,\theta)), \psi_i(x,\theta) = \frac{1}{f(x|\theta)}\frac{d^i}{d\theta^i} f(x|\theta).
</script>
</p>
<p>Esse resultado é uma consequência direta de uma desigualdade envolvendo variância de um estimador e covariâncias dele com outras funções dos dados e do parâmetro.</p>
<h3 id="desigualdade-de-cramer-rao-multidimensional">Desigualdade de Cramér-Rao multidimensional</h3>
<p>Assumindo as condições do caso unidimensional mais que a matriz informação de Fisher seja positiva definida, então
<script type="math/tex; mode=display">
\operatorname{Var}_{\theta}(\phi(X)) \ge \gamma^T(\theta) I_X^{-1}(\theta)\gamma(\theta),
</script>
em que <script type="math/tex">\gamma_i(\theta) = \frac{d}{d\theta_i} \mathbb{E}_{\theta} \phi(X)</script>.</p>
<h2 id="exemplo-numerico-do-limite-de-cramer-rao">Exemplo Numérico do limite de Cramér-Rao</h2>
<p><a href="http://michal.rawlik.pl/2014/02/21/numerical-cramer-rao-bound-in-python/">Referência</a></p>
<p>Considere um sinal (como uma música) com três parâmetros, amplitude, frequência e fase inicia.</p>
<p>Saberemos o número de amostras que sera 100Hz com nível de ruído de 0.1 </p>
<pre class="highlight"><code class="language-python">s = lambda t,a,f,ph: a*np.sin(2*np.pi*f*t + ph) # função que representa o sinal

p0 = [2,8,0]     # Amplitude, frequência e fase inicial para testar 
noise = 0.1

T = np.linspace(0,1,100)   #100 valores entre 0 e 1 igualmente espaçados
plt.plot(T, s(T, *p0), '.-k')
plt.xlabel('Tempo (s)')
plt.title('Sinal')
plt.show()</code></pre>
<p><img alt="png" src="../output_13_0.png" /></p>
<p>Vamos usar <a href="https://docs.python.org/3/library/inspect.html">inspect</a> para nos ajudar a pegar labels das funções, isto é, os parâmetros necessários das funções. Essa biblioteca fornece várias funções de ajuda desse tipo. Dê uma olhada. </p>
<pre class="highlight"><code class="language-python">parameters = str(inspect.signature(s)).strip('()').replace(' ', '').split(',')[1:]
p0dict = dict(zip(parameters, p0))
p0dict</code></pre>
<pre><code>{'a': 2, 'f': 8, 'ph': 0}
</code></pre>
<p>No caso geral, calcular a Matriz de Informação de Fisher não é trivial. Por isso, vamos calcular para o caso em que as medições são de uma amostra com distribuição multivariada normal, isto é, é uma distribuição normal, só que em mais dimensões, em particular, 441 dimensões (número de pontos no tempo)</p>
<p>Se calcularmos a informação de Fisher, podemos ver que:</p>
<p>
<script type="math/tex; mode=display">
\mathcal{I}_{mn} = \frac{1}{\sigma^2} \frac{\partial \mu^\mathrm{T}}{\partial \theta_m} \frac{\partial \mu}{\partial \theta_n} = \frac{1}{\sigma^2} \sum_k \frac{\partial \mu_k}{\partial \theta_m} \frac{\partial \mu_k}{\partial \theta_n}
</script>
</p>
<p>onde <script type="math/tex">\theta = [a,f,ph]^T</script>, <script type="math/tex">\mu = \mu(\theta)</script> é o vetor média da normal multivariada e <script type="math/tex">\sigma^2</script> é a variância de cada marginal da normal. Não se assuste. Na multivariada, temos uma matriz para indicar as variâncias (ela se chama Matriz de Covariâncias, na verdade). O que estou dizendo é que ela é <script type="math/tex">\sigma^2</script> vezes a identidade. É bom conhecer essa distribuição!</p>
<p>Por enquando acredite em mim! Ou no <a href="https://en.wikipedia.org/wiki/Fisher_information#Multivariate_normal_distribution">Wikipedia</a>.</p>
<p>Vou chamar <script type="math/tex">D_{ik} = \frac{\partial \mu_k}{\partial \theta_i}</script>
</p>
<pre class="highlight"><code class="language-python"># Usamos ** para desempacotar elementos de um dicionário.
string = "a: {a} f: {f} ph: {ph}".format(**p0dict)
print(string)</code></pre>
<pre><code>a: 2 f: 8 ph: 0
</code></pre>
<pre class="highlight"><code class="language-python">D = np.zeros((len(p0), len(T)))

# para cada parâmetro
for i, parameter in enumerate(parameters):
    # para cada ponto no tempo
    for k, t in enumerate(T):

        func = lambda x: s(t, **dict(p0dict, **{parameter: x}))
        # Calculamos a derivada com respeito a x, que nesse caso é o valor do parametro
        D[i,k] = derivative(func, p0dict[parameter], dx = 1e-4)   </code></pre>
<p>Veja que o tamanho de D é o seguinte:</p>
<pre class="highlight"><code class="language-python">D.shape</code></pre>
<pre><code>(3, 100)
</code></pre>
<pre class="highlight"><code class="language-python">plt.plot(T, s(T, *p0), '--k', lw=2, label='Sinal')

for Di, parameter in zip(D, parameters):
    # Estamos acessando Di = linha_i(D)
    plt.plot(T, Di, '.-', label=parameter)

plt.legend()
plt.xlabel('Tempo (s)')
plt.show()</code></pre>
<p><img alt="png" src="../output_21_0.png" /></p>
<p>O que <script type="math/tex">D_{ik}</script> indica? É a derivada da <script type="math/tex">k-ésima</script> média com respeito ao i-ésimo parâmetro. Logo indica o quanto o quando a amostra <script type="math/tex">k</script> afeta o parâmetro <script type="math/tex">i</script>. Veja que quando temos picos no seno, teremos pico na amplitude,. Também vemos que a fase inicial não tem essa relevância. Vemos também que o sinal se torna mais e mais sensível à frequência. </p>
<p>Assim, podemos calular a informação de fisher, usando <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html">einsum</a></p>
<pre class="highlight"><code class="language-python">I = 1/noise**2*np.einsum('mk,nk', D, D)
print(I)</code></pre>
<pre><code>[[ 4.95000000e+03 -5.64643569e+02 -3.43706036e-09]
 [-5.64643569e+02  2.68635205e+05  6.34601694e+04]
 [-3.43706036e-09  6.34601694e+04  2.01999999e+04]]
</code></pre>
<p>Podemos calcular o limite de Cramér-Rao para qualquer estimador não enviesado. Nesse caso, veja <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound#Multivariate_case">aqui</a> para mais detalhes. Mas não se incomode com os detalhes, se preferir. </p>
<pre class="highlight"><code class="language-python">iI = np.linalg.inv(I)  

print('Cramér-Rao Limite Inferior')
for parameter, variance in zip(parameters, iI.diagonal()):
    print('{}: {:.2g}'.format(parameter, np.sqrt(variance)))</code></pre>
<pre><code>Cramér-Rao Limite Inferior
a: 0.014
f: 0.0038
ph: 0.014
</code></pre></div>
        
        
    </div>

    
      <footer class="col-md-12 text-center">
          
          
            <hr>
            <p>
            <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</small>
            </p>
          

          
          
      </footer>
    
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="../../../js/bootstrap-3.0.3.min.js"></script>

    
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/highlight.min.js"></script>
        
    <script>hljs.initHighlightingOnLoad();</script>
    

    <script>var base_url = "../../.."</script>
    
    <script src="../../../js/base.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../../search/main.js"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal">
                    <span aria-hidden="true">&times;</span>
                    <span class="sr-only">Close</span>
                </button>
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>
    </body>

</html>
